{"1": {"topic": "Algorithm for separating nonsense text from meaningful text", "user_name": "Jason Day", "text": "\nI provided some of my programs with a feedback function. Unfortunately I forgot to include some sort of spam-protection - so users could send  anything they wanted to my server - where every feedback is stored in a huge db.  \nIn the beginning I periodically checked those feedbacks - I filtered out what was usable and deleted garbage. The problem is: I get 900 feedbacks per day. Only 4-5 are really useful, the other messages are mostly 2 type of gibberish:\n\nnonsense: jfvgasdjkfahs kdlfjhasdf  (People smashing their heads on the keyboard)\nlanguage i don't understand\n\nWhat I did so far: \n\nI installed a filter to delete any feedback containing \"asdf\", \"qwer\" etc... -> only 700 per day\nI installed a word filter to delte anything containing bad language -> 600 per day (don't ask - but there are many strange people out there)\nI filter out any messages containing letters not being used in my language -> 400 per day\n\nBut 400 per day is still way too much. So I'm wondering if anybody has dealt with such a problem before and knows some sort of algorithm to filter out senseless messages.\nAny help would really be appreciated!\n"}, "2": {"topic": "Algorithm for separating nonsense text from meaningful text", "user_name": "ChrisChris", "text": "\nHow about just using some existing implementation of a bayesian spam filter instead of implementing your own. I have had good results with DSpam\n"}, "3": {"topic": "Algorithm for separating nonsense text from meaningful text", "user_name": "John NilssonJohn Nilsson", "text": "\nA slightly different approach would be to set up a system to email the feedback messages to an account and use standard spam filtering.  You could send them through gmail and let their filtering take a shot at it.  Not perfect, but not too much effort to implement either.\n"}, "4": {"topic": "Algorithm for separating nonsense text from meaningful text", "user_name": "Rob WalkerRob Walker", "text": "\nIf you're only expecting (or care about) English comments, then why not simply count the number of valid words (with respect to some dictionary) in the feedback uploaded.  If the number passes some threshold, accept the feedback.  If not, trash it.  This simple heuristic could be extended to other languages by adding their dictionaries.\n"}, "5": {"topic": "Algorithm for separating nonsense text from meaningful text", "user_name": "maxaposteriorimaxaposteriori", "text": "\nYou might try the Bayesian algorithm used by many spam filters.\nBetter Bayesian Filtering\nWikipedia explanation\nSome open Source\n"}, "6": {"topic": "Algorithm for separating nonsense text from meaningful text", "user_name": "oglesteroglester", "text": "\nI had a spamming problem in a guestbook function on one of my sites a (quite long) while ago. my solution was simply to add a little captcha-like Q&A field asking the user \"Are you a spamming robot?\" Any answer containing the word \"no\" (letting through \"no, i'm not\", \"nope\" and \"not at all\" too, just for fun...) permitted the user to post...\nThe reason I chose not to use captcha was simply that my users wanted a more \"cozy\" feel to the site, and a captcha felt too formal. This was more personal =)\n"}, "7": {"topic": "Algorithm for separating nonsense text from meaningful text", "user_name": "Tomas AschanTomas Aschan", "text": "\nThe simplest method would be to count the occurrence of each letter. E is the most common letter in English, so it should be used the most. You could also check for word and digraph frequency. Have a look here to get the list of most frequently used anything in English\n"}, "8": {"topic": "Algorithm for separating nonsense text from meaningful text", "user_name": "MariusMarius", "text": "\nLook up Claude Shannon and Markov models. These lead to a statistical technique for assessing probabilities that letter combinations come from a specified language source.\nHere are some relevant course notes from Princeton University.\n"}, "9": {"topic": "Algorithm for separating nonsense text from meaningful text", "user_name": "", "text": "\nFidelis Assis and I have been adapting the spam filter OSBF-Lua so that it can easily be adapted to other applications including web applications.  This spam filter won the TREC spam contest three years running.  (I don't mind bragging because the algorithm is Fidelis's, not mine.)\nIf you want to try things out, we have \"nearly beta\" code at\ngit clone http://www.cs.tufts.edu/~nr/osbf-lua-temp\n\nWe are still a long way from having a tidy release, but the code should build provided you install automake 1.9.  Either of us would be happy to advise you on how to use it to clean your database and to integrate it into your application.\n"}, "10": {"topic": "Algorithm for separating nonsense text from meaningful text", "user_name": "joel.neelyjoel.neely", "text": "\nThe preceding answers about strapping up some spam filter Bayesian-inspired classfier are a good idea.  For your application, since you seem to get a lot of long nonsense words, it would be best to turn on an option in your parser to train on bigrams and trigrams; otherwise, many of the nonsense words will just be treated as \"never seen before\" which is not the most useful parse in your case.\n"}, "11": {"topic": "Algorithm for separating nonsense text from meaningful text", "user_name": "Norman RamseyNorman Ramsey", "text": "\nYes, like people pointed out, you could look at spam filters or Markov Models. \nSomething simpler would be to just count the different words in each response and sort by frequency. If words like the following are not at the top then it's probably not valid text:\nthe, a, in, of, and, or, ...\nThey are the most frequently used word in any usual English text.\n"}, "12": {"topic": "Algorithm for separating nonsense text from meaningful text", "user_name": "Liudvikas BukysLiudvikas Bukys", "text": "\nJust store comments in a pending state, pass them through Akismet or Defensio, and use the response to mark them as potential spam or mark them active.\nhttp://akismet.com/\nhttp://defensio.com/\nI personally prefer Defensio's API but they both work fantastically well.\n"}, "13": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "agiledatabase", "text": "\nI get the following error when trying to install Punkt for nltk:\nnltk.download('punkt')    \n [nltk_data] Error loading Punkt: <urlopen error [SSL:\n [nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed\n [nltk_data]     (_ssl.c:590)>\nFalse\n\n"}, "14": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "user3429986user3429986", "text": "\nTLDR: Here is a better solution: https://github.com/gunthercox/ChatterBot/issues/930#issuecomment-322111087\nNote that when you run nltk.download(), a window will pop up and let you select which packages to download (Download is not automatically started right away).\nTo complement the accepted answer, the following is a complete list of directories that will be searched on Mac (not limited to the one mentioned in the accepted answer):\n\n    - '/Users/YOUR_USERNAME/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/YOUR_USERNAME/YOUR_VIRTUAL_ENV_DIRECTORY/nltk_data'\n    - '/Users/YOUR_USERNAME/YOUR_VIRTUAL_ENV_DIRECTORY/share/nltk_data'\n    - '/Users/YOUR_USERNAME/YOUR_VIRTUAL_ENV_DIRECTORY/lib/nltk_data'\n\nIn case the link above dies, here is the solution pasted in its entirety:\nimport nltk\nimport ssl\n\ntry:\n    _create_unverified_https_context = ssl._create_unverified_context\nexcept AttributeError:\n    pass\nelse:\n    ssl._create_default_https_context = _create_unverified_https_context\n\nnltk.download()\n\nRun the above code in your favourite Python IDE or via the command line.\n"}, "15": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "pookie", "text": "\nThis works by disabling SSL check!\nimport nltk\nimport ssl\n\ntry:\n    _create_unverified_https_context = ssl._create_unverified_context\nexcept AttributeError:\n    pass\nelse:\n    ssl._create_default_https_context = _create_unverified_https_context\n\nnltk.download()\n\n"}, "16": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "fstangfstang", "text": "\nRun the Python interpreter and type the commands:\nimport nltk\nnltk.download()\n\nfrom here: http://www.nltk.org/data.html\nif you get an SSL/Certificate error, run the following command\nbash /Applications/Python 3.6/Install Certificates.command\nfrom here: ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:749)\n"}, "17": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "ishwardgretishwardgret", "text": "\nSearch 'Install Certificates.command' in the finder and open it.\nThen do the following steps in the terminal:\npython3\nimport nltk\nnltk.download()\n\n"}, "18": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "Selvaram G", "text": "\nThe downloader script is broken. As a temporal workaround can manually download the punkt tokenizer from here and then place the unzipped folder in the corresponding location. The default folders for each OS are:\n\nWindows: C:\\nltk_data\\tokenizers\nOSX: /usr/local/share/nltk_data/tokenizers\nUnix: /usr/share/nltk_data/tokenizers\n\n"}, "19": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "AdiAdi", "text": "\nThis is how I solved it for MAC OS.\nInitially after installing nltk, I was getting the SSL error.\nSolution:\nGoto\ncd /Applications/Python\\ 3.8\n\nRun the command\n./Install\\ Certificates.command\n\nNow if you try again, it should work!\nThanks a lot to this article!\n"}, "20": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "xxx", "text": "\nYou just need to Install the certificate doing this simple step \nIn the python application folder double-click on the file 'Certificates.command'\nthis will make a prompt window show in your screen and basically will automatically install the certificate for you, close this window and try again.\n"}, "21": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "yashyash", "text": "\nMy solution is:\n\nDownload punkt.zip from here and unzip\nCreate nltk_data/tokenizers folders under home folder\nPut punkt folder under tokenizers folder\n\n"}, "22": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "elyaseelyase", "text": "\nThere is a very simple way to fix all of this as written in the formal bug report for anyone else coming across this problem recently (e.g. 2019) and using MacOS. From the bug report at https://bugs.python.org/issue28150:\n\n...there is a simple double-clickable or command-line-runnable script (\"/Applications/Python 3.6/Install Certificates.command\") that does two things: 1. uses pip to install certifi and 2. creates a symlink in the OpenSSL directory to certifi's installed bundle location. \n\nSimply running the \"Install Certificates.command\" script worked for me on MacOS (10.15 beta as of this writing) and I was off and running.\n"}, "23": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "Sreekiran A RSreekiran A R", "text": "\nMy solution after nothing worked. I navigated, via the GUI to the Python 3.7 folder, opened the 'Certificates.command' file in terminal and the SSL issue was immediately resolved. \n"}, "24": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "thiago89thiago89", "text": "\nA bit late to the party but I just entered Certificates.command into Spotlight which found it and ran it. All fixed in seconds.\nI'm running mac Catalina and using python 3.7 installed by Homebrew\n"}, "25": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "gocengocen", "text": "\nIt means that you are not using HTTPS to work consistently with other run time dependencies for Python etc.\nIf you are using Linux (Ubuntu)\n~$ sudo apt-get install ca-certificates\n\nShould solve the issue.\nIf you are using this in a script with a docker file, you have to make sure you have install the the ca-certificates modules in your docker file.\n"}, "26": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "Michael HawkinsMichael Hawkins", "text": "\nFor mac users,\njust copy paste the following in the terminal:\n/Applications/Python\\ 3.10/Install\\ Certificates.command ; exit;\n\n"}, "27": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "Cormac O'KeeffeCormac O'Keeffe", "text": "\nFirst go to the path  /Applications/Python 3.6/ and run \nInstall Certificates.command\nYou will admin rights for the same.\nIf you are unable to download it, then as other answer suggest you can download directly and place it. You need to place them in the following directory structure.\n> nltk_data\n          > corpora\n                   > brown\n                   > conll2000\n                   > movie_reviews\n                   > wordnet\n          > taggers\n                   > averaged_perceptron_tagger\n          > tokenizers\n                      > punkt\n\n"}, "28": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "ChezChez", "text": "\nUpdating the python certificates worked for me.\nAt the top of your script, keep:\nimport nltk\nnltk.download('punkt')\n\nIn a separate terminal run (Mac):\nbash /Applications/Python <version>/Install Certificates.command\n\n"}, "29": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "Sibeesh Venu", "text": "\nFor me, the solution was much simpler: I was still connected to my corporate network/VPN which blocks certain types of downloads. Switching the network made the SSL error disappear.\n"}, "30": {"topic": "How to compute the similarity between two text documents?", "user_name": "Reily Bourne", "text": "\nI am looking at working on an NLP project, in any programming language (though Python will be my preference).\nI want to take two documents and determine how similar they are.\n"}, "31": {"topic": "How to compute the similarity between two text documents?", "user_name": "Reily BourneReily Bourne", "text": "\nThe common way of doing this is to transform the documents into TF-IDF vectors and then compute the cosine similarity between them. Any textbook on information retrieval (IR) covers this. See esp. Introduction to Information Retrieval, which is free and available online.\nComputing Pairwise Similarities\nTF-IDF (and similar text transformations) are implemented in the Python packages Gensim and scikit-learn. In the latter package, computing cosine similarities is as easy as\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndocuments = [open(f).read() for f in text_files]\ntfidf = TfidfVectorizer().fit_transform(documents)\n# no need to normalize, since Vectorizer will return normalized tf-idf\npairwise_similarity = tfidf * tfidf.T\n\nor, if the documents are plain strings,\n>>> corpus = [\"I'd like an apple\", \n...           \"An apple a day keeps the doctor away\", \n...           \"Never compare an apple to an orange\", \n...           \"I prefer scikit-learn to Orange\", \n...           \"The scikit-learn docs are Orange and Blue\"]                                                                                                                                                                                                   \n>>> vect = TfidfVectorizer(min_df=1, stop_words=\"english\")                                                                                                                                                                                                   \n>>> tfidf = vect.fit_transform(corpus)                                                                                                                                                                                                                       \n>>> pairwise_similarity = tfidf * tfidf.T \n\nthough Gensim may have more options for this kind of task.\nSee also this question.\n[Disclaimer: I was involved in the scikit-learn TF-IDF implementation.]\nInterpreting the Results\nFrom above, pairwise_similarity is a Scipy sparse matrix that is square in shape, with the number of rows and columns equal to the number of documents in the corpus.\n>>> pairwise_similarity                                                                                                                                                                                                                                      \n<5x5 sparse matrix of type '<class 'numpy.float64'>'\n    with 17 stored elements in Compressed Sparse Row format>\n\nYou can convert the sparse array to a NumPy array via .toarray() or .A:\n>>> pairwise_similarity.toarray()                                                                                                                                                                                                                            \narray([[1.        , 0.17668795, 0.27056873, 0.        , 0.        ],\n       [0.17668795, 1.        , 0.15439436, 0.        , 0.        ],\n       [0.27056873, 0.15439436, 1.        , 0.19635649, 0.16815247],\n       [0.        , 0.        , 0.19635649, 1.        , 0.54499756],\n       [0.        , 0.        , 0.16815247, 0.54499756, 1.        ]])\n\nLet's say we want to find the document most similar to the final document, \"The scikit-learn docs are Orange and Blue\".  This document has index 4 in corpus.  You can find the index of the most similar document by taking the argmax of that row, but first you'll need to mask the 1's, which represent the similarity of each document to itself.  You can do the latter through np.fill_diagonal(), and the former through np.nanargmax():\n>>> import numpy as np     \n                                                                                                                                                                                                                                  \n>>> arr = pairwise_similarity.toarray()     \n>>> np.fill_diagonal(arr, np.nan)                                                                                                                                                                                                                            \n                                                                                                                                                                                                                 \n>>> input_doc = \"The scikit-learn docs are Orange and Blue\"                                                                                                                                                                                                  \n>>> input_idx = corpus.index(input_doc)                                                                                                                                                                                                                      \n>>> input_idx                                                                                                                                                                                                                                                \n4\n\n>>> result_idx = np.nanargmax(arr[input_idx])                                                                                                                                                                                                                \n>>> corpus[result_idx]                                                                                                                                                                                                                                       \n'I prefer scikit-learn to Orange'\n\nNote: the purpose of using a sparse matrix is to save (a substantial amount of space) for a large corpus & vocabulary.  Instead of converting to a NumPy array, you could do:\n>>> n, _ = pairwise_similarity.shape                                                                                                                                                                                                                         \n>>> pairwise_similarity[np.arange(n), np.arange(n)] = -1.0\n>>> pairwise_similarity[input_idx].argmax()                                                                                                                                                                                                                  \n3\n\n"}, "32": {"topic": "How to compute the similarity between two text documents?", "user_name": "Savrige", "text": "\nIdentical to @larsman, but with some preprocessing\nimport nltk, string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nnltk.download('punkt') # if necessary...\n\n\nstemmer = nltk.stem.porter.PorterStemmer()\nremove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n\ndef stem_tokens(tokens):\n    return [stemmer.stem(item) for item in tokens]\n\n'''remove punctuation, lowercase, stem'''\ndef normalize(text):\n    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n\nvectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n\ndef cosine_sim(text1, text2):\n    tfidf = vectorizer.fit_transform([text1, text2])\n    return ((tfidf * tfidf.T).A)[0,1]\n\n\nprint cosine_sim('a little bird', 'a little bird')\nprint cosine_sim('a little bird', 'a little bird chirps')\nprint cosine_sim('a little bird', 'a big dog barks')\n\n"}, "33": {"topic": "How to compute the similarity between two text documents?", "user_name": "Fred FooFred Foo", "text": "\nIt's an old question, but I found this can be done easily with Spacy. Once the document is read, a simple api similarity can be used to find the cosine similarity between the document vectors.\nStart by installing the package and downloading the model:\npip install spacy\npython -m spacy download en_core_web_sm\n\nThen use like so:\nimport spacy\nnlp = spacy.load('en_core_web_sm')\ndoc1 = nlp(u'Hello hi there!')\ndoc2 = nlp(u'Hello hi there!')\ndoc3 = nlp(u'Hey whatsup?')\n\nprint (doc1.similarity(doc2)) # 0.999999954642\nprint (doc2.similarity(doc3)) # 0.699032527716\nprint (doc1.similarity(doc3)) # 0.699032527716\n\n"}, "34": {"topic": "How to compute the similarity between two text documents?", "user_name": "ldavid", "text": "\nIf you are looking for something very accurate, you need to use some better tool than tf-idf. Universal sentence encoder is one of the most accurate ones to find the similarity between any two pieces of text. Google provided pretrained models that you can use for your own application without a need to train from scratch anything. First, you have to install tensorflow and tensorflow-hub:\n    pip install tensorflow\n    pip install tensorflow_hub\n\nThe code below lets you convert any text to a fixed length vector representation and then you can use the dot product to find out the similarity between them\nimport tensorflow_hub as hub\nmodule_url = \"https://tfhub.dev/google/universal-sentence-encoder/1?tf-hub-format=compressed\"\n\n# Import the Universal Sentence Encoder's TF Hub module\nembed = hub.Module(module_url)\n\n# sample text\nmessages = [\n# Smartphones\n\"My phone is not good.\",\n\"Your cellphone looks great.\",\n\n# Weather\n\"Will it snow tomorrow?\",\n\"Recently a lot of hurricanes have hit the US\",\n\n# Food and health\n\"An apple a day, keeps the doctors away\",\n\"Eating strawberries is healthy\",\n]\n\nsimilarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\nsimilarity_message_encodings = embed(similarity_input_placeholder)\nwith tf.Session() as session:\n    session.run(tf.global_variables_initializer())\n    session.run(tf.tables_initializer())\n    message_embeddings_ = session.run(similarity_message_encodings, feed_dict={similarity_input_placeholder: messages})\n\n    corr = np.inner(message_embeddings_, message_embeddings_)\n    print(corr)\n    heatmap(messages, messages, corr)\n\nand the code for plotting:\ndef heatmap(x_labels, y_labels, values):\n    fig, ax = plt.subplots()\n    im = ax.imshow(values)\n\n    # We want to show all ticks...\n    ax.set_xticks(np.arange(len(x_labels)))\n    ax.set_yticks(np.arange(len(y_labels)))\n    # ... and label them with the respective list entries\n    ax.set_xticklabels(x_labels)\n    ax.set_yticklabels(y_labels)\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=10,\n         rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    for i in range(len(y_labels)):\n        for j in range(len(x_labels)):\n            text = ax.text(j, i, \"%.2f\"%values[i, j],\n                           ha=\"center\", va=\"center\", color=\"w\", \nfontsize=6)\n\n    fig.tight_layout()\n    plt.show()\n\nthe result would be:\n\nas you can see the most similarity is between texts with themselves and then with their close texts in meaning.\nIMPORTANT: the first time you run the code it will be slow because it needs to download the model. if you want to prevent it from downloading the model again and use the local model you have to create a folder for cache and add it to the environment variable and then after the first time running use that path:\n\ntf_hub_cache_dir = \"universal_encoder_cached/\"\nos.environ[\"TFHUB_CACHE_DIR\"] = tf_hub_cache_dir\n\n# pointing to the folder inside cache dir, it will be unique on your system\nmodule_url = tf_hub_cache_dir+\"/d8fbeb5c580e50f975ef73e80bebba9654228449/\"\nembed = hub.Module(module_url)\n\nMore information: https://tfhub.dev/google/universal-sentence-encoder/2\n"}, "35": {"topic": "How to compute the similarity between two text documents?", "user_name": "RenaudRenaud", "text": "\nGenerally a cosine similarity between two documents is used as a similarity measure of documents. In Java, you can use Lucene (if your collection is pretty large) or LingPipe to do this. The basic concept would be to count the terms in every document and calculate the dot product of the term vectors. The libraries do provide several improvements over this general approach, e.g. using inverse document frequencies and calculating tf-idf vectors. If you are looking to do something copmlex, LingPipe also provides methods to calculate LSA similarity between documents which gives better results than cosine similarity. \nFor Python, you can use NLTK.\n"}, "36": {"topic": "How to compute the similarity between two text documents?", "user_name": "Lu\u00eds de Sousa", "text": "\nFor Syntactic Similarity \nThere can be 3 easy ways of detecting similarity.\n\nWord2Vec\nGlove\nTfidf or countvectorizer\n\nFor Semantic Similarity\nOne can use BERT Embedding and try a different word pooling strategies to get document embedding and then apply cosine similarity on document embedding. \nAn advanced methodology can use BERT SCORE to get similarity. \n\nResearch Paper Link: https://arxiv.org/abs/1904.09675\n"}, "37": {"topic": "How to compute the similarity between two text documents?", "user_name": "Koustuv SinhaKoustuv Sinha", "text": "\nHere's a little app to get you started...\nimport difflib as dl\n\na = file('file').read()\nb = file('file1').read()\n\nsim = dl.get_close_matches\n\ns = 0\nwa = a.split()\nwb = b.split()\n\nfor i in wa:\n    if sim(i, wb):\n        s += 1\n\nn = float(s) / float(len(wa))\nprint '%d%% similarity' % int(n * 100)\n\n"}, "38": {"topic": "How to compute the similarity between two text documents?", "user_name": "user3723763", "text": "\nTo find sentence similarity with very less dataset and to get high accuracy you can use below python package which is using pre-trained BERT models,\npip install similar-sentences\n\n"}, "39": {"topic": "How to compute the similarity between two text documents?", "user_name": "Rohola ZandieRohola Zandie", "text": "\nIf you are more interested in measuring semantic similarity of two pieces of text, I suggest take a look at this gitlab project. You can run it as a server, there is also a pre-built model which you can use easily to measure the similarity of two pieces of text; even though it is mostly trained for measuring the similarity of two sentences, you can still use it in your case.It is written in java but you can run it as a RESTful service. \nAnother option also is DKPro Similarity which is a library with various algorithm to measure the similarity of texts. However, it is also written in java. \ncode example:\n// this similarity measure is defined in the dkpro.similarity.algorithms.lexical-asl package\n// you need to add that to your .pom to make that example work\n// there are some examples that should work out of the box in dkpro.similarity.example-gpl \nTextSimilarityMeasure measure = new WordNGramJaccardMeasure(3);    // Use word trigrams\n\nString[] tokens1 = \"This is a short example text .\".split(\" \");   \nString[] tokens2 = \"A short example text could look like that .\".split(\" \");\n\ndouble score = measure.getSimilarity(tokens1, tokens2);\n\nSystem.out.println(\"Similarity: \" + score);\n\n"}, "40": {"topic": "How to compute the similarity between two text documents?", "user_name": "Pulkit GoyalPulkit Goyal", "text": "\nCreator of the Simphile NLP text similarity Python package here. Simphile contains several text similarity methods that are language agnostic and less CPU-intensive than language embeddings.\nInstall:\npip install simphile\n\nChoose your favorite method.  This example shows three:\nfrom simphile import jaccard_similarity, euclidian_similarity, compression_similarity\n\ntext_a = \"I love dogs\"\ntext_b = \"I love cats\"\n\nprint(f\"Jaccard Similarity: {jaccard_similarity(text_a, text_b)}\")\nprint(f\"Euclidian Similarity: {euclidian_similarity(text_a, text_b)}\")\nprint(f\"Compression Similarity: {compression_similarity(text_a, text_b)}\")\n\n\nCompression Similairty \u2013 leverages the pattern recognition of compression algorithms\nEuclidian Similarity \u2013 Treats text like points in multi-dimensional space and calculates their closeness\nJaccard Similairy \u2013 Texts are more similar the more their words overlap\n\n"}, "41": {"topic": "How to compute the similarity between two text documents?", "user_name": "Shaurya UppalShaurya Uppal", "text": "\nYou might want to try this online service  for cosine document similarity http://www.scurtu.it/documentSimilarity.html\nimport urllib,urllib2\nimport json\nAPI_URL=\"http://www.scurtu.it/apis/documentSimilarity\"\ninputDict={}\ninputDict['doc1']='Document with some text'\ninputDict['doc2']='Other document with some text'\nparams = urllib.urlencode(inputDict)    \nf = urllib2.urlopen(API_URL, params)\nresponse= f.read()\nresponseObject=json.loads(response)  \nprint responseObject\n\n"}, "42": {"topic": "How to compute the similarity between two text documents?", "user_name": "Ben", "text": "\nI am combining the solutions from answers of @FredFoo and @Renaud. My solution is able to apply @Renaud's preprocessing on the text corpus of @FredFoo and then display pairwise similarities where the similarity is greater than 0. I ran this code on Windows by installing python and pip first. pip is installed as part of python but you may have to explicitly do it by re-running the installation package, choosing modify and then choosing pip. I use the command line to execute my python code saved in a file \"similarity.py\". I had to execute the following commands:\n>set PYTHONPATH=%PYTHONPATH%;C:\\_location_of_python_lib_\n>python -m pip install sklearn\n>python -m pip install nltk\n>py similarity.py\n\nThe code for similarity.py is as follows:\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk, string\nimport numpy as np\nnltk.download('punkt') # if necessary...\n\nstemmer = nltk.stem.porter.PorterStemmer()\nremove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n\ndef stem_tokens(tokens):\n    return [stemmer.stem(item) for item in tokens]\n\ndef normalize(text):\n    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n\ncorpus = [\"I'd like an apple\", \n           \"An apple a day keeps the doctor away\", \n           \"Never compare an apple to an orange\", \n           \"I prefer scikit-learn to Orange\", \n           \"The scikit-learn docs are Orange and Blue\"]  \n\nvect = TfidfVectorizer(tokenizer=normalize, stop_words='english')\ntfidf = vect.fit_transform(corpus)   \n                                                                                                                                                                                                                    \npairwise_similarity = tfidf * tfidf.T\n\n#view the pairwise similarities \nprint(pairwise_similarity)\n\n#check how a string is normalized\nprint(normalize(\"The scikit-learn docs are Orange and Blue\"))\n\n"}, "43": {"topic": "How to compute the similarity between two text documents?", "user_name": "        user963720user963720", "text": "\nWe can use sentencetransformer for this task\nlink\nA simple example from sbert as below:\nfrom sentence_transformers import SentenceTransformer, util\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n# Two lists of sentences\nsentences1 = ['The cat sits outside']\nsentences2 = ['The dog plays in the garden']\n#Compute embedding for both lists\nembeddings1 = model.encode(sentences1, convert_to_tensor=True)\nembeddings2 = model.encode(sentences2, convert_to_tensor=True)\n#Compute cosine-similarities\ncosine_scores = util.cos_sim(embeddings1, embeddings2)\n#Output the pairs with their score\nfor i in range(len(sentences1)):\n   print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], \n         sentences2[i], cosine_scores[i][i]))\n\n"}, "44": {"topic": "Semantic search with NLP and elasticsearch", "user_name": "Ivaylo Slavov", "text": "\nI am experimenting with elasticsearch as a search server and my task is to build a \"semantic\" search functionality. From a short text phrase like \"I have a burst pipe\" the system should infer that the user is searching for a plumber and return all plumbers indexed in elasticsearch. \nCan that be done directly in a search server like elasticsearch or do I have to use a natural language processing (NLP) tool like e.g. Maui Indexer. What is the exact terminology for my task at hand, text classification? Though the given text is very short as it is a search phrase.\n"}, "45": {"topic": "Semantic search with NLP and elasticsearch", "user_name": "user1089363user1089363", "text": "\nThere may be several approaches with different implementation complexity. \nThe easiest one is to create list of topics (like plumbing), attach bag of words (like \"pipe\"), identify search request by majority of keywords and search only in specified topic (you can add field topic to your elastic search documents and set it as mandatory with + during search). \nOf course, if you have lots of documents, manual creation of topic list and bag of words is very time expensive. You can use machine learning to automate some of tasks. Basically, it is enough to have distance measure between words and/or documents to automatically discover topics (e.g. by data clustering) and classify query to one of these topics. Mix of these techniques may also be a good choice (for example, you can manually create topics and assign initial documents to them, but use classification for query assignment). Take a look at Wikipedia's article on latent semantic analysis to better understand the idea. Also pay attention to the 2 linked articles on data clustering and document classification. And yes, Maui Indexer may become good helper tool this way. \nFinally, you can try to build an engine that \"understands\" meaning of the phrase (not just uses terms frequency) and searches appropriate topics. Most probably, this will involve natural language processing and ontology-based knowledgebases. But in fact, this field is still in active research and without previous experience it will be very hard for you to implement something like this. \n"}, "46": {"topic": "Semantic search with NLP and elasticsearch", "user_name": "ffriendffriend", "text": "\nYou may want to explore https://blog.conceptnet.io/2016/11/03/conceptnet-5-5-and-conceptnet-io/.\nIt combines semantic networks and distributional semantics.\n\nWhen most developers need word embeddings, the first and possibly only place they look is word2vec, a neural net algorithm from Google that computes word embeddings from distributional semantics. That is, it learns to predict words in a sentence from the other words around them, and the embeddings are the representation of words that make the best predictions. But even after terabytes of text, there are aspects of word meanings that you just won\u2019t learn from distributional semantics alone.\n\nSome results\n\nThe ConceptNet Numberbatch word embeddings, built into ConceptNet 5.5, solve these SAT analogies better than any previous system. It gets 56.4% of the questions correct. The best comparable previous system, Turney\u2019s SuperSim (2013), got 54.8%. And we\u2019re getting ever closer to \u201chuman-level\u201d performance on SAT analogies \u2014 while particularly smart humans can of course get a lot more questions right, the average college applicant gets 57.0%.\n\n"}, "47": {"topic": "Semantic search with NLP and elasticsearch", "user_name": "SemanticBeengSemanticBeeng", "text": "\nSemantic search is basically search with meaning. Elasticsearch uses JSON serialization by default, to apply search with meaning to JSON you would need to extend it to support edge relations via JSON-LD. You can then apply your semantic analysis over the JSON-LD schema to word disambiguate plumber entity and burst pipe contexts as a subject, predicate, object relationships. Elasticsearch has a very weak semantic search support but you can go around it using faceted searching and bag of words. You can index a thesaurus schema for plumbing terms, then do a semantic matching over the text phrases in your sentences. \n"}, "48": {"topic": "Semantic search with NLP and elasticsearch", "user_name": "        user3366107user3366107", "text": "\n\"Elasticsearch 7.3 introduced introduced text similarity search with vector fields\".\nThey describe the application of using text embeddings (e.g., word embeddings and sentence embeddings) to implement this sort of semantic similarity measure.\n"}, "49": {"topic": "Semantic search with NLP and elasticsearch", "user_name": "dfrankowdfrankow", "text": "\nA bit late to the party, but part II of this blog seems to address this through \"contextual searches\". It basically makes a two-part query to Elasticsearch in order to build a list of \"seed\" documents and then an expanded query via the more-like-this API. The result is a set of documents most contextually similar to the search query.\n"}, "50": {"topic": "Semantic search with NLP and elasticsearch", "user_name": "kd88kd88", "text": "\nit's possible. This GitHub repo shows how to integrate Elasticsearch with the current state-of-the-art on NLP for semantic representation of language: BERT (Bidirectional Encoder Representations from Transformers) https://github.com/Hironsan/bertsearch\nGood luck.\n"}, "51": {"topic": "Semantic search with NLP and elasticsearch", "user_name": "Vinicius WoloszynVinicius Woloszyn", "text": "\nMy suggestion is to use BERT embedding for your sentences and add an embedding field to your ElasticSearch, as it is described in https://www.elastic.co/blog/text-similarity-search-with-vectors-in-elasticsearch\nFor BERT embedding I suggest to use sentence-transformers from Huggingface library. You can find sample codes in https://towardsdatascience.com/how-to-build-a-semantic-search-engine-with-transformers-and-faiss-dcbea307a0e8\n"}, "52": {"topic": "Semantic search with NLP and elasticsearch", "user_name": "Ali Reza EbadatAli Reza Ebadat", "text": "\nThere are several options for that:\n\nYou can perform it in elasticsearch itself. Elasticsearch supports the indexing of Dense Embedding of docs. From there, you can write your own pipeline for search and use your preferred relevancy score formula ie. cosine similarity or something else.\n\nUse Haystack pipeline, refer to my blog which describes setting up a semantic search pipeline (end-to-end).\n\nYou can use Meta's Faiss\n\n\n"}, "53": {"topic": "Stanford nlp for python", "user_name": "90abyss90abyss", "text": "\nAll I want to do is find the sentiment (positive/negative/neutral) of any given string. On researching I came across Stanford NLP. But sadly its in Java. Any ideas on how can I make it work for python? \n"}, "54": {"topic": "Stanford nlp for python", "user_name": "", "text": "\nUse py-corenlp\nDownload Stanford CoreNLP\nThe latest version at this time (2020-05-25) is 4.0.0:\nwget https://nlp.stanford.edu/software/stanford-corenlp-4.0.0.zip https://nlp.stanford.edu/software/stanford-corenlp-4.0.0-models-english.jar\n\nIf you do not have wget, you probably have curl:\ncurl https://nlp.stanford.edu/software/stanford-corenlp-4.0.0.zip -O https://nlp.stanford.edu/software/stanford-corenlp-4.0.0-models-english.jar -O\n\nIf all else fails, use the browser ;-)\nInstall the package\nunzip stanford-corenlp-4.0.0.zip\nmv stanford-corenlp-4.0.0-models-english.jar stanford-corenlp-4.0.0\n\nStart the server\ncd stanford-corenlp-4.0.0\njava -mx5g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -timeout 10000\n\nNotes:\n\ntimeout is in milliseconds, I set it to 10 sec above.\nYou should increase it if you pass huge blobs to the server.\nThere are more options, you can list them with --help.\n-mx5g should allocate enough memory, but YMMV and you may need to modify the option if your box is underpowered.\n\nInstall the python package\nThe standard package\npip install pycorenlp\n\ndoes not work with Python 3.9, so you need to do\npip install git+https://github.com/sam-s/py-corenlp.git\n\n(See also the official list).\nUse it\nfrom pycorenlp import StanfordCoreNLP\n\nnlp = StanfordCoreNLP('http://localhost:9000')\nres = nlp.annotate(\"I love you. I hate him. You are nice. He is dumb\",\n                   properties={\n                       'annotators': 'sentiment',\n                       'outputFormat': 'json',\n                       'timeout': 1000,\n                   })\nfor s in res[\"sentences\"]:\n    print(\"%d: '%s': %s %s\" % (\n        s[\"index\"],\n        \" \".join([t[\"word\"] for t in s[\"tokens\"]]),\n        s[\"sentimentValue\"], s[\"sentiment\"]))\n\nand you will get:\n0: 'I love you .': 3 Positive\n1: 'I hate him .': 1 Negative\n2: 'You are nice .': 3 Positive\n3: 'He is dumb': 1 Negative\n\nNotes\n\nYou pass the whole text to the server and it splits it into sentences. It also splits sentences into tokens.\nThe sentiment is ascribed to each sentence, not the whole text. The mean sentimentValue across sentences can be used to estimate the sentiment of the whole text.\nThe average sentiment of a sentence is between Neutral (2) and Negative (1), the range is from VeryNegative (0) to VeryPositive (4) which appear to be quite rare.\nYou can stop the server either by typing Ctrl-C at the terminal you started it from or using the shell command kill $(lsof -ti tcp:9000). 9000 is the default port, you can change it using the -port option when starting the server.\nIncrease timeout (in milliseconds) in server or client if you get timeout errors.\nsentiment is just one annotator, there are many more, and you can request several, separating them by comma: 'annotators': 'sentiment,lemma'.\nBeware that the sentiment model is somewhat idiosyncratic (e.g., the result is different depending on whether you mention David or Bill).\n\nPS. I cannot believe that I added a 9th answer, but, I guess, I had to, since none of the existing answers helped me (some of the 8 previous answers have now been deleted, some others have been converted to comments).\n"}, "55": {"topic": "Stanford nlp for python", "user_name": "sdssds", "text": "\nNative Python implementation of NLP tools from Stanford\nRecently Stanford has released a new Python packaged implementing neural network (NN) based algorithms for the most important NLP tasks:\n\ntokenization\nmulti-word token (MWT) expansion\nlemmatization\npart-of-speech (POS) and morphological features tagging\ndependency parsing\n\nIt is implemented in Python and uses PyTorch as the NN library. The package contains accurate models for more than 50 languages. \nTo install you can use PIP:\npip install stanfordnlp\n\nTo perform basic tasks you can use native Python interface with many NLP algorithms:\nimport stanfordnlp\n\nstanfordnlp.download('en')   # This downloads the English models for the neural pipeline\nnlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English\ndoc = nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\ndoc.sentences[0].print_dependencies()\n\nEDIT:\nSo far, the library does not support sentiment analysis, yet I'm not deleting the answer, since it directly answers the \"Stanford nlp for python\" part of the question.\n"}, "56": {"topic": "Stanford nlp for python", "user_name": "", "text": "\nRight now they have STANZA.\nhttps://stanfordnlp.github.io/stanza/\nRelease History\nNote that prior to version 1.0.0, the Stanza library was named as \u201cStanfordNLP\u201d. To install historical versions prior to to v1.0.0, you\u2019ll need to run pip install stanfordnlp.\nSo, it confirms that Stanza is the full python version of stanford NLP.\n"}, "57": {"topic": "Stanford nlp for python", "user_name": "Aleksander PohlAleksander Pohl", "text": "\nTextblob is a great package for sentimental analysis written in Python. You can have the docs here . Sentimental analysis of any given sentence is carried out by inspecting words and their corresponding emotional score (sentiment). You can start with\n$ pip install -U textblob\n$ python -m textblob.download_corpora\n\nFirst pip install command will give you latest version of textblob installed in your (virtualenv) system since you pass -U will upgrade the pip package its latest available version . And the next will download all the data required, thecorpus . \n"}, "58": {"topic": "Stanford nlp for python", "user_name": "Syauqi HarisSyauqi Haris", "text": "\nI also faced similar situation. Most of my projects are in Python and sentiment part is Java. Luckily it's quite easy to lean how to use the stanford CoreNLP jar. \nHere is one of my scripts and you can download jars and run it. \nimport java.util.List;\nimport java.util.Properties;\nimport edu.stanford.nlp.ling.CoreAnnotations;\nimport edu.stanford.nlp.neural.rnn.RNNCoreAnnotations;\nimport edu.stanford.nlp.pipeline.Annotation;\nimport edu.stanford.nlp.pipeline.StanfordCoreNLP;\nimport edu.stanford.nlp.sentiment.SentimentCoreAnnotations.SentimentAnnotatedTree;\nimport edu.stanford.nlp.trees.Tree;\nimport edu.stanford.nlp.util.ArrayCoreMap;\nimport edu.stanford.nlp.util.CoreMap;\n\npublic class Simple_NLP {\nstatic StanfordCoreNLP pipeline;\n\n    public static void init() {\n        Properties props = new Properties();\n        props.setProperty(\"annotators\", \"tokenize, ssplit, parse, sentiment\");\n        pipeline = new StanfordCoreNLP(props);\n    }\n\n    public static String findSentiment(String tweet) {\n        String SentiReturn = \"\";\n        String[] SentiClass ={\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"};\n\n        //Sentiment is an integer, ranging from 0 to 4. \n        //0 is very negative, 1 negative, 2 neutral, 3 positive and 4 very positive.\n        int sentiment = 2;\n\n        if (tweet != null && tweet.length() > 0) {\n            Annotation annotation = pipeline.process(tweet);\n\n            List<CoreMap> sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);\n            if (sentences != null && sentences.size() > 0) {\n\n                ArrayCoreMap sentence = (ArrayCoreMap) sentences.get(0);                \n                Tree tree = sentence.get(SentimentAnnotatedTree.class);  \n                sentiment = RNNCoreAnnotations.getPredictedClass(tree);             \n                SentiReturn = SentiClass[sentiment];\n            }\n        }\n        return SentiReturn;\n    }\n\n}\n\n"}, "59": {"topic": "Stanford nlp for python", "user_name": "", "text": "\nI am facing the same problem : maybe a solution with stanford_corenlp_py that uses Py4j as pointed out by @roopalgarg.\n\nstanford_corenlp_py\nThis repo provides a Python interface for calling the \"sentiment\" and \"entitymentions\" annotators of Stanford's CoreNLP Java package, current as of v. 3.5.1. It uses py4j to interact with the JVM; as such, in order to run a script like scripts/runGateway.py, you must first compile and run the Java classes creating the JVM gateway.\n\n"}, "60": {"topic": "Stanford nlp for python", "user_name": "cutteethcutteeth", "text": "\nUse stanfordcore-nlp python library\nstanford-corenlp is a really good wrapper on top of the stanfordcore-nlp to use it in python.\nwget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\nUsage\n# Simple usage\nfrom stanfordcorenlp import StanfordCoreNLP\n\nnlp = StanfordCoreNLP('/Users/name/stanford-corenlp-full-2018-10-05')\n\nsentence = 'Guangdong University of Foreign Studies is located in Guangzhou.'\nprint('Tokenize:', nlp.word_tokenize(sentence))\nprint('Part of Speech:', nlp.pos_tag(sentence))\nprint('Named Entities:', nlp.ner(sentence))\nprint('Constituency Parsing:', nlp.parse(sentence))\nprint('Dependency Parsing:', nlp.dependency_parse(sentence))\n\nnlp.close() # Do not forget to close! The backend server will consume a lot memory.\n\nMore info\n"}, "61": {"topic": "Stanford nlp for python", "user_name": "Hao LyuHao Lyu", "text": "\nI would suggest using the TextBlob library. A sample implementation goes like this: \nfrom textblob import TextBlob\ndef sentiment(message):\n    # create TextBlob object of passed tweet text\n    analysis = TextBlob(message)\n    # set sentiment\n    return (analysis.sentiment.polarity)\n\n"}, "62": {"topic": "Stanford nlp for python", "user_name": "arnaudarnaud", "text": "\nThere is a very new progress on this issue:\nNow you can use stanfordnlp package inside the python:\nFrom the README:\n>>> import stanfordnlp\n>>> stanfordnlp.download('en')   # This downloads the English models for the neural pipeline\n>>> nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English\n>>> doc = nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n>>> doc.sentences[0].print_dependencies()\n\n"}, "63": {"topic": "Stanford nlp for python", "user_name": "CommunityBot", "text": "\nimport os\nimport numpy as np\nimport pandas as pd\n\ninputFile = 'senti_post3.csv'\n\n# Add empty column columns\ndf = pd.read_csv(inputFile)\ndf.head(5)\n\n# header_list_new = ['numSentence', 'numWords', 'totSentiment', 'avgSentiment', 'Sfreq0','Sfreq1','Sfreq2','Sfreq3','Sfreq4','Sfreq5']\n# for i, name in enumerate(header_list_new):\n#     df[name] = 0\n\nfrom pycorenlp import StanfordCoreNLP\nnlp = StanfordCoreNLP('http://localhost:9000')\n\n# Function; Output = # sentence, # words, avg.sentimentValue, sentimentHist\ndef stanford_sentiment(text_str):\n    res = nlp.annotate(text_str,\n                   properties={\n                       'annotators': 'sentiment',\n                       'outputFormat': 'json',\n                       'timeout': 40000,\n                   })\n    numSentence = len(res[\"sentences\"])\n    numWords = len(text_str.split())\n    \n    # data arrangement\n    arraySentVal = np.zeros(numSentence)\n\n    for i, s in enumerate(res[\"sentences\"]):\n        arraySentVal[i] = int(s[\"sentimentValue\"])\n\n    # sum of sentiment values \n    totSentiment = sum(arraySentVal)\n\n    # avg. of sentiment values \n    avgSentiment = np.mean(arraySentVal)\n\n    # frequency of sentimentValue\n    bins = [0,1,2,3,4,5,6]\n    freq = np.histogram(arraySentVal, bins)[0]    # getting freq. only w/o bins\n\n    return(numSentence, numWords, totSentiment, avgSentiment, freq)\n# dfLength = len(df)\n# for i in range(dfLength):\nfor i in range(54000,55284):\n    try:\n        numSentence, numWords, totSentiment, avgSentiment, freq = stanford_sentiment(df.clean_text[i].replace('\\n',\" \"))\n        df.loc[i,'numSentence'] = numSentence\n        df.loc[i,'numWords'] = numWords\n        df.loc[i,'totSentiment'] = totSentiment\n        df.loc[i,'avgSentiment'] = avgSentiment\n        df.loc[i,'Sfreq0'] = freq[0]\n        df.loc[i,'Sfreq1'] = freq[1]\n        df.loc[i,'Sfreq2'] = freq[2]\n        df.loc[i,'Sfreq3'] = freq[3]\n        df.loc[i,'Sfreq4'] = freq[4]\n        df.loc[i,'Sfreq5'] = freq[5]\n    except:\n        print(\"error where i =\", i)\n  \noutputFile = 'senti_post16.csv'\ndf.to_csv(outputFile, encoding='utf-8', index=False )\n\n"}, "64": {"topic": "Python NLP Intent Identification", "user_name": "Neo-coderNeo-coder", "text": "\nI am novice in Python and NLP, and my problem is how to finding out Intent of given questions, for example I have sets of questions and answers like this :\nquestion:What is NLP; answer: NLP stands for Natural Language Processing\nI did some basic POS tagger on given questions in above question I get entety [NLP] I also did String Matching using this algo. \nBasically I faced following issues :\n\nIf user ask what is NLP then it will return exact answers\nIf user ask meaning of NLP then it fail\nIf user ask Definition of NLP then it fail\nIf user ask What is Natural Language Processing then it fail\n\nSo how I should identify user intent of given questions because in my case String matching or pattern matching not works.  \n"}, "65": {"topic": "Python NLP Intent Identification", "user_name": "Vlad PVlad P", "text": "\nYou can do intent identification with DeepPavlov, it supports multi-label classification. More information can be found in http://docs.deeppavlov.ai/en/master/components/classifiers.html\nThe demo page https://demo.ipavlov.ai\n"}, "66": {"topic": "Python NLP Intent Identification", "user_name": "", "text": "\nyou can use spacy for training a custom parser for chat intent semantics.\nspaCy's parser component can be used to trained to predict any type of tree structure over your input text. You can also predict trees over whole documents or chat logs, with connections between the sentence-roots used to annotate discourse structure. \nfor example:\n\"show me the best hotel in berlin\"\n('show', 'ROOT', 'show')\n('best', 'QUALITY', 'hotel') --> hotel with QUALITY best\n('hotel', 'PLACE', 'show') --> show PLACE hotel\n('berlin', 'LOCATION', 'hotel') --> hotel with LOCATION berlin\n\nTo train the model you need data in this format:\n# training data: texts, heads and dependency labels\n# for no relation, we simply chose an arbitrary dependency label, e.g. '-'\nTRAIN_DATA = [\n    (\"find a cafe with great wifi\", {\n        'heads': [0, 2, 0, 5, 5, 2],  # index of token head\n        'deps': ['ROOT', '-', 'PLACE', '-', 'QUALITY', 'ATTRIBUTE']\n    }),\n    (\"find a hotel near the beach\", {\n        'heads': [0, 2, 0, 5, 5, 2],\n        'deps': ['ROOT', '-', 'PLACE', 'QUALITY', '-', 'ATTRIBUTE']\n    })]\n\nTEST_DATA:\ninput : show me the best hotel in berlin\noutput: [\n      ('show', 'ROOT', 'show'),\n      ('best', 'QUALITY', 'hotel'),\n      ('hotel', 'PLACE', 'show'),\n      ('berlin', 'LOCATION', 'hotel')\n    ]\n\nfor more details Please check the below link.\nhttps://spacy.io/usage/examples#intent-parser\n"}, "67": {"topic": "Python NLP Intent Identification", "user_name": "manish Prasadmanish Prasad", "text": "\nFor a general knowledge and list of excellent examples for question and answering based systems, the leaderboard of NLP in the industry are listed here: https://rajpurkar.github.io/SQuAD-explorer/\nThis process can actually get really complicated depending on the complexity and range of your domain.  For example, more advanced approaches apply first order + propositional logic and complex neural nets.\nOne of the more impressive solutions I've seen is bidirectional attention flow: https://github.com/allenai/bi-att-flow, demo is here: http://beta.moxel.ai/models/strin/bi-att-flow/latest\nIn practice, I have found that if your corpus has more domain-specific terms, you will need to build your own dictionary.  In your example, \"NLP\" and \"Natural Language Processing\" are the same entity, so you need to include this in a dictionary.  \nBasically, consider yourself really lucky if you can get away with just a pure statistical approach like cosine distance.  You'll likely need to combine with a lexicon-based approach as well.  All the NLP projects I have done have had domain-specific terminology and \"slang\", so I have used combined both statistical and lexicon based methods, especially for feature extraction like topics, intents, and entities.\n"}, "68": {"topic": "Python NLP Intent Identification", "user_name": "saucy wombatsaucy wombat", "text": "\nI think this really depends on how your frame your problem and your domain. Here is a dataset that might be useful for question type classification and here is an implementation. \nThese being said, I think you'll need to annotate your text, possibly by Chunker, SRL, etc and extract interesting pattern. \n"}, "69": {"topic": "Strategies for recognizing proper nouns in NLP", "user_name": "MaD70", "text": "\nI'm interested in learning more about Natural Language Processing (NLP) and am curious if there are currently any strategies for recognizing proper nouns in a text that aren't based on dictionary recognition? Also, could anyone explain or link to resources that explain the current dictionary-based methods? Who are the authoritative experts on NLP or what are the definitive resources on the subject?\n"}, "70": {"topic": "Strategies for recognizing proper nouns in NLP", "user_name": "VirtuosiMediaVirtuosiMedia", "text": "\nThe task of determining the proper part of speech for a word in a text is called Part of Speech Tagging. The Brill tagger, for example, uses a mixture of dictionary(vocabulary) words and contextual rules. I believe that some of the important initial dictionary words for this task are the stop words. \nOnce you have (mostly correct) parts of speech for your words, you can start building larger structures. This industry-oriented book differentiates between recognizing noun phrases (NPs) and recognizing named entities. \nAbout textbooks: Allen's Natural Language Understanding is a good, but a bit dated, book. Foundations of Statistical Natural Language Processing is a nice introduction to statistical NLP. Speech and Language Processing is a bit more rigorous and maybe more authoritative. The Association for Computational Linguistics is a leading scientific community on computational linguistics.\n"}, "71": {"topic": "Strategies for recognizing proper nouns in NLP", "user_name": "Yuval FYuval F", "text": "\nBesides the dictionary-based approach, two others come to my mind:\n\nPattern-based approaches (in a simple form: anything that is capitalized is a proper noun)\nMachine learning approaches (mark proper nouns in a training corpus and train a classifier)\n\nThe field is mostly called named-entity extraction and often considered a subfield of information extraction. A good starting point for the different fields of NLP is usually the according chapter in the Oxford Handbook of Computational Linguistics:\n\n(source: oup.com) \n"}, "72": {"topic": "Strategies for recognizing proper nouns in NLP", "user_name": "Glorfindel", "text": "\nTry searching for \"named entity recognition\"--that's the term that's used in the NLP literature for this sort of thing.\n"}, "73": {"topic": "Strategies for recognizing proper nouns in NLP", "user_name": "Fabian SteegFabian Steeg", "text": "\nIt depends on what you mean by dictionary-based.\nFor example, one strategy would be to take things that aren't in a dictionary and try to proceed on the assumption that they're proper nouns.  If this leads to a sensible parse, consider the assumption provisionally validated and keep going, otherwise conclude that they aren't.\nOther ideas:\n\nIn subject position, any simple subject without a determiner is a good candidate.\nDitto in prepositional phrases\nIn any position, the basis of a possessive determiner (e.g. Bob in \"Bob's sister\") is a good candidate \n\n-- MarkusQ\n"}, "74": {"topic": "Strategies for recognizing proper nouns in NLP", "user_name": "        user18015user18015", "text": "\nsome toolkits suggested:\n1. Opennlp: there is a Named Entity Recognition component for your task\n2. LingPipe: also a NER component for it\n3. Stanford NLP package: excellent package for academic usage, maybe not commercial friendly.\n4. nltk: a Python NLP package\n"}, "75": {"topic": "Strategies for recognizing proper nouns in NLP", "user_name": "", "text": "\nif you have sentence such as \"who is bill gates\"\nAnd if you apply part of speech tagger to it.\nIt will give answer as\n\"who/WP is/VBZ bill/NN gates/NNS ?/. \"\nU can try this online on \nhttp://cst.dk/online/pos_tagger/uk/\nSo you are getting what are all the nouns in this sentence. Now you can easily extract this nouns with some algorithm. I suggest to use python if you are using natural language processing. It has NLTK(Natural language toolkit) with which you can work.\n"}, "76": {"topic": "Strategies for recognizing proper nouns in NLP", "user_name": "MarkusQMarkusQ", "text": "\nIf you're interested in the implementation of natural language processing and python is your programming language, then this can be a very informative resource: http://www.youtube.com/watch?v=kKe4M4iSclc\n"}, "77": {"topic": "Strategies for recognizing proper nouns in NLP", "user_name": "WDongWDong", "text": "\nThough this is for Bengali language, but it can draw a common procedure identified proper noun. So I hope this will be helpful for you.\nPlease check the folowing link:\nhttp://www.mecs-press.org/ijmecs/ijmecs-v6-n8/v6n8-1.html\n"}, "78": {"topic": "Existing API for NLP in C++?", "user_name": "hyde", "text": "\nIs/are there existing C++ NLP API(s) out there? The closest thing I have found is CLucene, a port of Lucene. However, it seems a bit obsolete and the documentation is far from complete.\nIdeally, this/these API(s) would permit tokenization, stemming and PoS tagging.\n"}, "79": {"topic": "Existing API for NLP in C++?", "user_name": "meroursmerours", "text": "\nFreeling is written in C++ too, although most people just use their binaries to run the tools: http://devel.cpl.upc.edu/freeling/downloads?order=time&desc=1\nTry something like DyNet, it's a generic neural net framework but most of its processes are focusing on NLP because the maintainers are creators of the NLP community.\nOr perhaps Marian-NMT, it was designed for sequence-to-sequence model machine translation but potentially many NLP tasks can be structured as a sequence-to-sequence task.\n\nOutdated\nMaybe you can try Ellogon http://www.ellogon.org/ , they have GUI support and also C/C++ API for NLP too.\n"}, "80": {"topic": "Existing API for NLP in C++?", "user_name": "", "text": "\nif you remove the restriction on c++ , you get the perfect NLTK (python)\nthe remaining effort is then interfacing between python and c++. \n"}, "81": {"topic": "Existing API for NLP in C++?", "user_name": "alvasalvas", "text": "\nApache Lucy would get you part of the way there. It is under active development.\n"}, "82": {"topic": "Existing API for NLP in C++?", "user_name": "zinkingzinking", "text": "\nMaybe you can use Weka-C++. It's the very popular Weka library for machine learning and data mining (including NLP) ported from Java to C++.\nWeka supports tokenization and stemming, you'll probably need to train a classifier for PoS tagging.\nI only used Weka with Java though, so I'm afraid can't give you more details on this version.\n"}, "83": {"topic": "Existing API for NLP in C++?", "user_name": "Mark Leighton FisherMark Leighton Fisher", "text": "\nThere is TurboParser by Andr\u00e9 Martins at CMU, also has a Python wrapper. There is is an online demo for it.\n"}, "84": {"topic": "Existing API for NLP in C++?", "user_name": "", "text": "\nThis project provides free (even for commercial use) state-of-the-art information extraction tools. The current release includes tools for performing named entity extraction and binary relation detection as well as tools for training custom extractors and relation detectors.\nMITIE is built on top of dlib, a high-performance machine-learning library, MITIE makes use of several state-of-the-art techniques including the use of distributional word embeddings and Structural Support Vector Machines[3]. MITIE offers several pre-trained models providing varying levels of support for both English and Spanish, trained using a variety of linguistic resources (e.g., CoNLL 2003, ACE, Wikipedia, Freebase, and Gigaword). The core MITIE software is written in C++, but bindings for several other software languages including Python, R, Java, C, and MATLAB allow a user to quickly integrate MITIE into his/her own applications.\nhttps://github.com/mit-nlp/MITIE\n"}, "85": {"topic": "What is the difference between lemmatization vs stemming?", "user_name": "Shireen", "text": "\nWhen do I use each ?\nAlso...is the NLTK lemmatization dependent upon Parts of Speech?\nWouldn't it be more accurate if it was?\n"}, "86": {"topic": "What is the difference between lemmatization vs stemming?", "user_name": "TIMEXTIMEX", "text": "\nShort and dense: http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n\nThe goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\nHowever, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .\n\nFrom the NLTK docs:\n\nLemmatization and stemming are special cases of normalization. They identify a canonical representative for a set of related word forms.\n\n"}, "87": {"topic": "What is the difference between lemmatization vs stemming?", "user_name": "CommunityBot", "text": "\n\nLemmatisation is closely related to stemming. The difference is that a\n  stemmer operates on a single word without knowledge of the context,\n  and therefore cannot discriminate between words which have different\n  meanings depending on part of speech. However, stemmers are typically\n  easier to implement and run faster, and the reduced accuracy may not\n  matter for some applications.\nFor instance:\n\nThe word \"better\" has \"good\" as its lemma. This link is missed by\n  stemming, as it requires a dictionary look-up.\nThe word \"walk\" is the base form for word \"walking\", and hence this\n  is matched in both stemming and lemmatisation.\nThe word \"meeting\" can be either the base form of a noun or a form\n  of a verb (\"to meet\") depending on the context, e.g., \"in our last\n  meeting\" or \"We are meeting again tomorrow\". Unlike stemming,\n  lemmatisation can in principle select the appropriate lemma\n  depending on the context.\n\n\nSource: https://en.wikipedia.org/wiki/Lemmatisation\n"}, "88": {"topic": "What is the difference between lemmatization vs stemming?", "user_name": "mikumiku", "text": "\nStemming just removes or stems the last few characters of a word, often leading to incorrect meanings and spelling. Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma. Sometimes, the same word can have multiple different Lemmas. We should identify the Part of Speech (POS) tag for the word in that specific context.  Here are the examples to illustrate all the differences and use cases: \n\nIf you lemmatize the word 'Caring', it would return 'Care'. If you stem, it would return 'Car' and this is erroneous.\nIf you lemmatize the word 'Stripes' in verb context, it would return 'Strip'. If you lemmatize it in noun context, it would return 'Stripe'. If you just stem it, it would just return 'Strip'. \nYou would get same results whether you lemmatize or stem words such as walking, running, swimming... to walk, run, swim etc.\nLemmatization is computationally expensive since it involves look-up tables and what not. If you have large dataset and performance is an issue, go with Stemming. Remember you can also add your own rules to Stemming. If accuracy is paramount and dataset isn't humongous, go with Lemmatization.\n\n"}, "89": {"topic": "What is the difference between lemmatization vs stemming?", "user_name": "Mukesh ChapagainMukesh Chapagain", "text": "\nThere are two aspects to show their differences:\n\nA stemmer will return the stem of a word, which needn't be identical to the morphological root of the word. It usually sufficient that related words map to the same stem,even if the stem is not in itself a valid root, while in lemmatisation, it will return the dictionary form of a word, which must be a valid word. \nIn lemmatisation, the part of speech of a word should be first determined and the normalisation rules will be different for different part of speech, while the stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. \n\nReference http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization\n"}, "90": {"topic": "What is the difference between lemmatization vs stemming?", "user_name": "Sumit PokhrelSumit Pokhrel", "text": "\nThe purpose of both stemming and lemmatization is to reduce morphological variation. This is in contrast to the the more general \"term conflation\" procedures, which may also address lexico-semantic, syntactic, or orthographic variations. \nThe real difference between stemming and lemmatization is threefold:\n\nStemming reduces word-forms to (pseudo)stems, whereas lemmatization reduces the word-forms to linguistically valid lemmas. This difference is apparent in languages with more complex morphology, but may be irrelevant for many IR applications;\nLemmatization deals only with inflectional variance, whereas stemming may also deal with derivational variance;\nIn terms of implementation, lemmatization is usually more sophisticated (especially for morphologically complex languages) and usually requires some sort of lexica. Satisfatory stemming, on the other hand, can be achieved with rather simple rule-based approaches.\n\nLemmatization may also be backed up by a part-of-speech tagger in order to disambiguate homonyms.\n"}, "91": {"topic": "What is the difference between lemmatization vs stemming?", "user_name": "", "text": "\nAs MYYN pointed out, stemming is the process of removing inflectional and sometimes derivational affixes to a base form that all of the original words are probably related to.  Lemmatization is concerned with obtaining the single word that allows you to group together a bunch of inflected forms.  This is harder than stemming because it requires taking the context into account (and thus the meaning of the word), while stemming ignores context.\nAs for when you would use one or the other, it's a matter of how much your application depends on getting the meaning of a word in context correct.  If you're doing machine translation, you probably want lemmatization to avoid mistranslating a word.  If you're doing information retrieval over a billion documents with 99% of your queries ranging from 1-3 words, you can settle for stemming.\nAs for NLTK, the WordNetLemmatizer does use the part of speech, though you have to provide it (otherwise it defaults to nouns).  Passing it \"dove\" and \"v\" yields \"dive\" while \"dove\" and \"n\" yields \"dove\".\n"}, "92": {"topic": "What is the difference between lemmatization vs stemming?", "user_name": "Liang ZhangLiang Zhang", "text": "\nAn example-driven explanation on the differenes between lemmatization and stemming:\nLemmatization handles matching \u201ccar\u201d to \u201ccars\u201d along\nwith matching \u201ccar\u201d to \u201cautomobile\u201d. \nStemming handles matching \u201ccar\u201d to \u201ccars\u201d .\n\nLemmatization implies a broader scope of fuzzy word matching that is\n  still handled by the same subsystems.  It implies certain techniques\n  for low level processing within the engine, and may also reflect an\n  engineering preference for terminology.\n[...] Taking FAST as an example,\n  their lemmatization engine handles not only basic word variations like\n  singular vs. plural, but also thesaurus operators like having \u201chot\u201d\n  match \u201cwarm\u201d.\nThis is not to say that other engines don\u2019t handle synonyms, of course\n  they do, but the low level implementation may be in a different\n  subsystem than those that handle base stemming.\n\nhttp://www.ideaeng.com/stemming-lemmatization-0601\n"}, "93": {"topic": "What is the difference between lemmatization vs stemming?", "user_name": "hippietrail", "text": "\nStemming is the process of removing the last few characters of a given word, to obtain a shorter form, even if that form doesn't have any meaning.\nExamples,\n\"beautiful\" -> \"beauti\"\n\"corpora\" -> \"corpora\"\n\n\nStemming can be done very quickly.\nLemmatization on the other hand, is the process of converting the given word into it's base form according to the dictionary meaning of the word.\nExamples,\n\"beautiful\" -> \"beauty\"\n\"corpora\" -> \"corpus\"\n\n\nLemmatization takes more time than stemming.\n"}, "94": {"topic": "What is the difference between lemmatization vs stemming?", "user_name": "Jan SnajderJan Snajder", "text": "\nI think Stemming is a rough hack that people use to get all the different forms of the same word down to a base form which needs not be a legit word on its own.\nSomething like the Porter Stemmer can use simple regexes to eliminate common word suffixes.\nLemmatization brings a word down to its actual base form which, in the case of irregular verbs, might look nothing like the input word.\nSomething like Morpha which uses FSTs to bring nouns and verbs to their base form.\n"}, "95": {"topic": "What is the difference between lemmatization vs stemming?", "user_name": "ealdentealdent", "text": "\nHuang et al. describes the Stemming and Lemmatization as the following. The selection depends upon the problem and computational resource availability.\n\nStemming identifies the common root form of a word by removing or replacing word suffixes (e.g. \u201cflooding\u201d is stemmed as \u201cflood\u201d), while lemmatization identifies the inflected forms of a word and returns its base form (e.g. \u201cbetter\u201d is lemmatized as \u201cgood\u201d).\n\nHuang, X., Li, Z., Wang, C., & Ning, H. (2020). Identifying disaster related social media for rapid response: a visual-textual fused CNN architecture. International Journal of Digital Earth, 13(9), 1017\u20131039. https://doi.org/10.1080/17538947.2019.1633425\n"}, "96": {"topic": "What is the difference between lemmatization vs stemming?", "user_name": "majommajom", "text": "\nStemming and Lemmatization both generate the foundation sort of the inflected words and therefore the only difference is that stem may not be an actual word whereas, lemma is an actual language word.\nStemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used a corpus also to supply lemma which makes it slower than stemming. you furthermore might had to define a parts-of-speech to get the proper lemma.\nThe above points show that if speed is concentrated then stemming should be used since lemmatizers scan a corpus which consumes time and processing. It depends on the problem you\u2019re working on that decides if stemmers should be used or lemmatizers.\nfor more info visit the link:\nhttps://towardsdatascience.com/stemming-vs-lemmatization-2daddabcb221\n"}, "97": {"topic": "What is the difference between lemmatization vs stemming?", "user_name": "Antimony", "text": "\nStemming\nis the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers.\nOften when searching text for a certain keyword, it helps if the search returns variations of the word.\nFor instance, searching for \u201cboat\u201d might also return \u201cboats\u201d and \u201cboating\u201d. Here, \u201cboat\u201d would be the stem for [boat, boater, boating, boats].\nLemmatization\nlooks beyond word reduction and considers a language\u2019s full vocabulary to apply a morphological analysis to words. The lemma of \u2018was\u2019 is \u2018be\u2019 and the lemma of \u2018mice\u2019 is \u2018mouse\u2019.\nI did refer this link,\nhttps://towardsdatascience.com/stemming-vs-lemmatization-2daddabcb221\n"}, "98": {"topic": "What is the difference between lemmatization vs stemming?", "user_name": "siva pokalasiva pokala", "text": "\nIn short, the difference between these algorithms is that only lemmatization includes the meaning of the word in the evaluation. In stemming, only a certain number of letters are cut off from the end of the word to obtain a word stem. The meaning of the word does not play a role in it.\n"}, "99": {"topic": "What is the difference between lemmatization vs stemming?", "user_name": "Mehdi Charife", "text": "\nIn short:\n\nLemmatization: uses context to transform words to their\ndictionary(base) form also known as Lemma\n\n\nStemming: uses the stem of the word, most of the time removing derivational affixes.\n\nsource\n"}, "100": {"topic": "Python NLTK: SyntaxError: Non-ASCII character '\\xc3' in file (Sentiment Analysis -NLP)", "user_name": "bad_coder", "text": "\nI am playing around with NLTK to do an assignment on sentiment analysis. I am using Python 2.7. NLTK 3.0 and NumPy1.9.1 version. \nThis is the code :\n__author__ = 'karan'\nimport nltk\nimport re\nimport sys\n\n\n\ndef main():\n    print(\"Start\");\n    # getting the stop words\n    stopWords = open(\"english.txt\",\"r\");\n    stop_word = stopWords.read().split();\n    AllStopWrd = []\n    for wd in stop_word:\n        AllStopWrd.append(wd);\n    print(\"stop words-> \",AllStopWrd);\n\n    # sample and also cleaning it\n    tweet1= 'Love, my new toy\u00ed\u00a0\u00bd\u00ed\u00b8\u00ed\u00a0\u00bd\u00ed\u00b8#iPhone6. Its good https://twitter.com/Sandra_Ortega/status/513807261769424897/photo/1'\n    print(\"old tweet-> \",tweet1)\n    tweet1 = tweet1.lower()\n    tweet1 = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",tweet1).split())\n    print(tweet1);\n    tw = tweet1.split()\n    print(tw)\n\n\n    #tokenize\n    sentences = nltk.word_tokenize(tweet1)\n    print(\"tokenized ->\", sentences)\n\n\n    #remove stop words\n    Otweet =[]\n    for w in tw:\n        if w not in AllStopWrd:\n            Otweet.append(w);\n    print(\"sans stop word-> \",Otweet)\n\n\n    # get taggers for neg/pos/inc/dec/inv words\n    taggers ={}\n    negWords = open(\"neg.txt\",\"r\");\n    neg_word = negWords.read().split();\n    print(\"ned words-> \",neg_word)\n    posWords = open(\"pos.txt\",\"r\");\n    pos_word = posWords.read().split();\n    print(\"pos words-> \",pos_word)\n    incrWords = open(\"incr.txt\",\"r\");\n    inc_word = incrWords.read().split();\n    print(\"incr words-> \",inc_word)\n    decrWords = open(\"decr.txt\",\"r\");\n    dec_word = decrWords.read().split();\n    print(\"dec wrds-> \",dec_word)\n    invWords = open(\"inverse.txt\",\"r\");\n    inv_word = invWords.read().split();\n    print(\"inverse words-> \",inv_word)\n    for nw in neg_word:\n        taggers.update({nw:'negative'});\n    for pw in pos_word:\n        taggers.update({pw:'positive'});\n    for iw in inc_word:\n        taggers.update({iw:'inc'});\n    for dw in dec_word:\n        taggers.update({dw:'dec'});\n    for ivw in inv_word:\n        taggers.update({ivw:'inv'});\n    print(\"tagger-> \",taggers)\n    print(taggers.get('little'))\n\n    # get parts of speech\n    posTagger = [nltk.pos_tag(tw)]\n    print(\"posTagger-> \",posTagger)\n\nmain();\n\nThis is the error that I am getting when running my code:\nSyntaxError: Non-ASCII character '\\xc3' in file C:/Users/karan/PycharmProjects/mainProject/sentiment.py on line 19, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details\n\nHow do I fix this error?\nI also tried the code using Python 3.4.2 and with NLTK 3.0 and NumPy 1.9.1 but then I get the error:\nTraceback (most recent call last):\n  File \"C:/Users/karan/PycharmProjects/mainProject/sentiment.py\", line 80, in <module>\n    main();\n  File \"C:/Users/karan/PycharmProjects/mainProject/sentiment.py\", line 72, in main\n    posTagger = [nltk.pos_tag(tw)]\n  File \"C:\\Python34\\lib\\site-packages\\nltk\\tag\\__init__.py\", line 100, in pos_tag\n    tagger = load(_POS_TAGGER)\n  File \"C:\\Python34\\lib\\site-packages\\nltk\\data.py\", line 779, in load\n    resource_val = pickle.load(opened_resource)\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xcb in position 0: ordinal not in range(128)\n\n"}, "101": {"topic": "Python NLTK: SyntaxError: Non-ASCII character '\\xc3' in file (Sentiment Analysis -NLP)", "user_name": "rkbom9rkbom9", "text": "\nAdd the following to the top of your file   # coding=utf-8\nIf you go to the link in the error you can seen the reason why:\nDefining the Encoding\nPython will default to ASCII as standard encoding if no other\n    encoding hints are given.\n    To define a source code encoding, a magic comment must\n    be placed into the source files either as first or second\n    line in the file, such as:\n          # coding=\n"}, "102": {"topic": "googletrans stopped working with error 'NoneType' object has no attribute 'group'", "user_name": "Stevoisiak", "text": "\nI was trying googletrans and it was working quite well. Since this morning I started getting below error. I went through multiple posts from stackoverflow and other sites and found probably my ip is banned to use the service for sometime. I tried using multiple service provider internet that has different ip and stil facing the same issue ? I also tried to use googletrans on different laptops , still same issue ..Is googletrans package broken or something google did at their end ?\n>>> from googletrans import Translator\n>>> translator = Translator()\n>>> translator.translate('\uc548\ub155\ud558\uc138\uc694.')\n\nTraceback (most recent call last):\n  File \"<pyshell#2>\", line 1, in <module>\n    translator.translate('\uc548\ub155\ud558\uc138\uc694.')\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/googletrans/client.py\", line 172, in translate\n    data = self._translate(text, dest, src)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/googletrans/client.py\", line 75, in _translate\n    token = self.token_acquirer.do(text)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/googletrans/gtoken.py\", line 180, in do\n    self._update()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/googletrans/gtoken.py\", line 59, in _update\n    code = unicode(self.RE_TKK.search(r.text).group(1)).replace('var ', '')\nAttributeError: 'NoneType' object has no attribute 'group'\n\n"}, "103": {"topic": "googletrans stopped working with error 'NoneType' object has no attribute 'group'", "user_name": "steveJsteveJ", "text": "\nUpdate 06.12.20: A new 'official' alpha version of googletrans with a fix was released\nInstall the alpha version like this:\npip install googletrans==3.1.0a0\n\nTranslation example:\ntranslator = Translator()\ntranslation = translator.translate(\"Der Himmel ist blau und ich mag Bananen\", dest='en')\nprint(translation.text)\n#output: 'The sky is blue and I like bananas'\n\nIn case it does not work, try to specify the service url like this:\nfrom googletrans import Translator\ntranslator = Translator(service_urls=['translate.googleapis.com'])\ntranslator.translate(\"Der Himmel ist blau und ich mag Bananen\", dest='en')\n\nSee the discussion here for details and updates: https://github.com/ssut/py-googletrans/pull/237\nUpdate 10.12.20: Another fix was released\nAs pointed out by @DesiKeki and @Ahmed Breem, there is another fix which seems to work for several people:\npip install googletrans==4.0.0-rc1\n\nGithub discussion here: https://github.com/ssut/py-googletrans/issues/234#issuecomment-742460612\nIn case the fixes above don't work for you\nIf the above doesn't work for you, google_trans_new seems to be a good alternative that works for some people. It's unclear why the fix above works for some and doesn't for others. See details on installation and usage here: https://github.com/lushan88a/google_trans_new\n#pip install google_trans_new\n\nfrom google_trans_new import google_translator  \ntranslator = google_translator()  \ntranslate_text = translator.translate('\u0e2a\u0e27\u0e31\u0e2a\u0e14\u0e35\u0e08\u0e35\u0e19',lang_tgt='en')  \nprint(translate_text)\n#output: Hello china\n\n"}, "104": {"topic": "googletrans stopped working with error 'NoneType' object has no attribute 'group'", "user_name": "", "text": "\nUpdate 01/12/2020: This issue re-emerged lately, (apparently) caused once again by some changes on the Google translation API.\nA solution is being discussed (again) in this Github issue. Although there is not a definitive solution yet a Pull Request seem to be solving the problem: https://github.com/ssut/py-googletrans/pull/237.\nWhile we wait for it to be approved it can be installed like this:\n$ pip uninstall googletrans\n$ git clone https://github.com/alainrouillon/py-googletrans.git\n$ cd ./py-googletrans\n$ git checkout origin/feature/enhance-use-of-direct-api\n$ python setup.py install\n\nOriginal Answer:\nApparently it's a recent and widespread problem on Google's side.\nQuoting various Github discussions, it happens when Google sends you directly the raw token.\nIt's being discussed right now and there is already a pull request to fix it, so it should be resolved in the next few days.\nFor reference, see:\nhttps://github.com/ssut/py-googletrans/issues/48 <-- exact same problem reported on the Github repo\nhttps://github.com/pndurette/gTTS/issues/60 <-- seemingly same problem on a text-to-speech library\nhttps://github.com/ssut/py-googletrans/pull/78 <-- pull request to fix the issue\nTo apply this patch (without waiting for the pull request to be accepted) simply install the library from the forked repo https://github.com/BoseCorp/py-googletrans.git (uninstall the official library first):\n$ pip uninstall googletrans\n$ git clone https://github.com/BoseCorp/py-googletrans.git\n$ cd ./py-googletrans\n$ python setup.py install\n\nYou can clone it anywhere on your system and install it globally or while inside a virtualenv.\n"}, "105": {"topic": "googletrans stopped working with error 'NoneType' object has no attribute 'group'", "user_name": "MoritzMoritz", "text": "\nTry google_trans_new. It solved the problem for me\nhttps://github.com/lushan88a/google_trans_new\n\npip install google_trans_new\n\nfrom google_trans_new import google_translator  \n  \ntranslator = google_translator()  \ntranslate_text = translator.translate('Hola mundo!', lang_src='es', lang_tgt='en')  \nprint(translate_text)\n-> Hello world!\n\n"}, "106": {"topic": "googletrans stopped working with error 'NoneType' object has no attribute 'group'", "user_name": "", "text": "\nUnfortunately, I could get neither googletrans nor google_trans_new to work, despite the many proposed fixes that are around.\nMy solution was to switch to the deep_translator package:\npip install -U deep-translator\n\nThen you can use it like this:\n>>> from deep_translator import GoogleTranslator\n>>> GoogleTranslator(source='auto', target='de').translate(\"keep it up, you are awesome\") \n'weiter so, du bist toll'\n\nSee documentation for more info.\n"}, "107": {"topic": "googletrans stopped working with error 'NoneType' object has no attribute 'group'", "user_name": "MaldusMaldus", "text": "\nUpdated Answer as of 2021 Sept\npip uninstall googletrans==4.0.0-rc1\n\npip install googletrans==3.1.0a0\n\nThe 3.1.0a0 version works with bulk translation too!\n"}, "108": {"topic": "googletrans stopped working with error 'NoneType' object has no attribute 'group'", "user_name": "Ian", "text": "\nUpdate 10.12.20: New Alpha Version Release (Stable Release Candidate) is released: 4.0.0-rc1\nIt can be installed as follows:\npip install googletrans==4.0.0-rc1\n\nUsage:\ntranslation = translator.translate('\uc774 \ubb38\uc7a5\uc740 \ud55c\uae00\ub85c \uc4f0\uc5ec\uc84c\uc2b5\ub2c8\ub2e4.', dest='en')\nprint(translation.text)\n>>This sentence is written in Korean.\ndetected_lang = translator.detect('mein english me hindi likh raha hoon')\nprint(detected_lang)\n>>Detected(lang=hi, confidence=None)\ndetected_lang = translator.detect('\uc774 \ubb38\uc7a5\uc740 \ud55c\uae00\ub85c \uc4f0\uc5ec\uc84c\uc2b5\ub2c8\ub2e4.')\nprint(detected_lang)\n>>Detected(lang=ko, confidence=None)\n\n"}, "109": {"topic": "googletrans stopped working with error 'NoneType' object has no attribute 'group'", "user_name": "Frederick van LingenFrederick van Lingen", "text": "\nBy the time of this answer, you can solve it with the following:\nUninstall your installed version of\npip uninstall googletrans\n\nInstall the following version\npip install googletrans==4.0.0rc1\n\nI hope this will works for you as it worked for me.\nYou can try it now:\nfrom googletrans import Translator\ntranslator = Translator()\nar = translator.translate('\u0645\u0631\u062d\u0628\u0627').text\nprint(ar)\n\nNote that 4.0.0rc1 is using an old version of httpx, if you are getting an error regarding that, you need to edit the file client.py, fix httpcore.SyncHTTPTransport to httpcore.AsyncHTTPProxy.\n"}, "110": {"topic": "googletrans stopped working with error 'NoneType' object has no attribute 'group'", "user_name": "buddematbuddemat", "text": "\nHere is an unofficial fix to this problem as Darkblader24 stated in: https://github.com/ssut/py-googletrans/pull/78\nUpdate gtoken.py like this: \n    RE_TKK = re.compile(r'TKK=eval\\(\\'\\(\\(function\\(\\)\\{(.+?)\\}\\)\\(\\)\\)\\'\\);',\n                        re.DOTALL)\n    RE_RAWTKK = re.compile(r'TKK=\\'([^\\']*)\\';',re.DOTALL)\n\n    def __init__(self, tkk='0', session=None, host='translate.google.com'):\n        self.session = session or requests.Session()\n        self.tkk = tkk\n        self.host = host if 'http' in host else 'https://' + host\n\n    def _update(self):\n        \"\"\"update tkk\n        \"\"\"\n        # we don't need to update the base TKK value when it is still valid\n        now = math.floor(int(time.time() * 1000) / 3600000.0)\n        if self.tkk and int(self.tkk.split('.')[0]) == now:\n            return\n\n        r = self.session.get(self.host)\n\n        rawtkk = self.RE_RAWTKK.search(r.text)\n        if rawtkk:\n            self.tkk = rawtkk.group(1)\n            return\n\n"}, "111": {"topic": "googletrans stopped working with error 'NoneType' object has no attribute 'group'", "user_name": "Harrish SelvarajahHarrish Selvarajah", "text": "\nThis worked for me:\npip install googletrans==4.0.0-rc1\n\nOriginal answer can be found here:\nhttps://github.com/ssut/py-googletrans/issues/234#issuecomment-742460612\n"}, "112": {"topic": "googletrans stopped working with error 'NoneType' object has no attribute 'group'", "user_name": "DesiKekiDesiKeki", "text": "\nFixed is here https://pypi.org/project/py-translator/\n$ pip3 install py_translator==1.8.9 \nfrom py_translator import Translator\ns = Translator().translate(text='Hello my friend', dest='es').text\nprint(s)\n\n\nout:Hola mi amigo\n\n"}, "113": {"topic": "googletrans stopped working with error 'NoneType' object has no attribute 'group'", "user_name": "", "text": "\npip uninstall googletrans googletrans-temp\npip install googletrans-temp\n\nWorked for me in Win10 and Ubuntu 16 (Python 3.6) as of 2019.2.24 -- Refer to one of the replies in https://github.com/ssut/py-googletrans/issues/94. The old fix pip install git+https://github.com/BoseCorp/py-googletrans.git --upgrade does not work any more over here.\n"}, "114": {"topic": "googletrans stopped working with error 'NoneType' object has no attribute 'group'", "user_name": "AKMalkadiAKMalkadi", "text": "\ngoogletrans is not supported in latest python so you need to unistall it\ninstall new googletrans ( pip install googletrans==3.1.0a0)\n"}, "115": {"topic": "googletrans stopped working with error 'NoneType' object has no attribute 'group'", "user_name": "KeremKerem", "text": "\nThis is how I fixed my problem.\npip3 uninstall googletrans\npip3 install googletrans==3.1.0a0\n\nFirst you need to uninstall the previous version and the install the 3.1.0 version.\n"}, "116": {"topic": "googletrans stopped working with error 'NoneType' object has no attribute 'group'", "user_name": "Ahmet BirimAhmet Birim", "text": "\nUse the translators package from here\n\nIt works (;\nSupports more then google\n\nInstallation:\npip install translators --upgrade\nUsage:\n\n    >>> import translators as ts\n    Using Israel server backend.\n    >>> ts.google('\u05e9\u05dc\u05d5\u05dd' , to_language = 'es')\n    'Hola'\n    \n\n\n"}, "117": {"topic": "googletrans stopped working with error 'NoneType' object has no attribute 'group'", "user_name": "\u05e1\u05d8\u05e0\u05dc\u05d9 \u05d2\u05e8\u05d5\u05e0\u05df", "text": "\nMaking the following change to gtoken made it work for me:\nRE_TKK = re.compile(r'tkk:\\'(.+?)\\'')      \n\ndef __init__(self, tkk='0', session=None, host='translate.google.com'):\n    self.session = session or requests.Session()\n    self.tkk = tkk\n    self.host = host if 'http' in host else 'https://' + host\n\ndef _update(self):\n    \"\"\"update tkk\n    \"\"\"\n    # we don't need to update the base TKK value when it is still valid\n    r = self.session.get(self.host)        \n\n    self.tkk = self.RE_TKK.findall(r.text)[0]\n\n    now = math.floor(int(time.time() * 1000) / 3600000.0)\n    if self.tkk and int(self.tkk.split('.')[0]) == now:\n        return\n\n    # this will be the same as python code after stripping out a reserved word 'var'\n    code = unicode(self.RE_TKK.search(r.text).group(1)).replace('var ', '')\n    # unescape special ascii characters such like a \\x3d(=)\n\nI obtained this snippet from the ticket here. \nNote that this is slightly different from other change suggested earlier by Kerem. \nFor other uninitiated folks like me, gtoken.py can be found within AppData\\Local\\Continuum\\anaconda3\\site-packages\\googletrans on a Windows machine using Anaconda. To find AppData, go into the address bar in file explorer, type '%AppData%', and hit Enter. \n"}, "118": {"topic": "googletrans stopped working with error 'NoneType' object has no attribute 'group'", "user_name": "Georgi Ivanov DimitrovGeorgi Ivanov Dimitrov", "text": "\nIt turns out putting the call whithin a try/except block solved the problem for me\ntry:\n    langs = translator.detect(update.message.text)\n    if langs.lang == 'en':\n        foo(translator.translate(update.message.text,dest='zh-cn').text)\n    else:\n        bar(translator.translate(update.message.text,dest='en').text)\nexcept Exception as e:\n    print(e)\n\n"}, "119": {"topic": "googletrans stopped working with error 'NoneType' object has no attribute 'group'", "user_name": "", "text": "\nIf you are using Google Colab or Jupyter Notebook, then run:\n!pip uninstall googletrans\n\nRestart the runtime, and then execute:\n!pip install googletrans==4.0.0rc1\n\n"}, "120": {"topic": "googletrans stopped working with error 'NoneType' object has no attribute 'group'", "user_name": "mikeymikey", "text": "\nTry - pip install googletrans==3.1.0a0\n"}, "121": {"topic": "How to determine the language of a piece of text?", "user_name": "John Kugelman", "text": "\nI want to get this:\nInput text: \"\u0440\u0443\u0301\u0441\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u0301\u043a\"\nOutput text: \"Russian\" \n\nInput text: \"\u4e2d\u6587\"\nOutput text: \"Chinese\" \n\nInput text: \"\u306b\u307b\u3093\u3054\"\nOutput text: \"Japanese\" \n\nInput text: \"\u0627\u0644\u0639\u064e\u0631\u064e\u0628\u0650\u064a\u064e\u0651\u0629\"\nOutput text: \"Arabic\"\n\nHow can I do it in python?\n"}, "122": {"topic": "How to determine the language of a piece of text?", "user_name": "RitaRita", "text": "\n1. TextBlob. (Deprecated - Use official Google Translate API instead)\nRequires NLTK package, uses Google.\nfrom textblob import TextBlob\nb = TextBlob(\"bonjour\")\nb.detect_language()\n\npip install textblob\nNote: This solution requires internet access and Textblob is using Google Translate's language detector by calling the API.\n2. Polyglot.\nRequires numpy and some arcane libraries, unlikely to get it work for Windows. (For Windows, get an appropriate versions of PyICU, Morfessor and PyCLD2 from here, then just pip install downloaded_wheel.whl.) Able to detect texts with mixed languages.\nfrom polyglot.detect import Detector\n\nmixed_text = u\"\"\"\nChina (simplified Chinese: \u4e2d\u56fd; traditional Chinese: \u4e2d\u570b),\nofficially the People's Republic of China (PRC), is a sovereign state\nlocated in East Asia.\n\"\"\"\nfor language in Detector(mixed_text).languages:\n        print(language)\n\n# name: English     code: en       confidence:  87.0 read bytes:  1154\n# name: Chinese     code: zh_Hant  confidence:   5.0 read bytes:  1755\n# name: un          code: un       confidence:   0.0 read bytes:     0\n\npip install polyglot\nTo install the dependencies, run:\nsudo apt-get install python-numpy libicu-dev\nNote: Polyglot is using pycld2, see https://github.com/aboSamoor/polyglot/blob/master/polyglot/detect/base.py#L72 for details.\n3. chardet\nChardet has also a feature of detecting languages if there are character bytes in range (127-255]:\n>>> chardet.detect(\"\u042f \u043b\u044e\u0431\u043b\u044e \u0432\u043a\u0443\u0441\u043d\u044b\u0435 \u043f\u0430\u043c\u043f\u0443\u0448\u043a\u0438\".encode('cp1251'))\n{'encoding': 'windows-1251', 'confidence': 0.9637267119204621, 'language': 'Russian'}\n\npip install chardet\n4. langdetect\nRequires large portions of text. It uses non-deterministic approach under the hood. That means you get different results for the same text sample. Docs say you have to use following code to make it determined:\nfrom langdetect import detect, DetectorFactory\nDetectorFactory.seed = 0\ndetect('\u4eca\u4e00\u306f\u304a\u524d\u3055\u3093')\n\npip install langdetect\n5. guess_language\nCan detect very short samples by using this spell checker with dictionaries.\npip install guess_language-spirit\n6. langid\nlangid.py provides both a module\nimport langid\nlangid.classify(\"This is a test\")\n# ('en', -54.41310358047485)\n\nand a command-line tool:\n$ langid < README.md\n\npip install langid\n7. FastText\nFastText is a text classifier, can be used to recognize 176 languages with a proper models for language classification. Download this model, then:\nimport fasttext\nmodel = fasttext.load_model('lid.176.ftz')\nprint(model.predict('\u0627\u0644\u0634\u0645\u0633 \u062a\u0634\u0631\u0642', k=2))  # top 2 matching languages\n\n(('__label__ar', '__label__fa'), array([0.98124713, 0.01265871]))\n\npip install fasttext\n8. pyCLD3\npycld3 is a neural network model for language identification. This package contains the inference code and a trained model.\nimport cld3\ncld3.get_language(\"\u5f71\u97ff\u5305\u542b\u5c0d\u6c23\u5019\u7684\u8b8a\u5316\u4ee5\u53ca\u81ea\u7136\u8cc7\u6e90\u7684\u67af\u7aed\u7a0b\u5ea6\")\n\nLanguagePrediction(language='zh', probability=0.999969482421875, is_reliable=True, proportion=1.0)\n\npip install pycld3\n"}, "123": {"topic": "How to determine the language of a piece of text?", "user_name": "Adrien Arcuri", "text": "\nHave you had a look at langdetect?\nfrom langdetect import detect\n\nlang = detect(\"Ein, zwei, drei, vier\")\n\nprint lang\n#output: de\n\n"}, "124": {"topic": "How to determine the language of a piece of text?", "user_name": "RabashRabash", "text": "\n@Rabash had a good list of tools on https://stackoverflow.com/a/47106810/610569\nAnd @toto_tico did a nice job in presenting the speed comparison.\nHere's a summary to complete the great answers above (as of 2021)\n\n\n\n\nLanguage ID software\nUsed by\nOpen Source / Model\nRule-based\nStats-based\nCan train/tune\n\n\n\n\nGoogle Translate Language Detection\nTextBlob (limited usage)\n\u2715\n-\n-\n\u2715\n\n\nChardet\n-\n\u2713\n\u2713\n\u2715\n\u2715\n\n\nGuess Language (non-active development)\nspirit-guess (updated rewrite)\n\u2713\n\u2713\nMinimally\n\u2715\n\n\npyCLD2\nPolyglot\n\u2713\nSomewhat\n\u2713\nNot sure\n\n\nCLD3\n-\n\u2713\n\u2715\n\u2713\nPossibly\n\n\nlangid-py\n-\n\u2713\nNot sure\n\u2713\n\u2713\n\n\nlangdetect\nSpaCy-langdetect\n\u2713\n\u2715\n\u2713\n\u2713\n\n\nFastText\nWhat The Lang\n\u2713\n\u2715\n\u2713\nNot sure\n\n\n\n "}, "125": {"topic": "How to determine the language of a piece of text?", "user_name": "dheibergdheiberg", "text": "\nIf you are looking for a library that is fast with long texts, polyglot and fastext are doing the best job here.\nI sampled 10000 documents from a collection of dirty and random HTMLs, and here are the results:\n+------------+----------+\n| Library    | Time     |\n+------------+----------+\n| polyglot   | 3.67 s   |\n+------------+----------+\n| fasttext   | 6.41     |\n+------------+----------+\n| cld3       | 14 s     |\n+------------+----------+\n| langid     | 1min 8s  |\n+------------+----------+\n| langdetect | 2min 53s |\n+------------+----------+\n| chardet    | 4min 36s |\n+------------+----------+\n\n\nI have noticed that a lot of the methods focus on short texts, probably because it is the hard problem to solve: if you have a lot of text, it is really easy to detect languages (e.g. one could just use a dictionary!). However, this makes it difficult to find for an easy and suitable method for long texts. \n"}, "126": {"topic": "How to determine the language of a piece of text?", "user_name": "David Beauchemin", "text": "\nThere is an issue with langdetect when it is being used for parallelization and it fails. But spacy_langdetect is a wrapper for that and you can use it for that purpose. You can use the following snippet as well:\nimport spacy\nfrom spacy_langdetect import LanguageDetector\n\nnlp = spacy.load(\"en\")\nnlp.add_pipe(LanguageDetector(), name=\"language_detector\", last=True)\ntext = \"This is English text Er lebt mit seinen Eltern und seiner Schwester in Berlin. Yo me divierto todos los d\u00edas en el parque. Je m'appelle Ang\u00e9lica Summer, j'ai 12 ans et je suis canadienne.\"\ndoc = nlp(text)\n# document level language detection. Think of it like average language of document!\nprint(doc._.language['language'])\n# sentence level language detection\nfor i, sent in enumerate(doc.sents):\n    print(sent, sent._.language)\n\n"}, "127": {"topic": "How to determine the language of a piece of text?", "user_name": "alvasalvas", "text": "\nYou can use Googletrans (unofficial) a free and unlimited Google translate API for Python.\nYou can make as many requests as you want, there are no limits\nInstallation:\n$ pip install googletrans\n\nLanguage detection:\n>>> from googletrans import Translator\n>>> t = Translator().detect(\"hello world!\")\n>>> t.lang\n'en'\n>>> t.confidence\n0.8225234\n\n"}, "128": {"topic": "How to determine the language of a piece of text?", "user_name": "toto_ticototo_tico", "text": "\nPretrained Fast Text Model Worked Best For My Similar Needs\nI arrived at your question with a very similar need. I found the most help from Rabash's answers for my specific needs.\nAfter experimenting to find what worked best among his recommendations, which was making sure that text files were in English in 60,000+ text files, I found that fasttext was an excellent tool for such a task.\nWith a little work, I had a tool that worked very fast over many files. But it could be easily modified for something like your case, because fasttext works over a list of lines easily. \nMy code with comments is among the answers on THIS post. I believe that you and others can easily modify this code for other specific needs.\n"}, "129": {"topic": "How to determine the language of a piece of text?", "user_name": "Habib KarbasianHabib Karbasian", "text": "\nDepending on the case, you might be interested in using one of the following methods:\nMethod 0: Use an API or library\n\ncld2-cffi\nGoogle Cloud Translation - Basic (v2)\nTextBlob\nlangdetect\netc.\n\nUsually, there are a few problems with these libraries because some of them are not accurate for small texts, some languages are missing, are slow, require internet connection, are non-free,... But generally speaking, they will suit most needs.\nMethod 1: Language models\nA language model gives us the probability of a sequence of words. This is important because it allows us to robustly detect the language of a text, even when the text contains words in other languages (e.g.: \"'Hola' means 'hello' in spanish\").\nYou can use N language models (one per language), to score your text. The detected language will be the language of the model that gave you the highest score.\nIf you want to build a simple language model for this, I'd go for 1-grams. To do this, you only need to count the number of times each word from a big text (e.g. Wikipedia Corpus in \"X\" language) has appeared.\nThen, the probability of a word will be its frequency divided by the total number of words analyzed (sum of all frequencies).\nthe 23135851162\nof  13151942776\nand 12997637966\nto  12136980858\na   9081174698\nin  8469404971\nfor 5933321709\n...\n\n=> P(\"'Hola' means 'hello' in spanish\") = P(\"hola\") * P(\"means\") * P(\"hello\") * P(\"in\") * P(\"spanish\")\n\nIf the text to detect is quite big, I recommend sampling N random words and then use the sum of logarithms instead of multiplications to avoid floating-point precision problems.\nP(s) = 0.03 * 0.01 * 0.014 = 0.0000042\nP(s) = log10(0.03) + log10(0.01) + log10(0.014) = -5.376\n\nMethod 2: Intersecting sets\nAn even simpler approach is to prepare N sets (one per language) with the top M most frequent words. Then intersect your text with each set. The set with the highest number of intersections will be your detected language.\nspanish_set = {\"de\", \"hola\", \"la\", \"casa\",...}\nenglish_set = {\"of\", \"hello\", \"the\", \"house\",...}\nczech_set = {\"z\", \"ahoj\", \"z\u00e1v\u011brky\", \"d\u016fm\",...}\n...\n\ntext_set = {\"hola\", \"means\", \"hello\", \"in\", \"spanish\"}\n\nspanish_votes = text_set.intersection(spanish_set)  # 1\nenglish_votes = text_set.intersection(english_set)  # 4\nczech_votes = text_set.intersection(czech_set)  # 0\n...\n\nMethod 3: Zip compression\nThis more a curiosity than anything else, but here it goes... You can compress your text (e.g LZ77) and then measure the zip-distance with regards to a reference compressed text (target language). Personally, I didn't like it because it's slower, less accurate and less descriptive than other methods. Nevertheless, there might be interesting applications for this method. \nTo read more: Language Trees and Zipping\n"}, "130": {"topic": "How to determine the language of a piece of text?", "user_name": "h3t1h3t1", "text": "\nPolygot or Cld2 are among the best suggestions because they can detect multiple language in text. But, they are not easy to be installed on Windows because of \"building wheel fail\".\nA solution that worked for me ( I am using Windows 10 ) is installing CLD2-CFFI\nso first install cld2-cffi\npip install cld2-cffi\n\nand then use it like this:\ntext_content = \"\"\" A acc\u00e8s aux chiens et aux frontaux qui lui ont \u00e9t\u00e9 il peut \nconsulter et modifier ses collections et exporter Cet article concerne le pays \neurop\u00e9en aujourd\u2019hui appel\u00e9 R\u00e9publique fran\u00e7aise. \nPour d\u2019autres usages du nom France, Pour une aide rapide et effective, veuiller \ntrouver votre aide dans le menu ci-dessus. \nWelcome, to this world of Data Scientist. Today is a lovely day.\"\"\"\n\nimport cld2\n\nisReliable, textBytesFound, details = cld2.detect(text_content)\nprint('  reliable: %s' % (isReliable != 0))\nprint('  textBytes: %s' % textBytesFound)\nprint('  details: %s' % str(details))\n\nTh output is like this:\nreliable: True\ntextBytes: 377\ndetails: (Detection(language_name='FRENCH', language_code='fr', percent=74, \nscore=1360.0), Detection(language_name='ENGLISH', language_code='en', \npercent=25, score=1141.0), Detection(language_name='Unknown', \nlanguage_code='un', percent=0, score=0.0))\n\n"}, "131": {"topic": "How to determine the language of a piece of text?", "user_name": "Thom IvesThom Ives", "text": "\nI like the approach offered by TextBlob for language detection. Its quite simple and easy to implement and uses fewer lines of code. before you begin. you will need to install the textblob python library for the below code to work.\nfrom textblob import TextBlob\ntext = \"\u044d\u0442\u043e \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u044b\u0439 \u043f\u043e\u0440\u0442\u0430\u043b \u0434\u043b\u044f \u0433\u0438\u043a\u043e\u0432.\"\nlang = TextBlob(text)\nprint(lang.detect_language())\n\nOn the other hand, if you have a combination of various languages used, you might want to try pycld2 that allows language detection by defining parts of the sentence or paragraph with accuracy.\n"}, "132": {"topic": "How to determine the language of a piece of text?", "user_name": "", "text": "\nYou can try determining the Unicode group of chars in input string to point out type of language, (Cyrillic for Russian, for example), and then search for language-specific symbols in text.\n"}, "133": {"topic": "How to determine the language of a piece of text?", "user_name": "Salva Carri\u00f3nSalva Carri\u00f3n", "text": "\nIf the language you want to detect is among these...\n\narabic (ar)\nbulgarian (bg)\ngerman (de)\nmodern greek (el)\nenglish (en)\nspanish (es)\nfrench (fr)\nhindi (hi)\nitalian (it)\njapanese (ja)\ndutch (nl)\npolish (pl)\nportuguese (pt)\nrussian (ru)\nswahili (sw)\nthai (th)\nturkish (tr)\nurdu (ur)\nvietnamese (vi)\nchinese (zh)\n\n...then it is relatively easy with HuggingFace libraries and models (Deep Learning Natural Language Processing, if you are not familiar with it):\n# Import libraries\nfrom transformers import pipeline\n# Load pipeline\nclassifier = pipeline(\"text-classification\", model = \"papluca/xlm-roberta-base-language-detection\")\n# Example sentence\nsentence1 = 'Ciao, come stai?'\n# Get language\nclassifier(sentence1)\n\nOutput:\n[{'label': 'it', 'score': 0.9948362112045288}]\n\nlabel is the predicted language, and score is the assigned score to it: you can think of it as a confidence measure.\nSome details:\n\nThe training set contains 70k samples, while the validation and test\nsets 10k each. The average accuracy on the test set is 99.6%\n\nYou can finde more info at the model's page, and I suppose that you could find other models that fit you needs.\n"}, "134": {"topic": "How to determine the language of a piece of text?", "user_name": "parvaneh shayeghparvaneh shayegh", "text": "\nYou can install the pycld2 python library\npip install pycld2\n\nor\npython -m pip install -U pycld2\n\nfor the below code to work.\nimport pycld2 as cld2\n\nisReliable, textBytesFound, details = cld2.detect(\n    \"\u0430 \u043d\u0435\u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u0439 \u0444\u043e\u0440\u043c\u0430\u0442 \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430 \u0434\u043d \u043d\u0430\u0437\u0430\u0434\"\n)\n\nprint(isReliable)\n# True\ndetails[0]\n# ('RUSSIAN', 'ru', 98, 404.0)\n\nfr_en_Latn = \"\"\"\\\nFrance is the largest country in Western Europe and the third-largest in Europe as a whole.\nA acc\u00e8s aux chiens et aux frontaux qui lui ont \u00e9t\u00e9 il peut consulter et modifier ses collections\net exporter Cet article concerne le pays europ\u00e9en aujourd\u2019hui appel\u00e9 R\u00e9publique fran\u00e7aise.\nPour d\u2019autres usages du nom France, Pour une aide rapide et effective, veuiller trouver votre aide\ndans le menu ci-dessus.\nMotoring events began soon after the construction of the first successful gasoline-fueled automobiles.\nThe quick brown fox jumped over the lazy dog.\"\"\"\n\nisReliable, textBytesFound, details, vectors = cld2.detect(\n    fr_en_Latn, returnVectors=True\n)\nprint(vectors)\n# ((0, 94, 'ENGLISH', 'en'), (94, 329, 'FRENCH', 'fr'), (423, 139, 'ENGLISH', 'en'))\n\nPycld2 python library is a python binding for the Compact Language Detect 2 (CLD2). You can explore the different functionality of Pycld2. Know about the Pycld2 here.\n"}, "135": {"topic": "How to determine the language of a piece of text?", "user_name": "SaaSy MonsterSaaSy Monster", "text": "\nThe best way to determine the laguage of a text is to implement the following function:\nfrom langdetect import detect\n\ndef get_language(text):\n\n    keys =['ab', 'aa', 'af', 'ak', 'sq', 'am', 'ar', 'an', 'hy', 'as', 'av', 'ae', 'ay', 'az', 'bm', 'ba', 'eu', 'be', 'bn', 'bi', 'bs', 'br', 'bg', 'my', 'ca', 'ch', 'ce', 'ny', 'zh', 'cu', 'cv', 'kw', 'co', 'cr', 'hr', 'cs', 'da', 'dv', 'nl', 'dz', 'en', 'eo', 'et', 'ee', 'fo', 'fj', 'fi', 'fr', 'fy', 'ff', 'gd', 'gl', 'lg', 'ka', 'de', 'el', 'kl', 'gn', 'gu', 'ht', 'ha', 'he', 'hz', 'hi', 'ho', 'hu', 'is', 'io', 'ig', 'id', 'ia', 'ie', 'iu', 'ik', 'ga', 'it', 'ja', 'jv', 'kn', 'kr', 'ks', 'kk', 'km', 'ki', 'rw', 'ky', 'kv', 'kg', 'ko', 'kj', 'ku', 'lo', 'la', 'lv', 'li', 'ln', 'lt', 'lu', 'lb', 'mk', 'mg', 'ms', 'ml', 'mt', 'gv', 'mi', 'mr', 'mh', 'mn', 'na', 'nv', 'nd', 'nr', 'ng', 'ne', 'no', 'nb', 'nn', 'ii', 'oc', 'oj', 'or', 'om', 'os', 'pi', 'ps', 'fa', 'pl', 'pt', 'pa', 'qu', 'ro', 'rm', 'rn', 'ru', 'se', 'sm', 'sg', 'sa', 'sc', 'sr', 'sn', 'sd', 'si', 'sk', 'sl', 'so', 'st', 'es', 'su', 'sw', 'ss', 'sv', 'tl', 'ty', 'tg', 'ta', 'tt', 'te', 'th', 'bo', 'ti', 'to', 'ts', 'tn', 'tr', 'tk', 'tw', 'ug', 'uk', 'ur', 'uz', 've', 'vi', 'vo', 'wa', 'cy', 'wo', 'xh', 'yi', 'yo', 'za', 'zu']\n    \n    langs = ['Abkhazian', 'Afar', 'Afrikaans', 'Akan', 'Albanian', 'Amharic', 'Arabic', 'Aragonese', 'Armenian', 'Assamese', 'Avaric', 'Avestan', 'Aymara', 'Azerbaijani', 'Bambara', 'Bashkir', 'Basque', 'Belarusian', 'Bengali', 'Bislama', 'Bosnian', 'Breton', 'Bulgarian', 'Burmese', 'Catalan, Valencian', 'Chamorro', 'Chechen', 'Chichewa, Chewa, Nyanja', 'Chinese', 'Church\u00a0Slavonic, Old Slavonic,\u00a0Old\u00a0Church\u00a0Slavonic', 'Chuvash', 'Cornish', 'Corsican', 'Cree', 'Croatian', 'Czech', 'Danish', 'Divehi, Dhivehi, Maldivian', 'Dutch,\u00a0Flemish', 'Dzongkha', 'English', 'Esperanto', 'Estonian', 'Ewe', 'Faroese', 'Fijian', 'Finnish', 'French', 'Western Frisian', 'Fulah', 'Gaelic, Scottish Gaelic', 'Galician', 'Ganda', 'Georgian', 'German', 'Greek, Modern (1453\u2013)', 'Kalaallisut, Greenlandic', 'Guarani', 'Gujarati', 'Haitian, Haitian Creole', 'Hausa', 'Hebrew', 'Herero', 'Hindi', 'Hiri Motu', 'Hungarian', 'Icelandic', 'Ido', 'Igbo', 'Indonesian', 'Interlingua\u00a0(International Auxiliary Language Association)', 'Interlingue, Occidental', 'Inuktitut', 'Inupiaq', 'Irish', 'Italian', 'Japanese', 'Javanese', 'Kannada', 'Kanuri', 'Kashmiri', 'Kazakh', 'Central Khmer', 'Kikuyu, Gikuyu', 'Kinyarwanda', 'Kirghiz, Kyrgyz', 'Komi', 'Kongo', 'Korean', 'Kuanyama, Kwanyama', 'Kurdish', 'Lao', 'Latin', 'Latvian', 'Limburgan, Limburger, Limburgish', 'Lingala', 'Lithuanian', 'Luba-Katanga', 'Luxembourgish, Letzeburgesch', 'Macedonian', 'Malagasy', 'Malay', 'Malayalam', 'Maltese', 'Manx', 'Maori', 'Marathi', 'Marshallese', 'Mongolian', 'Nauru', 'Navajo, Navaho', 'North Ndebele', 'South Ndebele', 'Ndonga', 'Nepali', 'Norwegian', 'Norwegian Bokm\u00e5l', 'Norwegian Nynorsk', 'Sichuan Yi, Nuosu', 'Occitan', 'Ojibwa', 'Oriya', 'Oromo', 'Ossetian, Ossetic', 'Pali', 'Pashto, Pushto', 'Persian', 'Polish', 'Portuguese', 'Punjabi, Panjabi', 'Quechua', 'Romanian,\u00a0Moldavian, Moldovan', 'Romansh', 'Rundi', 'Russian', 'Northern Sami', 'Samoan', 'Sango', 'Sanskrit', 'Sardinian', 'Serbian', 'Shona', 'Sindhi', 'Sinhala, Sinhalese', 'Slovak', 'Slovenian', 'Somali', 'Southern Sotho', 'Spanish, Castilian', 'Sundanese', 'Swahili', 'Swati', 'Swedish', 'Tagalog', 'Tahitian', 'Tajik', 'Tamil', 'Tatar', 'Telugu', 'Thai', 'Tibetan', 'Tigrinya', 'Tonga\u00a0(Tonga Islands)', 'Tsonga', 'Tswana', 'Turkish', 'Turkmen', 'Twi', 'Uighur, Uyghur', 'Ukrainian', 'Urdu', 'Uzbek', 'Venda', 'Vietnamese', 'Volap\u00fck', 'Walloon', 'Welsh', 'Wolof', 'Xhosa', 'Yiddish', 'Yoruba', 'Zhuang, Chuang', 'Zulu']\n    \n    lang_dict = {key : lan for (key, lan) in zip(keys, langs)}\n    \n    return lang_dict[detect(text)]\n\nLet's try it:\n>>> get_language(\"Ich liebe meine Frau\")\n\n... 'German'\n\n"}, "136": {"topic": "How to determine the language of a piece of text?", "user_name": "KerbiterKerbiter", "text": "\nI have tried all the libraries out there, and i concluded that pycld2 is the best one, fast and accurate.\nyou can install it like this:\npython -m pip install -U pycld2\n\nyou can use it like this:\nisReliable, textBytesFound, details = cld2.detect(your_sentence)\n\nprint(isReliable, details[0][1]) # reliablity(bool),lang abbrev.(en/es/de...)   \n\n"}, "137": {"topic": "How to determine the language of a piece of text?", "user_name": "SilentCloudSilentCloud", "text": "\nI would say lingua.py all the way. It is much faster and more accurate than fasttext. Definately deserves to be listed here.\nInstallation\npoety add lingua-language-detector\n\nUsage\nfrom typing import List\nfrom lingua.language import Language\nfrom lingua.builder import LanguageDetectorBuilder\nlanguages: List[Language] = [Language.ENGLISH, Language.TURKISH, Language.PERSIAN]\ndetector = LanguageDetectorBuilder.from_languages(*languages).build()\n\nif __name__ == \"__main__\":\n    print(detector.detect_language_of(\"Ben de iyiyim. Tesekkurler.\")) # Language.TURKISH\n    print(detector.detect_language_of(\"I'm fine and you?\")) # Language.ENGLISH\n    print(detector.detect_language_of(\"\u062d\u0627\u0644 \u0645\u0646 \u062e\u0648\u0628\u0647\u061f \u0634\u0645\u0627 \u0686\u0637\u0648\u0631\u06cc\u062f\u061f\")) # Language.PERSIAN\n\n"}, "138": {"topic": "What does Keras Tokenizer method exactly do?", "user_name": "Jack Fleeting", "text": "\nOn occasion, circumstances require us to do the following:\nfrom keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(num_words=my_max)\n\nThen, invariably, we chant this mantra:\ntokenizer.fit_on_texts(text) \nsequences = tokenizer.texts_to_sequences(text)\n\nWhile I (more or less) understand what the total effect is, I can't figure out what each one does separately, regardless of how much research I do (including, obviously, the documentation). I don't think I've ever seen one without the other. \nSo what does each do? Are there any circumstances where you would use either one without the other? If not, why aren't they simply combined into something like:\nsequences = tokenizer.fit_on_texts_to_sequences(text)\n\nApologies if I'm missing something obvious, but I'm pretty new at this.\n"}, "139": {"topic": "What does Keras Tokenizer method exactly do?", "user_name": "Jack FleetingJack Fleeting", "text": "\nFrom the source code:\n\nfit_on_texts Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word (often the first few are stop words because they appear a lot).\ntexts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.\n\nWhy don't combine them? Because you almost always fit once and convert to sequences many times. You will fit on your training corpus once and use that exact same word_index dictionary at train / eval / testing / prediction time to convert actual text into sequences to feed them to the network. So it makes sense to keep those methods separate.\n"}, "140": {"topic": "What does Keras Tokenizer method exactly do?", "user_name": "", "text": "\nAdding more to above answers with examples will help in better understanding:\nExample 1:\nt  = Tokenizer()\nfit_text = \"The earth is an awesome place live\"\nt.fit_on_texts(fit_text)\ntest_text = \"The earth is an great place live\"\nsequences = t.texts_to_sequences(test_text)\n\nprint(\"sequences : \",sequences,'\\n')\n\nprint(\"word_index : \",t.word_index)\n#[] specifies : 1. space b/w the words in the test_text    2. letters that have not occured in fit_text\n\nOutput :\n\n       sequences :  [[3], [4], [1], [], [1], [2], [8], [3], [4], [], [5], [6], [], [2], [9], [], [], [8], [1], [2], [3], [], [13], [7], [2], [14], [1], [], [7], [5], [15], [1]] \n\n       word_index :  {'e': 1, 'a': 2, 't': 3, 'h': 4, 'i': 5, 's': 6, 'l': 7, 'r': 8, 'n': 9, 'w': 10, 'o': 11, 'm': 12, 'p': 13, 'c': 14, 'v': 15}\n\nExample 2:\nt  = Tokenizer()\nfit_text = [\"The earth is an awesome place live\"]\nt.fit_on_texts(fit_text)\n\n#fit_on_texts fits on sentences when list of sentences is passed to fit_on_texts() function. \n#ie - fit_on_texts( [ sent1, sent2, sent3,....sentN ] )\n\n#Similarly, list of sentences/single sentence in a list must be passed into texts_to_sequences.\ntest_text1 = \"The earth is an great place live\"\ntest_text2 = \"The is my program\"\nsequences = t.texts_to_sequences([test_text1, test_text2])\n\nprint('sequences : ',sequences,'\\n')\n\nprint('word_index : ',t.word_index)\n#texts_to_sequences() returns list of list. ie - [ [] ]\n\nOutput:\n\n        sequences :  [[1, 2, 3, 4, 6, 7], [1, 3]] \n\n        word_index :  {'the': 1, 'earth': 2, 'is': 3, 'an': 4, 'awesome': 5, 'place': 6, 'live': 7}\n\n"}, "141": {"topic": "What does Keras Tokenizer method exactly do?", "user_name": "nuricnuric", "text": "\nLets see what this line of code does.\ntokenizer.fit_on_texts(text) \n\nFor example, consider the sentence \" The earth is an awesome place live\"\ntokenizer.fit_on_texts(\"The earth is an awesome place live\") fits [[1,2,3,4,5,6,7]] where 3 -> \"is\" , 6 -> \"place\", so on.\nsequences = tokenizer.texts_to_sequences(\"The earth is an great place live\")\n\nreturns [[1,2,3,4,6,7]].\nYou see what happened here. The word \"great\" is not fit initially, so it does not recognize the word \"great\". Meaning, fit_on_text can be used independently on train data and then the fitted vocabulary index can be used to represent a completely new set of word sequence. These are two different processes. Hence the two lines of code.\n"}, "142": {"topic": "What does Keras Tokenizer method exactly do?", "user_name": "ajaysinghnegiajaysinghnegi", "text": "\nnuric already satistified the question, but I would add something.\nPlease focus on both word frequency-based encoding and OOV in this example:\nfrom tensorflow.keras.preprocessing.text        import Tokenizer\n\ncorpus =['The', 'cat', 'is', 'on', 'the', 'table', 'a', 'very', 'long', 'table']\n\ntok_obj = Tokenizer(num_words=10, oov_token='<OOV>')\ntok_obj.fit_on_texts(corpus)\n\n[TL;DR] The tokenizer will include the first 10 words appearing in the corpus. Here 10 words, but only 8 are unique. The most frequent 10 words will be encoded, if they are more than this number they will go OOV (Out Of Vocabulary).\nBuilt dictionary:\nPlease note the frequency\n{'<OOV>': 1, 'the': 2, 'table': 3, 'cat': 4, 'is': 5, 'on': 6, 'a': 7, 'very': 8, 'long': 9}\n\nSentence(s) processing:\nprocessed_seq = tok_obj.texts_to_sequences(['The dog is on the bed'])\n\nWhich gives:\n>>> processed_seq\n    [[2, 1, 5, 6, 2, 1]]\n\nHow to retrieve the sentence?\nBuild the dictionary inv_map and use It! list comprehension can be used below to compress the code.\ninv_map = {v: k for k, v in tok_obj.word_index.items()}\n\nfor seq in processed_seq:\n    for tok in seq:\n        print(inv_map[tok])\n\nwhich gives:\n>>> the\n<OOV>\nis\non\nthe\n<OOV>\n\nbecause dog and bed are not in the dictionary.\nList comprehension can be used to compress the code. Here obtaining a list as output.\n[inv_map[tok] for seq in processed_seq for tok in seq]\n\nwhich gives:\n>>> ['the', '<OOV>', 'is', 'on', 'the', '<OOV>']\n\n"}, "143": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "lizarisk", "text": "\nI'm just starting to use NLTK and I don't quite understand how to get a list of words from text. If I use nltk.word_tokenize(), I get a list of words and punctuation. I need only the words instead. How can I get rid of punctuation? Also word_tokenize doesn't work with multiple sentences: dots are added to the last word.\n"}, "144": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "lizarisklizarisk", "text": "\nTake a look at the other tokenizing options that nltk provides here. For example, you can define a tokenizer that picks out sequences of alphanumeric characters as tokens and drops everything else:\nfrom nltk.tokenize import RegexpTokenizer\n\ntokenizer = RegexpTokenizer(r'\\w+')\ntokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')\n\nOutput:\n['Eighty', 'seven', 'miles', 'to', 'go', 'yet', 'Onward']\n\n"}, "145": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "kgraney", "text": "\nYou do not really need NLTK to remove punctuation. You can remove it with simple python. For strings:\nimport string\ns = '... some string with punctuation ...'\ns = s.translate(None, string.punctuation)\n\nOr for unicode:\nimport string\ntranslate_table = dict((ord(char), None) for char in string.punctuation)   \ns.translate(translate_table)\n\nand then use this string in your tokenizer.\nP.S. string module have some other sets of elements that can be removed (like digits).\n"}, "146": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "rmaloufrmalouf", "text": "\nBelow code will remove all punctuation marks as well as non alphabetic characters. Copied from their book.\nhttp://www.nltk.org/book/ch01.html \nimport nltk\n\ns = \"I can't do this now, because I'm so tired.  Please give me some time. @ sd  4 232\"\n\nwords = nltk.word_tokenize(s)\n\nwords=[word.lower() for word in words if word.isalpha()]\n\nprint(words)\n\noutput\n['i', 'ca', 'do', 'this', 'now', 'because', 'i', 'so', 'tired', 'please', 'give', 'me', 'some', 'time', 'sd']\n\n"}, "147": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "Eli", "text": "\nAs noticed in comments start with sent_tokenize(), because word_tokenize() works only on a single sentence. You can filter out punctuation with filter(). And if you have an unicode strings make sure that is a unicode object (not a 'str' encoded with some encoding like 'utf-8'). \nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\ntext = '''It is a blue, small, and extraordinary ball. Like no other'''\ntokens = [word for sent in sent_tokenize(text) for word in word_tokenize(sent)]\nprint filter(lambda word: word not in ',-', tokens)\n\n"}, "148": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "Salvador DaliSalvador Dali", "text": "\nI just used the following code, which removed all the punctuation:\ntokens = nltk.wordpunct_tokenize(raw)\n\ntype(tokens)\n\ntext = nltk.Text(tokens)\n\ntype(text)  \n\nwords = [w.lower() for w in text if w.isalpha()]\n\n"}, "149": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "Madura PradeepMadura Pradeep", "text": "\nSincerely asking, what is a word? If your assumption is that a word consists of alphabetic characters only, you are wrong since words such as can't will be destroyed into pieces (such as can and t) if you remove punctuation before tokenisation, which is very likely to affect your program negatively.\nHence the solution is to tokenise and then remove punctuation tokens.\nimport string\n\nfrom nltk.tokenize import word_tokenize\n\ntokens = word_tokenize(\"I'm a southern salesman.\")\n# ['I', \"'m\", 'a', 'southern', 'salesman', '.']\n\ntokens = list(filter(lambda token: token not in string.punctuation, tokens))\n# ['I', \"'m\", 'a', 'southern', 'salesman']\n\n...and then if you wish, you can replace certain tokens such as 'm with am.\n"}, "150": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "paloohpalooh", "text": "\nI think you need some sort of regular expression matching (the following code is in Python 3):\nimport string\nimport re\nimport nltk\n\ns = \"I can't do this now, because I'm so tired.  Please give me some time.\"\nl = nltk.word_tokenize(s)\nll = [x for x in l if not re.fullmatch('[' + string.punctuation + ']+', x)]\nprint(l)\nprint(ll)\n\nOutput:\n['I', 'ca', \"n't\", 'do', 'this', 'now', ',', 'because', 'I', \"'m\", 'so', 'tired', '.', 'Please', 'give', 'me', 'some', 'time', '.']\n['I', 'ca', \"n't\", 'do', 'this', 'now', 'because', 'I', \"'m\", 'so', 'tired', 'Please', 'give', 'me', 'some', 'time']\n\nShould work well in most cases since it removes punctuation while preserving tokens like \"n't\", which can't be obtained from regex tokenizers such as wordpunct_tokenize.\n"}, "151": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "", "text": "\nYou can do it in one line without nltk (python 3.x).\nimport string\nstring_text= string_text.translate(str.maketrans('','',string.punctuation))\n\n"}, "152": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "vishvish", "text": "\nI use this code to remove punctuation:\nimport nltk\ndef getTerms(sentences):\n    tokens = nltk.word_tokenize(sentences)\n    words = [w.lower() for w in tokens if w.isalnum()]\n    print tokens\n    print words\n\ngetTerms(\"hh, hh3h. wo shi 2 4 A . fdffdf. A&&B \")\n\nAnd If you want to check whether a token is a valid English word or not, you may need PyEnchant\nTutorial:\n import enchant\n d = enchant.Dict(\"en_US\")\n d.check(\"Hello\")\n d.check(\"Helo\")\n d.suggest(\"Helo\")\n\n"}, "153": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "Bora M. AlperBora M. Alper", "text": "\nJust adding to the solution by @rmalouf, this will not include any numbers because \\w+ is equivalent to [a-zA-Z0-9_]\nfrom nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'[a-zA-Z]')\ntokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')\n\n"}, "154": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "Quan GanQuan Gan", "text": "\nRemove punctuaion(It will remove . as well as part of punctuation handling using below code)\n        tbl = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n        text_string = text_string.translate(tbl) #text_string don't have punctuation\n        w = word_tokenize(text_string)  #now tokenize the string \n\nSample Input/Output:\ndirect flat in oberoi esquire. 3 bhk 2195 saleable 1330 carpet. rate of 14500 final plus 1% floor rise. tax approx 9% only. flat cost with parking 3.89 cr plus taxes plus possession charger. middle floor. north door. arey and oberoi woods facing. 53% paymemt due. 1% transfer charge with buyer. total cost around 4.20 cr approx plus possession charges. rahul soni\n\n['direct', 'flat', 'oberoi', 'esquire', '3', 'bhk', '2195', 'saleable', '1330', 'carpet', 'rate', '14500', 'final', 'plus', '1', 'floor', 'rise', 'tax', 'approx', '9', 'flat', 'cost', 'parking', '389', 'cr', 'plus', 'taxes', 'plus', 'possession', 'charger', 'middle', 'floor', 'north', 'door', 'arey', 'oberoi', 'woods', 'facing', '53', 'paymemt', 'due', '1', 'transfer', 'charge', 'buyer', 'total', 'cost', 'around', '420', 'cr', 'approx', 'plus', 'possession', 'charges', 'rahul', 'soni']\n\n"}, "155": {"topic": "Detecting syllables in a word", "user_name": "Kevin Brown-Silva", "text": "\nI need to find a fairly efficient  way to detect syllables in a word. E.g.,\nInvisible -> in-vi-sib-le\nThere are some syllabification rules that could be used:\nV\nCV\nVC\nCVC\nCCV\nCCCV\nCVCC\n*where V is a vowel and C is a consonant.\nE.g., \nPronunciation (5 Pro-nun-ci-a-tion; CV-CVC-CV-V-CVC)\nI've tried few methods, among which were using regex (which helps only if you want to count syllables) or hard coded rule definition (a brute force approach which proves to be very inefficient) and finally using a finite state automata (which did not result with anything useful).\nThe purpose of my application is to create a dictionary of all syllables in a given language. This dictionary will later be used for spell checking applications (using Bayesian classifiers) and text to speech synthesis. \nI would appreciate if one could give me tips on an alternate way to solve this problem besides my previous approaches. \nI work in Java, but any tip in C/C++, C#, Python, Perl... would work for me.\n"}, "156": {"topic": "Detecting syllables in a word", "user_name": "user50705user50705", "text": "\nRead about the TeX approach to this problem for the purposes of hyphenation. Especially see Frank Liang's thesis dissertation Word Hy-phen-a-tion by Com-put-er. His algorithm is very accurate, and then includes a small exceptions dictionary for cases where the algorithm does not work.\n"}, "157": {"topic": "Detecting syllables in a word", "user_name": "jasonjason", "text": "\nI stumbled across this page looking for the same thing, and found a few implementations of the Liang paper here:\nhttps://github.com/mnater/hyphenator or the successor: https://github.com/mnater/Hyphenopoly\nThat is unless you're the type that enjoys reading a 60 page thesis instead of adapting freely available code for non-unique problem. :)\n"}, "158": {"topic": "Detecting syllables in a word", "user_name": "tkruse", "text": "\nHere is a solution using NLTK:\nfrom nltk.corpus import cmudict\nd = cmudict.dict()\ndef nsyl(word):\n  return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]] \n\n"}, "159": {"topic": "Detecting syllables in a word", "user_name": "SeanSean", "text": "\nI'm trying to tackle this problem for a program that will calculate the flesch-kincaid and flesch reading score of a block of text. My algorithm uses what I found on this website: http://www.howmanysyllables.com/howtocountsyllables.html and it gets reasonably close. It still has trouble on complicated words like invisible and hyphenation, but I've found it gets in the ballpark for my purposes. \nIt has the upside of being easy to implement. I found the \"es\" can be either syllabic or not. It's a gamble, but I decided to remove the es in my algorithm.\nprivate int CountSyllables(string word)\n    {\n        char[] vowels = { 'a', 'e', 'i', 'o', 'u', 'y' };\n        string currentWord = word;\n        int numVowels = 0;\n        bool lastWasVowel = false;\n        foreach (char wc in currentWord)\n        {\n            bool foundVowel = false;\n            foreach (char v in vowels)\n            {\n                //don't count diphthongs\n                if (v == wc && lastWasVowel)\n                {\n                    foundVowel = true;\n                    lastWasVowel = true;\n                    break;\n                }\n                else if (v == wc && !lastWasVowel)\n                {\n                    numVowels++;\n                    foundVowel = true;\n                    lastWasVowel = true;\n                    break;\n                }\n            }\n\n            //if full cycle and no vowel found, set lastWasVowel to false;\n            if (!foundVowel)\n                lastWasVowel = false;\n        }\n        //remove es, it's _usually? silent\n        if (currentWord.Length > 2 && \n            currentWord.Substring(currentWord.Length - 2) == \"es\")\n            numVowels--;\n        // remove silent e\n        else if (currentWord.Length > 1 &&\n            currentWord.Substring(currentWord.Length - 1) == \"e\")\n            numVowels--;\n\n        return numVowels;\n    }\n\n"}, "160": {"topic": "Detecting syllables in a word", "user_name": "", "text": "\nThis is a particularly difficult problem which is not completely solved by the LaTeX hyphenation algorithm.  A good summary of some available methods and the challenges involved can be found in the paper Evaluating Automatic Syllabification Algorithms for English (Marchand, Adsett, and Damper 2007).\n"}, "161": {"topic": "Detecting syllables in a word", "user_name": "hojuhoju", "text": "\nWhy calculate it? Every online dictionary has this info. http://dictionary.reference.com/browse/invisible\nin\u00b7vis\u00b7i\u00b7ble\n"}, "162": {"topic": "Detecting syllables in a word", "user_name": "", "text": "\nBumping @Tihamer and @joe-basirico.  Very useful function, not perfect, but good for most small-to-medium projects.  Joe, I have re-written an implementation of your code in Python:\ndef countSyllables(word):\n    vowels = \"aeiouy\"\n    numVowels = 0\n    lastWasVowel = False\n    for wc in word:\n        foundVowel = False\n        for v in vowels:\n            if v == wc:\n                if not lastWasVowel: numVowels+=1   #don't count diphthongs\n                foundVowel = lastWasVowel = True\n                        break\n        if not foundVowel:  #If full cycle and no vowel found, set lastWasVowel to false\n            lastWasVowel = False\n    if len(word) > 2 and word[-2:] == \"es\": #Remove es - it's \"usually\" silent (?)\n        numVowels-=1\n    elif len(word) > 1 and word[-1:] == \"e\":    #remove silent e\n        numVowels-=1\n    return numVowels\n\nHope someone finds this useful!\n"}, "163": {"topic": "Detecting syllables in a word", "user_name": "Joe BasiricoJoe Basirico", "text": "\nI ran into this exact same issue a little while ago. \nI ended up using the CMU Pronunciation Dictionary for quick and accurate lookups of most words. For words not in the dictionary, I fell back to a machine learning model that's ~98% accurate at predicting syllable counts.\nI wrapped the whole thing up in an easy-to-use python module here: https://github.com/repp/big-phoney \nInstall:\npip install big-phoney\nCount Syllables:    \nfrom big_phoney import BigPhoney\nphoney = BigPhoney()\nphoney.count_syllables('triceratops')  # --> 4\n\nIf you're not using Python and you want to try the ML-model-based approach, I did a pretty detailed write up on how the syllable counting model works on Kaggle.\n"}, "164": {"topic": "Detecting syllables in a word", "user_name": "ChrisChris", "text": "\nThanks Joe Basirico, for sharing your quick and dirty implementation in C#. I've used the big libraries, and they work, but they're usually a bit slow, and for quick projects, your method works fine.\nHere is your code in Java, along with test cases:\npublic static int countSyllables(String word)\n{\n    char[] vowels = { 'a', 'e', 'i', 'o', 'u', 'y' };\n    char[] currentWord = word.toCharArray();\n    int numVowels = 0;\n    boolean lastWasVowel = false;\n    for (char wc : currentWord) {\n        boolean foundVowel = false;\n        for (char v : vowels)\n        {\n            //don't count diphthongs\n            if ((v == wc) && lastWasVowel)\n            {\n                foundVowel = true;\n                lastWasVowel = true;\n                break;\n            }\n            else if (v == wc && !lastWasVowel)\n            {\n                numVowels++;\n                foundVowel = true;\n                lastWasVowel = true;\n                break;\n            }\n        }\n        // If full cycle and no vowel found, set lastWasVowel to false;\n        if (!foundVowel)\n            lastWasVowel = false;\n    }\n    // Remove es, it's _usually? silent\n    if (word.length() > 2 && \n            word.substring(word.length() - 2) == \"es\")\n        numVowels--;\n    // remove silent e\n    else if (word.length() > 1 &&\n            word.substring(word.length() - 1) == \"e\")\n        numVowels--;\n    return numVowels;\n}\n\npublic static void main(String[] args) {\n    String txt = \"what\";\n    System.out.println(\"txt=\"+txt+\" countSyllables=\"+countSyllables(txt));\n    txt = \"super\";\n    System.out.println(\"txt=\"+txt+\" countSyllables=\"+countSyllables(txt));\n    txt = \"Maryland\";\n    System.out.println(\"txt=\"+txt+\" countSyllables=\"+countSyllables(txt));\n    txt = \"American\";\n    System.out.println(\"txt=\"+txt+\" countSyllables=\"+countSyllables(txt));\n    txt = \"disenfranchized\";\n    System.out.println(\"txt=\"+txt+\" countSyllables=\"+countSyllables(txt));\n    txt = \"Sophia\";\n    System.out.println(\"txt=\"+txt+\" countSyllables=\"+countSyllables(txt));\n}\n\nThe result was as expected (it works good enough for Flesch-Kincaid):\ntxt=what countSyllables=1\ntxt=super countSyllables=2\ntxt=Maryland countSyllables=3\ntxt=American countSyllables=3\ntxt=disenfranchized countSyllables=5\ntxt=Sophia countSyllables=2\n\n"}, "165": {"topic": "Detecting syllables in a word", "user_name": "CerinCerin", "text": "\nToday I found this Java implementation of Frank Liang's hyphenation algorithmn with pattern for English or German, which works quite well and is available on Maven Central.\nCave: It is important to remove the last lines of the .tex pattern files, because otherwise those files can not be loaded with the current version on Maven Central.\nTo load and use the hyphenator, you can use the following Java code snippet. texTable is the name of the .tex files containing the needed patterns. Those files are available on the project github site.\n private Hyphenator createHyphenator(String texTable) {\n        Hyphenator hyphenator = new Hyphenator();\n        hyphenator.setErrorHandler(new ErrorHandler() {\n            public void debug(String guard, String s) {\n                logger.debug(\"{},{}\", guard, s);\n            }\n\n            public void info(String s) {\n                logger.info(s);\n            }\n\n            public void warning(String s) {\n                logger.warn(\"WARNING: \" + s);\n            }\n\n            public void error(String s) {\n                logger.error(\"ERROR: \" + s);\n            }\n\n            public void exception(String s, Exception e) {\n                logger.error(\"EXCEPTION: \" + s, e);\n            }\n\n            public boolean isDebugged(String guard) {\n                return false;\n            }\n        });\n\n        BufferedReader table = null;\n\n        try {\n            table = new BufferedReader(new InputStreamReader(Thread.currentThread().getContextClassLoader()\n                    .getResourceAsStream((texTable)), Charset.forName(\"UTF-8\")));\n            hyphenator.loadTable(table);\n        } catch (Utf8TexParser.TexParserException e) {\n            logger.error(\"error loading hyphenation table: {}\", e.getLocalizedMessage(), e);\n            throw new RuntimeException(\"Failed to load hyphenation table\", e);\n        } finally {\n            if (table != null) {\n                try {\n                    table.close();\n                } catch (IOException e) {\n                    logger.error(\"Closing hyphenation table failed\", e);\n                }\n            }\n        }\n\n        return hyphenator;\n    }\n\nAfterwards the Hyphenator is ready to use. To detect syllables, the basic idea is to split the term at the provided hyphens.\n    String hyphenedTerm = hyphenator.hyphenate(term);\n\n    String hyphens[] = hyphenedTerm.split(\"\\u00AD\");\n\n    int syllables = hyphens.length;\n\nYou need to split on \"\\u00AD\", since the API does not return a normal \"-\".\nThis approach outperforms the answer of Joe Basirico, since it supports many different languages and detects German hyphenation more accurate.\n"}, "166": {"topic": "Detecting syllables in a word", "user_name": "", "text": "\nPerl has Lingua::Phonology::Syllable module. You might try that, or try looking into its algorithm. I saw a few other older modules there, too.\nI don't understand why a regular expression gives you only a count of syllables. You should be able to get the syllables themselves using capture parentheses. Assuming you can construct a regular expression that works, that is.\n"}, "167": {"topic": "Detecting syllables in a word", "user_name": "TersosaurosTersosauros", "text": "\nThank you @joe-basirico and @tihamer. I have ported @tihamer's code to Lua 5.1, 5.2 and luajit 2 (most likely will run on other versions of lua as well):\ncountsyllables.lua\nfunction CountSyllables(word)\n  local vowels = { 'a','e','i','o','u','y' }\n  local numVowels = 0\n  local lastWasVowel = false\n\n  for i = 1, #word do\n    local wc = string.sub(word,i,i)\n    local foundVowel = false;\n    for _,v in pairs(vowels) do\n      if (v == string.lower(wc) and lastWasVowel) then\n        foundVowel = true\n        lastWasVowel = true\n      elseif (v == string.lower(wc) and not lastWasVowel) then\n        numVowels = numVowels + 1\n        foundVowel = true\n        lastWasVowel = true\n      end\n    end\n\n    if not foundVowel then\n      lastWasVowel = false\n    end\n  end\n\n  if string.len(word) > 2 and\n    string.sub(word,string.len(word) - 1) == \"es\" then\n    numVowels = numVowels - 1\n  elseif string.len(word) > 1 and\n    string.sub(word,string.len(word)) == \"e\" then\n    numVowels = numVowels - 1\n  end\n\n  return numVowels\nend\n\nAnd some fun tests to confirm it works (as much as it's supposed to):\ncountsyllables.tests.lua\nrequire \"countsyllables\"\n\ntests = {\n  { word = \"what\", syll = 1 },\n  { word = \"super\", syll = 2 },\n  { word = \"Maryland\", syll = 3},\n  { word = \"American\", syll = 4},\n  { word = \"disenfranchized\", syll = 5},\n  { word = \"Sophia\", syll = 2},\n  { word = \"End\", syll = 1},\n  { word = \"I\", syll = 1},\n  { word = \"release\", syll = 2},\n  { word = \"same\", syll = 1},\n}\n\nfor _,test in pairs(tests) do\n  local resultSyll = CountSyllables(test.word)\n  assert(resultSyll == test.syll,\n    \"Word: \"..test.word..\"\\n\"..\n    \"Expected: \"..test.syll..\"\\n\"..\n    \"Result: \"..resultSyll)\nend\n\nprint(\"Tests passed.\")\n\n"}, "168": {"topic": "Detecting syllables in a word", "user_name": "Ryan EppRyan Epp", "text": "\nYou can try Spacy Syllables. This works on Python 3.9:\nSetup:\npip install spacy\npip install spacy_syllables\npython -m spacy download en_core_web_md\n\nCode:\nimport spacy\nfrom spacy_syllables import SpacySyllables\nnlp = spacy.load('en_core_web_md')\nsyllables = SpacySyllables(nlp)\nnlp.add_pipe('syllables', after='tagger')\n\n\ndef spacy_syllablize(word):\n    token = nlp(word)[0]\n    return token._.syllables\n\n\nfor test_word in [\"trampoline\", \"margaret\", \"invisible\", \"thought\", \"Pronunciation\", \"couldn't\"]:\n    print(f\"{test_word} -> {spacy_syllablize(test_word)}\")\n\nOutput:\ntrampoline -> ['tram', 'po', 'line']\nmargaret -> ['mar', 'garet']\ninvisible -> ['in', 'vis', 'i', 'ble']\nthought -> ['thought']\nPronunciation -> ['pro', 'nun', 'ci', 'a', 'tion']\ncouldn't -> ['could']\n\n"}, "169": {"topic": "Detecting syllables in a word", "user_name": "TihamerTihamer", "text": "\nI could not find an adequate way to count syllables, so I designed a method myself. \nYou can view my method here: https://stackoverflow.com/a/32784041/2734752\nI use a combination of a dictionary and algorithm method to count syllables. \nYou can view my library here: https://github.com/troywatson/Lawrence-Style-Checker\nI just tested my algorithm and had a 99.4% strike rate!   \nLawrence lawrence = new Lawrence();\n\nSystem.out.println(lawrence.getSyllable(\"hyphenation\"));\nSystem.out.println(lawrence.getSyllable(\"computer\"));\n\nOutput:\n4\n3\n\n"}, "170": {"topic": "Detecting syllables in a word", "user_name": "MWiesner", "text": "\nAfter doing a lot of testing and trying out hyphenation packages as well, I wrote my own based on a number of examples. I also tried the pyhyphen and pyphen packages that interfaces with hyphenation dictionaries, but they produce the wrong number of syllables in many cases. The nltk package was simply too slow for this use case.\nMy implementation in Python is part of a class i wrote, and the syllable counting routine is pasted below. It over-estimates the number of syllables a bit as I still haven't found a good way to account for silent word endings.\nThe function returns the ratio of syllables per word as it is used for a Flesch-Kincaid readability score. The number doesn't have to be exact, just close enough for an estimate.\nOn my 7th generation i7 CPU, this function took 1.1-1.2 milliseconds for a 759 word sample text.\ndef _countSyllablesEN(self, theText):\n\n    cleanText = \"\"\n    for ch in theText:\n        if ch in \"abcdefghijklmnopqrstuvwxyz'\u2019\":\n            cleanText += ch\n        else:\n            cleanText += \" \"\n\n    asVow    = \"aeiouy'\u2019\"\n    dExep    = (\"ei\",\"ie\",\"ua\",\"ia\",\"eo\")\n    theWords = cleanText.lower().split()\n    allSylls = 0\n    for inWord in theWords:\n        nChar  = len(inWord)\n        nSyll  = 0\n        wasVow = False\n        wasY   = False\n        if nChar == 0:\n            continue\n        if inWord[0] in asVow:\n            nSyll += 1\n            wasVow = True\n            wasY   = inWord[0] == \"y\"\n        for c in range(1,nChar):\n            isVow  = False\n            if inWord[c] in asVow:\n                nSyll += 1\n                isVow = True\n            if isVow and wasVow:\n                nSyll -= 1\n            if isVow and wasY:\n                nSyll -= 1\n            if inWord[c:c+2] in dExep:\n                nSyll += 1\n            wasVow = isVow\n            wasY   = inWord[c] == \"y\"\n        if inWord.endswith((\"e\")):\n            nSyll -= 1\n        if inWord.endswith((\"le\",\"ea\",\"io\")):\n            nSyll += 1\n        if nSyll < 1:\n            nSyll = 1\n        # print(\"%-15s: %d\" % (inWord,nSyll))\n        allSylls += nSyll\n\n    return allSylls/len(theWords)\n\n"}, "171": {"topic": "Detecting syllables in a word", "user_name": "rzo1rzo1", "text": "\nI am including a solution that works \"okay\" in R.  Far from perfect.\ncountSyllablesInWord = function(words)\n  {\n  #word = \"super\";\n  n.words = length(words);\n  result = list();\n  for(j in 1:n.words)\n    {\n    word = words[j];\n    vowels = c(\"a\",\"e\",\"i\",\"o\",\"u\",\"y\");\n    \n    word.vec = strsplit(word,\"\")[[1]];\n    word.vec;\n    \n    n.char = length(word.vec);\n    \n    is.vowel = is.element(tolower(word.vec), vowels);\n    n.vowels = sum(is.vowel);\n    \n    \n    # nontrivial problem \n    if(n.vowels <= 1)\n      {\n      syllables = 1;\n      str = word;\n      } else {\n              # syllables = 0;\n              previous = \"C\";\n              # on average ? \n              str = \"\";\n              n.hyphen = 0;\n        \n              for(i in 1:n.char)\n                {\n                my.char = word.vec[i];\n                my.vowel = is.vowel[i];\n                if(my.vowel)\n                  {\n                  if(previous == \"C\")\n                    {\n                    if(i == 1)\n                      {\n                      str = paste0(my.char, \"-\");\n                      n.hyphen = 1 + n.hyphen;\n                      } else {\n                              if(i < n.char)\n                                {\n                                if(n.vowels > (n.hyphen + 1))\n                                  {\n                                  str = paste0(str, my.char, \"-\");\n                                  n.hyphen = 1 + n.hyphen;\n                                  } else {\n                                           str = paste0(str, my.char);\n                                          }\n                                } else {\n                                        str = paste0(str, my.char);\n                                        }\n                              }\n                     # syllables = 1 + syllables;\n                     previous = \"V\";\n                    } else {  # \"VV\"\n                          # assume what  ?  vowel team?\n                          str = paste0(str, my.char);\n                          }\n            \n                } else {\n                            str = paste0(str, my.char);\n                            previous = \"C\";\n                            }\n                #\n                }\n        \n              syllables = 1 + n.hyphen;\n              }\n  \n      result[[j]] = list(\"syllables\" = syllables, \"vowels\" = n.vowels, \"word\" = str);\n      }\n  \n  if(n.words == 1) { result[[1]]; } else { result; }\n  }\n\nHere are some results:\nmy.count = countSyllablesInWord(c(\"America\", \"beautiful\", \"spacious\", \"skies\", \"amber\", \"waves\", \"grain\", \"purple\", \"mountains\", \"majesty\"));\n\nmy.count.df = data.frame(matrix(unlist(my.count), ncol=3, byrow=TRUE));\ncolnames(my.count.df) = names(my.count[[1]]);\n\nmy.count.df;\n\n#    syllables vowels         word\n# 1          4      4   A-me-ri-ca\n# 2          4      5 be-auti-fu-l\n# 3          3      4   spa-ci-ous\n# 4          2      2       ski-es\n# 5          2      2       a-mber\n# 6          2      2       wa-ves\n# 7          2      2       gra-in\n# 8          2      2      pu-rple\n# 9          3      4  mo-unta-ins\n# 10         3      3    ma-je-sty\n\nI didn't realize how big of a \"rabbit hole\" this is, seems so easy.\n\n################ hackathon #######\n\n\n# https://en.wikipedia.org/wiki/Gunning_fog_index\n# THIS is a CLASSIFIER PROBLEM ...\n# https://stackoverflow.com/questions/405161/detecting-syllables-in-a-word\n\n\n\n# http://www.speech.cs.cmu.edu/cgi-bin/cmudict\n# http://www.syllablecount.com/syllables/\n\n\n  # https://enchantedlearning.com/consonantblends/index.shtml\n  # start.digraphs = c(\"bl\", \"br\", \"ch\", \"cl\", \"cr\", \"dr\", \n  #                   \"fl\", \"fr\", \"gl\", \"gr\", \"pl\", \"pr\",\n  #                   \"sc\", \"sh\", \"sk\", \"sl\", \"sm\", \"sn\",\n  #                   \"sp\", \"st\", \"sw\", \"th\", \"tr\", \"tw\",\n  #                   \"wh\", \"wr\");\n  # start.trigraphs = c(\"sch\", \"scr\", \"shr\", \"sph\", \"spl\",\n  #                     \"spr\", \"squ\", \"str\", \"thr\");\n  # \n  # \n  # \n  # end.digraphs = c(\"ch\",\"sh\",\"th\",\"ng\",\"dge\",\"tch\");\n  # \n  # ile\n  # \n  # farmer\n  # ar er\n  # \n  # vowel teams ... beaver1\n  # \n  # \n  # # \"able\"\n  # # http://www.abcfastphonics.com/letter-blends/blend-cial.html\n  # blends = c(\"augh\", \"ough\", \"tien\", \"ture\", \"tion\", \"cial\", \"cian\", \n  #             \"ck\", \"ct\", \"dge\", \"dis\", \"ed\", \"ex\", \"ful\", \n  #             \"gh\", \"ng\", \"ous\", \"kn\", \"ment\", \"mis\", );\n  # \n  # glue = c(\"ld\", \"st\", \"nd\", \"ld\", \"ng\", \"nk\", \n  #           \"lk\", \"lm\", \"lp\", \"lt\", \"ly\", \"mp\", \"nce\", \"nch\", \n  #           \"nse\", \"nt\", \"ph\", \"psy\", \"pt\", \"re\", )\n  # \n  # \n  # start.graphs = c(\"bl, br, ch, ck, cl, cr, dr, fl, fr, gh, gl, gr, ng, ph, pl, pr, qu, sc, sh, sk, sl, sm, sn, sp, st, sw, th, tr, tw, wh, wr\");\n  # \n  # # https://mantra4changeblog.wordpress.com/2017/05/01/consonant-digraphs/\n  # digraphs.start = c(\"ch\",\"sh\",\"th\",\"wh\",\"ph\",\"qu\");\n  # digraphs.end = c(\"ch\",\"sh\",\"th\",\"ng\",\"dge\",\"tch\");\n  # # https://www.education.com/worksheet/article/beginning-consonant-blends/\n  # blends.start = c(\"pl\", \"gr\", \"gl\", \"pr\",\n  #                 \n  # blends.end = c(\"lk\",\"nk\",\"nt\",\n  # \n  # \n  # # https://sarahsnippets.com/wp-content/uploads/2019/07/ScreenShot2019-07-08at8.24.51PM-817x1024.png\n  # # Monte     Mon-te\n  # # Sophia    So-phi-a\n  # # American  A-mer-i-can\n  # \n  # n.vowels = 0;\n  # for(i in 1:n.char)\n  #   {\n  #   my.char = word.vec[i];\n  # \n  # \n  # \n  # \n  # \n  # n.syll = 0;\n  # str = \"\";\n  # \n  # previous = \"C\"; # consonant vs \"V\" vowel\n  # \n  # for(i in 1:n.char)\n  #   {\n  #   my.char = word.vec[i];\n  #   \n  #   my.vowel = is.element(tolower(my.char), vowels);\n  #   if(my.vowel)\n  #     {\n  #     n.vowels = 1 + n.vowels;\n  #     if(previous == \"C\")\n  #       {\n  #       if(i == 1)\n  #         {\n  #         str = paste0(my.char, \"-\");\n  #         } else {\n  #                 if(n.syll > 1)\n  #                   {\n  #                   str = paste0(str, \"-\", my.char);\n  #                   } else {\n  #                          str = paste0(str, my.char);\n  #                         }\n  #                 }\n  #        n.syll = 1 + n.syll;\n  #        previous = \"V\";\n  #       } \n  #     \n  #   } else {\n  #               str = paste0(str, my.char);\n  #               previous = \"C\";\n  #               }\n  #   #\n  #   }\n  # \n  # \n  # \n  # \n## https://jzimba.blogspot.com/2017/07/an-algorithm-for-counting-syllables.html\n# AIDE   1\n# IDEA   3\n# IDEAS  2\n# IDEE   2\n# IDE   1\n# AIDA   2\n# PROUSTIAN 3\n# CHRISTIAN 3\n# CLICHE  1\n# HALIDE  2\n# TELEPHONE 3\n# TELEPHONY 4\n# DUE   1\n# IDEAL  2\n# DEE   1\n# UREA  3\n# VACUO  3\n# SEANCE  1\n# SAILED  1\n# RIBBED  1\n# MOPED  1\n# BLESSED  1\n# AGED  1\n# TOTED  2\n# WARRED  1\n# UNDERFED 2\n# JADED  2\n# INBRED  2\n# BRED  1\n# RED   1\n# STATES  1\n# TASTES  1\n# TESTES  1\n# UTILIZES  4\n\nAnd for good measure, a simple kincaid readability function ... syllables is a list of counts returned from the first function ...\nSince my function is a bit biased towards more syllables, that will give an inflated readability score ... which for now is fine ... if the goal is to make text more readable, this is not the worst thing.\ncomputeReadability = function(n.sentences, n.words, syllables=NULL)\n  {\n  n = length(syllables);\n  n.syllables = 0;\n  for(i in 1:n)\n    {\n    my.syllable = syllables[[i]];\n    n.syllables = my.syllable$syllables + n.syllables;\n    }\n  # Flesch Reading Ease (FRE):\n  FRE = 206.835 - 1.015 * (n.words/n.sentences) - 84.6 * (n.syllables/n.words);\n  # Flesh-Kincaid Grade Level (FKGL):\n  FKGL = 0.39 * (n.words/n.sentences) + 11.8 * (n.syllables/n.words) - 15.59; \n  # FKGL = -0.384236 * FRE - 20.7164 * (n.syllables/n.words) + 63.88355;\n  # FKGL = -0.13948  * FRE + 0.24843 * (n.words/n.sentences) + 13.25934;\n  \n  list(\"FRE\" = FRE, \"FKGL\" = FKGL); \n  }\n\n"}, "172": {"topic": "Detecting syllables in a word", "user_name": "crenate", "text": "\nI used jsoup to do this once. Here's a sample syllable parser:\npublic String[] syllables(String text){\n        String url = \"https://www.merriam-webster.com/dictionary/\" + text;\n        String relHref;\n        try{\n            Document doc = Jsoup.connect(url).get();\n            Element link = doc.getElementsByClass(\"word-syllables\").first();\n            if(link == null){return new String[]{text};}\n            relHref = link.html(); \n        }catch(IOException e){\n            relHref = text;\n        }\n        String[] syl = relHref.split(\"\u00b7\");\n        return syl;\n    }\n\n"}, "173": {"topic": "How does Apple find dates, times and addresses in emails?", "user_name": "Martin", "text": "\nIn the iOS email client, when an email contains a date, time or location, the text becomes a hyperlink and it is possible to create an appointment or look at a map simply by tapping the link. It not only works for emails in English, but in other languages also. I love this feature and would like to understand how they do it. \nThe naive way to do this would be to have many regular expressions and run them all. However I  this is not going to scale very well and will work for only a specific language or date format, etc. I think that Apple must be using some concept of machine learning to extract entities (8:00PM, 8PM, 8:00, 0800, 20:00, 20h, 20h00, 2000 etc.).\nAny idea how Apple is able to extract entities so quickly in its email client? What machine learning algorithm would you to apply accomplish such task? \n"}, "174": {"topic": "How does Apple find dates, times and addresses in emails?", "user_name": "MartinMartin", "text": "\nThey likely use Information Extraction techniques for this.\nHere is a demo of Stanford's SUTime tool:\nhttp://nlp.stanford.edu:8080/sutime/process\nYou would extract attributes about n-grams (consecutive words) in a document:\n\nnumberOfLetters\nnumberOfSymbols\nlength\npreviousWord\nnextWord\nnextWordNumberOfSymbols\n...\n\nAnd then use a classification algorithm, and feed it positive and negative examples:\nObservation  nLetters  nSymbols  length  prevWord  nextWord isPartOfDate  \n\"Feb.\"       3         1         4       \"Wed\"     \"29th\"   TRUE  \n\"DEC\"        3         0         3       \"company\" \"went\"   FALSE  \n...\n\nYou might get away with 50 examples of each, but the more the merrier. Then, the algorithm learns based on those examples, and can apply to future examples that it hasn't seen before.\nIt might learn rules such as \n\nif previous word is only characters and maybe periods...\nand current word is in \"february\", \"mar.\", \"the\" ...\nand next word is in \"twelfth\", any_number ...\nthen is date\n\nHere is a decent video by a Google engineer on the subject\n"}, "175": {"topic": "How does Apple find dates, times and addresses in emails?", "user_name": "", "text": "\nThat's a technology Apple actually developed a very long time ago called Apple Data Detectors. You can read more about it here:\nhttp://www.miramontes.com/writing/add-cacm/\nEssentially it parses the text and detects patterns that represent specific pieces of data, then applies OS-contextual actions to it. It's neat.\n"}, "176": {"topic": "How does Apple find dates, times and addresses in emails?", "user_name": "Neil McGuiganNeil McGuigan", "text": "\nThis is called temporal expression identification and parsing.  Here are some Google searches to get you started: \nhttps://www.google.com/#hl=en&safe=off&sclient=psy-ab&q=timebank+timeml+timex\nhttps://www.google.com/#hl=en&safe=off&sclient=psy-ab&q=temporal+expression+tagger\n"}, "177": {"topic": "How does Apple find dates, times and addresses in emails?", "user_name": "Soner G\u00f6n\u00fcl", "text": "\nOne part of the puzzle could be the NSDataDetector class. Its used to recognize some standard types like phone numbers.\n"}, "178": {"topic": "How does Apple find dates, times and addresses in emails?", "user_name": "jeffehobbsjeffehobbs", "text": "\nI once wrote a parser to do this, using pyparsing. It's really very simple, you just need to get all the different ways right, but there aren't that many. It only took a few hours and was pretty fast.\n"}, "179": {"topic": "How does Apple find dates, times and addresses in emails?", "user_name": "arturomp", "text": "\nApple has a patent on how they did it System and method for performing an action on a structure in computer data, and here's a story on this patent apples-patent-on-nsdatadetector\n"}, "180": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "sophros", "text": "\nwhat is difference between spacy.load('en_core_web_sm') and spacy.load('en')? This link explains different model sizes. But i am still not clear how spacy.load('en_core_web_sm') and spacy.load('en') differ\nspacy.load('en') runs fine for me. But the spacy.load('en_core_web_sm') throws error\ni have installed spacyas below. when i go to jupyter notebook and run command nlp = spacy.load('en_core_web_sm') I get the below error \n---------------------------------------------------------------------------\nOSError                                   Traceback (most recent call last)\n<ipython-input-4-b472bef03043> in <module>()\n      1 # Import spaCy and load the language library\n      2 import spacy\n----> 3 nlp = spacy.load('en_core_web_sm')\n      4 \n      5 # Create a Doc object\n\nC:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder\\lib\\site-packages\\spacy\\__init__.py in load(name, **overrides)\n     13     if depr_path not in (True, False, None):\n     14         deprecation_warning(Warnings.W001.format(path=depr_path))\n---> 15     return util.load_model(name, **overrides)\n     16 \n     17 \n\nC:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder\\lib\\site-packages\\spacy\\util.py in load_model(name, **overrides)\n    117     elif hasattr(name, 'exists'):  # Path or Path-like to model data\n    118         return load_model_from_path(name, **overrides)\n--> 119     raise IOError(Errors.E050.format(name=name))\n    120 \n    121 \n\nOSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n\nhow I installed Spacy ---\n(C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder) C:\\Users\\nikhizzz>conda install -c conda-forge spacy\nFetching package metadata .............\nSolving package specifications: .\n\nPackage plan for installation in environment C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder:\n\nThe following NEW packages will be INSTALLED:\n\n    blas:           1.0-mkl\n    cymem:          1.31.2-py35h6538335_0    conda-forge\n    dill:           0.2.8.2-py35_0           conda-forge\n    msgpack-numpy:  0.4.4.2-py_0             conda-forge\n    murmurhash:     0.28.0-py35h6538335_1000 conda-forge\n    plac:           0.9.6-py_1               conda-forge\n    preshed:        1.0.0-py35h6538335_0     conda-forge\n    pyreadline:     2.1-py35_1000            conda-forge\n    regex:          2017.11.09-py35_0        conda-forge\n    spacy:          2.0.12-py35h830ac7b_0    conda-forge\n    termcolor:      1.1.0-py_2               conda-forge\n    thinc:          6.10.3-py35h830ac7b_2    conda-forge\n    tqdm:           4.29.1-py_0              conda-forge\n    ujson:          1.35-py35hfa6e2cd_1001   conda-forge\n\nThe following packages will be UPDATED:\n\n    msgpack-python: 0.4.8-py35_0                         --> 0.5.6-py35he980bc4_3 conda-forge\n\nThe following packages will be DOWNGRADED:\n\n    freetype:       2.7-vc14_2               conda-forge --> 2.5.5-vc14_2\n\nProceed ([y]/n)? y\n\nblas-1.0-mkl.t 100% |###############################| Time: 0:00:00   0.00  B/s\ncymem-1.31.2-p 100% |###############################| Time: 0:00:00   1.65 MB/s\nmsgpack-python 100% |###############################| Time: 0:00:00   5.37 MB/s\nmurmurhash-0.2 100% |###############################| Time: 0:00:00   1.49 MB/s\nplac-0.9.6-py_ 100% |###############################| Time: 0:00:00   0.00  B/s\npyreadline-2.1 100% |###############################| Time: 0:00:00   4.62 MB/s\nregex-2017.11. 100% |###############################| Time: 0:00:00   3.31 MB/s\ntermcolor-1.1. 100% |###############################| Time: 0:00:00 187.81 kB/s\ntqdm-4.29.1-py 100% |###############################| Time: 0:00:00   2.51 MB/s\nujson-1.35-py3 100% |###############################| Time: 0:00:00   1.66 MB/s\ndill-0.2.8.2-p 100% |###############################| Time: 0:00:00   4.34 MB/s\nmsgpack-numpy- 100% |###############################| Time: 0:00:00   0.00  B/s\npreshed-1.0.0- 100% |###############################| Time: 0:00:00   0.00  B/s\nthinc-6.10.3-p 100% |###############################| Time: 0:00:00   5.49 MB/s\nspacy-2.0.12-p 100% |###############################| Time: 0:00:10   7.42 MB/s\n\n(C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder) C:\\Users\\nikhizzz>python -V\nPython 3.5.3 :: Anaconda custom (64-bit)\n\n(C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder) C:\\Users\\nikhizzz>python -m spacy download en\nCollecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n    100% |################################| 37.4MB ...\nInstalling collected packages: en-core-web-sm\n  Running setup.py install for en-core-web-sm ... done\nSuccessfully installed en-core-web-sm-2.0.0\n\n    Linking successful\n    C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder\\lib\\site-packages\\en_core_web_sm\n    -->\n    C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder\\lib\\site-packages\\spacy\\data\\en\n\n    You can now load the model via spacy.load('en')\n\n\n(C:\\Users\\nikhizzz\\AppData\\Local\\conda\\conda\\envs\\tensorflowspyder) C:\\Users\\nikhizzz>\n\n"}, "181": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "user2543622user2543622", "text": "\nInitially I downloaded two en packages using following statements in anaconda prompt.\npython -m spacy download en_core_web_lg\npython -m spacy download en_core_web_sm\n\nBut, I kept on getting linkage error and finally running below command helped me to establish link and solved error.\npython -m spacy download en\n\nAlso make sure you to restart your runtime if working with Jupyter.\n-PS : If you get linkage error try giving admin previlages.\n"}, "182": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "Sandalu Pabasara", "text": "\nThe answer to your misunderstanding is a Unix concept, softlinks which we could say that in Windows are similar to shortcuts. Let's explain this.\nWhen you spacy download en, spaCy tries to find the best small model that matches your spaCy distribution. The small model that I am talking about defaults to en_core_web_sm which can be found in different variations which correspond to the different spaCy versions (for example spacy, spacy-nightly have en_core_web_sm of different sizes). \nWhen spaCy finds the best model for you, it downloads it and then links the name en to the package it downloaded, e.g. en_core_web_sm. That basically means that whenever you refer to en you will be referring to en_core_web_sm. In other words, en after linking is not a \"real\" package, is just a name for en_core_web_sm.\nHowever, it doesn't work the other way. You can't refer directly to en_core_web_sm because your system doesn't know you have it installed. When you did spacy download en you basically did a pip install. So pip knows that you have a package named en installed for your python distribution, but knows nothing about the package en_core_web_sm. This package is just replacing package en when you import it, which means that package en is just a softlink to en_core_web_sm.\nOf course, you can directly download en_core_web_sm, using the command: python -m spacy download en_core_web_sm, or you can even link the name en to other models as well. For example, you could do python -m spacy download en_core_web_lg and then python -m spacy link en_core_web_lg en. That would make \nen a name for en_core_web_lg, which is a large spaCy model for the English language.\nHope it is clear now :) \n"}, "183": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "Tarun ReddyTarun Reddy", "text": "\nThe below worked for me :\nimport en_core_web_sm\n\nnlp = en_core_web_sm.load()\n\n"}, "184": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "", "text": "\nUsing the Spacy language model in Colab requires only the following two steps:\n\nDownload the model (change the name according to the size of the model)\n\n!python -m spacy download en_core_web_lg \n\n\nRestart the colab runtime!\nPerform shortcut key\uff1a Ctrl + M + .\n\nTest\nimport spacy\nnlp = spacy.load(\"en_core_web_lg\")\n\nsuccessful!!!\n"}, "185": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "gdarasgdaras", "text": "\nFor those who are still facing problems even after installing it as administrator from Anaconda prompt, here's a quick fix:\n\nGot to the path where it is downloaded. For e.g.\nC:\\Users\\name\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.2.0\n\n\nCopy the path.\n\nPaste it in:\nnlp = spacy.load(r'C:\\Users\\name\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.2.0')\n\n\nWorks like a charm :)\n\n\nPS: Check for spacy version\n"}, "186": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "jla", "text": "\nTry this method as this worked like a charm to me:\nIn your Anaconda Prompt, run the command:\n!python -m spacy download en\n\nAfter running the above command, you should be able to execute the below in your jupyter notebook:\nspacy.load('en_core_web_sm')\n\n"}, "187": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "Dipanwita MallickDipanwita Mallick", "text": "\nDon't run !python -m spacy download en_core_web_lg from inside jupyter.\nDo this instead:\nimport spacy.cli\nspacy.cli.download(\"en_core_web_lg\")\n\nYou may need to restart the kernel before running the above two commands for it to work.\n"}, "188": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "yinghao zhiyinghao zhi", "text": "\nSteps to load up modules based on different versions of spacy\ndownload the best-matching version of a specific model for your spaCy installation\npython -m spacy download en_core_web_sm\npip install .tar.gz archive from path or URL\npip install /Users/you/en_core_web_sm-2.2.0.tar.gz\n\nor\npip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n\nAdd to your requirements file or environment yaml file. Theres range of version that one spacy version is comptable with you can view more under https://github.com/explosion/spacy-models/releases\nif your not sure running below code\nnlp = spacy.load('en_core_web_sm') \n\nwill give off a warning telling what version model will be compatible with your installed spacy verion\nenironment.yml example\nname: root\nchannels:\n  - defaults\n  - conda-forge\n  - anaconda\ndependencies:\n  - python=3.8.3\n  - pip\n  - spacy=2.3.2\n  - scikit-learn=0.23.2\n  - pip:\n    - https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm\n\n"}, "189": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "General Grievance", "text": "\nFirst of all, install spacy using the following command for jupyter notebook\npip install -U spacy\nThen write the following code:\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\n"}, "190": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "Harshit SinghHarshit Singh", "text": "\nI am running Jupyter Notebook on Windows.\nFinally, its a version issue, Need to execute below commands in conda cmd prompt( open as admin)\n\npip install spacy==2.3.5\n\npython -m spacy download en_core_web_sm\n\npython -m spacy download en\n\n\nfrom chatterbot import ChatBot\nimport spacy\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\nChatBot(\"hello\")\n\nOutput -\n\n"}, "191": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "jeffhale", "text": "\nimport spacy\n\nnlp = spacy.load('/opt/anaconda3/envs/NLPENV/lib/python3.7/site-packages/en_core_web_sm/en_core_web_sm-2.3.1')\n\nTry giving the absolute path of the package with the version as shown in the image.\nIt works perfectly fine.\n"}, "192": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "Pallavi BanerjeePallavi Banerjee", "text": "\na simple solution for this which I saw on spacy.io\nfrom spacy.lang.en import English\nnlp=English()\n\nhttps://course.spacy.io/en/chapter1\n"}, "193": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "AzzedineAzzedine", "text": "\nAs for Windows based Anaconda,\n\nOpen Anaconda Prompt\n\nActivate your environment. Ex: active myspacyenv\n\npip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n\npython -m spacy download en_core_web_sm\n\nOpen Jupyter Notebook ex: active myspacyenv and then jupyter notebook on Anaconda Promt\n\n\n\nimport spacy spacy.load('en_core_web_sm')\n\nand it will run peacefully!\n"}, "194": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "Timothy MugayiTimothy Mugayi", "text": "\nOpen Anaconda Navigator. Click on any IDE. Run the code: \n!pip install -U spacy download en_core_web_sm\n!pip install -U spacy download en_core_web_sm\n\nIt will work. If you are open IDE directly close it and follow this procedure once.\n"}, "195": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "Davit Tovmasyan", "text": "\nLoading the module using the different syntax worked for me.\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\n"}, "196": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "mekonen mokemekonen moke", "text": "\nAnaconda Users\n\nIf you're using a conda virtual environment, be sure that its the same version of Python as that in your base environment. To verify this, run python --version in each environment. If not the same, create a new virtual environment with that version of Python (Ex. conda create --name myenv python=x.x.x).\nActivate the virtual environment (conda activate myenv)\nconda install -c conda-forge spacy\npython -m spacy download en_core_web_sm\n\nI just ran into this issue, and the above worked for me. This addresses the issue of the download occurring in an area that is not accessible to your current virtual environment.\nYou should then be able to run the following:\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\n"}, "197": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "Chandan", "text": "\nOpen command prompt or terminal and execute the below code:\npip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n\nExecute the below chunk in your Jupiter notebook.\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nHope the above code works for all:)\n"}, "198": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "Prashant MalanPrashant Malan", "text": "\nI had also same issue as I couldnt load module using '''spacy.load()'''\nYou can follow below steps to solve this on windows:\n\ndownload using !python -m spacy download en_core_web_sm\nimport en_core_web_sm as import en_core_web_sm\nload using en_core_web_sm.load() to some variable\n\nComplete code will be:\npython -m spacy download en_core_web_sm\n\nimport en_core_web_sm\n\nnlp = en_core_web_sm.load()\n\n"}, "199": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "fcdt", "text": "\nThis works with colab:\n!python -m spacy download en\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\nOr for the medium:\nimport en_core_web_md\nnlp = en_core_web_md.load()\n\n"}, "200": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "lovenish gaurlovenish gaur", "text": "\nInstead of any of the above, this solved my error.\nconda install -c conda-forge spacy-model-en_core_web_sm\nIf you are an anaconda user, this is the solution.\n"}, "201": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "General Grievance", "text": "\nI'm running PyCharm on MacOS and while none of the above answers completely worked for me, they did provide enough clues and I was finally able to everything working.  I am connecting to an ec2 instance and have configured PyCharm such that I can edit on my Mac and it automatically updates the files on my ec2 instance.  Thus, the problem was on the ec2 side where it was not finding Spacy even though I installed it several different times and ways.  If I ran my python script from the command line, everything worked fine.  However, from within PyCharm, it was initially not finding Spacy and the models.  I eventually fixed the \"finding\" spacy issue using the above recommendation of adding a \"requirements.txt\" file.  But the models were still not recognized.\nMy solution: download the models manually and place them in the file system on the ec2 instance and explicitly point to them when loaded.  I downloaded the files from here:\nhttps://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\nhttps://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0.tar.gz\nAfter downloading, I dropped moved them to my ec2 instance, decompressed and untared them in my filesystem, e.g. /path_to_models/en_core_web_lg-3.0.0/\nI then load a model using the explicit path and it worked from within PyCharm (note the path used goes all the way to en_core_web_lg-3.0.0; you will get an error if you do not use the folder with the config.cfg file):\nnlpObject = spacy.load('/path_to_models/en_core_web_lg-3.0.0/en_core_web_lg/en_core_web_lg-3.0.0')\n\n"}, "202": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "user15341444user15341444", "text": "\nCheck installed version of spacy\npip show spacy\nYou will get something like this:\nName: spacy\nVersion: 3.1.3\nSummary: Industrial-strength Natural Language Processing (NLP) in Python\nInstall the relevant version of the model using:\n!pip install -U https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n"}, "203": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "General Grievance", "text": "\nI tried all the above answers but could not succeed. Below worked for me :\n(Specific to WINDOWS os)\n\nRun anaconda command prompt with admin privilege(Important)\nThen run below commands:\n\n  pip install -U --user spacy    \n  python -m spacy download en\n\n\nTry below command for verification:\n\nimport spacy\nspacy.load('en')\n\n\nIt might work for others versions as well:\n\n\n"}, "204": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "VoontentVoontent", "text": "\nIf you have already downloaded spacy and the language model (E.g., en_core_web_sm or en_core_web_md), then you can follow these steps:\n\nOpen Anaconda prompt as admin\n\nThen type : python -m spacy link [package name or path] [shortcut]\nFor E.g., python -m spacy link /Users/you/model en\n\n\nThis will create a symlink to the your language model. Now you can load the model using spacy.load(\"en\") in your notebooks or scripts\n"}, "205": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "erncyp", "text": "\nThis is what I did:\n\nWent to the virtual environment where I was working on Anaconda Prompt / Command Line\n\nRan this: python -m spacy download en_core_web_sm\n\n\nAnd was done\n"}, "206": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "saikrishna pulipatisaikrishna pulipati", "text": "\nTRY THIS :-\n!python -m spacy download en_core_web_md\n"}, "207": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "JaskiratSraJaskiratSra", "text": "\nEven I faced similar issue. How I resolved it\n\nstart anaconda prompt in admin mode.\ninstalled both\npython -m spacy download en\nand\npython -m spacy download en_core_web_sm\nafter above steps only I started jupyter notebook where I am accessing this package.\nNow I can access both\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nor\nnlp = spacy.load('en')\nBoth are working for me.\n\n"}, "208": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "Colonel_OldColonel_Old", "text": "\nI faced a similar issue. I installed spacy and en_core_web_sm from a specific conda environment. However, I got two(02) differents issues as following:\n[Errno 2] No such file or directory: '....\\en_core_web_sm\\en_core_web_sm-2.3.1\\vocab\\lexemes.bin'\nor\nOSError: [E050] Can't find model 'en_core_web_sm'.... It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\nI did the following:\n\nOpen Command Prompt as Administrator\nGo to c:>\nActivate my Conda environment (If you work in a specific conda environment):\n\nc:\\>activate <conda environment name>\n\n\n(conda environment name)c:\\>python -m spacy download en\nReturn to Jupyter Notebook and you can load the language library:\n\nnlp = en_core_web_sm.load()\n\nFor me, it works :)\n"}, "209": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "Ankit JainAnkit Jain", "text": "\nDownload en_core_web_sm tar file\nOpen terminal from anaconda or open anaconda evn.\nRun this:\npip3 install /Users/yourpath/Downloads/en_core_web_sm-3.1.0.tar.gz;\n\nor\npip install /Users/yourpath/Downloads/en_core_web_sm-3.1.0.tar.gz;\n\nRestart jupyter, it will work.\n"}, "210": {"topic": "spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)", "user_name": "", "text": "\nRun this in os console:\npython -m spacy download en\npython -m spacy link en_core_web_sm en_core_web_sm\n\nThen run this in python console or on your python IDE:\nimport spacy\nspacy.load('en_core_web_sm')\n\n"}, "211": {"topic": "Understanding min_df and max_df in scikit CountVectorizer", "user_name": "Troll", "text": "\nI have five text files that I input to a CountVectorizer. When specifying min_df and max_df to the CountVectorizer instance what does the min/max document frequency exactly mean? Is it the frequency of a word in its particular text file or is it the frequency of the word in the entire overall corpus (five text files)?\nWhat are the differences when min_df and max_df are provided as integers or as floats?\nThe documentation doesn't seem to provide a thorough explanation nor does it supply an example to demonstrate the use of these two parameters. Could someone provide an explanation or example demonstrating min_df and max_df?\n"}, "212": {"topic": "Understanding min_df and max_df in scikit CountVectorizer", "user_name": "moeabdolmoeabdol", "text": "\nmax_df is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\". For example:\n\nmax_df = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\nmax_df = 25 means \"ignore terms that appear in more than 25 documents\".\n\nThe default max_df is 1.0, which means \"ignore terms that appear in more than 100% of the documents\". Thus, the default setting does not ignore any terms.\n\nmin_df is used for removing terms that appear too infrequently. For example:\n\nmin_df = 0.01 means \"ignore terms that appear in less than 1% of the documents\".\nmin_df = 5 means \"ignore terms that appear in less than 5 documents\".\n\nThe default min_df is 1, which means \"ignore terms that appear in less than 1 document\". Thus, the default setting does not ignore any terms.\n"}, "213": {"topic": "Understanding min_df and max_df in scikit CountVectorizer", "user_name": "Kevin MarkhamKevin Markham", "text": "\nI would add this point also for understanding min_df and max_df in tf-idf better.\nIf you go with the default values, meaning considering all terms, you have generated definitely more tokens. So your clustering process (or any other thing you want to do with those terms later) will take a longer time. \nBUT the quality of your clustering should NOT be reduced. \nOne might think that allowing all terms (e.g. too frequent terms or stop-words) to be present might lower the quality but in tf-idf it doesn't. Because tf-idf measurement instinctively will give a low score to those terms, effectively making them not influential (as they appear in many documents).\nSo to sum it up, pruning the terms via min_df and max_df is to improve the performance, not the quality of clusters (as an example).\nAnd the crucial point is that if you set the min and max mistakenly, you would lose some important terms and thus lower the quality. So if you are unsure about the right threshold (it depends on your documents set), or if you are sure about your machine's processing capabilities, leave the min, max parameters unchanged.\n"}, "214": {"topic": "Understanding min_df and max_df in scikit CountVectorizer", "user_name": "Fabio says Reinstate Monica", "text": "\nAs per the CountVectorizer documentation here.\nWhen using a float in the range [0.0, 1.0] they refer to the document frequency. That is the percentage of documents that contain the term.\nWhen using an int it refers to absolute number of documents that hold this term.\nConsider the example where you have 5 text files (or documents). If you set max_df = 0.6 then that would translate to 0.6*5=3 documents. If you set max_df = 2 then that would simply translate to 2 documents.\nThe source code example below is copied from Github here and shows how the max_doc_count is constructed from the max_df. The code for min_df is similar and can be found on the GH page.\nmax_doc_count = (max_df\n                 if isinstance(max_df, numbers.Integral)\n                 else max_df * n_doc)\n\nThe defaults for min_df and max_df are 1 and 1.0, respectively. This basically says \"If my term is found in only 1 document, then it's ignored. Similarly if it's found in all documents (100% or 1.0) then it's ignored.\"\nmax_df and min_df are both used internally to calculate max_doc_count and min_doc_count, the maximum and minimum number of documents that a term must be found in. This is then passed to self._limit_features as the keyword arguments high and low respectively, the docstring for self._limit_features is\n\"\"\"Remove too rare or too common features.\n\nPrune features that are non zero in more samples than high or less\ndocuments than low, modifying the vocabulary, and restricting it to\nat most the limit most frequent.\n\nThis does not prune samples with zero features.\n\"\"\"\n\n"}, "215": {"topic": "Understanding min_df and max_df in scikit CountVectorizer", "user_name": "Amirabbas AskaryAmirabbas Askary", "text": "\nThe defaults for min_df and max_df are 1 and 1.0, respectively. These defaults really don't do anything at all.  \nThat being said, I believe the currently accepted answer by @Ffisegydd answer isn't quite correct.\nFor example, run this using the defaults, to see that when min_df=1 and max_df=1.0, then \n1) all tokens that appear in at least one document are used  (e.g., all tokens!)\n2) all tokens that appear in all documents are used (we'll test with one candidate: everywhere).  \ncv = CountVectorizer(min_df=1, max_df=1.0, lowercase=True) \n# here is just a simple list of 3 documents.\ncorpus = ['one two three everywhere', 'four five six everywhere', 'seven eight nine everywhere']\n# below we call fit_transform on the corpus and get the feature names.\nX = cv.fit_transform(corpus)\nvocab = cv.get_feature_names()\nprint vocab\nprint X.toarray()\nprint cv.stop_words_\n\nWe get:  \n[u'eight', u'everywhere', u'five', u'four', u'nine', u'one', u'seven', u'six', u'three', u'two']\n[[0 1 0 0 0 1 0 0 1 1]\n [0 1 1 1 0 0 0 1 0 0]\n [1 1 0 0 1 0 1 0 0 0]]\nset([])\n\nAll tokens are kept. There are no stopwords.  \nFurther messing around with the arguments will clarify other configurations.  \nFor fun and insight, I'd also recommend playing around with stop_words = 'english' and seeing that, peculiarly, all the words except 'seven' are removed! Including `everywhere'.\n"}, "216": {"topic": "Understanding min_df and max_df in scikit CountVectorizer", "user_name": "", "text": "\nThe goal of MIN_DF is to ignore words that have very few occurrences to be considered meaningful. For example, in your text you may have names of people that may appear in only 1 or two documents. In some applications, this may qualify as noise and could be eliminated from further analysis. Similarly, you can ignore words that are too common with MAX_DF.\nInstead of using a minimum/maximum term frequency (total occurrences of a word) to eliminate words, MIN_DF and MAX_DF look at how many documents contained a term, better known as document frequency. The threshold values can be an absolute value (e.g. 1, 2, 3, 4) or a value representing proportion of documents (e.g. 0.25 meaning, ignore words that have appeared in 25% of the documents) .\nSee some usage examples here.\n"}, "217": {"topic": "Understanding min_df and max_df in scikit CountVectorizer", "user_name": "FfisegyddFfisegydd", "text": "\nI just looked at the documentation for sklearn CountVectorizer.This is how I think about it.\nCommon words have higher frequency values, while rare words have lower frequency values. The frequency values range between 0 - 1 as fractions.\nmax_df is the upper ceiling value of the frequency values, while min_df is just the lower cutoff value of the frequency values.\nIf we want to remove more common words, we set max_df to a lower ceiling value between 0 and 1. If we want to remove more rare words, we set min_df to a higher cutoff value between 0 and 1. We keep everything between max_df and min_df.\nLet me know, not sure if this makes sense.\n"}, "218": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "CommunityBot", "text": "\n\n\n\n\n\n\nThis question already has answers here:                    \n\n\nClosed 10 years ago.\n\n\n\n\nPossible Duplicate:\nHow does the Google \u201cDid you mean?\u201d Algorithm work? \n\nSuppose you have a search system already in your website. How can you implement the \"Did you mean:<spell_checked_word>\" like Google does in some search queries?\n"}, "219": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "pekpek", "text": "\nActually what Google does is very much non-trivial and also at first counter-intuitive. They don't do anything like check against a dictionary, but rather they make use of statistics to identify \"similar\" queries that returned more results than your query, the exact algorithm is of course not known.\nThere are different sub-problems to solve here, as a fundamental basis for all Natural Language Processing statistics related there is one must have book: Foundation of Statistical Natural Language Processing.\nConcretely to solve the problem of word/query similarity I have had good results with using Edit Distance, a mathematical measure of string similarity that works surprisingly well. I used to use Levenshtein but the others may be worth looking into.\nSoundex - in my experience - is crap.\nActually efficiently storing and searching a large dictionary of misspelled words and having sub second retrieval is again non-trivial, your best bet is to make use of existing  full text indexing and retrieval engines (i.e. not your database's one), of which Lucene is currently one of the best and coincidentally ported to many many platforms.\n"}, "220": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "Boris TerzicBoris Terzic", "text": "\nGoogle's Dr Norvig has outlined how it works; he even gives a 20ish line Python implementation:\nhttp://googlesystem.blogspot.com/2007/04/simplified-version-of-googles-spell.html\nhttp://www.norvig.com/spell-correct.html\nDr Norvig also discusses the \"did you mean\" in this excellent talk.   Dr Norvig is head of research at Google - when asked how \"did you mean\" is implemented, his answer is authoritive.\nSo its spell-checking, presumably with a dynamic dictionary build from other searches or even actual internet phrases and such.  But that's still spell checking.\nSOUNDEX and other guesses don't get a look in, people!\n"}, "221": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "", "text": "\nCheck this article on wikipedia about the Levenshtein distance. Make sure you take a good look at Possible improvements.\n"}, "222": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "WillWill", "text": "\nI was pleasantly surprised that someone has asked how to create a state-of-the-art spelling suggestion system for search engines. I have been working on this subject for more than a year for a search engine company and I can point to information on the public domain on the subject.\nAs was mentioned in a previous post, Google (and Microsoft and Yahoo!) do not use any predefined dictionary nor do they employ hordes of linguists that ponder over the possible misspellings of queries. That would be impossible due to the scale of the problem but also because it is not clear that people could actually correctly identify when and if a query is misspelled.\nInstead there is a simple and rather effective principle that is also valid for all European languages. Get all the unique queries on your search logs, calculate the edit distance between all pairs of queries, assuming that the reference query is the one that has the highest count. \nThis simple algorithm will work great for many types of queries. If you want to take it to the next level then I suggest you read the paper by Microsoft Research on that subject. You can find it here \nThe paper has a great introduction but after that you will need to be knowledgeable with concepts such as the Hidden Markov Model.\n"}, "223": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "        Ionut AnghelcoviciIonut Anghelcovici", "text": "\nI would suggest looking at SOUNDEX to find similar words in your database.\nYou can also access google own dictionary by using the Google API spelling suggestion request.\n"}, "224": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "Costas BoulisCostas Boulis", "text": "\nYou may want to look at Peter Norvig's \"How to Write a Spelling Corrector\" article.\n"}, "225": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "", "text": "\nI believe Google logs all queries and identifies when someone makes a spelling correction. This correction may then be suggested when others supply the same first query. This will work for any language, in fact any string of any characters.\n"}, "226": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "EspoEspo", "text": "\nhttp://en.wikipedia.org/wiki/N-gram#Google_use_of_N-gram\n"}, "227": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "FA.FA.", "text": "\nI think this depends on how big your website it. On our local Intranet which is used by about 500 member of staff, I simply look at the search phrases that returned zero results and enter that search phrase with the new suggested search phrase into a SQL table.\nI them call on that table if no search results has been returned, however, this only works if the site is relatively small and I only do it for search phrases which are the most common.\nYou might also want to look at my answer to a similar question:\n\n\"Similar Posts\" like functionality using MS SQL Server?\n\n"}, "228": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "LiamLiam", "text": "\nIf you have industry specific translations, you will likely need a thesaurus. For example, I worked in the jewelry industry and there were abbreviate in our descriptions such as kt - karat, rd - round, cwt - carat weight... Endeca (the search engine at that job) has a thesaurus that will translate  from common misspellings, but it does require manual intervention.\n"}, "229": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "robakerrobaker", "text": "\nI do it with Lucene's Spell Checker.\n"}, "230": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "CommunityBot", "text": "\nSoundex is good for phonetic matches, but works best with peoples' names (it was originally developed for census data)\nAlso check out Full-Text-Indexing, the syntax is different from Google logic, but it's very quick and can deal with similar language elements.\n"}, "231": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "GateKillerGateKiller", "text": "\nSoundex and \"Porter stemming\" (soundex is trivial, not sure about porter stemming).\n"}, "232": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "oglesteroglester", "text": "\nThere's something called aspell that might help:\nhttp://blog.evanweaver.com/files/doc/fauna/raspell/classes/Aspell.html\nThere's a ruby gem for it, but I don't know how to talk to it from python\nhttp://blog.evanweaver.com/files/doc/fauna/raspell/files/README.html\nHere's a quote from the ruby implementation\n\nUsage\nAspell lets you check words and suggest corrections. For example:\n  string = \"my haert wil go on\"\n\n  string.gsub(/[\\w\\']+/) do |word|\n    if !speller.check(word)\n      # word is wrong\n      puts \"Possible correction for #{word}:\"\n      puts speller.suggest(word).first\n    end\n  end\n\n\nThis outputs:\nPossible correction for haert:\nheart\nPossible correction for wil:\nWill\n"}, "233": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "cherouvimcherouvim", "text": "\nImplementing spelling correction for search engines in an effective way is not trivial (you can't just compute the edit/levenshtein distance to every possible word). A solution based on k-gram indexes is described in Introduction to Information Retrieval (full text available online).\n"}, "234": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "KeithKeith", "text": "\nU could use ngram for the comparisment: http://en.wikipedia.org/wiki/N-gram\nUsing python ngram module: http://packages.python.org/ngram/index.html\nimport ngram\n\nG2 = ngram.NGram([  \"iis7 configure ftp 7.5\",\n                    \"ubunto configre 8.5\",\n                    \"mac configure ftp\"])\n\nprint \"String\", \"\\t\", \"Similarity\"\nfor i in G2.search(\"iis7 configurftp 7.5\", threshold=0.1):\n    print i[1], \"\\t\", i[0]\n\nU get:\n>>> \nString  Similarity\n0.76    \"iis7 configure ftp 7.5\"    \n0.24    \"mac configure ftp\"\n0.19    \"ubunto configre 8.5\"   \n\n"}, "235": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "Michael NealeMichael Neale", "text": "\nWhy not use google's did you mean in your code.For how see here\nhttp://narenonit.blogspot.com/2012/08/trick-for-using-googles-did-you-mean.html\n"}, "236": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "CommunityBot", "text": "\nI've tried PorterStemmer and Snowball but both don't work on all words, missing some very common ones. \nMy test words are: \"cats running ran cactus cactuses cacti community communities\", and both get less than half right.\nSee also:\n\nStemming algorithm that produces real words\nStemming - code examples or open source projects?\n\n"}, "237": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "manixrockmanixrock", "text": "\nIf you know Python, The Natural Language Toolkit (NLTK) has a very powerful lemmatizer that makes use of WordNet.\nNote that if you are using this lemmatizer for the first time, you must download the corpus prior to using it.  This can be done by:\n>>> import nltk\n>>> nltk.download('wordnet')\n\nYou only have to do this once.  Assuming that you have now downloaded the corpus, it works like this:\n>>> from nltk.stem.wordnet import WordNetLemmatizer\n>>> lmtzr = WordNetLemmatizer()\n>>> lmtzr.lemmatize('cars')\n'car'\n>>> lmtzr.lemmatize('feet')\n'foot'\n>>> lmtzr.lemmatize('people')\n'people'\n>>> lmtzr.lemmatize('fantasized','v')\n'fantasize'\n\nThere are other lemmatizers in the nltk.stem module, but I haven't tried them myself.\n"}, "238": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "rayryeng", "text": "\nI use stanford nlp to perform lemmatization. I have been stuck up with a similar problem in the last few days. All thanks to stackoverflow to help me solve the issue .  \nimport java.util.*; \nimport edu.stanford.nlp.pipeline.*;\nimport edu.stanford.nlp.ling.*; \nimport edu.stanford.nlp.ling.CoreAnnotations.*;  \n\npublic class example\n{\n    public static void main(String[] args)\n    {\n        Properties props = new Properties(); \n        props.put(\"annotators\", \"tokenize, ssplit, pos, lemma\"); \n        pipeline = new StanfordCoreNLP(props, false);\n        String text = /* the string you want */; \n        Annotation document = pipeline.process(text);  \n\n        for(CoreMap sentence: document.get(SentencesAnnotation.class))\n        {    \n            for(CoreLabel token: sentence.get(TokensAnnotation.class))\n            {       \n                String word = token.get(TextAnnotation.class);      \n                String lemma = token.get(LemmaAnnotation.class); \n                System.out.println(\"lemmatized version :\" + lemma);\n            }\n        }\n    }\n}\n\nIt also might be a good idea to use stopwords to minimize output lemmas if it's used later in classificator. Please take a look at coreNlp extension written by John Conwell.\n"}, "239": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "theycallmemortytheycallmemorty", "text": "\nI tried your list of terms on this snowball demo site and the results look okay.... \n\ncats -> cat \nrunning -> run \nran -> ran\ncactus -> cactus \ncactuses -> cactus\ncommunity -> communiti \ncommunities -> communiti\n\nA stemmer is supposed to turn inflected forms of words down to some common root. It's not really a stemmer's job to make that root a 'proper' dictionary word. For that you need to look at morphological/orthographic analysers.\nI think this question is about more or less the same thing, and Kaarel's answer to that question is where I took the second link from.\n"}, "240": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "expert", "text": "\nThe stemmer vs lemmatizer debates goes on. It's a matter of preferring precision over efficiency. You should lemmatize to achieve linguistically meaningful units and stem to use minimal computing juice and still index a word and its variations under the same key. \nSee Stemmers vs Lemmatizers\nHere's an example with python NLTK:\n>>> sent = \"cats running ran cactus cactuses cacti community communities\"\n>>> from nltk.stem import PorterStemmer, WordNetLemmatizer\n>>>\n>>> port = PorterStemmer()\n>>> \" \".join([port.stem(i) for i in sent.split()])\n'cat run ran cactu cactus cacti commun commun'\n>>>\n>>> wnl = WordNetLemmatizer()\n>>> \" \".join([wnl.lemmatize(i) for i in sent.split()])\n'cat running ran cactus cactus cactus community community'\n\n"}, "241": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "CTsiddharthCTsiddharth", "text": "\nMartin Porter's official page contains a Porter Stemmer in PHP as well as other languages.\nIf you're really serious about good stemming though you're going to need to start with something like the Porter Algorithm, refine it by adding rules to fix incorrect cases common to your dataset, and then finally add a lot of exceptions to the rules.  This can be easily implemented with key/value pairs (dbm/hash/dictionaries) where the key is the word to look up and the value is the stemmed word to replace the original. A commercial search engine I worked on once ended up with 800 some exceptions to a modified Porter algorithm.\n"}, "242": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "CommunityBot", "text": "\nBased on various answers on Stack Overflow and blogs I've come across, this is the method I'm using, and it seems to return real words quite well. The idea is to split the incoming text into an array of words (use whichever method you'd like), and then find the parts of speech (POS) for those words and use that to help stem and lemmatize the words.\nYou're sample above doesn't work too well, because the POS can't be determined. However, if we use a real sentence, things work much better.\nimport nltk\nfrom nltk.corpus import wordnet\n\nlmtzr = nltk.WordNetLemmatizer().lemmatize\n\n\ndef get_wordnet_pos(treebank_tag):\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\n\ndef normalize_text(text):\n    word_pos = nltk.pos_tag(nltk.word_tokenize(text))\n    lemm_words = [lmtzr(sw[0], get_wordnet_pos(sw[1])) for sw in word_pos]\n\n    return [x.lower() for x in lemm_words]\n\nprint(normalize_text('cats running ran cactus cactuses cacti community communities'))\n# ['cat', 'run', 'ran', 'cactus', 'cactuses', 'cacti', 'community', 'community']\n\nprint(normalize_text('The cactus ran to the community to see the cats running around cacti between communities.'))\n# ['the', 'cactus', 'run', 'to', 'the', 'community', 'to', 'see', 'the', 'cat', 'run', 'around', 'cactus', 'between', 'community', '.']\n\n"}, "243": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "StompchickenStompchicken", "text": "\nhttp://wordnet.princeton.edu/man/morph.3WN\nFor a lot of my projects, I prefer the lexicon-based WordNet lemmatizer over the more aggressive porter stemming. \nhttp://wordnet.princeton.edu/links#PHP has a link to a PHP interface to the WN APIs.\n"}, "244": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "CommunityBot", "text": "\nLook into WordNet, a large lexical database for the English language:\nhttp://wordnet.princeton.edu/\nThere are APIs for accessing it in several languages.\n"}, "245": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "alvasalvas", "text": "\nThis looks interesting:\nMIT Java WordnetStemmer:\nhttp://projects.csail.mit.edu/jwi/api/edu/mit/jwi/morph/WordnetStemmer.html\n"}, "246": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "Van GaleVan Gale", "text": "\nTake a look at LemmaGen - open source library written in C# 3.0.\nResults for your test words (http://lemmatise.ijs.si/Services)\n\ncats -> cat\nrunning\nran -> run\ncactus\ncactuses -> cactus\ncacti -> cactus\ncommunity\ncommunities -> community\n\n"}, "247": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "", "text": "\nThe top python packages (in no specific order) for lemmatization are: spacy, nltk, gensim, pattern, CoreNLP and TextBlob. I prefer spaCy and gensim's implementation (based on pattern) because they identify the POS tag of the word and assigns the appropriate lemma automatically. The gives more relevant lemmas, keeping the meaning intact.\nIf you plan to use nltk or TextBlob, you need to take care of finding the right POS tag manually and the find the right lemma. \nLemmatization Example with spaCy:\n# Run below statements in terminal once. \npip install spacy\nspacy download en\n\nimport spacy\n\n# Initialize spacy 'en' model\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\nsentence = \"The striped bats are hanging on their feet for best\"\n\n# Parse\ndoc = nlp(sentence)\n\n# Extract the lemma\n\" \".join([token.lemma_ for token in doc])\n#> 'the strip bat be hang on -PRON- foot for good'\n\nLemmatization Example With Gensim:\nfrom gensim.utils import lemmatize\nsentence = \"The striped bats were hanging on their feet and ate best fishes\"\nlemmatized_out = [wd.decode('utf-8').split('/')[0] for wd in lemmatize(sentence)]\n#> ['striped', 'bat', 'be', 'hang', 'foot', 'eat', 'best', 'fish']\n\nThe above examples were borrowed from in this lemmatization page.\n"}, "248": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "cjbarthcjbarth", "text": "\nIf I may quote my answer to the question StompChicken mentioned:\nThe core issue here is that stemming algorithms operate on a phonetic basis with no actual understanding of the language they're working with.\nAs they have no understanding of the language and do not run from a dictionary of terms, they have no way of recognizing and responding appropriately to irregular cases, such as \"run\"/\"ran\".\nIf you need to handle irregular cases, you'll need to either choose a different approach or augment your stemming with your own custom dictionary of corrections to run after the stemmer has done its thing.\n"}, "249": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "msbmsbmsbmsb", "text": "\nThe most current version of the stemmer in NLTK is Snowball.\nYou can find examples on how to use it here:\nhttp://nltk.googlecode.com/svn/trunk/doc/api/nltk.stem.snowball2-pysrc.html#demo \n"}, "250": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "Ricardo J. M\u00e9ndezRicardo J. M\u00e9ndez", "text": "\nYou could use the Morpha stemmer.  UW has uploaded morpha stemmer to Maven central if you plan to use it from a Java application.  There's a wrapper that makes it much easier to use.  You just need to add it as a dependency and use the edu.washington.cs.knowitall.morpha.MorphaStemmer class.  Instances are threadsafe (the original JFlex had class fields for local variables unnecessarily).  Instantiate a class and run morpha and the word you want to stem.\nnew MorphaStemmer().morpha(\"climbed\") // goes to \"climb\"\n\n"}, "251": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "user382903user382903", "text": "\nDo a search for Lucene, im not sure if theres a PHP port but I do know Lucene is available for many platforms. Lucene is an OSS (from Apache) indexing and search library. Naturally it and community extras might have something interesting to look at. At the very least you can learn how it's done in one language so you can translate the \"idea\" into PHP.\n"}, "252": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "AlexAlex", "text": "\n.Net lucene has an inbuilt porter stemmer. You can try that. But note that porter stemming does not consider word context when deriving the lemma. (Go through the algorithm and its implementation and you will see how it works)\n"}, "253": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "SelvaSelva", "text": "\nMartin Porter wrote Snowball (a language for stemming algorithms) and rewrote the \"English Stemmer\" in Snowball. There are is an English Stemmer for C and Java.\nHe explicitly states that the Porter Stemmer has been reimplemented only for historical reasons, so testing stemming correctness against the Porter Stemmer will get you results that you (should) already know.\n\nFrom http://tartarus.org/~martin/PorterStemmer/index.html (emphasis mine)\nThe Porter stemmer should be regarded as \u2018frozen\u2019, that is, strictly defined, and not amenable to further modification. As a stemmer, it is slightly inferior to the Snowball English or Porter2 stemmer, which derives from it, and which is subjected to occasional improvements. For practical work, therefore, the new Snowball stemmer is recommended. The Porter stemmer is appropriate to IR research work involving stemming where the experiments need to be exactly repeatable.\n\nDr. Porter suggests to use the English or Porter2 stemmers instead of the Porter stemmer. The English stemmer is what's actually used in the demo site as @StompChicken has answered earlier.\n"}, "254": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "Dave SherohmanDave Sherohman", "text": "\nIn Java, i use tartargus-snowball to stemming words\nMaven:\n<dependency>\n        <groupId>org.apache.lucene</groupId>\n        <artifactId>lucene-snowball</artifactId>\n        <version>3.0.3</version>\n        <scope>test</scope>\n</dependency>\n\nSample code:\nSnowballProgram stemmer = new EnglishStemmer();\nString[] words = new String[]{\n    \"testing\",\n    \"skincare\",\n    \"eyecare\",\n    \"eye\",\n    \"worked\",\n    \"read\"\n};\nfor (String word : words) {\n    stemmer.setCurrent(word);\n    stemmer.stem();\n    //debug\n    logger.info(\"Origin: \" + word + \" > \" + stemmer.getCurrent());// result: test, skincar, eyecar, eye, work, read\n}\n\n"}, "255": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "EdmonEdmon", "text": "\nTry this one here: http://www.twinword.com/lemmatizer.php\nI entered your query in the demo \"cats running ran cactus cactuses cacti community communities\" and got [\"cat\", \"running\", \"run\", \"cactus\", \"cactus\", \"cactus\", \"community\", \"community\"] with the optional flag ALL_TOKENS.\nSample Code\nThis is an API so you can connect to it from any environment. Here is what the PHP REST call may look like.\n// These code snippets use an open-source library. http://unirest.io/php\n$response = Unirest\\Request::post([ENDPOINT],\n  array(\n    \"X-Mashape-Key\" => [API KEY],\n    \"Content-Type\" => \"application/x-www-form-urlencoded\",\n    \"Accept\" => \"application/json\"\n  ),\n  array(\n    \"text\" => \"cats running ran cactus cactuses cacti community communities\"\n  )\n);\n\n"}, "256": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "schmmdschmmd", "text": "\nI highly recommend using Spacy (base text parsing & tagging) and Textacy (higher level text processing built on top of Spacy).\nLemmatized words are available by default in Spacy as a token's .lemma_ attribute and text can be lemmatized while doing a lot of other text preprocessing with textacy.  For example while creating a bag of terms or words or generally just before performing some processing that requires it.\nI'd encourage you to check out both before writing any code, as this may save you a lot of time!\n"}, "257": {"topic": "How do I do word Stemming or Lemmatization?", "user_name": "Wai Ha Lee", "text": "\ndf_plots = pd.read_excel(\"Plot Summary.xlsx\", index_col = 0)\ndf_plots\n# Printing first sentence of first row and last sentence of last row\nnltk.sent_tokenize(df_plots.loc[1].Plot)[0] + nltk.sent_tokenize(df_plots.loc[len(df)].Plot)[-1]\n\n# Calculating length of all plots by words\ndf_plots[\"Length\"] = df_plots.Plot.apply(lambda x : \nlen(nltk.word_tokenize(x)))\n\nprint(\"Longest plot is for season\"),\nprint(df_plots.Length.idxmax())\n\nprint(\"Shortest plot is for season\"),\nprint(df_plots.Length.idxmin())\n\n\n\n#What is this show about? (What are the top 3 words used , excluding the #stop words, in all the #seasons combined)\n\nword_sample = list([\"struggled\", \"died\"])\nword_list = nltk.pos_tag(word_sample)\n[wnl.lemmatize(str(word_list[index][0]), pos = word_list[index][1][0].lower()) for index in range(len(word_list))]\n\n# Figure out the stop words\nstop = (stopwords.words('english'))\n\n# Tokenize all the plots\ndf_plots[\"Tokenized\"] = df_plots.Plot.apply(lambda x : nltk.word_tokenize(x.lower()))\n\n# Remove the stop words\ndf_plots[\"Filtered\"] = df_plots.Tokenized.apply(lambda x : (word for word in x if word not in stop))\n\n# Lemmatize each word\nwnl = WordNetLemmatizer()\ndf_plots[\"POS\"] = df_plots.Filtered.apply(lambda x : nltk.pos_tag(list(x)))\n# df_plots[\"POS\"] = df_plots.POS.apply(lambda x : ((word[1] = word[1][0] for word in word_list) for word_list in x))\ndf_plots[\"Lemmatized\"] = df_plots.POS.apply(lambda x : (wnl.lemmatize(x[index][0], pos = str(x[index][1][0]).lower()) for index in range(len(list(x)))))\n\n\n\n#Which Season had the highest screenplay of \"Jesse\" compared to \"Walt\"\u00a0\n#Screenplay of Jesse =(Occurences of \"Jesse\")/(Occurences of \"Jesse\"+ #Occurences of \"Walt\")\n\ndf_plots.groupby(\"Season\").Tokenized.sum()\n\ndf_plots[\"Share\"] = df_plots.groupby(\"Season\").Tokenized.sum().apply(lambda x : float(x.count(\"jesse\") * 100)/float(x.count(\"jesse\") + x.count(\"walter\") + x.count(\"walt\")))\n\nprint(\"The highest times Jesse was mentioned compared to Walter/Walt was in season\"),\nprint(df_plots[\"Share\"].idxmax())\n#float(df_plots.Tokenized.sum().count('jesse')) * 100 / #float((df_plots.Tokenized.sum().count('jesse') + #df_plots.Tokenized.sum().count('walt') + #df_plots.Tokenized.sum().count('walter')))\n\n"}, "258": {"topic": "gensim word2vec: Find number of words in vocabulary", "user_name": "kmario23", "text": "\nAfter training a word2vec model using python gensim, how do you find the number of words in the model's vocabulary?\n"}, "259": {"topic": "gensim word2vec: Find number of words in vocabulary", "user_name": "hlin117hlin117", "text": "\nIn recent versions, the model.wv property holds the words-and-vectors, and can itself can report a length \u2013 the number of words it contains. So if w2v_model is your Word2Vec (or Doc2Vec or FastText) model, it's enough to just do:\nvocab_len = len(w2v_model.wv)\n\nIf your model is just a raw set of word-vectors, like a KeyedVectors instance rather than a full Word2Vec/etc model, it's just:\nvocab_len = len(kv_model)\n\nOther useful internals in Gensim 4.0+ include model.wv.index_to_key, a plain list of the key (word) in each index position, and model.wv.key_to_index, a plain dict mapping keys (words) to their index positions.\nIn pre-4.0 versions, the vocabulary was in the vocab field of the Word2Vec model's wv property, as a dictionary, with the keys being each token (word). So there it was just the usual Python for getting a dictionary's length:\nlen(w2v_model.wv.vocab)\n\nIn very-old gensim versions before 0.13 vocab appeared directly on the model. So way back then you would use w2v_model.vocab instead of w2v_model.wv.vocab.\nBut if you're still using anything from before Gensim 4.0, you should definitely upgrade! There are big memory & performance improvements, and the changes required in calling code are relatively small \u2013 some renamings & moves, covered in the 4.0 Migration Notes.\n"}, "260": {"topic": "gensim word2vec: Find number of words in vocabulary", "user_name": "", "text": "\nOne more way to get the vocabulary size is from the embedding matrix itself as in:\nIn [33]: from gensim.models import Word2Vec\n\n# load the pretrained model\nIn [34]: model = Word2Vec.load(pretrained_model)\n\n# get the shape of embedding matrix    \nIn [35]: model.wv.vectors.shape\nOut[35]: (662109, 300)\n\n# `vocabulary_size` is just the number of rows (i.e. axis 0)\nIn [36]: model.wv.vectors.shape[0]\nOut[36]: 662109\n\n"}, "261": {"topic": "gensim word2vec: Find number of words in vocabulary", "user_name": "gojomogojomo", "text": "\nGojomo's answer raises an AttributeError for Gensim 4.0.0+.\nFor these versions, you can get the length of the vocabulary as follows:\nlen(w2v_model.wv.index_to_key)\n(which is slightly faster than: len(w2v_model.wv.key_to_index))\n"}, "262": {"topic": "gensim word2vec: Find number of words in vocabulary", "user_name": "kmario23kmario23", "text": "\nLatest:\nUse model.wv.key_to_index, after creating gensim model\nvocab dict became key_to_index for looking up a key's integer index, or get_vecattr() and set_vecattr() for other per-key attributes:https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#4-vocab-dict-became-key_to_index-for-looking-up-a-keys-integer-index-or-get_vecattr-and-set_vecattr-for-other-per-key-attributes\n"}, "263": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\n\n\n\n\n\n\n\n\n\r\nAs it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.                    \n\n\nClosed 11 years ago.\n\n\n\nI mean, is there a coded language with human style coding?\nFor example:\nCreate an object called MyVar and initialize it to 10;\nTake MyVar and call MyMethod() with parameters. . .\n\nI know it's not so useful, but it can be interesting to create such a grammar.\n"}, "264": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nHow about LOLCODE?\nHAI\nCAN HAS STDIO?\nVISIBLE \"HAI WORLD!\"\nKTHXBYE\n\nSimplicity itself!\n"}, "265": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nCOBOL is a lot like that.\nSET MYVAR TO 10.\nEXECUTE MYMETHOD with 10, MYVAR.\n\nAnother sample from Wikipedia:\nADD YEARS TO AGE.\nMULTIPLY PRICE BY QUANTITY GIVING COST.\nSUBTRACT DISCOUNT FROM COST GIVING FINAL-COST.\n\nOddly enough though, despite its design to be readable as English, most programmers completely undermined this with bizarre naming conventions:\nSET VAR_00_MYVAR_PIC99 TO 10.\nEXECUTE PROC_10_MYMETHOD with 10, VAR_00_MYVAR_PIC99.\n\n"}, "266": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nInform 7\nInform 7 is perhaps the language I feel is most appropriately designed in a human language fashion. It is quite application specific for writing adventure games.\nIt is based on rule-based semantics, where you write a lot of rules describing the relationship between objects and their location. For instance, the section below is an Inform 7 program:\n\"Hello Deductible\" by \"I.F. Author\"\n\nThe story headline is \"An Interactive Example\".\n\nThe Living Room is a room. \"A comfortably furnished living room.\"\nThe Kitchen is north of the Living Room.\nThe Front Door is south of the Living Room.\nThe Front Door is a door. The Front Door is closed and locked.\n\nThe insurance salesman is a man in the Living Room. The description is \"An insurance salesman in a tacky polyester suit. He seems eager to speak to you.\" Understand \"man\" as the insurance salesman.\n\nA briefcase is carried by the insurance salesman. The description is \"A slightly worn, black briefcase.\"  Understand \"case\" as the briefcase.\n\nThe insurance paperwork is in the briefcase. The description is \"Page after page of small legalese.\" Understand \"papers\" or \"documents\" or \"forms\" as the paperwork.\n\nInstead of listening to the insurance salesman: \n    say \"The salesman bores you with a discussion of life  insurance policies.  From his briefcase he pulls some paperwork which he hands to you.\";\n    move the insurance paperwork to the player.\n\nExample cited from Wikipedia\n"}, "267": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nAppleScript is pretty close to that, though that is obviously platform dependent.\nHere's a script for opening iTunes and playing a playlist\ntell application \"iTunes\"\n    activate\n    play playlist \"Party Shuffle\"\nend tell\n\nSource: AppleScript Examples\n"}, "268": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\n\nProjects promoting programming in\n  \"natural language\" are intrinsically\n  doomed to fail.\n\n-- Edsger W.Dijkstra, How do we tell truths that might hurt?\n"}, "269": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nThis was \"the next big thing\" around about the early 1980s and I spent much of my first couple of years as a a coder working in \"NATURAL\", which was the supposedly the best of the new crop of 4GLs (fourth generation languages) which were designed to make data access (in this case to an ADABAS database) human readable.\nOf course it did absolutely nothing of the type.  All we ended up with was verbose badly structured code.  Both of these products are still around, but you've never heard of them, which sort of proves the what a dead end it was.\nActually at that period there appeared to be a general desire to move beyond 'programming' into some sort of 2001 inspired AI heaven.  Oracle were really keen on code generation and I remember with some interest a product called 'the last one' that was being marketed to managers as a product that would automatically generate any program you wanted and make all your programming staff redundant.  Seems not to have lived up to expectations ;-)\nIt's worth remembering to that SQL was originally marketed in some quarters as a way to allow management to directly query their data.  I was even sent on a course to learn basic SQL (in a large national transport organization that ran on rails - the steel variety) where junior management types were included because they had plans to put basic query tools in their hands.  What a disaster that was.\nMaybe it might be different in 50 years, but at the current stage of play coding demands a certain clarity of thought and implementation which is best mediated through a dedicated syntax designed for those ends, not any approximation to a natural language which is unclear and ambiguous.  The nearest approximation is possibly physics where the essence of the subject is in the mathematics used (think a programming language for physics) not verbose wordage. \nADDED\nI was forgetting, apart from COBOL there was also PL/1, sometime credited with allowing NASA to put a man on the moon it was just as verbose as COBOL and tried even harder to be 'Manager-readable'.  Which is why no-one has really heard of it now either :-)\n"}, "270": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nChef! Anyone can read recipes right? Behold hello world!\nIngredients.\n72 g haricot beans\n101 eggs\n108 g lard\n111 cups oil\n32 zucchinis\n119 ml water\n114 g red salmon\n100 g dijon mustard\n33 potatoes\n\nMethod.\nPut potatoes into the mixing bowl. Put dijon mustard into the mixing bowl. \nPut lard into the mixing bowl. Put red salmon into the mixing bowl. Put oil into the mixing bowl. \nPut water into the mixing bowl. Put zucchinis into the mixing bowl. Put oil into the mixing bowl. \nPut lard into the mixing bowl. Put lard into the mixing bowl. Put eggs into the mixing bowl. \nPut haricot beans into the mixing bowl. Liquefy contents of the mixing bowl. \nPour contents of the mixing bowl into the baking dish.\n\nSorry if it's not a serious answer, but this is way awesome. :-)\n"}, "271": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nAll languages are 'human readable'. :) How else would someone be able to create it? That being said, languages that support DSLs can be incredibly intuitive such as Boo.\n"}, "272": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nHaving a programming language read like a (verbose) normal language, would be like requiring people to converse all the time in legalese.  All the extra verbiage just gets in the way.\nAn ideal programming language should have syntax that is as transparent as possible and let the concepts behind the program stand out.  Obviously there is a trade off between having a quick learning curve and having minimal but obscure syntax (think Perl, or even K).\n"}, "273": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nBy creating a set of rules, it is possible to do logic programming in Prolog like this.  You can build a grammar (or download one) for a particular domain, create a knowledge base and then query it.  After defining your grammar you could do something like:\nbob is a parent of tim.\nmary is a parent of bob.\n\n?- X is a grandparent of tim.\nX = mary\n\n?- jim is a parent of bob.\nfalse\n\n"}, "274": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nI see the Shakespeare programming language have yet to be mentioned.\nThese programs are coded to look like shakespear plays, the individial characters in the play being variables that can hold numbers and the various phrases in the play manipulate the characters and the number they hold. For instance, \"Speak your mind\" orders a character to output his value.\n"}, "275": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nApplescript:\ntell application \"Finder\"\n set the percent_free to \u00ac\n (((the free space of the startup disk) / (the capacity of the startup disk)) * 100) div 1\nend tell\nif the percent_free is less than 10 then\n tell application (path to frontmost application as text)\n display dialog \"The startup disk has only \" & the percent_free & \u00ac\n \" percent of its capacity available.\" & return & return & \u00ac\n \"Should this script continue?\" with icon 1\n end tell\nend if\n\n"}, "276": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nI can read C. That means it's human-readable(because I'm a human). It's just too terse for the average person. The general concept of programming languages is to maximize the information about how the computer should operate in a given line.\nThis is why Ruby is so popular; it maximizes the functionality in minimal text. English(or any other other natural language) is a pretty imprecise, low-information/character language.\nIn sum, it is: (i)done before and (ii)a known weaker idea. \n"}, "277": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nThis is actually a hot topic.\nFor starters - What is Human readable?\nA Chinese-reader cannot read Russian and vice versa. \nIt you narrow your domain for example to Chinese pharmacists writing a perscription you could design a language around that. And that would be human readable.\nSuch as language would fall under a the umbrella of Domain Specific Languages.\n"}, "278": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nSQL\nSELECT name, address FROM customers WHERE region = 'Europe'\n\n"}, "279": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nYes.  It's called COBOL, and people generally detest it.\n"}, "280": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nHyperTalk and its descendant AppleScript were designed to be similar to the English language.\n"}, "281": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nInform 7 is the most successful such system I've seen. It has two advantages over the cruder systems listed in other answers here: it's for a domain particularly appropriate for natural language (interactive fiction), and it does a fancier analysis of the input code based on more computational-linguistics lore, not just a conventional programming-language grammar that happens to use English words instead of braces, etc.\n"}, "282": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nPerl, some people claim.\nprint \"hello!\" and open my $File, '<', $path or die \"Couldn't open the file after saying hello!\";\n\n"}, "283": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nDo a google search for \"natural language programming\" and you'll find lots of information (including why this is a bad idea).\n"}, "284": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nClarity of Expression is important.\nBut Clarity of Thought is far, far more important.\n"}, "285": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nVB is as close as I can think of one:\nIf MyLife.Sucks Then MyLife.End Else MyLife.Continue\n"}, "286": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nSure, Erlang.\n-module(listsort).\n-export([by_length/1]).\n\n by_length(Lists) ->\n    F = fun(A,B) when is_list(A), is_list(B) ->\n            length(A) < length(B)\n        end,\n    qsort(Lists, F).\n\n qsort([], _)-> [];\n qsort([Pivot|Rest], Smaller) ->\n     qsort([ X || X <- Rest, Smaller(X,Pivot)], Smaller)\n     ++ [Pivot] ++\n     qsort([ Y ||Y <- Rest, not(Smaller(Y, Pivot))], Smaller).\n\nI'm a human, it's a programming language, and I can read it. I don't know what any of it means, but I see a lot of English words in there, I think.\n(Tongue firmly in cheek.)\n"}, "287": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nDSLs can be very natural-looking. See this example created with MGrammar:\ntest \"Searching google for watin\"\n    goto \"http://www.google.se\"\n    type \"watin\" into \"q\"\n    click \"btnG\"\n    assert that text \"WatiN Home\" exists\n    assert that element \"res\" exists\nend\n\n"}, "288": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nCOBOL was intended to be read by managers, and has \"noise words\" to make it more readable.\nThe funny thing is, it reads a bit like a verbose DSL.\n"}, "289": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nBeing more human-readable than most was one of the early selling points of Ada. I find it a silly argument these days, as any sufficently complex task in any language is going to require a competent practicioner to understand. However, it does beat the bejeezus out of C-syntax languages. Its dominant coding styles can enhance this effect too. For example, comparing loops in an if statement:\nAda:\nif Time_To_Loop then\n   for i in Some_Array loop\n      Some_Array(i) := i;\n   end loop;\nend if;\n\nC:\nif (timeToLoop != 0) {\n   for (int i=0;i<SOME_ARRAY_LENGTH;i++) {\n      someArray[i] = i;\n   }\n}\n\nThe C code would look even worse if I used Hungarian notation like Microsoft, but I'm trying to be nice. :-)\n"}, "290": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nInteresting question. Your question can be read as \"Is there any programming language that is easily readable by humans?\", OR ELSE as \"Is there a human language that can be used for programming?\". All the answers here have focused on the former, so let me try answering the latter.\nHave you heard of Sanskrit? It is an ancient Indian language on which modern Indian languages like Hindi are based.\nwiki/Sanskrit\nI've been hearing for years that it is precise and complete enough to be used, as it is, as a high-level language on a computer. Ofcourse, you need a compiler to convert Sanskrit instructions to machine language.  I know the script & yes, it is precise (entirely phonetic so you never have to ask \"how do you spell that\"), but I don't know the grammer well enough.\nThis is completeley anecdotal, so I don't vouch for the accuracy of this. Just wanted to share what I know regarding this. :-)\n"}, "291": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nI agree with the general consensus here. \"Human readable\" general purpose programming languages are mostly a bad idea, but human readable Domain Specific Languages are very worthwhile.\nREBOL has a great system for creating DSLs.\n"}, "292": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nGradStudent\nIt only has one statement: \"you - write me a program to do x\"\nIt's valid for all values of X and has the advantage that x doesn't have to be defined and can be changed after the program is written.\nA commercial dialect is available called intern: development cost is lower but it isn't guaranteed to work\n"}, "293": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nCobol was kind of like that.\n"}, "294": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "Fred Foo", "text": "\nI have a .NET application where, given a noun, I want it to correctly prefix that word with \"a\" or \"an\". How would I do that?\nBefore you think the answer is to simply check if the first letter is a vowel, consider phrases like:\n\nan honest mistake\na used car\n\n"}, "295": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "ryeguyryeguy", "text": "\n\nDownload Wikipedia\nUnzip it and write a quick filter program that spits out only article text (the download is generally in XML format, along with non-article metadata too).\nFind all instances of a(n).... and make an index on the following word and all of its prefixes (you can use a simple suffixtrie for this). This should be case sensitive, and you'll need a maximum word-length - 15 letters?\n(optional) Discard all those prefixes which occur less than 5 times or where \"a\" vs. \"an\" achieves less than 2/3 majority (or some other threshholds - tweak here).  Preferably keep the empty prefix to avoid corner-cases.\nYou can optimize your prefix database by discarding all those prefixes whose parent shares the same \"a\" or \"an\" annotation.\nWhen determining whether to use \"A\" or \"AN\" find the longest matching prefix, and follow its lead.  If you didn't discard the empty prefix in step 4, then there will always be a matching prefix (namely the empty prefix), otherwise you may need a special case for a completely-non matching string (such input should be very rare).\n\nYou probably can't get much better than this - and it'll certainly beat most rule-based systems.\nEdit: I've implemented this in JS/C#.  You can try it in your browser, or download the small, reusable javascript implementation it uses.  The .NET implementation is package AvsAn on nuget.  The implementations are trivial, so it should be easy to port to any other language if necessary.\nTurns out the \"rules\" are quite a bit more complex than I thought:\n\nit's an unanticipated result but it's a unanimous vote\nit's an honest decision but a honeysuckle shrub\nSymbols: It's an 0800 number, or an \u221e of oregano.\nAcronyms: It's a NASA scientist, but an NSA analyst; a FIAT car but an FAA policy.\n\n...which just goes to underline that a rule based system would be tricky to build!\n"}, "296": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "Gary.Ray", "text": "\nYou need to use a list of exceptions. I don't think all of the exceptions are well defined, because it sometimes depends on the accent of the person saying the word.\nOne stupid way is to ask Google for the two possibilities (using the one of the search APIs) and use the most popular:\n\nhttp://www.google.co.uk/search?q=%22a+europe%22 - 841,000 hits\nhttp://www.google.co.uk/search?q=%22an+europe%22 - 25,000 hits\n\nOr:\n\nhttp://www.google.co.uk/search?q=%22a+honest%22 - 797,000 hits\nhttp://www.google.co.uk/search?q=%22an+honest%22 - 8,220,000 hits\n\nTherefore \"a europe\" and \"an honest\" are the correct versions.\n"}, "297": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "Eamon NerbonneEamon Nerbonne", "text": "\nIf you could find a source of word spellings to word pronunciations, like:\n\"honest\":\"on-ist\"\n\"horrible\":\"hawr-uh-buhl, hor-\"\n\nYou could base your decision on the first character of the spelled pronunciation string.\nFor performance, perhaps you could use such a lookup to pre-generate exception sets and use those smaller lookup sets during execution instead.\nEdited to add:\n!!! - I think you could use this to generate your exceptions:\nhttp://www.speech.cs.cmu.edu/cgi-bin/cmudict\nNot everything will be in the dictionary, of course - meaning not every possible exception would wind up in your exceptions sets - but in that case, you could just default to an for vowels/ a for consonants or use some other heuristic with better odds.\n(Looking through the CMU dictionary, I was pleased to see it includes proper nouns for countries and some other places - so it will hande examples like \"a Ukrainian\", \"a USA Today paper\", \"a Urals-inspired painting\".)\nEditing once more to add:  The CMU dictionary does not contain common acronyms, and you have to worry about those starting with s,f,l,m,n,u,and x.  But there are plenty of acronym lists out there, like in Wikipedia, which you could use to add to the exceptions.\n"}, "298": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "", "text": "\nYou have to implemented manually and add the exceptions you want like for example if the first letter is 'H' and followed by an 'O' like honest, hour ... and also the opposite ones like europe, university, used ...\n"}, "299": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "rjmunrorjmunro", "text": "\nSince \"a\" and \"an\" is determined by phonetic rules and not spelling conventions, I would probably do it like this:\n\nIf the first letter of the word is a consonant -> 'a'\nIf the first letter of the word is a vowel-> 'an'\nKeep a list of exceptions (heart, x-ray, house) as rjumnro says.\n\n"}, "300": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "", "text": "\nYou need to look at the grammatical rules for indefinite articles (there are only two indefinite articles in English grammar - \"a\" and \"an). You may not agree these sound correct, but the rules of English grammar are very clear:\n\n\"The words a and an are indefinite\n  articles. We use the indefinite\n  article an before words that begin\n  with a vowel sound (a, e, i, o, u) and\n  the indefinite article a before words\n  that begin with a consonant sound (all\n  other letters).\"\n\nNote this means a vowel sound, and not a vowel letter. For instance, words beginning with a silent \"h\", such as \"honour\" or \"heir\" are treated as vowels an so are proceeded with \"an\" - for example, \"It is an honour to meet you\". Words beginning with a consonant sound are prefixed with a - which is why you say \"a used car\" rather than \"an used car\" - because \"used\" has a \"yoose\" sound rather than a \"uhh\" sound.\nSo, as a programmer, these are the rules to follow. You just need to work out a way of determining what sound a word begins with, rather than what letter. I've seen examples of this, such as this one in PHP by Jaimie Sirovich :\nfunction aOrAn($next_word) \n{ \n    $_an = array('hour', 'honest', 'heir', 'heirloom'); \n    $_a = array('use', 'useless', 'user'); \n    $_vowels = array('a','e','i','o','u'); \n\n    $_endings = array('ly', 'ness', 'less', 'lessly', 'ing', 'ally', 'ially'); \n    $_endings_regex = implode('|', $_endings); \n\n    $tmp = preg_match('#(.*?)(-| |$)#', $next_word, $captures); \n    $the_word = trim($captures[1]); \n    //$the_word = Format::trimString(Utils::pregGet('#(.*?)(-| |$)#', $next_word, 1)); \n\n    $_an_regex = implode('|', $_an); \n    if (preg_match(\"#($_an_regex)($_endings_regex)#i\", $the_word)) { \n        return 'an'; \n    } \n\n    $_a_regex = implode('|', $_a); \n    if (preg_match(\"#($_a_regex)($_endings_regex)#i\", $the_word)) { \n        return 'a'; \n    } \n\n    if (in_array(strtolower($the_word{0}), $_vowels)) { \n        return 'an';     \n    } \n\n    return 'a'; \n}\n\nIt's probably easiest to create the rule and then create a list of exceptions and use that. I don't imagine there will be that many.\n"}, "301": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "AnonAnon", "text": "\nMan, I realize that this is probably a settled argument, but I think it can be settled easier than using ad hoc grammar rules from Wikipedia, which would derive vernacular grammar, at best. \nThe best solution, it seems, is to have the use of a or an trigger a phoneme-based matching of the following word, with certain phonemes always associated with \"an\" and the remaining belonging to \"a\". \nCarnegie Mellon University has a great online tool for these kind of checks - http://www.speech.cs.cmu.edu/cgi-bin/cmudict - and at 125k words with the matching 39 phonemes. Plugging a word in provides the entire phonemic set, of which only the first is important. \nIf the word does not appear in the dictionary, such as \"NSA\" and is all capitalized, then the system can assume the word is an Acronym and use the first letter to determine which indefinite article to use based on the same original rule set. \n"}, "302": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "Ahmad FaridAhmad Farid", "text": "\n@Nathan Long:\nDownloading wikipedia is actually not a bad idea. All images, videos and other media is not needed. \nI wrote a (crappy) program in php and javascript(!) to read the entire Swedish wikipedia (or at least all aricles that could be reached from the aricle about math, which was the start for my spider.)\nI collected all words and internal links in a database, and also kept track of the frequency of every word. I now use that as a word database for various tasks:\n* Finding all words that can be created from a given set of letters (including wildcard)\n* Created a simple syntax file for Swedish (all words not in the database are considered incorrect).\nOh, and downloading the entire wiki took about one week, using my laptop running most of the time, with 10Mbit connection. \nWhen you're at it, log all occurrences that are inconsistent with the english language and see if some of them are mistakes. Go fix 'em and give something back to the community.\n"}, "303": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "CommunityBot", "text": "\nNote that there are differences between American and British dialects, as Grammar Girl pointed out in her episode A Versus An.\n\nOne complication is when words are pronounced differently in British and American English. For example, the word for a certain kind of plant is pronounced \u201cerb\u201d in American English and \u201cherb\u201d in British English. In the rare cases where this is a problem, use the form that will be expected in your country or by the majority of your readers.\n\n"}, "304": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "Patrik SvenssonPatrik Svensson", "text": "\nTake a look at Perl's Lingua::EN::Inflect. See sub _indef_article in the source code.\n"}, "305": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "Dan DiploDan Diplo", "text": "\nI've ported a function from Python (originally from CPAN package Lingua-EN-Inflect) that correctly determines vowel sounds in C# and posted it as an answer to the question Programmatically determine whether to describe an object with a or an?.  You can see the code snippet here.\n"}, "306": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "knownhumanknownhuman", "text": "\nCould you get a English dictionary that stores the words written in our regular alphabet, and the International Phoenetic Alphabet?\nThen use the phoenetics to figure out the beginning sound of the word, and thus whether \u201ca\u201d or \u201can\u201d is appropriate?\nNot sure if that would actually be easier than (or as much fun as) the statistical Wikipedia approach.\n"}, "307": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "Per AlexanderssonPer Alexandersson", "text": "\nI would use a rule-based algorithm to cover as many as I could, then use a list of exceptions.  If you wanted to get fancy, you could try to determine some new \"rules\" from your exception list.\n"}, "308": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "Jan AagaardJan Aagaard", "text": "\nI just looks like a set of heuristics. It needs be a bit more complicated and answer some things which I never got a good answer for, for example how do you treat abbreviations (\"a RPM\" or \"an RPM\"? I always thought the latter one makes more sense).\nA quick search yielded on linguistic libraries that talk about how to handle the English singular prefix, but you can probably find something if you dig dip enough. And if not - you can always write your own inflection library and gain world fame :-) .\n"}, "309": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "Sinan \u00dcn\u00fcrSinan \u00dcn\u00fcr", "text": "\nI don't suppose you can just fill-in some boiler plate stuff like 'a/an' as a one step cover-all. Otherwise you will end up with assumption errors like all words with 'h' proceed by 'o' get 'an' instead of 'a' like 'home' - (an home?). Basically, you will end up including the logic of the english language or occassionally find rare cases that will make you look foolish.\n"}, "310": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "CommunityBot", "text": "\nCheck for whether a word starts with a vowel or a consonent. A \"u\" is generally a consonant and a vowel (\"yu\"), hence belongs in the consonant group for your purposes.\nThe letter \"h\" stands for a gottal stop (a consonant) in French and in French words used in English. You can make a list of those (in fact, including \"honor\", \"honour\", and \"hour\" might be sufficient) and count them as starting with vowels (since English doesn't recognise a glottal stop).\nAlso count \"eu\" as a consonant etc.\nIt's not too difficult.\n"}, "311": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "StuartStuart", "text": "\nchoice of an or a depends on the way the word is pronounced. By looking at the word you can't necessarily tell its correct pronunciation e.g. a Jargon or abbreviation etc.\nOne of the ways can be to have a dictionary with support for phonemes and use the phoneme information associated with the word to determine whether an \"a\" or an \"an\" should be used.\n"}, "312": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "Paul D. WaitePaul D. Waite", "text": "\nI can't be certain that it has the appropriate information in it to differentiate \"a\" and \"an\", but Princeton's WordNet database exists precisely for the purpose of similar sorts of tasks, so I think it's likely that the data is in there.  It has some tens of thousands of words and hundreds of thousands of relationships between said words (IIRC; I can't find the current statistics on the site).  Give it a look.  It's freely downloadable.\n"}, "313": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "A. L. FlanaganA. L. Flanagan", "text": "\nHow? How about when? Get the noun with article attached. Ask for it in a specific form.\nAsk for the noun with the article. Many a MUD codebase store items as information consisting of:\n\none or more keywords\na short form\na long form\n\nThe keyword form might be \"short sword rusty\". The short form will be \"a sword\". The long form will be \"a rusty short sword\".\nAre you writing an \"a vs. an\" Web service? Take a step back and look at if you can attack this leak further upstream. You can build a dam, but unless you stop it from flowing, it will spill over eventually.\nDetermine how critical this is, and as others have suggested, go for \"quick but crude\", or \"expensive but sturdy\".\n"}, "314": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "GussGuss", "text": "\nThe rule is very simple. If the next word starts with a vowel sound then use 'an', if it starts with a consonant then use 'a'. The hard thing is that our school classification of vowels and consonants doesn't work. The 'h' in 'honour' is a vowel, but the 'h' in 'hospital' is a consonant.\nEven worse, some words like 'honest' start with a vowel or a consonant depending on who is saying them. Even worse, some words change depending on the words around them for some speakers.\nThe problem is bounded only by how much time and effort you want to put into it. You can write something in a couple using 'aeiou' as vowels in a couple of minutes, or you can spends months doing linguistic analysis of your target audience. Between them are a huge number of heuristics which will be right for some speakers and wrong for others -- but because different speakers have different determinations for the same word it simply isn't possible to be right all of the time no matter how you do it.\n"}, "315": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "        BadfishBadfish", "text": "\nThe ideal approach would be to find someplace online that can give you the answers, dynamically query them and cache the answers.  You can prime the system with a few hundred words for starters.\n(I don't know of such an online source, but I wouldn't be surprised if there is one.)\n"}, "316": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "Andrew J. BrehmAndrew J. Brehm", "text": "\nSo, a reasonable solution is possible without downloading all of the internet. Here's what I did:\nI remembered that Google published their raw data for Google Books N-Gram frequencies here.  So I downloaded the 2-gram files for \"a_\" and \"an\".  It's about 26 gigs if I recall correctly.  From that I produced a list of strings where they were overwhelmingly preceded by the opposite article you'd expect (if we were to expect vowels take an \"an\").  That final list of words I was able to store in under 7 kilobytes.\n"}, "317": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "RohinRohin", "text": "\nRather than writing code that could be culture-dependent and have numerous exceptions I tend to rework the statement that includes the indefinite article. For example, rather than saying \"This customer wants to live in a Single-Family Home.\", you could say \"This customer wants a housing type of 'Single-Family Home'.\"  That way, the indefinite article is not dependent on the variable - e.g., \"This customer wants a housing type of 'Apartment'.\"\n"}, "318": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "rmeadorrmeador", "text": "\nI'd like to synthesize a few of the given answers, and contribute my own solutions as well.\nLet's start with some basic heuristics:\n\nStart with the first letter of the word.\n\nIf it starts with an \"a\", \"i\" or \"o\", then use \"an\". As far as I know, those letters always begin with an actual vowel.\n\nIf it starts with an \"e\", then it will be pronounced as a vowel, unless it is followed by a \"u\" (e.g., euphonium, eugenics, euphoric, euphemism, etc.). This would be the case with \"i\" as well, in the unlikely cases of \"Iuka\", \"Iuliyanov\", and \"IUPAC\". (https://en.wiktionary.org/w/index.php?title=Category:English_terms_with_IPA_pronunciation&from=iu)\n\n\nIf it starts with a \"b\", \"c\", \"d\", \"g\", \"k\", \"p\", \"q\", \"t\", \"v\", \"w\", or \"z\", then it is guaranteed to be a consonant, and pronounced like a consonant.\nIf it starts with an \"f\", \"l\", \"m\", \"n\", \"r\", \"s\", or \"x\", it may be pronounced with a vowel, but only if it's in an acronym. Otherwise, it's guaranteed to be pronounced as a consonant.\nIf it begins with a \"u\", or with an \"h\", \"j\", or \"y\", then it falls into a corner case.\n\n\nDetermine whether the word is an acronym.\n\n\n\nIf the word is an acronym, then assume that it contains more than one consecutive capital letter, or contains periods. This could be solved via a simple regex (e.g. [A-Z][A-Z]+).\n\nIf the word is an acronym, then first turn it into a more \"word-like\" form (i.e., not all capitalized, not containing periods) before going to Step 3. If it isn't an acronym, then refer back to the information in Step 1.\n\n\n\n\nUse a dictionary!\n\nIf the word is in this dictionary, and begins with an \"a\", \"e\", \"i\", \"o\", or \"u\", then it begins with a vowel. Otherwise, it's a consonant.\nWiktionary and Wikipedia use the IPA to represent the pronunciations of words. If the word begins with one of these letters, then it begins with a vowel.\n\n\n\nHopefully this helps. I suspect that it will be less resource intensive than any single option, given that much of it can be solved by either a simple \"equals\" statement (e.g. word[0] == 'a'), or by a regex expression (e.g. [aioAIO]), and by some simple knowledge of linguistics and the pronunciations of the English letter names. If the word doesn't fall into a simple case, then use one of the more complex solutions that the other answerers have provided.\n"}, "319": {"topic": "How can I correctly prefix a word with \"a\" and \"an\"?", "user_name": "maxwellbmaxwellb", "text": "\nYou use \"a\" whenever the next word isn't a vowel? And you use \"an\" whenever there is a vowel?\nWith that said, couldn't you just do a regular expression like \"a\\s[a,e,i,o,u].*\"? And then replace it with an \"an?\"\n"}, "320": {"topic": "How to calculate the sentence similarity using word2vec model of gensim with python", "user_name": "analyticalpicasso", "text": "\nAccording to the Gensim Word2Vec, I can use the word2vec model in gensim package to calculate the similarity between 2 words.\ne.g.\ntrained_model.similarity('woman', 'man') \n0.73723527\n\nHowever, the word2vec model fails to predict the sentence similarity. I find out the LSI model with sentence similarity in gensim, but, which doesn't seem that can be combined with word2vec model. The length of corpus of each sentence I have is not very long (shorter than 10 words).  So, are there any simple ways to achieve the goal?\n"}, "321": {"topic": "How to calculate the sentence similarity using word2vec model of gensim with python", "user_name": "zhfktzhfkt", "text": "\nThis is actually a pretty challenging problem that you are asking. Computing sentence similarity requires building a grammatical model of the sentence, understanding equivalent structures (e.g. \"he walked to the store yesterday\" and \"yesterday, he walked to the store\"), finding similarity not just in the pronouns and verbs but also in the proper nouns, finding statistical co-occurences / relationships in lots of real textual examples, etc.\nThe simplest thing you could try -- though I don't know how well this would perform and it would certainly not give you the optimal results -- would be to first remove all \"stop\" words (words like \"the\", \"an\", etc. that don't add much meaning to the sentence) and then run word2vec on the words in both sentences, sum up the vectors in the one sentence, sum up the vectors in the other sentence, and then find the difference between the sums. By summing them up instead of doing a word-wise difference, you'll at least not be subject to word order. That being said, this will fail in lots of ways and isn't a good solution by any means (though good solutions to this problem almost always involve some amount of NLP, machine learning, and other cleverness).\nSo, short answer is, no, there's no easy way to do this (at least not to do it well).\n"}, "322": {"topic": "How to calculate the sentence similarity using word2vec model of gensim with python", "user_name": "Michael Aaron SafyanMichael Aaron Safyan", "text": "\nSince you're using gensim, you should probably use it's doc2vec implementation. doc2vec is an extension of word2vec to the phrase-, sentence-, and document-level. It's a pretty simple extension, described here\nhttp://cs.stanford.edu/~quocle/paragraph_vector.pdf\nGensim is nice because it's intuitive, fast, and flexible. What's great is that you can grab the pretrained word embeddings from the official word2vec page and the syn0 layer of gensim's Doc2Vec model is exposed so that you can seed the word embeddings with these high quality vectors!\nGoogleNews-vectors-negative300.bin.gz (as linked in Google Code)\nI think gensim is definitely the easiest (and so far for me, the best) tool for embedding a sentence in a vector space.\nThere exist other sentence-to-vector techniques than the one proposed in Le & Mikolov's paper above. Socher and Manning from Stanford are certainly two of the most famous researchers working in this area. Their work has been based on the principle of compositionally - semantics of the sentence come from:\n1. semantics of the words\n\n2. rules for how these words interact and combine into phrases\n\nThey've proposed a few such models (getting increasingly more complex) for how to use compositionality to build sentence-level representations.\n2011 - unfolding recursive autoencoder (very comparatively simple. start here if interested)\n2012 - matrix-vector neural network\n2013 - neural tensor network\n2015 - Tree LSTM\nhis papers are all available at socher.org. Some of these models are available, but I'd still recommend gensim's doc2vec. For one, the 2011 URAE isn't particularly powerful. In addition, it comes pretrained with weights suited for paraphrasing news-y data. The code he provides does not allow you to retrain the network. You also can't swap in different word vectors, so you're stuck with 2011's pre-word2vec embeddings from Turian. These vectors are certainly not on the level of word2vec's or GloVe's.\nHaven't worked with the Tree LSTM yet, but it seems very promising!\ntl;dr Yeah, use gensim's doc2vec. But other methods do exist!\n"}, "323": {"topic": "How to calculate the sentence similarity using word2vec model of gensim with python", "user_name": "EliadL", "text": "\nIf you are using word2vec, you need to calculate the average vector for all words in every sentence/document and use cosine similarity between vectors:\nimport numpy as np\nfrom scipy import spatial\n\nindex2word_set = set(model.wv.index2word)\n\ndef avg_feature_vector(sentence, model, num_features, index2word_set):\n    words = sentence.split()\n    feature_vec = np.zeros((num_features, ), dtype='float32')\n    n_words = 0\n    for word in words:\n        if word in index2word_set:\n            n_words += 1\n            feature_vec = np.add(feature_vec, model[word])\n    if (n_words > 0):\n        feature_vec = np.divide(feature_vec, n_words)\n    return feature_vec\n\nCalculate similarity:\ns1_afv = avg_feature_vector('this is a sentence', model=model, num_features=300, index2word_set=index2word_set)\ns2_afv = avg_feature_vector('this is also sentence', model=model, num_features=300, index2word_set=index2word_set)\nsim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\nprint(sim)\n\n> 0.915479828613\n\n"}, "324": {"topic": "How to calculate the sentence similarity using word2vec model of gensim with python", "user_name": "WillieWillie", "text": "\nyou can use Word Mover's Distance algorithm. here is an easy description about WMD.\n#load word2vec model, here GoogleNews is used\nmodel = gensim.models.KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)\n#two sample sentences \ns1 = 'the first sentence'\ns2 = 'the second text'\n\n#calculate distance between two sentences using WMD algorithm\ndistance = model.wmdistance(s1, s2)\n\nprint ('distance = %.3f' % distance)\n\nP.s.: if you face an error about import pyemd library, you can install it using following command:\npip install pyemd\n\n"}, "325": {"topic": "How to calculate the sentence similarity using word2vec model of gensim with python", "user_name": "", "text": "\nOnce you compute the sum of the two sets of word vectors, you should take the cosine between the vectors, not the diff. The cosine can be computed by taking the dot product of the two vectors normalized. Thus, the word count is not a factor. \n"}, "326": {"topic": "How to calculate the sentence similarity using word2vec model of gensim with python", "user_name": "\r", "text": "\nI would like to update the existing solution to help the people who are going to calculate the semantic similarity of sentences.\nStep 1:\nLoad the suitable model using gensim and calculate the word vectors for words in the sentence and store them as a word list\nStep 2 :\nComputing the sentence vector\nThe calculation of semantic similarity between sentences was difficult before but recently a paper named \"A SIMPLE BUT TOUGH-TO-BEAT BASELINE FOR SENTENCE\nEMBEDDINGS\" was proposed which suggests a simple approach by computing the weighted average of word vectors in the sentence and then remove the projections of the average vectors on their first principal component.Here the weight of a word w is a/(a + p(w)) with a being a parameter and p(w) the (estimated) word frequency called smooth inverse frequency.this method performing significantly better.\nA simple code to calculate the sentence vector using SIF(smooth inverse frequency) the method proposed in the paper has been given here\nStep 3:\nusing sklearn cosine_similarity load two vectors for the sentences and compute the similarity.\nThis is the most simple and efficient method to compute the sentence similarity.\n"}, "327": {"topic": "How to calculate the sentence similarity using word2vec model of gensim with python", "user_name": "\r", "text": "\nThere is a function from the documentation taking a list of words and comparing their similarities.\ns1 = 'This room is dirty'\ns2 = 'dirty and disgusting room' #corrected variable name\n\ndistance = model.wv.n_similarity(s1.lower().split(), s2.lower().split())\n\n"}, "328": {"topic": "How to calculate the sentence similarity using word2vec model of gensim with python", "user_name": "EhsanEhsan", "text": "\nI am using the following method and it works well.\nYou first need to run a POSTagger and then filter your sentence to get rid of the stop words (determinants, conjunctions, ...). I recommend TextBlob APTagger.\nThen you build a word2vec by taking the mean of each word vector in the sentence. The n_similarity method in Gemsim word2vec does exactly that by allowing to pass two sets of words to compare.\n"}, "329": {"topic": "How to calculate the sentence similarity using word2vec model of gensim with python", "user_name": "Rani NelkenRani Nelken", "text": "\nThere are extensions of Word2Vec intended to solve the problem of comparing longer pieces of text like phrases or sentences. One of them is paragraph2vec or doc2vec.\n\"Distributed Representations of Sentences and Documents\"\nhttp://cs.stanford.edu/~quocle/paragraph_vector.pdf\nhttp://rare-technologies.com/doc2vec-tutorial/\n"}, "330": {"topic": "How to calculate the sentence similarity using word2vec model of gensim with python", "user_name": "Poorna PrudhviPoorna Prudhvi", "text": "\nGensim implements a model called Doc2Vec for paragraph embedding.\nThere are different tutorials presented as IPython notebooks:\n\nDoc2Vec Tutorial on the Lee Dataset \nGensim Doc2Vec Tutorial on the IMDB Sentiment Dataset\nDoc2Vec to wikipedia articles\n\nAnother method would rely on Word2Vec and Word Mover's Distance (WMD), as shown in this tutorial:\n\nFinding similar documents with Word2Vec and WMD \n\nAn alternative solution would be to rely on average vectors:\nfrom gensim.models import KeyedVectors\nfrom gensim.utils import simple_preprocess    \n\ndef tidy_sentence(sentence, vocabulary):\n    return [word for word in simple_preprocess(sentence) if word in vocabulary]    \n\ndef compute_sentence_similarity(sentence_1, sentence_2, model_wv):\n    vocabulary = set(model_wv.index2word)    \n    tokens_1 = tidy_sentence(sentence_1, vocabulary)    \n    tokens_2 = tidy_sentence(sentence_2, vocabulary)    \n    return model_wv.n_similarity(tokens_1, tokens_2)\n\nwv = KeyedVectors.load('model.wv', mmap='r')\nsim = compute_sentence_similarity('this is a sentence', 'this is also a sentence', wv)\nprint(sim)\n\nFinally, if you can run Tensorflow, you may try:\nhttps://tfhub.dev/google/universal-sentence-encoder/2\n"}, "331": {"topic": "How to calculate the sentence similarity using word2vec model of gensim with python", "user_name": "Dreams", "text": "\nI have tried the methods provided by the previous answers. It works, but the main drawback of it is that the longer the sentences the larger similarity will be(to calculate the similarity I use the cosine score of the two mean embeddings of any two sentences) since the more the words the more positive semantic effects will be added to the sentence. \nI thought I should change my mind and use the sentence embedding instead as studied in this paper and this. \n"}, "332": {"topic": "How to calculate the sentence similarity using word2vec model of gensim with python", "user_name": "AstariulAstariul", "text": "\nIf not using Word2Vec we have other model to find it using BERT for embed.\nBelow are reference link \nhttps://github.com/UKPLab/sentence-transformers\npip install -U sentence-transformers\n\nfrom sentence_transformers import SentenceTransformer\nimport scipy.spatial\n\nembedder = SentenceTransformer('bert-base-nli-mean-tokens')\n\n# Corpus with example sentences\ncorpus = ['A man is eating a food.',\n          'A man is eating a piece of bread.',\n          'The girl is carrying a baby.',\n          'A man is riding a horse.',\n          'A woman is playing violin.',\n          'Two men pushed carts through the woods.',\n          'A man is riding a white horse on an enclosed ground.',\n          'A monkey is playing drums.',\n          'A cheetah is running behind its prey.'\n          ]\ncorpus_embeddings = embedder.encode(corpus)\n\n# Query sentences:\nqueries = ['A man is eating pasta.', 'Someone in a gorilla costume is playing a set of drums.', 'A cheetah chases prey on across a field.']\nquery_embeddings = embedder.encode(queries)\n\n# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\nclosest_n = 5\nfor query, query_embedding in zip(queries, query_embeddings):\n    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n\n    results = zip(range(len(distances)), distances)\n    results = sorted(results, key=lambda x: x[1])\n\n    print(\"\\n\\n======================\\n\\n\")\n    print(\"Query:\", query)\n    print(\"\\nTop 5 most similar sentences in corpus:\")\n\n    for idx, distance in results[0:closest_n]:\n        print(corpus[idx].strip(), \"(Score: %.4f)\" % (1-distance))\n\nOther Link to follow\nhttps://github.com/hanxiao/bert-as-service\n"}, "333": {"topic": "How to calculate the sentence similarity using word2vec model of gensim with python", "user_name": "lechatpitolechatpito", "text": "\nFacebook Research group released a new solution called InferSent \nResults and code are published on Github, check their repo. It is pretty awesome. I am planning to use it. \nhttps://github.com/facebookresearch/InferSent \ntheir paper\nhttps://arxiv.org/abs/1705.02364 \nAbstract: \nMany modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.\n"}, "334": {"topic": "How to calculate the sentence similarity using word2vec model of gensim with python", "user_name": "MaxMax", "text": "\nYou can just add the word vectors of one sentence together. Then count the Cosine similarity of two sentence vector as the similarity of two sentence. I think that's  the most easy way.\n"}, "335": {"topic": "How to config nltk data directory from code?", "user_name": "alvas", "text": "\nHow to config nltk data directory from code?\n"}, "336": {"topic": "How to config nltk data directory from code?", "user_name": "Juanjo ContiJuanjo Conti", "text": "\nJust change items of nltk.data.path, it's a simple list.\n"}, "337": {"topic": "How to config nltk data directory from code?", "user_name": "Tim McNamaraTim McNamara", "text": "\nFrom the code, http://www.nltk.org/_modules/nltk/data.html: \n\n``nltk:path``: Specifies the file stored in the NLTK data\n package at *path*.  NLTK will search for these files in the\n directories specified by ``nltk.data.path``.\n\n\nThen within the code:\n######################################################################\n# Search Path\n######################################################################\n\npath = []\n\"\"\"A list of directories where the NLTK data package might reside.\n   These directories will be checked in order when looking for a\n   resource in the data package.  Note that this allows users to\n   substitute in their own versions of resources, if they have them\n   (e.g., in their home directory under ~/nltk_data).\"\"\"\n\n# User-specified locations:\npath += [d for d in os.environ.get('NLTK_DATA', str('')).split(os.pathsep) if d]\nif os.path.expanduser('~/') != '~/':\n    path.append(os.path.expanduser(str('~/nltk_data')))\n\nif sys.platform.startswith('win'):\n    # Common locations on Windows:\n    path += [\n        str(r'C:\\nltk_data'), str(r'D:\\nltk_data'), str(r'E:\\nltk_data'),\n        os.path.join(sys.prefix, str('nltk_data')),\n        os.path.join(sys.prefix, str('lib'), str('nltk_data')),\n        os.path.join(os.environ.get(str('APPDATA'), str('C:\\\\')), str('nltk_data'))\n    ]\nelse:\n    # Common locations on UNIX & OS X:\n    path += [\n        str('/usr/share/nltk_data'),\n        str('/usr/local/share/nltk_data'),\n        str('/usr/lib/nltk_data'),\n        str('/usr/local/lib/nltk_data')\n    ]\n\nTo modify the path, simply append to the list of possible paths:\nimport nltk\nnltk.data.path.append(\"/home/yourusername/whateverpath/\")\n\nOr in windows:\nimport nltk\nnltk.data.path.append(\"C:\\somewhere\\farfar\\away\\path\")\n\n"}, "338": {"topic": "How to config nltk data directory from code?", "user_name": "alvasalvas", "text": "\nI use append, example\nnltk.data.path.append('/libs/nltk_data/')\n\n"}, "339": {"topic": "How to config nltk data directory from code?", "user_name": "Tushar Gupta - curioustushar", "text": "\nInstead of adding nltk.data.path.append('your/path/to/nltk_data') to every script, NLTK accepts NLTK_DATA environment variable. (code link)\nOpen ~/.bashrc (or ~/.profile) with text editor (e.g. nano, vim, gedit), and add following line:  \nexport NLTK_DATA=\"your/path/to/nltk_data\"\n\nExecute source to load environmental variable  \nsource ~/.bashrc\n\n\nTest\nOpen python and execute following lines\nimport nltk\nnltk.data.path\n\nYour can see your nltk data path already in there.\nReference: @alvations's answer on\nnltk/nltk #1997 \n"}, "340": {"topic": "How to config nltk data directory from code?", "user_name": "bahlumbahlum", "text": "\nFor those using uwsgi: \nI was having trouble because I wanted a uwsgi app (running as a different user than myself) to have access to nltk data that I had previously downloaded. What worked for me was adding the following line to myapp_uwsgi.ini:\nenv = NLTK_DATA=/home/myuser/nltk_data/\n\nThis sets the environment variable NLTK_DATA, as suggested by @schemacs.\nYou may need to restart your uwsgi process after making this change. \n"}, "341": {"topic": "How to config nltk data directory from code?", "user_name": "", "text": "\nUsing fnjn's advice above on printing out the path:\nprint(nltk.data.path)\n\nI saw the path strings in this type of format on windows:\nC:\\\\Users\\\\my_user_name\\\\AppData\\\\Roaming\\\\SPB_Data\n\nSo I switched my path from the python type forward slash '/', to a double backslash '\\\\' when I used path.append:\nnltk.data.path.append(\"C:\\\\workspace\\\\my_project\\\\data\\\\nltk_books\")\n\nThe exception went away.\n"}, "342": {"topic": "How to config nltk data directory from code?", "user_name": "fnjnfnjn", "text": "\nAnother solution is to get ahead of it. \ntry \n    import nltk \n    nltk.download()  \nWhen the window box pops up asking if you want to download the corpus , you can specify there which directory it is to be downloaded to. \n"}, "343": {"topic": "Load Pretrained glove vectors in python", "user_name": "user", "text": "\nI have downloaded pretrained glove vector file from the internet. It is a .txt file. I am unable to load and access it. It is easy to load and access a word vector binary file using gensim but I don't know how to do it when it is a text file format.\n"}, "344": {"topic": "Load Pretrained glove vectors in python", "user_name": "SameSame", "text": "\nglove model files are in a word - vector format. You can open the textfile to verify this. Here is a small snippet of code you can use to load a pretrained glove file:\nimport numpy as np\n\ndef load_glove_model(File):\n    print(\"Loading Glove Model\")\n    glove_model = {}\n    with open(File,'r') as f:\n        for line in f:\n            split_line = line.split()\n            word = split_line[0]\n            embedding = np.array(split_line[1:], dtype=np.float64)\n            glove_model[word] = embedding\n    print(f\"{len(glove_model)} words loaded!\")\n    return glove_model\n\nYou can then access the word vectors by simply using the gloveModel variable.\nprint(gloveModel['hello'])\n"}, "345": {"topic": "Load Pretrained glove vectors in python", "user_name": "Jules G.M.", "text": "\nYou can do it much faster with pandas:\nimport pandas as pd\nimport csv\n\nwords = pd.read_table(glove_data_file, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n\nThen to get the vector for a word:\ndef vec(w):\n  return words.loc[w].as_matrix()\n\nAnd to find the closest word to a vector:\nwords_matrix = words.as_matrix()\n\ndef find_closest_word(v):\n  diff = words_matrix - v\n  delta = np.sum(diff * diff, axis=1)\n  i = np.argmin(delta)\n  return words.iloc[i].name\n\n"}, "346": {"topic": "Load Pretrained glove vectors in python", "user_name": "Karishma MalkanKarishma Malkan", "text": "\nI suggest using gensim to do everything. You can read the file, and also benefit from having a lot of methods already implemented on this great package.\nSuppose you generated GloVe vectors using the C++ program and that your \"-save-file\" parameter is \"vectors\". Glove executable will generate you two files, \"vectors.bin\" and \"vectors.txt\".\nUse glove2word2vec to convert GloVe vectors in text format into the word2vec text format:\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nglove2word2vec(glove_input_file=\"vectors.txt\", word2vec_output_file=\"gensim_glove_vectors.txt\")\n\nFinally, read the word2vec txt to a gensim model using KeyedVectors:\nfrom gensim.models.keyedvectors import KeyedVectors\nglove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)\n\nNow you can use gensim word2vec methods (for example, similarity) as you'd like.\n"}, "347": {"topic": "Load Pretrained glove vectors in python", "user_name": "Steven", "text": "\nI found this approach faster. \nimport pandas as pd\n\ndf = pd.read_csv('glove.840B.300d.txt', sep=\" \", quoting=3, header=None, index_col=0)\nglove = {key: val.values for key, val in df.T.items()}\n\nSave the dictionary:\nimport pickle\nwith open('glove.840B.300d.pkl', 'wb') as fp:\n    pickle.dump(glove, fp)\n\n"}, "348": {"topic": "Load Pretrained glove vectors in python", "user_name": "PetterPetter", "text": "\nHere's a one liner if all you want is the embedding matrix\nnp.loadtxt(path, usecols=range(1, dim+1), comments=None)\nwhere path is path to your downloaded GloVe file and dim is the dimension of the word embedding.\nIf you want both the words and corresponding vectors you can do\nglove = np.loadtxt(path, dtype='str', comments=None)\nand seperate the words and vectors as follows\nwords = glove[:, 0]\nvectors = glove[:, 1:].astype('float')\n\n"}, "349": {"topic": "Load Pretrained glove vectors in python", "user_name": "BenBen", "text": "\nLoading word embedding from a text file (in my case the glove.42B.300d embeddings) takes a bit long (147.2s on my machine).\nWhat helps is converting the text file first into two new files: a text file that contains the words only (e.g. embeddings.vocab) and a binary file which contains the embedding vectors as numpy-structure (e.g. embeddings.npy).\nOnce converted, it takes me only 4.96s to load the same embeddings into the memory. This approach ends a up with exactly the same dictionary as if you load it from the text file. It is as efficient in access time and does not require any additional frameworks, but a lot faster in loading time.\nWith this code you convert your embedding text file to the two new files: \ndef convert_to_binary(embedding_path):\n    f = codecs.open(embedding_path + \".txt\", 'r', encoding='utf-8')\n    wv = []\n\n    with codecs.open(embedding_path + \".vocab\", \"w\", encoding='utf-8') as vocab_write:\n        count = 0\n        for line in f:\n            splitlines = line.split()\n            vocab_write.write(splitlines[0].strip())\n            vocab_write.write(\"\\n\")\n            wv.append([float(val) for val in splitlines[1:]])\n        count += 1\n\n    np.save(embedding_path + \".npy\", np.array(wv))\n\nAnd with this method you load it efficiently into your memory:\ndef load_word_emb_binary(embedding_file_name_w_o_suffix):\n    print(\"Loading binary word embedding from {0}.vocab and {0}.npy\".format(embedding_file_name_w_o_suffix))\n\n    with codecs.open(embedding_file_name_w_o_suffix + '.vocab', 'r', 'utf-8') as f_in:\n        index2word = [line.strip() for line in f_in]\n\n    wv = np.load(embedding_file_name_w_o_suffix + '.npy')\n    word_embedding_map = {}\n    for i, w in enumerate(index2word):\n        word_embedding_map[w] = wv[i]\n\n    return word_embedding_map\n\nDisclaimer: This code is shamelessly stolen from https://blog.ekbana.com/loading-glove-pre-trained-word-embedding-model-from-text-file-faster-5d3e8f2b8455. But it might help in this thread.\n"}, "350": {"topic": "Load Pretrained glove vectors in python", "user_name": "IndraIndra", "text": "\nPython3 version which also handles bigrams and trigrams:\nimport numpy as np\n\n\ndef load_glove_model(glove_file):\n    print(\"Loading Glove Model\")\n    f = open(glove_file, 'r')\n    model = {}\n    vector_size = 300\n    for line in f:\n        split_line = line.split()\n        word = \" \".join(split_line[0:len(split_line) - vector_size])\n        embedding = np.array([float(val) for val in split_line[-vector_size:]])\n        model[word] = embedding\n    print(\"Done.\\n\" + str(len(model)) + \" words loaded!\")\n    return model\n\n"}, "351": {"topic": "Load Pretrained glove vectors in python", "user_name": "Abhai KollaraAbhai Kollara", "text": "\nimport os\nimport numpy as np\n\n# store all the pre-trained word vectors\nprint('Loading word vectors...')\nword2vec = {}\nwith open(os.path.join('glove/glove.6B.%sd.txt' % EMBEDDING_DIM)) as f: #enter the path where you unzipped the glove file\n  # is just a space-separated text file in the format:\n  # word vec[0] vec[1] vec[2] ...\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float32')\n        word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))\n\n"}, "352": {"topic": "Load Pretrained glove vectors in python", "user_name": "Ursin BrunnerUrsin Brunner", "text": "\nThis code takes some time to store glove embeddings on shelf, but loading it is quite faster as compared to other approaches.\nimport os\nimport numpy as np\nfrom contextlib import closing\nimport shelve\n\ndef store_glove_to_shelf(glove_file_path,shelf):\n    print('Loading Glove')\n    with open(os.path.join(glove_file_path)) as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vec = np.asarray(values[1:], dtype='float32')\n            shelf[word] = vec\n\nshelf_file_name = \"glove_embeddings\"\nglove_file_path = \"glove/glove.840B.300d.txt\"\n\n# Storing glove embeddings to shelf for faster load\nwith closing(shelve.open(shelf_file_name + '.shelf', 'c')) as shelf:\n    store_glove_to_shelf(glove_file_path,shelf)\n    print(\"Stored glove embeddings from {} to {}\".format(glove_file_path,shelf_file_name+'.shelf'))\n\n# To reuse the glove embeddings stored in shelf\nwith closing(shelve.open(shelf_file_name + '.shelf')) as embeddings_index:\n    # USE embeddings_index here , which is a dictionary\n    print(\"Loaded glove embeddings from {}\".format(shelf_file_name+'.shelf'))\n    print(\"Found glove embeddings with {} words\".format(len(embeddings_index)))\n\n"}, "353": {"topic": "Load Pretrained glove vectors in python", "user_name": "alabroskialabroski", "text": "\nEach corpus need to start with a line containing the vocab size and the vector size in that order.\nOpen the .txt file of the glove model and enter the dimension of the vector at the first line by pressing Enter first:\nExample, for glove.6B.50d.txt, just add 400000 50 in the first line.\nThen use gensim to transform that raw .txt vector file to gensim vector format:\nimport gensim\n\nword_vectors = gensim.models.KeyedVectors.load_word2vec_format('path/glove.6B.50d.txt', binary=False)\nword_vectors.save('path/glove_gensim.txt')\n\n"}, "354": {"topic": "Load Pretrained glove vectors in python", "user_name": "Ankan DattaAnkan Datta", "text": "\nSome of the other approaches here required more storage space (e.g. to split files) or were quite slow to run on my personal laptop. I tried shelf db but it seemed to blow up in storage size. Here's an \"in-place\" approach with one-time file-read time cost and very low additional storage cost. We treat the original text file as a database and just store the position location for each of the words. This works really well when you're, e.g., investigating properties of word vectors.\n# First create a map from words to position in the file\ndef get_db_mapping(fname):\n    char_ct = 0    # cumulative position in file\n    pos_map = dict()\n\n    with open(fname + \".txt\", 'r', encoding='utf-8') as f:\n        for line in tqdm(f):\n            new_len = len(line)     # len of line\n\n            # get the word\n            splitlines = line.split()\n            word = splitlines[0].strip()\n\n            # store and increment counter\n            pos_map[word] = char_ct\n            char_ct += new_len\n\n    # write dict\n    with open(fname + '.db', 'wb') as handle:\n        pickle.dump(pos_map, handle)\n\n\nclass Embedding:\n\"\"\"Small wrapper so that we can use [] notation to fetch word vectors.\nIt would be better to just have the file pointer and the pos_map as part\nof this class, but that's not how I wrote it initially.\"\"\"\n    def __init__(self, emb_fn):\n        self.emb_fn = emb_fn\n\n    def __getitem__(self, item):\n        return self.emb_fn(item)\n\n\ndef load_db_mapping(fname, cache_size=1000) -> Embedding:\n    \"\"\"Creates a function closure that wraps access to the db mapping\n    and the text file that functions as db. Returns them as an\n    Embedding object\"\"\"\n    # get the two state objects: mapping and file pointer\n    with open(fname + '.db', 'rb') as handle:\n        pos_map = pickle.load(handle)\n    f = open(fname + \".txt\", 'r', encoding='utf-8')\n\n    @lru_cache(maxsize=cache_size)\n    def get_vector(word: str):\n        pos = pos_map[word]\n        f.seek(pos, 0)\n\n        # special logic needed because of small count errors\n        fail_ct = 0\n        read_word = \"\"\n        while fail_ct < 5 and read_word != word:\n            fail_ct += 1\n            l = f.readline()\n            try:\n                splitlines = l.split()\n                read_word = splitlines[0].strip()\n            except:\n                continue\n        if read_word != word:\n            raise ValueError('word not found')\n\n        # actually return\n        return np.array([float(val) for val in splitlines[1:]])\n\n    return Embedding(get_vector)\n\n# to run\nk_glove_vector_name = 'glove.42B.300d'   # omit .txt\nget_db_mapping(k_glove_vector_name)      # run only once; creates .db\nword_embedding = load_db_mapping(k_glove_vector_name)\nword_embedding['hello']\n\n"}, "355": {"topic": "Load Pretrained glove vectors in python", "user_name": "Rudra DesaiRudra Desai", "text": "\na tool with an easy implementation of GloVe is zeulgma\nhttps://pypi.org/project/zeugma/\nfrom zeugma.embeddings import EmbeddingTransformer\nglove = EmbeddingTransformer('glove')\n\nthe implementation is really very easy\n"}, "356": {"topic": "Load Pretrained glove vectors in python", "user_name": "Dharman\u2666", "text": "\ndef create_embedding_matrix(word_to_index):\n# word_to_index is dictionary containing \"word:token\" pairs\nnb_words = len(word_to_index)+1\n\nembeddings_index = {}\nwith open('C:/Users/jayde/Desktop/IISc/DLNLP/Assignment1/glove.840B.300d/glove.840B.300d.txt', encoding=\"utf-8\", errors='ignore') as f:\n    for line in f:\n        values = line.split()\n        word = ''.join(values[:-300])\n        coefs = np.asarray(values[-300:], dtype='float32')\n        embeddings_index[word] = coefs\n\nembedding_matrix = np.zeros((nb_words, 300))\n\nfor word, i in word_to_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\nreturn embedding_matrix\n\nemb_matrix = create_embedding_matrix(vocab_to_int)\n"}, "357": {"topic": "Load Pretrained glove vectors in python", "user_name": "StrayhornStrayhorn", "text": "\nEMBEDDING_LIFE = 'path/to/your/glove.txt'\n\ndef get_coefs(word,*arr): \n      return word, np.asarray(arr, dtype='float32')\n\nembeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n\nfor word, i in word_index.items():\nif i >= max_features: continue\nembedding_vector = embeddings_index.get(word)\nif embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\n"}, "358": {"topic": "Is there a good natural language processing library [closed]", "user_name": "Kai", "text": "\n\n\n\n\n\n\n\n\n\r\nAs it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.                    \n\n\nClosed 10 years ago.\n\n\n\nI need to implement some NLP in my current module. I am looking for some good library that can help me here. I came across 'LingPipe' but could not completely follow on how to use it.\nBasically, we need to implement a feature where the application can decipher customer instructions (delivery instructions) typed in plain english. Eg:\n\nWill pick up at 12:00 noon tomorrow\nRequest delivery after 10th June\nPlease do not send before Wednesday\nAdd 10 more units of XYZ to the order\n\n"}, "359": {"topic": "Is there a good natural language processing library [closed]", "user_name": "ViniVini", "text": "\nLingPipe is very nice and well documented. You can also take a look at:\n\nOpenNLP\nStanford NLP\nApache UIMA\nGATE\nCogComp-NLP\nFrameNet\n\nThe last one specifically might be of interest to you, although I don't know whether there are any readily available Java implementations (and maybe that's too big of a gun for your problem anyway :-)\nPaul's idea of using a DSL is probably easier and faster to implement, and more reliable to use for your customers. I, too, would recommend looking into that first.\n"}, "360": {"topic": "Is there a good natural language processing library [closed]", "user_name": "Martin Peck", "text": "\nI think whether or not you choose to use NLP might depend on the specific requirements for your system.  Is it a requirement that free-form english should be accepted, or will only a certain set of instructions be accepted?  It might be easier to build a domain specific language that supports what your users need to convey than to do full-on semantic analysis of free-form text.\nIn the Java space, Groovy has some support for building DSLs.\n"}, "361": {"topic": "Is there a good natural language processing library [closed]", "user_name": "n3rdn3rd", "text": "\nFor other JVM languages see \n\nScala: Scala NLP -  Breeze and Epic (related spark-project.org ) \nClojure: clojure-opennlp\n\n"}, "362": {"topic": "Creating a new corpus with NLTK", "user_name": "halfer", "text": "\nI reckoned that often the answer to my title is to go and read the documentations, but I ran through the NLTK book but it doesn't give the answer. I'm kind of new to Python.\nI have a bunch of .txt files and I want to be able to use the corpus functions that NLTK provides for the corpus nltk_data. \nI've tried PlaintextCorpusReader but I couldn't get further than:\n>>>import nltk\n>>>from nltk.corpus import PlaintextCorpusReader\n>>>corpus_root = './'\n>>>newcorpus = PlaintextCorpusReader(corpus_root, '.*')\n>>>newcorpus.words()\n\nHow do I segment the newcorpus sentences using punkt? I tried using the punkt functions but the punkt functions couldn't read PlaintextCorpusReader class?\nCan you also lead me to how I can write the segmented data into text files?\n"}, "363": {"topic": "Creating a new corpus with NLTK", "user_name": "alvasalvas", "text": "\nAfter some years of figuring out how it works, here's the updated tutorial of \nHow to create an NLTK corpus with a directory of textfiles?\nThe main idea is to make use of the nltk.corpus.reader package. In the case that you have a directory of textfiles in English, it's best to use the PlaintextCorpusReader. \nIf you have a directory that looks like this:\nnewcorpus/\n         file1.txt\n         file2.txt\n         ...\n\nSimply use these lines of code and you can get a corpus:\nimport os\nfrom nltk.corpus.reader.plaintext import PlaintextCorpusReader\n\ncorpusdir = 'newcorpus/' # Directory of corpus.\n\nnewcorpus = PlaintextCorpusReader(corpusdir, '.*')\n\nNOTE: that the PlaintextCorpusReader will use the default nltk.tokenize.sent_tokenize() and nltk.tokenize.word_tokenize() to split your texts into sentences and words and these functions are build for English, it may NOT work for all languages.\nHere's the full code with creation of test textfiles and how to create a corpus with NLTK and how to access the corpus at different levels:\nimport os\nfrom nltk.corpus.reader.plaintext import PlaintextCorpusReader\n\n# Let's create a corpus with 2 texts in different textfile.\ntxt1 = \"\"\"This is a foo bar sentence.\\nAnd this is the first txtfile in the corpus.\"\"\"\ntxt2 = \"\"\"Are you a foo bar? Yes I am. Possibly, everyone is.\\n\"\"\"\ncorpus = [txt1,txt2]\n\n# Make new dir for the corpus.\ncorpusdir = 'newcorpus/'\nif not os.path.isdir(corpusdir):\n    os.mkdir(corpusdir)\n\n# Output the files into the directory.\nfilename = 0\nfor text in corpus:\n    filename+=1\n    with open(corpusdir+str(filename)+'.txt','w') as fout:\n        print>>fout, text\n\n# Check that our corpus do exist and the files are correct.\nassert os.path.isdir(corpusdir)\nfor infile, text in zip(sorted(os.listdir(corpusdir)),corpus):\n    assert open(corpusdir+infile,'r').read().strip() == text.strip()\n\n\n# Create a new corpus by specifying the parameters\n# (1) directory of the new corpus\n# (2) the fileids of the corpus\n# NOTE: in this case the fileids are simply the filenames.\nnewcorpus = PlaintextCorpusReader('newcorpus/', '.*')\n\n# Access each file in the corpus.\nfor infile in sorted(newcorpus.fileids()):\n    print infile # The fileids of each file.\n    with newcorpus.open(infile) as fin: # Opens the file.\n        print fin.read().strip() # Prints the content of the file\nprint\n\n# Access the plaintext; outputs pure string/basestring.\nprint newcorpus.raw().strip()\nprint \n\n# Access paragraphs in the corpus. (list of list of list of strings)\n# NOTE: NLTK automatically calls nltk.tokenize.sent_tokenize and \n#       nltk.tokenize.word_tokenize.\n#\n# Each element in the outermost list is a paragraph, and\n# Each paragraph contains sentence(s), and\n# Each sentence contains token(s)\nprint newcorpus.paras()\nprint\n\n# To access pargraphs of a specific fileid.\nprint newcorpus.paras(newcorpus.fileids()[0])\n\n# Access sentences in the corpus. (list of list of strings)\n# NOTE: That the texts are flattened into sentences that contains tokens.\nprint newcorpus.sents()\nprint\n\n# To access sentences of a specific fileid.\nprint newcorpus.sents(newcorpus.fileids()[0])\n\n# Access just tokens/words in the corpus. (list of strings)\nprint newcorpus.words()\n\n# To access tokens of a specific fileid.\nprint newcorpus.words(newcorpus.fileids()[0])\n\nFinally, to read a directory of texts and create an NLTK corpus in another languages, you must first ensure that you have a python-callable word tokenization and sentence tokenization modules that takes string/basestring input and produces such output:\n>>> from nltk.tokenize import sent_tokenize, word_tokenize\n>>> txt1 = \"\"\"This is a foo bar sentence.\\nAnd this is the first txtfile in the corpus.\"\"\"\n>>> sent_tokenize(txt1)\n['This is a foo bar sentence.', 'And this is the first txtfile in the corpus.']\n>>> word_tokenize(sent_tokenize(txt1)[0])\n['This', 'is', 'a', 'foo', 'bar', 'sentence', '.']\n\n"}, "364": {"topic": "Creating a new corpus with NLTK", "user_name": "", "text": "\nI think the PlaintextCorpusReader already segments the input with a punkt tokenizer, at least if your input language is english.\nPlainTextCorpusReader's constructor\ndef __init__(self, root, fileids,\n             word_tokenizer=WordPunctTokenizer(),\n             sent_tokenizer=nltk.data.LazyLoader(\n                 'tokenizers/punkt/english.pickle'),\n             para_block_reader=read_blankline_block,\n             encoding='utf8'):\n\nYou can pass the reader a word and sentence tokenizer, but for the latter the default already is nltk.data.LazyLoader('tokenizers/punkt/english.pickle').\nFor a single string, a tokenizer would be used as follows (explained here, see section 5 for punkt tokenizer).\n>>> import nltk.data\n>>> text = \"\"\"\n... Punkt knows that the periods in Mr. Smith and Johann S. Bach\n... do not mark sentence boundaries.  And sometimes sentences\n... can start with non-capitalized words.  i is a good variable\n... name.\n... \"\"\"\n>>> tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n>>> tokenizer.tokenize(text.strip())\n\n"}, "365": {"topic": "Creating a new corpus with NLTK", "user_name": "alvasalvas", "text": "\n >>> import nltk\n >>> from nltk.corpus import PlaintextCorpusReader\n >>> corpus_root = './'\n >>> newcorpus = PlaintextCorpusReader(corpus_root, '.*')\n \"\"\"\n if the ./ dir contains the file my_corpus.txt, then you \n can view say all the words it by doing this \n \"\"\"\n >>> newcorpus.words('my_corpus.txt')\n\n"}, "366": {"topic": "Creating a new corpus with NLTK", "user_name": "Skippy le Grand Gourou", "text": "\nfrom nltk.corpus.reader.plaintext import PlaintextCorpusReader\n\n\nfilecontent1 = \"This is a cow\"\nfilecontent2 = \"This is a Dog\"\n\ncorpusdir = 'nltk_data/'\nwith open(corpusdir + 'content1.txt', 'w') as text_file:\n    text_file.write(filecontent1)\nwith open(corpusdir + 'content2.txt', 'w') as text_file:\n    text_file.write(filecontent2)\n\ntext_corpus = PlaintextCorpusReader(corpusdir, [\"content1.txt\", \"content2.txt\"])\n\nno_of_words_corpus1 = len(text_corpus.words(\"content1.txt\"))\nprint(no_of_words_corpus1)\nno_of_unique_words_corpus1 = len(set(text_corpus.words(\"content1.txt\")))\n\nno_of_words_corpus2 = len(text_corpus.words(\"content2.txt\"))\nno_of_unique_words_corpus2 = len(set(text_corpus.words(\"content2.txt\")))\n\nenter code here\n\n"}, "367": {"topic": "Ordinal numbers replacement", "user_name": "skornosskornos", "text": "\nI am currently looking for the way to replace words like first, second, third,...with appropriate ordinal number representation (1st, 2nd, 3rd).\nI have been googling for the last week and I didn't find any useful standard tool or any function from NLTK.\nSo is there any or should I write some regular expressions manually?\nThanks for any advice\n"}, "368": {"topic": "Ordinal numbers replacement", "user_name": "", "text": "\nThe package number-parser can parse ordinal words (\"first\", \"second\", etc) to integers.\nfrom number_parser import parse_ordinal\nn = parse_ordinal(\"first\")\n\nTo convert an integer to \"1st\", \"2nd\", etc, you can use the following:\ndef ordinal(n: int):\n    if 11 <= (n % 100) <= 13:\n        suffix = 'th'\n    else:\n        suffix = ['th', 'st', 'nd', 'rd', 'th'][min(n % 10, 4)]\n    return str(n) + suffix\n\nHere is a more terse but less readable version (taken from Gareth on codegolf):\nordinal = lambda n: \"%d%s\" % (n,\"tsnrhtdd\"[(n//10%10!=1)*(n%10<4)*n%10::4])\n\nThis works on any number:\nprint([ordinal(n) for n in range(1,32)])\n\n['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th',\n '11th', '12th', '13th', '14th', '15th', '16th', '17th', '18th', '19th',\n '20th', '21st', '22nd', '23rd', '24th', '25th', '26th', '27th', '28th',\n '29th', '30th', '31st']\n\n"}, "369": {"topic": "Ordinal numbers replacement", "user_name": "Ben DavisBen Davis", "text": "\nIf you don't want to pull in an additional dependency on an external library (as suggested by luckydonald) but also don't want the future maintainer of the code to haunt you down and kill you (because you used golfed code in production) then here's a short-but-maintainable variant:\ndef make_ordinal(n):\n    '''\n    Convert an integer into its ordinal representation::\n\n        make_ordinal(0)   => '0th'\n        make_ordinal(3)   => '3rd'\n        make_ordinal(122) => '122nd'\n        make_ordinal(213) => '213th'\n    '''\n    n = int(n)\n    if 11 <= (n % 100) <= 13:\n        suffix = 'th'\n    else:\n        suffix = ['th', 'st', 'nd', 'rd', 'th'][min(n % 10, 4)]\n    return str(n) + suffix\n\n"}, "370": {"topic": "Ordinal numbers replacement", "user_name": "", "text": "\nHow about this:\nsuf = lambda n: \"%d%s\"%(n,{1:\"st\",2:\"nd\",3:\"rd\"}.get(n%100 if (n%100)<20 else n%10,\"th\"))\nprint [suf(n) for n in xrange(1,32)]\n\n['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th',\n '11th', '12th', '13th', '14th', '15th', '16th', '17th', '18th', '19th',\n '20th', '21st', '22nd', '23rd', '24th', '25th', '26th', '27th', '28th',\n '29th', '30th', '31st']\n\n"}, "371": {"topic": "Ordinal numbers replacement", "user_name": "Florian BruckerFlorian Brucker", "text": "\nAnother solution to format numbers to 1th, 2nd, 3rd, ... is the num2words library (pip | github).\nIt especially offers different languages, so localization/internationalization (aka. l10n/i18n) is a no-brainer.\nUsage is easy after you installed it with pip install num2words:\nfrom num2words import num2words\n# english is default\nnum2words(4458, to=\"ordinal_num\")\n'4458th'\n\n# examples for other languages\nnum2words(4458, lang=\"en\", to=\"ordinal_num\")\n'4458th'\n\nnum2words(4458, lang=\"es\", to=\"ordinal_num\")\n'4458\u00ba'\n\nnum2words(4458, lang=\"de\", to=\"ordinal_num\")\n'4458.'\n\nnum2words(4458, lang=\"id\", to=\"ordinal_num\")\n'ke-4458'\n\nBonus:\nnum2words(4458, lang=\"en\", to=\"ordinal\")\n'four thousand, four hundred and fifty-eighth'\n\n\nIf you need to parse the words \"first\", \"second\", \"third\", ... to numbers 1, 2, 3 first (as asked in the question, too), you can use the number-parser library (pip | github) to do that:\nfrom number_parser import parse_ordinal\nparse_ordinal(\"twenty third\")\n23\n\nNote that it only supports English, Hindi, Spanish, Ukrainian and Russian at the time of writing this answer.\n"}, "372": {"topic": "Ordinal numbers replacement", "user_name": "t\u00f6rzsm\u00f3kus", "text": "\nThe accepted answer to a previous question has an algorithm for half of this: it turns \"first\" into 1. To go from there to \"1st\", do something like:\nsuffixes = [\"th\", \"st\", \"nd\", \"rd\", ] + [\"th\"] * 16\nsuffixed_num = str(num) + suffixes[num % 100]\n\nThis only works for numbers 0-19.\n"}, "373": {"topic": "Ordinal numbers replacement", "user_name": "evandrixevandrix", "text": "\nI found myself doing something similar, needing to convert addresses with ordinal numbers ('Third St') to a format that a geocoder could comprehend ('3rd St').  While this isn't very elegant, one quick and dirty solution is to use the inflect.py to generate a dictionary for translation.\ninflect.py has a number_to_words() function, that will turn a number (e.g. 2) to its word form (e.g. 'two').  Additionally, there is an ordinal() function that will take any number (numeral or word form) and turn it into its ordinal form (e.g. 4 -> fourth, six -> sixth).  Neither of those, on their own, do what you're looking for, but together you can use them to generate a dictionary to translate any supplied ordinal-number-word (within a reasonable range) to its respective numeral ordinal.  Take a look:\n>>> import inflect\n>>> p = inflect.engine()\n>>> word_to_number_mapping = {}\n>>>\n>>> for i in range(1, 100):\n...     word_form = p.number_to_words(i)  # 1 -> 'one'\n...     ordinal_word = p.ordinal(word_form)  # 'one' -> 'first'\n...     ordinal_number = p.ordinal(i)  # 1 -> '1st'\n...     word_to_number_mapping[ordinal_word] = ordinal_number  # 'first': '1st'\n...\n>>> print word_to_number_mapping['sixth']\n6th\n>>> print word_to_number_mapping['eleventh']\n11th\n>>> print word_to_number_mapping['forty-third']\n43rd\n\nIf you're willing to commit some time, it might be possible to examine inflect.py's inner-workings in both of those functions and build your own code to do this dynamically (I haven't tried to do this).\n"}, "374": {"topic": "Ordinal numbers replacement", "user_name": "", "text": "\nI wanted to use ordinals for a project of mine and after a few prototypes I think this method although not small will work for any positive integer, yes any integer.\nIt works by determiniting if the number is above or below 20, if the number is below 20 it will turn the int 1 into the string 1st , 2 , 2nd; 3, 3rd; and the rest will have \"st\" added to it. \nFor numbers over 20 it will take the last and second to last digits, which I have called the tens and unit respectively and test them to see what to add to the number. \nThis is in python by the way, so I'm not sure if other languages will be able to find the last or second to last digit on a string if they do it should translate pretty easily.\ndef o(numb):\n    if numb < 20: #determining suffix for < 20\n        if numb == 1: \n            suffix = 'st'\n        elif numb == 2:\n            suffix = 'nd'\n        elif numb == 3:\n            suffix = 'rd'\n        else:\n            suffix = 'th'  \n    else:   #determining suffix for > 20\n        tens = str(numb)\n        tens = tens[-2]\n        unit = str(numb)\n        unit = unit[-1]\n        if tens == \"1\":\n           suffix = \"th\"\n        else:\n            if unit == \"1\": \n                suffix = 'st'\n            elif unit == \"2\":\n                suffix = 'nd'\n            elif unit == \"3\":\n                suffix = 'rd'\n            else:\n                suffix = 'th'\n    return str(numb)+ suffix\n\nI called the function \"o\" for ease of use and can be called by importing the file name which I called \"ordinal\" by import ordinal then ordinal.o(number).\nLet me know what you think :D\n"}, "375": {"topic": "Ordinal numbers replacement", "user_name": "luckydonaldluckydonald", "text": "\nIf using django, you could do:\nfrom django.contrib.humanize.templatetags.humanize import ordinal\nvar = ordinal(number)\n\n(or use ordinal in a django template as the template filter it was intended to be, though calling it like this from python code works as well)\nIf not using django you could steal their implementation which is very neat.\n"}, "376": {"topic": "Ordinal numbers replacement", "user_name": "CommunityBot", "text": "\nThere's an ordinal function in humanize\npip install humanize\n>>> [(x, humanize.ordinal(x)) for x in (1, 2, 3, 4, 20, 21, 22, 23, 24, 100, 101,\n...                                     102, 103, 113, -1, 0, 1.2, 13.6)]\n[(1, '1st'), (2, '2nd'), (3, '3rd'), (4, '4th'), (20, '20th'), (21, '21st'),\n (22, '22nd'), (23, '23rd'), (24, '24th'), (100, '100th'), (101, '101st'),\n (102, '102nd'), (103, '103rd'), (113, '113th'), (-1, '-1th'), (0, '0th'),\n (1.2, '1st'), (13.6, '13th')]\n\n\n"}, "377": {"topic": "Ordinal numbers replacement", "user_name": "lvclvc", "text": "\nHere is a more complicated solution I just wrote that takes into account compounded ordinals. So it works from first all the way to nine hundred and ninety ninth. I needed it to convert string street names to the number ordinals:\nimport re\nfrom collections import OrderedDict\n\nONETHS = {\n    'first': '1ST', 'second': '2ND', 'third': '3RD', 'fourth': '4TH', 'fifth': '5TH', 'sixth': '6TH', 'seventh': '7TH',\n    'eighth': '8TH', 'ninth': '9TH'\n}\n\nTEENTHS = {\n    'tenth': '10TH', 'eleventh': '11TH', 'twelfth': '12TH', 'thirteenth': '13TH',\n    'fourteenth': '14TH', 'fifteenth': '15TH', 'sixteenth': '16TH', 'seventeenth': '17TH', 'eighteenth': '18TH',\n    'nineteenth': '19TH'\n}\n\nTENTHS = {\n    'twentieth': '20TH', 'thirtieth': '30TH', 'fortieth': '40TH', 'fiftieth': '50TH', 'sixtieth': '60TH',\n    'seventieth': '70TH', 'eightieth': '80TH', 'ninetieth': '90TH',\n}\n\nHUNDREDTH = {'hundredth': '100TH'}  # HUNDREDTH not s\n\nONES = {'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7', 'eight': '8',\n        'nine': '9'}\n\nTENS = {'twenty': '20', 'thirty': '30', 'forty': '40', 'fifty': '50', 'sixty': '60', 'seventy': '70', 'eighty': '80',\n        'ninety': '90'}\n\nHUNDRED = {'hundred': '100'}\n\n# Used below for ALL_ORDINALS\nALL_THS = {}\nALL_THS.update(ONETHS)\nALL_THS.update(TEENTHS)\nALL_THS.update(TENTHS)\nALL_THS.update(HUNDREDTH)\n\nALL_ORDINALS = OrderedDict()\nALL_ORDINALS.update(ALL_THS)\nALL_ORDINALS.update(TENS)\nALL_ORDINALS.update(HUNDRED)\nALL_ORDINALS.update(ONES)\n\n\ndef split_ordinal_word(word):\n    ordinals = []\n    if not word:\n        return ordinals \n\n    for key, value in ALL_ORDINALS.items():\n        if word.startswith(key):\n            ordinals.append(key)\n            ordinals += split_ordinal_word(word[len(key):])\n            break\n    return ordinals\n\ndef get_ordinals(s):\n    ordinals, start, end = [], [], []\n    s = s.strip().replace('-', ' ').replace('and', '').lower()\n    s = re.sub(' +',' ', s)  # Replace multiple spaces with a single space\n    s = s.split(' ')\n\n    for word in s:\n        found_ordinals = split_ordinal_word(word)\n        if found_ordinals:\n            ordinals += found_ordinals\n        else:  # else if word, for covering blanks\n            if ordinals:  # Already have some ordinals\n                end.append(word)\n            else:\n                start.append(word)\n    return start, ordinals, end\n\n\ndef detect_ordinal_pattern(ordinals):\n    ordinal_length = len(ordinals)\n    ordinal_string = '' # ' '.join(ordinals)\n    if ordinal_length == 1:\n        ordinal_string = ALL_ORDINALS[ordinals[0]]\n    elif ordinal_length == 2:\n        if ordinals[0] in ONES.keys() and ordinals[1] in HUNDREDTH.keys():\n            ordinal_string = ONES[ordinals[0]] + '00TH'\n        elif ordinals[0] in HUNDRED.keys() and ordinals[1] in ONETHS.keys():\n            ordinal_string = HUNDRED[ordinals[0]][:-1] + ONETHS[ordinals[1]]\n        elif ordinals[0] in TENS.keys() and ordinals[1] in ONETHS.keys():\n            ordinal_string = TENS[ordinals[0]][0] + ONETHS[ordinals[1]]\n    elif ordinal_length == 3:\n        if ordinals[0] in HUNDRED.keys() and ordinals[1] in TENS.keys() and ordinals[2] in ONETHS.keys():\n            ordinal_string = HUNDRED[ordinals[0]][0] + TENS[ordinals[1]][0] + ONETHS[ordinals[2]]\n        elif ordinals[0] in ONES.keys() and ordinals[1] in HUNDRED.keys() and ordinals[2] in ALL_THS.keys():\n            ordinal_string =  ONES[ordinals[0]] + ALL_THS[ordinals[2]]\n    elif ordinal_length == 4:\n        if ordinals[0] in ONES.keys() and ordinals[1] in HUNDRED.keys() and ordinals[2] in TENS.keys() and \\\n           ordinals[3] in ONETHS.keys():\n                ordinal_string = ONES[ordinals[0]] + TENS[ordinals[2]][0] + ONETHS[ordinals[3]]\n\n    return ordinal_string\n\nAnd here is some sample usage:\n# s = '32 one   hundred and forty-third st toronto, on'\n#s = '32 forty-third st toronto, on'\n#s = '32 one-hundredth st toronto, on'\n#s = '32 hundred and third st toronto, on'\n#s = '32 hundred and thirty first st toronto, on'\n# s = '32 nine hundred and twenty third st toronto, on'\n#s = '32 nine hundred and ninety ninth st toronto, on'\ns = '32 sixty sixth toronto, on'\n\nst, ords, en = get_ordinals(s)\nprint st, detect_ordinal_pattern(ords), en\n\n"}, "378": {"topic": "Ordinal numbers replacement", "user_name": "", "text": "\nthis function works well for each number n.  If n is negative, it is converted to positive. If n is not integer, it is converted to integer. \ndef ordinal( n ):\n\n    suffix = ['th', 'st', 'nd', 'rd', 'th', 'th', 'th', 'th', 'th', 'th']\n\n    if n < 0:\n        n *= -1\n\n    n = int(n)\n\n    if n % 100 in (11,12,13):\n        s = 'th'\n    else:\n        s = suffix[n % 10]\n\n    return str(n) + s\n\n"}, "379": {"topic": "Ordinal numbers replacement", "user_name": "alukachalukach", "text": "\nIf you don't want to import an external module and prefer a one-line solution, then the following is probably (slightly) more readable than the accepted answer:\ndef suffix(i):\n    return {1:\"st\", 2:\"nd\", 3:\"rd\"}.get(i%10*(i%100 not in [11,12,13]), \"th\"))\n\nIt uses dictionary .get, as suggested by https://codereview.stackexchange.com/a/41300/90593 and https://stackoverflow.com/a/36977549/5069869.\nI made use of multiplication with a boolean to handle the special cases (11,12,13) without having to start an if-block. If the condition (i%100 not in [11,12,13]) evaluates to False, the whole number is 0 and we get the default 'th' case.\n"}, "380": {"topic": "Ordinal numbers replacement", "user_name": "", "text": "\nGareth's code expressed using the modern .format()\nordinal = lambda n: \"{}{}\".format(n,\"tsnrhtdd\"[(n/10%10!=1)*(n%10<4)*n%10::4])\n\n"}, "381": {"topic": "Ordinal numbers replacement", "user_name": "HounganHoungan", "text": "\nThis can handle any length number, the exceptions for ...#11 to ...#13 and negative integers.\ndef ith(i):return(('th'*(10<(abs(i)%100)<14))+['st','nd','rd',*['th']*7][(abs(i)-1)%10])[0:2]\n\nI suggest using ith() as a name to avoid overriding the builtin ord().\n# test routine\nfor i in range(-200,200):\n    print(i,ith(i))\n\nNote: Tested with Python 3.6; The abs() function was available without explicitly including a math module.\n"}, "382": {"topic": "Ordinal numbers replacement", "user_name": "Monika SulikMonika Sulik", "text": "\nTry this \nimport sys\n\na = int(sys.argv[1])\n\nfor i in range(1,a+1):\n\nj = i\nif(j%100 == 11 or j%100 == 12 or j%100 == 13):\n    print(\"%dth Hello\"%(j))\n    continue            \ni %= 10\nif ((j%10 == 1) and ((i%10 != 0) or (i%10 != 1))):\n    print(\"%dst Hello\"%(j))\nelif ((j%10 == 2) and ((i%10 != 0) or (i%10 != 1))):\n    print(\"%dnd Hello\"%(j))\nelif ((j%10 == 3) and ((i%10 != 0) or (i%10 != 1))):\n    print(\"%drd Hello\"%(j))\nelse:\n    print(\"%dth Hello\"%(j))\n\n"}, "383": {"topic": "Ordinal numbers replacement", "user_name": "Tim DielsTim Diels", "text": "\nI salute Gareth's lambda code. So elegant. I only half-understand how it works though. So I tried to deconstruct it and came up with this:\ndef ordinal(integer):\n\n    int_to_string = str(integer)\n\n    if int_to_string == '1' or int_to_string == '-1':\n        print int_to_string+'st'\n        return int_to_string+'st';\n    elif int_to_string == '2' or int_to_string == '-2':\n        print int_to_string+'nd'\n        return int_to_string+'nd';\n    elif int_to_string == '3' or int_to_string == '-3':\n        print int_to_string+'rd'\n        return int_to_string+'rd';\n\n    elif int_to_string[-1] == '1' and int_to_string[-2] != '1':\n        print int_to_string+'st'\n        return int_to_string+'st';\n    elif int_to_string[-1] == '2' and int_to_string[-2] != '1':\n        print int_to_string+'nd'\n        return int_to_string+'nd';\n    elif int_to_string[-1] == '3' and int_to_string[-2] != '1':\n        print int_to_string+'rd'\n        return int_to_string+'rd';\n\n    else:\n        print int_to_string+'th'\n        return int_to_string+'th';\n\n\n>>> print [ordinal(n) for n in range(1,25)]\n1st\n2nd\n3rd\n4th\n5th\n6th\n7th\n8th\n9th\n10th\n11th\n12th\n13th\n14th\n15th\n16th\n17th\n18th\n19th\n20th\n21st\n22nd\n23rd\n24th\n['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th',             \n'11th', '12th', '13th', '14th', '15th', '16th', '17th', '18th', '19th', \n'20th', '21st', '22nd', '23rd', '24th']\n\n"}, "384": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "Benyamin Jafari", "text": "\n\n\n\n\n\n\nClosed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.                    \n\n\n\n\n\n\n\n\n\n\n We don\u2019t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\n\n\nClosed 8 years ago.\n\n\n\n\n\n\n\r\n                        Improve this question\r\n                    \n\n\n\nI'm looking for an open source implementation, preferably in python, of Textual Sentiment Analysis (http://en.wikipedia.org/wiki/Sentiment_analysis). Is anyone familiar with such open source implementation I can use?\nI'm writing an application that searches twitter for some search term, say \"youtube\", and counts \"happy\" tweets vs. \"sad\" tweets. \nI'm using Google's appengine, so it's in python. I'd like to be able to classify the returned search results from twitter and I'd like to do that in python.\nI haven't been able to find such sentiment analyzer so far, specifically not in python. \nAre you familiar with such open source implementation I can use? Preferably this is already in python, but if not, hopefully I can translate it to python.\nNote, the texts I'm analyzing are VERY short, they are tweets. So ideally, this classifier is optimized for such short texts.\nBTW, twitter does support the \":)\" and \":(\" operators in search, which aim to do just this, but unfortunately, the classification provided by them isn't that great, so I figured I might give this a try myself.\nThanks!\nBTW, an early demo is here and the code I have so far is here and I'd love to opensource it with any interested developer.\n"}, "385": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "RanRan", "text": "\nGood luck with that.\nSentiment is enormously contextual, and tweeting culture makes the problem worse because you aren't given the context for most tweets.  The whole point of twitter is that you can leverage the huge amount of shared \"real world\" context to pack meaningful communication in a very short message.\nIf they say the video is bad, does that mean bad, or bad?\n\nA linguistics professor was lecturing\n  to her class one day. \"In English,\"\n  she said, \"A double negative forms a\n  positive. In some languages, though,\n  such as Russian, a double negative is\n  still a negative. However, there is no\n  language wherein a double positive can\n  form a negative.\"\nA voice from the back of the room\n  piped up, \"Yeah . . .right.\"\n\n"}, "386": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "dbr", "text": "\nWith most of these kinds of applications, you'll have to roll much of your own code for a statistical classification task. As Lucka suggested, NLTK is the perfect tool for natural language manipulation in Python, so long as your goal doesn't interfere with the non commercial nature of its license.  However, I would suggest other software packages for modeling.  I haven't found many strong advanced machine learning models available for Python, so I'm going to suggest some standalone binaries that easily cooperate with it.\nYou may be interested in The Toolkit for Advanced Discriminative Modeling, which can be easily interfaced with Python.  This has been used for classification tasks in various areas of natural language processing.  You also have a pick of a number of different models.  I'd suggest starting with Maximum Entropy classification so long as you're already familiar with implementing a Naive Bayes classifier.  If not, you may want to look into it and code one up to really get a decent understanding of statistical classification as a machine learning task.\nThe University of Texas at Austin computational linguistics groups have held classes where most of the projects coming out of them have used this great tool.  You can look at the course page for Computational Linguistics II to get an idea of how to make it work and what previous applications it has served.\nAnother great tool which works in the same vein is Mallet.  The difference between Mallet is that there's a bit more documentation and some more models available, such as decision trees, and it's in Java, which, in my opinion, makes it a little slower.  Weka is a whole suite of different machine learning models in one big package that includes some graphical stuff, but it's really mostly meant for pedagogical purposes, and isn't really something I'd put into production.\nGood luck with your task.  The real difficult part will probably be the amount of knowledge engineering required up front for you to classify the 'seed set' off of which your model will learn.  It needs to be pretty sizeable, depending on whether you're doing binary classification (happy vs sad) or a whole range of emotions (which will require even more).  Make sure to hold out some of this engineered data for testing, or run some tenfold or remove-one tests to make sure you're actually doing a good job predicting before you put it out there. And most of all, have fun!  This is the best part of NLP and AI, in my opinion.\n"}, "387": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "MarkusQMarkusQ", "text": "\nThanks everyone for your suggestions, they were indeed very useful!\nI ended up using a Naive Bayesian classifier, which I borrowed from here. \nI started by feeding it with a list of good/bad keywords and then added a \"learn\" feature by employing user feedback. It turned out to work pretty nice.\nThe full details of my work as in a blog post.\nAgain, your help was very useful, so thank you!\n"}, "388": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "Robert ElwellRobert Elwell", "text": "\nI have constructed a word list labeled with sentiment. You can access it from here:\nhttp://www2.compute.dtu.dk/pubdb/views/edoc_download.php/6010/zip/imm6010.zip\nYou will find a short Python program on my blog: \nhttp://finnaarupnielsen.wordpress.com/2011/06/20/simplest-sentiment-analysis-in-python-with-af/\nThis post displays how to use the word list with single sentences as well as with Twitter.\nWord lists approaches have their limitations. You will find a investigation of the limitations of my word list in the article \"A new ANEW: Evaluation of a word list for sentiment analysis in microblogs\". That article is available from my homepage.\nPlease note a unicode(s, 'utf-8') is missing from the code (for paedagogic reasons).\n"}, "389": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "RanRan", "text": "\nA lot of research papers indicate that a good starting point for sentiment analysis is looking at adjectives, e.g., are they positive adjectives or negative adjectives. For a short block of text this is pretty much your only option... There are papers that look at entire documents, or sentence level analysis, but as you say tweets are quite short... There is no real magic approach to understanding the sentiment of a sentence, so I think your best bet would be hunting down one of these research papers and trying to get their data-set of positively/negatively oriented adjectives.\nNow, this having been said, sentiment is domain specific, and you might find it difficult to get a high-level of accuracy with a general purpose data-set.\nGood luck.\n"}, "390": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "", "text": "\nI think you may find it difficult to find what you're after. The closest thing that I know of is LingPipe, which has some sentiment analysis functionality and is available under a limited kind of open-source licence, but is written in Java.\nAlso, sentiment analysis systems are usually developed by training a system on product/movie review data which is significantly different from the average tweet. They are going to be optimised for text with several sentences, all about the same topic. I suspect you would do better coming up with a rule-based system yourself, perhaps based on a lexicon of sentiment terms like the one the University of Pittsburgh provide.\nCheck out We Feel Fine for an implementation of similar idea with a really beautiful interface (and twitrratr).\n"}, "391": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "Finn \u00c5rup NielsenFinn \u00c5rup Nielsen", "text": "\nTake a look at Twitter sentiment analysis tool. It's written in python, and it uses Naive Bayes classifier with semi-supervised machine learning. The source can be found here.\n"}, "392": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "        Ben CoeBen Coe", "text": "\nMaybe TextBlob (based on NLTK and pattern) is the right sentiment analysis tool for you.\n"}, "393": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "", "text": "\nI came across Natural Language Toolkit a while ago. You could probably use it as a starting point. It also has a lot of modules and addons, so maybe they already have something similar.\n"}, "394": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "StompchickenStompchicken", "text": "\nSomewhat wacky thought: you could try using the Twitter API to download a large set of tweets, and then classifying a subset of that set using emoticons: one positive group for  \":)\", \":]\", \":D\", etc, and another negative group with \":(\", etc.\nOnce you have that crude classification, you could search for more clues with frequency or ngram analysis or something along those lines.\nIt may seem silly, but serious research has been done on this (search for \"sentiment analysis\" and emoticon). Worth a look.  \n"}, "395": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "Garrett Hyde", "text": "\nThere's a Twitter Sentiment API by TweetFeel that does advanced linguistic analysis of tweets, and can retrieve positive/negative tweets. See http://www.webservius.com/corp/docs/tweetfeel_sentiment.htm\n"}, "396": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "cyhexcyhex", "text": "\nFor those interested in coding Twitter Sentiment Analyis from scratch, there is a Coursera course \"Data Science\" with python code on GitHub (as part of assignment 1 - link). The sentiments are part of the AFINN-111.\nYou can find working solutions, for example here. In addition to the AFINN-111 sentiment list, there is a simple implementation of builing a dynamic term list based on frequency of terms in tweets that have a pos/neg score (see here).\n"}, "397": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "CommunityBot", "text": "\n\n\n\n\n\n\nThis question already has answers here:                    \n\n\nClosed 10 years ago.\n\n\n\n\nPossible Duplicate:\nHow does the Google \u201cDid you mean?\u201d Algorithm work? \n\nSuppose you have a search system already in your website. How can you implement the \"Did you mean:<spell_checked_word>\" like Google does in some search queries?\n"}, "398": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "pekpek", "text": "\nActually what Google does is very much non-trivial and also at first counter-intuitive. They don't do anything like check against a dictionary, but rather they make use of statistics to identify \"similar\" queries that returned more results than your query, the exact algorithm is of course not known.\nThere are different sub-problems to solve here, as a fundamental basis for all Natural Language Processing statistics related there is one must have book: Foundation of Statistical Natural Language Processing.\nConcretely to solve the problem of word/query similarity I have had good results with using Edit Distance, a mathematical measure of string similarity that works surprisingly well. I used to use Levenshtein but the others may be worth looking into.\nSoundex - in my experience - is crap.\nActually efficiently storing and searching a large dictionary of misspelled words and having sub second retrieval is again non-trivial, your best bet is to make use of existing  full text indexing and retrieval engines (i.e. not your database's one), of which Lucene is currently one of the best and coincidentally ported to many many platforms.\n"}, "399": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "Boris TerzicBoris Terzic", "text": "\nGoogle's Dr Norvig has outlined how it works; he even gives a 20ish line Python implementation:\nhttp://googlesystem.blogspot.com/2007/04/simplified-version-of-googles-spell.html\nhttp://www.norvig.com/spell-correct.html\nDr Norvig also discusses the \"did you mean\" in this excellent talk.   Dr Norvig is head of research at Google - when asked how \"did you mean\" is implemented, his answer is authoritive.\nSo its spell-checking, presumably with a dynamic dictionary build from other searches or even actual internet phrases and such.  But that's still spell checking.\nSOUNDEX and other guesses don't get a look in, people!\n"}, "400": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "", "text": "\nCheck this article on wikipedia about the Levenshtein distance. Make sure you take a good look at Possible improvements.\n"}, "401": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "WillWill", "text": "\nI was pleasantly surprised that someone has asked how to create a state-of-the-art spelling suggestion system for search engines. I have been working on this subject for more than a year for a search engine company and I can point to information on the public domain on the subject.\nAs was mentioned in a previous post, Google (and Microsoft and Yahoo!) do not use any predefined dictionary nor do they employ hordes of linguists that ponder over the possible misspellings of queries. That would be impossible due to the scale of the problem but also because it is not clear that people could actually correctly identify when and if a query is misspelled.\nInstead there is a simple and rather effective principle that is also valid for all European languages. Get all the unique queries on your search logs, calculate the edit distance between all pairs of queries, assuming that the reference query is the one that has the highest count. \nThis simple algorithm will work great for many types of queries. If you want to take it to the next level then I suggest you read the paper by Microsoft Research on that subject. You can find it here \nThe paper has a great introduction but after that you will need to be knowledgeable with concepts such as the Hidden Markov Model.\n"}, "402": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "        Ionut AnghelcoviciIonut Anghelcovici", "text": "\nI would suggest looking at SOUNDEX to find similar words in your database.\nYou can also access google own dictionary by using the Google API spelling suggestion request.\n"}, "403": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "Costas BoulisCostas Boulis", "text": "\nYou may want to look at Peter Norvig's \"How to Write a Spelling Corrector\" article.\n"}, "404": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "", "text": "\nI believe Google logs all queries and identifies when someone makes a spelling correction. This correction may then be suggested when others supply the same first query. This will work for any language, in fact any string of any characters.\n"}, "405": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "EspoEspo", "text": "\nhttp://en.wikipedia.org/wiki/N-gram#Google_use_of_N-gram\n"}, "406": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "FA.FA.", "text": "\nI think this depends on how big your website it. On our local Intranet which is used by about 500 member of staff, I simply look at the search phrases that returned zero results and enter that search phrase with the new suggested search phrase into a SQL table.\nI them call on that table if no search results has been returned, however, this only works if the site is relatively small and I only do it for search phrases which are the most common.\nYou might also want to look at my answer to a similar question:\n\n\"Similar Posts\" like functionality using MS SQL Server?\n\n"}, "407": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "LiamLiam", "text": "\nIf you have industry specific translations, you will likely need a thesaurus. For example, I worked in the jewelry industry and there were abbreviate in our descriptions such as kt - karat, rd - round, cwt - carat weight... Endeca (the search engine at that job) has a thesaurus that will translate  from common misspellings, but it does require manual intervention.\n"}, "408": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "robakerrobaker", "text": "\nI do it with Lucene's Spell Checker.\n"}, "409": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "CommunityBot", "text": "\nSoundex is good for phonetic matches, but works best with peoples' names (it was originally developed for census data)\nAlso check out Full-Text-Indexing, the syntax is different from Google logic, but it's very quick and can deal with similar language elements.\n"}, "410": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "GateKillerGateKiller", "text": "\nSoundex and \"Porter stemming\" (soundex is trivial, not sure about porter stemming).\n"}, "411": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "oglesteroglester", "text": "\nThere's something called aspell that might help:\nhttp://blog.evanweaver.com/files/doc/fauna/raspell/classes/Aspell.html\nThere's a ruby gem for it, but I don't know how to talk to it from python\nhttp://blog.evanweaver.com/files/doc/fauna/raspell/files/README.html\nHere's a quote from the ruby implementation\n\nUsage\nAspell lets you check words and suggest corrections. For example:\n  string = \"my haert wil go on\"\n\n  string.gsub(/[\\w\\']+/) do |word|\n    if !speller.check(word)\n      # word is wrong\n      puts \"Possible correction for #{word}:\"\n      puts speller.suggest(word).first\n    end\n  end\n\n\nThis outputs:\nPossible correction for haert:\nheart\nPossible correction for wil:\nWill\n"}, "412": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "cherouvimcherouvim", "text": "\nImplementing spelling correction for search engines in an effective way is not trivial (you can't just compute the edit/levenshtein distance to every possible word). A solution based on k-gram indexes is described in Introduction to Information Retrieval (full text available online).\n"}, "413": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "KeithKeith", "text": "\nU could use ngram for the comparisment: http://en.wikipedia.org/wiki/N-gram\nUsing python ngram module: http://packages.python.org/ngram/index.html\nimport ngram\n\nG2 = ngram.NGram([  \"iis7 configure ftp 7.5\",\n                    \"ubunto configre 8.5\",\n                    \"mac configure ftp\"])\n\nprint \"String\", \"\\t\", \"Similarity\"\nfor i in G2.search(\"iis7 configurftp 7.5\", threshold=0.1):\n    print i[1], \"\\t\", i[0]\n\nU get:\n>>> \nString  Similarity\n0.76    \"iis7 configure ftp 7.5\"    \n0.24    \"mac configure ftp\"\n0.19    \"ubunto configre 8.5\"   \n\n"}, "414": {"topic": "How do you implement a \"Did you mean\"? [duplicate]", "user_name": "Michael NealeMichael Neale", "text": "\nWhy not use google's did you mean in your code.For how see here\nhttp://narenonit.blogspot.com/2012/08/trick-for-using-googles-did-you-mean.html\n"}, "415": {"topic": "Fuzzy String Comparison", "user_name": "San Jacinto", "text": "\nWhat I am striving to complete is a program which reads in a file and will compare each sentence according to the original sentence. The sentence which is a perfect match to the original will receive a score of 1 and a sentence which is the total opposite will receive a 0. All other fuzzy sentences will receive a grade in between 1 and 0. \nI am unsure which operation to use to allow me to complete this in Python 3. \nI have included the sample text in which the Text 1 is the original and the other preceding strings are the comparisons.  \nText: Sample\nText 1: It was a dark and stormy night. I was all alone sitting on a red chair. I was not completely alone as I had three cats.\nText 20: It was a murky and stormy night. I was all alone sitting on a crimson chair. I was not completely alone as I had three felines\n// Should score high point but not 1\nText 21: It was a murky and tempestuous night. I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines\n// Should score lower than text 20\nText 22: I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines. It was a murky and tempestuous night.\n// Should score lower than text 21 but NOT 0\nText 24: It was a dark and stormy night. I was not alone. I was not sitting on a red chair. I had three cats.\n// Should score a 0!\n"}, "416": {"topic": "Fuzzy String Comparison", "user_name": "jacksonstephencjacksonstephenc", "text": "\nThere is a package called fuzzywuzzy. Install via pip:\npip install fuzzywuzzy\n\nSimple usage:\n>>> from fuzzywuzzy import fuzz\n>>> fuzz.ratio(\"this is a test\", \"this is a test!\")\n    96\n\nThe package is built on top of difflib. Why not just use that, you ask? Apart from being a bit simpler, it has a number of different matching methods (like token order insensitivity, partial string matching) which make it more powerful in practice. The process.extract functions are especially useful: find the best matching strings and ratios from a set. From their readme:\n\nPartial Ratio\n\n>>> fuzz.partial_ratio(\"this is a test\", \"this is a test!\")\n    100\n\n\nToken Sort Ratio\n\n>>> fuzz.ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\")\n    90\n>>> fuzz.token_sort_ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\")\n    100\n\n\nToken Set Ratio\n\n>>> fuzz.token_sort_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\")\n    84\n>>> fuzz.token_set_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\")\n    100\n\n\nProcess\n\n>>> choices = [\"Atlanta Falcons\", \"New York Jets\", \"New York Giants\", \"Dallas Cowboys\"]\n>>> process.extract(\"new york jets\", choices, limit=2)\n    [('New York Jets', 100), ('New York Giants', 78)]\n>>> process.extractOne(\"cowboys\", choices)\n    (\"Dallas Cowboys\", 90)\n\n"}, "417": {"topic": "Fuzzy String Comparison", "user_name": "congusbonguscongusbongus", "text": "\nThere is a module in the standard library (called difflib) that can compare strings and return a score based on their similarity. The SequenceMatcher class should do what you want.\nSmall example from Python prompt:\n>>> from difflib import SequenceMatcher as SM\n>>> s1 = ' It was a dark and stormy night. I was all alone sitting on a red chair. I was not completely alone as I had three cats.'\n>>> s2 = ' It was a murky and stormy night. I was all alone sitting on a crimson chair. I was not completely alone as I had three felines.'\n>>> SM(None, s1, s2).ratio()\n0.9112903225806451\n\n"}, "418": {"topic": "Fuzzy String Comparison", "user_name": "        user17242583\r", "text": "\nfuzzyset is much faster than fuzzywuzzy (difflib) for both indexing and searching.\nfrom fuzzyset import FuzzySet\ncorpus = \"\"\"It was a murky and stormy night. I was all alone sitting on a crimson chair. I was not completely alone as I had three felines\n    It was a murky and tempestuous night. I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines\n    I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines. It was a murky and tempestuous night.\n    It was a dark and stormy night. I was not alone. I was not sitting on a red chair. I had three cats.\"\"\"\ncorpus = [line.lstrip() for line in corpus.split(\"\\n\")]\nfs = FuzzySet(corpus)\nquery = \"It was a dark and stormy night. I was all alone sitting on a red chair. I was not completely alone as I had three cats.\"\nfs.get(query)\n# [(0.873015873015873, 'It was a murky and stormy night. I was all alone sitting on a crimson chair. I was not completely alone as I had three felines')]\n\nWarning: Be careful not to mix unicode and bytes in your fuzzyset.\n"}, "419": {"topic": "Fuzzy String Comparison", "user_name": "macmac", "text": "\nThe task is called Paraphrase Identification which is an active area of research in Natural Language Processing. I have linked several state of the art papers many of which you can find open source code on GitHub for.\nNote that all the answered question assume that there is some string/surface similarity between the two sentences while in reality two sentences with little string similarity can be semantically similar.\nIf you're interested in that kind of similarity you can use Skip-Thoughts.\nInstall the software according to the GitHub guides and go to paraphrase detection section in readme:\nimport skipthoughts\nmodel = skipthoughts.load_model()\nvectors = skipthoughts.encode(model, X_sentences)\n\nThis converts your sentences (X_sentences) to vectors. Later you can find the similarity of two vectors by:\nsimilarity = 1 - scipy.spatial.distance.cosine(vectors[0], vectors[1])\n\nwhere we are assuming vector[0] and vector1 are the corresponding vector to X_sentences[0], X_sentences1 which you wanted to find their scores.\nThere are other models to convert a sentence to a vector which you can find here.\nOnce you convert your sentences into vectors the similarity is just a matter of finding the Cosine similarity between those vectors.\nUpdate in 2020\nThere is this new model called BERT released by Google based on a deep learning framework called Tensorflow. There is also an implementation that many people find easier to use called Transformers. What these programs do, is that they accept two phrases or sentences, and they are able to be trained to say if these two phrases/sentences are the same or not. To train them, you need a number of sentences with labels 1 or 0 (if they have the same meaning or not). You train these models using your training data (already labelled data), and then you'll be able to use the trained model to make prediction for a new pair of phrases/sentences. You can find how to train (they call it fine-tune) these models on their corresponding github pages or in many other places such as this.\nThere are also already labelled training data available in English called MRPC (microsoft paraphrase identification corpus). Note that there  multilingual or language-specific versions of BERT also exists so this model can be extended (e.g. trained) in other languages as well.  \n"}, "420": {"topic": "Fuzzy String Comparison", "user_name": "", "text": "\nThere is also this fast and accurate fuzzy comparison library licensed by MIT:\nhttps://github.com/maxbachmann/RapidFuzz\n"}, "421": {"topic": "Fuzzy string search library in Java [closed]", "user_name": "", "text": "\n\n\n\n\n\n\nClosed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.                    \n\n\n\n\n\n\n\n\n\n\n We don\u2019t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\n\n\nClosed 5 years ago.\n\n\n\n\n\n\n\r\n                        Improve this question\r\n                    \n\n\n\nI'm looking for a high performance Java library for fuzzy string search.\nThere are numerous algorithms to find similar strings, Levenshtein distance, Daitch-Mokotoff Soundex, n-grams etc.\nWhat Java implementations exists? Pros and cons for them? I'm aware of Lucene, any other solution or Lucene is best?\nI found these, does anyone have experience with them?  \n\nSimMetrics \nNGramJ \n\n"}, "422": {"topic": "Fuzzy string search library in Java [closed]", "user_name": "\r", "text": "\nCommons Lang has an implementation of Levenshtein distance.\nCommons Codec has an implementation of soundex and metaphone.\n"}, "423": {"topic": "Fuzzy string search library in Java [closed]", "user_name": "\r", "text": "\nIf you are mostly comparing short strings and want something portable and lightweight you can use the well known python algorithm fuzzywuzzy ported to Java.\nYou can read more about it here\n"}, "424": {"topic": "Fuzzy string search library in Java [closed]", "user_name": "", "text": "\nYou can use Apache Lucene, but depending on the use case this may be too heavy weight. For very simple fuzzy searches it may be a bit complex to use and (correct me if I'm wrong) it requires you to build an index.\nIf you need a simple online (= not maintaining an index) algorithm you can use the fuzzy Bitap algorithm. I found an implementation in Java here. It's code fits in a single relatively short method with an almost self-explaining signature:\npublic static List<Integer> find(String doc, String pattern, int k)\n\nApache Commons StringUtils has an implementation of the Levenshtein algorithm for fuzzy String matching. It can be seen as the fuzzy version of String.equals, Bitap is like the fuzzy version of String.indexOf and still uses the Levenshtein distance measure. It is generally more efficient than naively using Levenshtein to compare the search pattern with each substring that could possibly match.\nNotes: \n\nThe Bitap algorithm seems to be mostly useful for relatively small\nalphabets, e.g. plain ASCII. In fact the Simon Watiau version I linked to throws an ArrayIndexOutOfBoundsException on non-ASCII characters (>= 128) so you will have to filter these out.\nI tried using Bimap in an application to search an in-memory list of persons by name. I found that a Levenhstein distance of 2\ngives way too many false positives. A Levenhstein distance of 1 works\nbetter, but it cannot detect a typo where you swap two letters, e.g.\n\"William\" and \"Willaim\".  I can think of a few ways to solve this,\ne.g. \n\ndo a fuzzy search only when an exact search finds no matches (and show a message to the user about this)\nadjust Bitap to use Damerau-Levenshtein distance where a swap has distance 1 instead of 2. According to wikipedia, this is possible, but I could not find an existing implementation in Java.\ninstead of \"contains\" do a \"startsWith\". The fuzzy search tools contains a prefix version of Damerau-Levenshtein, but it gave me an ArrayIndexOutOfBoundsException\nadjust the algorithm to introduce search result ranking where exact matches score higher\n\nIf you are going to do 2 or 4, it may\nbe better to use a proper full-text search library like Lucene\nanyway.\nMore information on fuzzy search can be found on this blog. It's author\nalso created an implementation in Java called BitapOnlineSearcher,\nbut requires you to use java.io.Reader together with an Alphabet\nclass. It's Javadoc is written in Russian.\n\n"}, "425": {"topic": "Fuzzy string search library in Java [closed]", "user_name": "\r", "text": "\nSimMetrics is probably what you need: http://sourceforge.net/projects/simmetrics/\nIt has several algorithms for calculating various flavours of edit-distance.\nLucene is a very powerful full-text search engine, but FT search isn't exactly the same thing as fuzzy string matching (eg. given a list of strings find me the one that is most similar to some candidate string).\n"}, "426": {"topic": "Fuzzy string search library in Java [closed]", "user_name": "\r", "text": "\nTo Lucene I would add SOLR http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters\n"}, "427": {"topic": "Fuzzy string search library in Java [closed]", "user_name": "", "text": "\nYou can try the Completely library, it relies on text preprocessing to create an in-memory index for efficiently answering (fuzzy) searches in large data sets. Unlike Lucene and other full featured text search libraries, the API is small and easy to get started.\n"}, "428": {"topic": "Fuzzy string search library in Java [closed]", "user_name": "\r", "text": "\nApache Lucene is the only way, I think. I don't know any better search lib.\n\nApache Lucene(TM) is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.\n\n"}, "429": {"topic": "Fuzzy string search library in Java [closed]", "user_name": "\r", "text": "\nYou can try bitap. I was playing with bitap written in  ANSI C and it was pretty fast there is java implementation in http://www.crosswire.org. \n"}, "430": {"topic": "nginx not listening to port 80", "user_name": "NyxynyxNyxynyx", "text": "\nI've just installed a Ubuntu 12.04 server and nginx 1.2.7, removed default from sites-enabled and added my own file into sites-available and symlink at sites-enabled. Then restarted nginx.\nProblem: However going to the URL does not load the site. netstat -nlp | grep nginx and netstat -nlp | grep 80 both returns no results! lsof -i :80 also returns nothing. A dig from another server returns the correct ip address so it shouldn't be a DNS problem. I was able to connect to apache which I have now stopped its service. nginx logs also show nothing.\nHow should I troubleshoot this problem?\n/etc/nginx/site-available/mysite.com\nserver {\n    listen   80;\n    server_name www.mysite.com mysite.com *.mysite.com;\n    access_log /var/log/nginx/access.log;\n    error_log /var/log/nginx/error.log;\n    root /var/www/mysite/public;\n\n    index index.php index.html;\n\n    location / {\n        try_files $uri $uri/ /index.php?$args ;\n    }\n    location ~ \\.php$ {\n        fastcgi_pass unix:/var/run/php5-fpm.sock;\n        fastcgi_index index.php;\n        include fastcgi_params;\n        fastcgi_read_timeout 300;\n    }\n\n}\n\n"}, "431": {"topic": "nginx not listening to port 80", "user_name": "Steve Davis", "text": "\nI had this same problem, the solution was that I had not symlinked my siteconf file correctly. Try running vim /etc/nginx/sites-enabled/mysite.com\u2014can you get to it? I was getting \"Permission Denied.\"\nIf not run:\nrm /etc/nginx/sites-enabled/mysite.com\nln -s /etc/nginx/sites-available/mysite.com /etc/nginx/sites-enabled/mysite.com\n\n"}, "432": {"topic": "nginx not listening to port 80", "user_name": "thciprianithcipriani", "text": "\nIf your logs are silent on the issue, you may not be including the sites-enabled directory. One simple way to tell that the site is being loaded is to set the error/access log path within your server block to a unique path, reload nginx, and check if the files are created.\nEnsure the following include directive exists within the http context in /etc/nginx/nginx.conf.\nhttp {\n  ...\n  include /etc/nginx/sites-enabled/*;\n}\n\n"}, "433": {"topic": "nginx not listening to port 80", "user_name": "PatrickPatrick", "text": "\nI've found it helpful to approach debugging nginx with the following steps:\n1... Make sure nginx is running.\nps aux | grep nginx\n\n2... Check for processes already bound to the port in question.\nlsof -n -i:80\n\n3... Make sure nginx has been reloaded.\nsudo nginx -t\nsudo nginx -s reload\n\n\nOn Mac, brew services restart nginx is not sufficient to reload nginx.\n\n4... Try creating simple responses manually to make sure your location path isn't messed up. This is especially helpful when problems arise while using proxy_pass to forward requests to other running apps.\nlocation / {\n    add_header Content-Type text/html;\n    return 200 'Here I am!';\n}\n\n"}, "434": {"topic": "nginx not listening to port 80", "user_name": "spencer.smspencer.sm", "text": "\nI ran into the same problem, I got a Failed to load resource: net::ERR_CONNECTION_REFUSED error when connecting over HTTP, but fine over HTTPS. Ran netstat -tulpn and saw nginx not binding to port 80 for IPv4. Done everything described here. Turned out to be something very stupid:\nMake sure the sites-available file with the default_server is actually enabled.\nHope this saved some other poor idiot out there some time.\n"}, "435": {"topic": "nginx not listening to port 80", "user_name": "dayulolidayuloli", "text": "\nYou are probably binding nginx to port 80 twice. Is that your full config file? Don't you have another statement listening to port 80?\n"}, "436": {"topic": "nginx not listening to port 80", "user_name": "AndresAndres", "text": "\nA semi-colon ; missing in /etc/nginx/nginx.conf for exemple on the line before include /etc/nginx/servers-enabled/*; can just bypass this intruction and nginx -t check will be successful anyway.\nSo just check that all instructions in /etc/nginx/nginx.conf are ended with a semi-colon ;.\n"}, "437": {"topic": "nginx not listening to port 80", "user_name": "norajnoraj", "text": "\nI had faced the same problem over the server, here I am listing the how I had solved it :\nStep 1 :: Installing the Ngnix\nsudo apt update\nsudo apt install nginx\n\nStep 2 \u2013 Adjusting the Firewall\nsudo ufw app list\n\nYou should get a listing of the application profiles:\n\nOutput\n  Available applications:\n    Nginx Full\n    Nginx HTTP\n    Nginx HTTPS\n    OpenSSH\n\nAs you can see, there are three profiles available for Nginx:\nNginx Full: This profile opens both port 80 (normal, unencrypted web traffic) and port 443 (TLS/SSL encrypted traffic)\nNginx HTTP: This profile opens only port 80 (normal, unencrypted web traffic)\nNginx HTTPS: This profile opens only port 443 (TLS/SSL encrypted traffic)\nSince I haven\u2019t configured SSL for our server yet in this guide, we will only need to allow traffic on port 80.You can enable this by typing:\nsudo ufw allow 'Nginx HTTP'\n\nYou can verify the change by typing:\nsudo ufw status\n\nStep 3 \u2013 Checking your Web Server\nsystemctl status nginx\n\nNow Check port 80 , It worked for me hope will work for you as well.\n"}, "438": {"topic": "nginx not listening to port 80", "user_name": "kuldeep Mishrakuldeep Mishra", "text": "\nHave you checked if your nginx binary really exists? please check if\n#whereis nginx\n\noutputs the binary path and check this path with your init script from /etc/init.d/nginx. e.g.\nDAEMON=/usr/sbin/nginx\n\n(In my init script \"test -x $DAEMON || exit 0\" is invoked and in any case this script returned nothing - my binary was completely missing)\n"}, "439": {"topic": "nginx not listening to port 80", "user_name": "DionysiusDionysius", "text": "\nWhile we all think we don't make silly mistakes, we do.\nSo, if you are looking into NGINX issues and all signs are showing it should work then you should take a step away from the files and look downstream.\nSystem Firewall, Hardware Firewall, Nat router/firewall.\nFor myself this issue was my router, I run a home lab and so I can access services behind my router from afar I use NGINX to reverse proxy as my router only handles incoming based on IP and doesn't do any handling of hostnames, I'm sure this is all fairly normal.\nIn any case my issue cropped up as I was securing my network a few days ago, removing some port forwarding that isnt needed any longer and I accidentally removed port 80.\nYes it was as simple as forwarding that port again to NGINX and all was fixed.\nI will now walk away with my head hung in extreme shame though I leave this answer to show my gratitude to the people in this thread that lead me to find my own error.\nSo thank you.\n"}, "440": {"topic": "nginx not listening to port 80", "user_name": "CodingInTheUKCodingInTheUK", "text": "\nIn my case those network command's outputs showed nginx was correctly binding to port 80, yet the ports weren't externally accessible or visible with nmap.\nWhile I suspected a firewall,  it turns out that old iptables rules on the machine were redirecting traffic from those ports and conflicting with nginx. Use sudo iptables-save to view all currently applicable rules.\n"}, "441": {"topic": "nginx not listening to port 80", "user_name": "SilveriSilveri", "text": "\nI am facing the same issue. Just reload the nginx help me\nsudo nginx -t\n\nIf you got error then just delete the log.txt file\nthen,\nsudo nginx -s reload\n\n"}, "442": {"topic": "How to return history of validation loss in Keras", "user_name": "Marcin Mo\u017cejko", "text": "\nUsing Anaconda Python 2.7 Windows 10.\nI am training a language model using the Keras exmaple:\nprint('Build model...')\nmodel = Sequential()\nmodel.add(GRU(512, return_sequences=True, input_shape=(maxlen, len(chars))))\nmodel.add(Dropout(0.2))\nmodel.add(GRU(512, return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(len(chars)))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\ndef sample(a, temperature=1.0):\n    # helper function to sample an index from a probability array\n    a = np.log(a) / temperature\n    a = np.exp(a) / np.sum(np.exp(a))\n    return np.argmax(np.random.multinomial(1, a, 1))\n\n\n# train the model, output generated text after each iteration\nfor iteration in range(1, 3):\n    print()\n    print('-' * 50)\n    print('Iteration', iteration)\n    model.fit(X, y, batch_size=128, nb_epoch=1)\n    start_index = random.randint(0, len(text) - maxlen - 1)\n\n    for diversity in [0.2, 0.5, 1.0, 1.2]:\n        print()\n        print('----- diversity:', diversity)\n\n        generated = ''\n        sentence = text[start_index: start_index + maxlen]\n        generated += sentence\n        print('----- Generating with seed: \"' + sentence + '\"')\n        sys.stdout.write(generated)\n\n        for i in range(400):\n            x = np.zeros((1, maxlen, len(chars)))\n            for t, char in enumerate(sentence):\n                x[0, t, char_indices[char]] = 1.\n\n            preds = model.predict(x, verbose=0)[0]\n            next_index = sample(preds, diversity)\n            next_char = indices_char[next_index]\n\n            generated += next_char\n            sentence = sentence[1:] + next_char\n\n            sys.stdout.write(next_char)\n            sys.stdout.flush()\n        print()\n\nAccording to Keras documentation, the model.fit method returns a History callback, which has a history attribute containing the lists of successive losses and other metrics.\nhist = model.fit(X, y, validation_split=0.2)\nprint(hist.history)\n\nAfter training my model, if I run print(model.history) I get the error:\n AttributeError: 'Sequential' object has no attribute 'history'\n\nHow do I return my model history after training my model with the above code?\nUPDATE\nThe issue was that:\nThe following had to first be defined:\nfrom keras.callbacks import History \nhistory = History()\n\nThe callbacks option had to be called\nmodel.fit(X_train, Y_train, nb_epoch=5, batch_size=16, callbacks=[history])\n\nBut now if I print\nprint(history.History)\n\nit returns\n{}\n\neven though I ran an iteration. \n"}, "443": {"topic": "How to return history of validation loss in Keras", "user_name": "ishidoishido", "text": "\nJust an example started from\nhistory = model.fit(X, Y, validation_split=0.33, nb_epoch=150, batch_size=10, verbose=0)\n\nYou can use\nprint(history.history.keys())\n\nto list all data in history.\nThen, you can print the history of validation loss like this: \nprint(history.history['val_loss'])\n\n"}, "444": {"topic": "How to return history of validation loss in Keras", "user_name": "Sahil Mittal", "text": "\nIt's been solved.\nThe losses only save to the History over the epochs. I was running iterations instead of using the Keras built in epochs option.\nso instead of doing 4 iterations I now have\nmodel.fit(......, nb_epoch = 4)\n\nNow it returns the loss for each epoch run:\nprint(hist.history)\n{'loss': [1.4358016599558268, 1.399221191623641, 1.381293383180471, 1.3758836857303727]}\n\n"}, "445": {"topic": "How to return history of validation loss in Keras", "user_name": "Jeremy AnifaccJeremy Anifacc", "text": "\nThe following simple code works great for me:\n    seqModel =model.fit(x_train, y_train,\n          batch_size      = batch_size,\n          epochs          = num_epochs,\n          validation_data = (x_test, y_test),\n          shuffle         = True,\n          verbose=0, callbacks=[TQDMNotebookCallback()]) #for visualization\n\nMake sure you assign the fit function to an output variable. Then you can access that variable very easily\n# visualizing losses and accuracy\ntrain_loss = seqModel.history['loss']\nval_loss   = seqModel.history['val_loss']\ntrain_acc  = seqModel.history['acc']\nval_acc    = seqModel.history['val_acc']\nxc         = range(num_epochs)\n\nplt.figure()\nplt.plot(xc, train_loss)\nplt.plot(xc, val_loss)\n\nHope this helps.\nsource: https://keras.io/getting-started/faq/#how-can-i-record-the-training-validation-loss-accuracy-at-each-epoch\n"}, "446": {"topic": "How to return history of validation loss in Keras", "user_name": "craymichael", "text": "\nThe dictionary with histories of \"acc\", \"loss\", etc. is available and saved in hist.history variable.\n"}, "447": {"topic": "How to return history of validation loss in Keras", "user_name": "ishidoishido", "text": "\nI have also found that you can use verbose=2 to make keras print out the Losses:\nhistory = model.fit(X, Y, validation_split=0.33, nb_epoch=150, batch_size=10, verbose=2)\n\nAnd that would print nice lines like this:\nEpoch 1/1\n - 5s - loss: 0.6046 - acc: 0.9999 - val_loss: 0.4403 - val_acc: 0.9999\n\nAccording to their documentation:\nverbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n\n"}, "448": {"topic": "How to return history of validation loss in Keras", "user_name": "", "text": "\nFor plotting the loss directly the following works:\nimport matplotlib.pyplot as plt\n...    \nmodel_ = model.fit(X, Y, epochs= ..., verbose=1 )\nplt.plot(list(model_.history.values())[0],'k-o')\n\n"}, "449": {"topic": "How to return history of validation loss in Keras", "user_name": "Rami AlloushRami Alloush", "text": "\nAnother option is CSVLogger: https://keras.io/callbacks/#csvlogger.\nIt creates a csv file appending the result of each epoch. Even if you interrupt training, you get to see how it evolved.\n"}, "450": {"topic": "How to return history of validation loss in Keras", "user_name": "", "text": "\nActually, you can also do it with the iteration method. Because sometimes we might need to use the iteration method instead of the built-in epochs method to visualize the training results after each iteration.\nhistory = [] #Creating a empty list for holding the loss later\nfor iteration in range(1, 3):\n    print()\n    print('-' * 50)\n    print('Iteration', iteration)\n    result = model.fit(X, y, batch_size=128, nb_epoch=1) #Obtaining the loss after each training\n    history.append(result.history['loss']) #Now append the loss after the training to the list.\n    start_index = random.randint(0, len(text) - maxlen - 1)\nprint(history)\n\nThis way allows you to get the loss you want while maintaining your iteration method.\n"}, "451": {"topic": "How to return history of validation loss in Keras", "user_name": "Marcin Mo\u017cejkoMarcin Mo\u017cejko", "text": "\nThanks to Alloush,\nFollowing parameter must be included in model.fit():\nvalidation_data = (x_test, y_test)\n\nIf it is not defined, val_acc and val_loss will not\nbe exist at output.\n"}, "452": {"topic": "How to return history of validation loss in Keras", "user_name": "Roozbeh ZabihollahiRoozbeh Zabihollahi", "text": "\nThose who got still error like me:\nConvert model.fit_generator() to model.fit()\n"}, "453": {"topic": "How to return history of validation loss in Keras", "user_name": "", "text": "\nyou can get loss and metrics like below:\nreturned history object is dictionary and you can access model loss( val_loss) or accuracy(val_accuracy) like below:\nmodel_hist=model.fit(train_data,train_lbl,epochs=my_epoch,batch_size=sel_batch_size,validation_data=val_data)\n\nacc=model_hist.history['accuracy']\n\nval_acc=model_hist.history['val_accuracy']\n\nloss=model_hist.history['loss']\n\nval_loss=model_hist.history['val_loss']\n\ndont forget that for getting val_loss or val_accuracy you should specify validation data in the \"fit\" function.\n"}, "454": {"topic": "How to return history of validation loss in Keras", "user_name": "horseshoehorseshoe", "text": "\nhistory = model.fit(partial_train_data, partial_train_targets,\nvalidation_data=(val_data, val_targets),\nepochs=num_epochs, batch_size=1, verbose=0)\nmae_history = history.history['val_mean_absolute_error']\n\nI had the same problem. The following code worked  for me.\n\nmae_history = history.history['val_mae']\n\n"}, "455": {"topic": "English grammar for parsing in NLTK", "user_name": "Fred Foo", "text": "\nIs there a ready-to-use English grammar that I can just load it and use in NLTK? I've searched around examples of parsing with NLTK, but it seems like that I have to manually specify grammar before parsing a sentence. \nThanks a lot!\n"}, "456": {"topic": "English grammar for parsing in NLTK", "user_name": "roborenroboren", "text": "\nYou can take a look at pyStatParser, a simple python statistical parser that returns NLTK parse Trees. It comes with public treebanks and it generates the grammar model only the first time you instantiate a Parser object (in about 8 seconds). It uses a CKY algorithm and it parses average length sentences (like the one below) in under a second.\n>>> from stat_parser import Parser\n>>> parser = Parser()\n>>> print parser.parse(\"How can the net amount of entropy of the universe be massively decreased?\")\n(SBARQ\n  (WHADVP (WRB how))\n  (SQ\n    (MD can)\n    (NP\n      (NP (DT the) (JJ net) (NN amount))\n      (PP\n        (IN of)\n        (NP\n          (NP (NNS entropy))\n          (PP (IN of) (NP (DT the) (NN universe))))))\n    (VP (VB be) (ADJP (RB massively) (VBN decreased))))\n  (. ?))\n\n"}, "457": {"topic": "English grammar for parsing in NLTK", "user_name": "emilmontemilmont", "text": "\nMy library, spaCy, provides a high performance dependency parser.\nInstallation:\npip install spacy\npython -m spacy.en.download all\n\nUsage:\nfrom spacy.en import English\nnlp = English()\ndoc = nlp(u'A whole document.\\nNo preprocessing require.   Robust to arbitrary formating.')\nfor sent in doc:\n    for token in sent:\n        if token.is_alpha:\n            print token.orth_, token.tag_, token.head.lemma_\n\nChoi et al. (2015) found spaCy to be the fastest dependency parser available. It processes over 13,000 sentences a second, on a single thread. On the standard WSJ evaluation it scores 92.7%, over 1% more accurate than any of CoreNLP's models.\n"}, "458": {"topic": "English grammar for parsing in NLTK", "user_name": "syllogism_syllogism_", "text": "\nThere are a few grammars in the nltk_data distribution. In your Python interpreter, issue nltk.download().\n"}, "459": {"topic": "English grammar for parsing in NLTK", "user_name": "Fred FooFred Foo", "text": "\nThere is a Library called Pattern. It is quite fast and easy to use.\n>>> from pattern.en import parse\n>>>  \n>>> s = 'The mobile web is more important than mobile apps.'\n>>> s = parse(s, relations=True, lemmata=True)\n>>> print s\n\n'The/DT/B-NP/O/NP-SBJ-1/the mobile/JJ/I-NP/O/NP-SBJ-1/mobile' ... \n\n"}, "460": {"topic": "English grammar for parsing in NLTK", "user_name": "user3798928user3798928", "text": "\nUse the MaltParser, there you have a pretrained english-grammar, and also some other pretrained languages.\nAnd the Maltparser is a dependency parser and not some simple bottom-up, or top-down Parser.\nJust download the MaltParser from http://www.maltparser.org/index.html and use the NLTK like this:\nimport nltk\nparser = nltk.parse.malt.MaltParser()\n\n"}, "461": {"topic": "English grammar for parsing in NLTK", "user_name": "blackmambablackmamba", "text": "\nI've tried NLTK, PyStatParser, Pattern. IMHO Pattern is best English parser introduced in above article. Because it supports pip install and There is a fancy document on the website (http://www.clips.ua.ac.be/pages/pattern-en). I couldn't find reasonable document for NLTK (And it gave me inaccurate result for me by its default. And I couldn't find how to tune it). pyStatParser is much slower than described above in my Environment. (About one minute for initialization and It took couple of seconds to parse long sentences. Maybe I didn't use it correctly).  \n"}, "462": {"topic": "English grammar for parsing in NLTK", "user_name": "Piyo HogePiyo Hoge", "text": "\nDid you try POS tagging in NLTK?\ntext = word_tokenize(\"And now for something completely different\")\nnltk.pos_tag(text)\n\nThe answer is something like this\n[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'),('completely', 'RB'), ('different', 'JJ')]\n\nGot this example from here NLTK_chapter03\n"}, "463": {"topic": "English grammar for parsing in NLTK", "user_name": "maverik_akagamimaverik_akagami", "text": "\nI'm found out that nltk working good with parser grammar developed by Stanford.\nSyntax Parsing with Stanford CoreNLP and NLTK\nIt is very easy to start to use Stanford CoreNLP and NLTK. All you need is small preparation, after that you can parse sentences with following code:\nfrom nltk.parse.corenlp import CoreNLPParser\nparser = CoreNLPParser()\nparse = next(parser.raw_parse(\"I put the book in the box on the table.\"))\n\nPreparation:\n\nDownload Java Stanford model\nRun CoreNLPServer\n\nYou can use following code to run CoreNLPServer:\nimport os\nfrom nltk.parse.corenlp import CoreNLPServer\n# The server needs to know the location of the following files:\n#   - stanford-corenlp-X.X.X.jar\n#   - stanford-corenlp-X.X.X-models.jar\nSTANFORD = os.path.join(\"models\", \"stanford-corenlp-full-2018-02-27\")\n# Create the server\nserver = CoreNLPServer(\n   os.path.join(STANFORD, \"stanford-corenlp-3.9.1.jar\"),\n   os.path.join(STANFORD, \"stanford-corenlp-3.9.1-models.jar\"),    \n)\n# Start the server in the background\nserver.start()\n\n\nDo not forget stop server with executing server.stop() \n\n"}, "464": {"topic": "How to use Bert for long text classification?", "user_name": "Mahmoud", "text": "\nWe know that BERT has a max length limit of tokens = 512, So if an article has a length of much bigger than 512, such as 10000 tokens in text\nHow can BERT be used?\n"}, "465": {"topic": "How to use Bert for long text classification?", "user_name": "user1337896user1337896", "text": "\nYou have basically three options:\n\nYou can cut the longer texts off and only use the first 512 Tokens. The original BERT implementation (and probably the others as well) truncates longer sequences automatically. For most cases, this option is sufficient.\nYou can split your text in multiple subtexts, classify each of them and combine the results back together ( choose the class which was predicted for most of the subtexts for example). This option is obviously more expensive.\nYou can even feed the output token for each subtext (as in option 2) to another network (but you won't be able to fine-tune) as described in this discussion.\n\nI would suggest to try option 1, and only if this is not good enough to consider the other options.\n"}, "466": {"topic": "How to use Bert for long text classification?", "user_name": "MohammadAli Zeraatkar", "text": "\nThis paper compared a few different strategies: How to Fine-Tune BERT for Text Classification?. \nOn the IMDb movie review dataset, they actually found that cutting out the middle of the text (rather than truncating the beginning or the end) worked best! It even outperformed more complex \"hierarchical\" approaches involving breaking the article into chunks and then recombining the results.\nAs another anecdote, I applied BERT to the Wikipedia Personal Attacks dataset here, and found that simple truncation worked well enough that I wasn't motivated to try other approaches :) \n"}, "467": {"topic": "How to use Bert for long text classification?", "user_name": "chefhosechefhose", "text": "\nIn addition to chunking data and passing it to BERT, check the following new approaches.\nThere are new researches for long document analysis. As you've asked for Bert a similar pre-trained transformer Longformer has recently been made available from ALLEN NLP (https://arxiv.org/abs/2004.05150). Check out this link for the paper.\nThe related work section also mentions some previous work on long sequences. Google them too. I'll suggest at least go through Transformer XL (https://arxiv.org/abs/1901.02860). As far I know it was one of the initial models for long sequences, so would be good to use it as a foundation before moving into 'Longformers'.\n"}, "468": {"topic": "How to use Bert for long text classification?", "user_name": "Domi W", "text": "\nYou can leverage from the HuggingFace Transformers library that includes the following list of Transformers that work with long texts (more than 512 tokens):\n\nReformer: that combines the modeling capacity of a Transformer with an architecture that can be executed efficiently on long sequences.\nLongformer: with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer.\n\nEight other recently proposed efficient Transformer models include Sparse Transformers (Child et al.,2019), Linformer (Wang et al., 2020), Sinkhorn Transformers (Tay et al., 2020b), Performers (Choromanski et al., 2020b), Synthesizers (Tay et al., 2020a), Linear Transformers (Katharopoulos et al., 2020), and BigBird (Zaheeret al., 2020).\nThe paper from the authors from Google Research and DeepMind tries to make a comparison between these Transformers based on Long-Range Arena \"aggregated metrics\":\n\nThey also suggest that Longformers have better performance than Reformer when it comes to the classification task.\n"}, "469": {"topic": "How to use Bert for long text classification?", "user_name": "chrismccchrismcc", "text": "\nI have recently (April 2021) published a paper regarding this topic that you can find on arXiv (https://arxiv.org/abs/2104.07225).\nThere, Table 1 allows to review previous approaches to the problem in question, and the whole manuscript is about long text classification and proposing a new method called Text Guide. This new method claims to improve performance over naive and semi-naive text selection methods used in the paper (https://arxiv.org/abs/1905.05583) that was mentioned in one of the previous answers to this question.\nLong story short about your options:\n\nLow computational cost: use naive/semi naive approaches to select a part of original text instance. Examples include choosing first n tokens, or compiling a new text instance out of the beginning and end of original text instance.\n\nMedium to high computational cost: use recent transformer models (like Longformer) that have 4096 token limit instead of 512. In some cases this will allow for covering the whole text instance and the modified attention mechanism decreases computational cost, and\n\nHigh computational cost: divide the text instance into chunks that fit a model like BERT with \u2018standard\u2019 512 limit of tokens per instance, deploy the model on each part separately, join the resulting vector representations.\n\n\nNow, in my recently published paper there is a new method proposed called Text Guide. Text Guide is a text selection method that allows for improved performance when compared to naive or semi-naive truncation methods. As a text selection method, Text Guide doesn\u2019t interfere with the language model, so it can be used to improve performance of models with \u2018standard\u2019 limit of tokens (512 for transformer models) or \u2018extended\u2019 limit (4096 as for instance for the Longformer model). Summary: Text Guide is a low-computational-cost method that improves performance over naive and semi-naive truncation methods. If text instances are exceeding the limit of models deliberately developed for long text classification like Longformer (4096 tokens), it can also improve their performance.\n"}, "470": {"topic": "How to use Bert for long text classification?", "user_name": "Moradnejad", "text": "\nThere are two main methods:\n\nConcatenating 'short' BERT altogether (which consists of 512 tokens max)\nConstructing a real long BERT (CogLTX, Blockwise BERT, Longformer, Big Bird)\n\nI resumed some typical papers of BERT for long text in this post : https://lethienhoablog.wordpress.com/2020/11/19/paper-dissected-and-recap-4-which-bert-for-long-text/\nYou can have an overview of all methods there.\n"}, "471": {"topic": "How to use Bert for long text classification?", "user_name": "Gandharv SuriGandharv Suri", "text": "\nThere is an approach used in the paper Defending Against Neural Fake News ( https://arxiv.org/abs/1905.12616)\nTheir generative model was producing outputs of 1024 tokens and they wanted to use BERT for human vs machine generations. They extended the sequence length which BERT uses simply by initializing 512 more embeddings and training them while they were fine-tuning BERT on their dataset.\n"}, "472": {"topic": "How to use Bert for long text classification?", "user_name": "SvitlanaGA...supportsUkraineSvitlanaGA...supportsUkraine", "text": "\nU can use the max_position_embeddings argument in the configuration while downloading the BERT model into your kernel. with this argument you can choose 512, 1024, 2048\nas max sequence length\nmax_position_embeddings (int, optional, defaults to 512) \u2013 The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).\nhttps://huggingface.co/transformers/model_doc/bert.html\n"}, "473": {"topic": "How to use Bert for long text classification?", "user_name": "krzysztoffiokkrzysztoffiok", "text": "\nA relatively straightforward way to go is altering the input. For example, you can truncate the input or separately classify multiple parts of the input and aggregate the results. However, you would probably lose some useful information this way.\nThe main obstacle of applying Bert on long texts is that attention needs O(n^2) operations for n input tokens. Some newer methods try to subtly change the Bert's architecture and make it compatible for longer texts. For instance, Longformer limits the attention span to a fixed value so every token would only be related to a set of nearby tokens. This table (Longformer 2020, Iz Beltagy et al.) demonstrates a set of attention-based models for long-text classification:\n\nLTR methods process the input in chunks from left to right and are suitable for auto-regressive applications. Sparse methods mostly reduce the computational order to O(n) by avoiding a full quadratic attention\nmatrix calculation.\n"}, "474": {"topic": "Unsupervised Sentiment Analysis", "user_name": "Holger Just", "text": "\nI've been reading a lot of articles that explain the need for an initial set of texts that are classified as either 'positive' or 'negative' before a sentiment analysis system will really work.\nMy question is: Has anyone attempted just doing a rudimentary check of 'positive' adjectives vs 'negative' adjectives, taking into account any simple negators to avoid classing 'not happy' as positive? If so, are there any articles that discuss just why this strategy isn't realistic?\n"}, "475": {"topic": "Unsupervised Sentiment Analysis", "user_name": "TrindazTrindaz", "text": "\nA classic paper by Peter Turney (2002) explains a method to do unsupervised sentiment analysis (positive/negative classification) using only the words excellent and poor as a seed set. Turney uses the mutual information of other words with these two adjectives to achieve an accuracy of 74%.\n"}, "476": {"topic": "Unsupervised Sentiment Analysis", "user_name": "ruben_pants", "text": "\nI haven't tried doing untrained sentiment analysis such as you are describing, but off the top of my head I'd say you're oversimplifying the problem.  Simply analyzing adjectives is not enough to get a good grasp of the sentiment of a text; for example, consider the word 'stupid.'  Alone, you would classify that as negative, but if a product review were to have '... [x] product makes their competitors look stupid for not thinking of this feature first...' then the sentiment in there would definitely be positive. The greater context in which words appear definitely matters in something like this.  This is why an untrained bag-of-words approach alone (let alone an even more limited bag-of-adjectives) is not enough to tackle this problem adequately.\nThe pre-classified data ('training data') helps in that the problem shifts from trying to determine whether a text is of positive or negative sentiment from scratch, to trying to determine if the text is more similar to positive texts or negative texts, and classify it that way.  The other big point is that textual analyses such as sentiment analysis are often affected greatly by the differences of the characteristics of texts depending on domain.  This is why having a good set of data to train on (that is, accurate data from within the domain in which you are working, and is hopefully representative of the texts you are going to have to classify) is as important as building a good system to classify with.\nNot exactly an article, but hope that helps.\n"}, "477": {"topic": "Unsupervised Sentiment Analysis", "user_name": "Fred FooFred Foo", "text": "\nThe paper of Turney (2002) mentioned by larsmans is a good basic one. In a newer research, Li and He [2009] introduce an approach using Latent Dirichlet Allocation (LDA) to train a model that can classify an article's overall sentiment and topic simultaneously in a totally unsupervised manner. The accuracy they achieve is 84.6%.\n"}, "478": {"topic": "Unsupervised Sentiment Analysis", "user_name": "waffle paradoxwaffle paradox", "text": "\nI tried several methods of Sentiment Analysis for opinion mining in Reviews. \nWhat worked the best for me is the method described in Liu book: http://www.cs.uic.edu/~liub/WebMiningBook.html In this Book Liu and others, compared many strategies and discussed different papers on Sentiment Analysis and Opinion Mining.\nAlthough my main goal was to extract features in the opinions, I implemented a sentiment classifier to detect positive and negative classification of this features. \nI used NLTK for the pre-processing (Word tokenization, POS tagging) and the trigrams creation. Then also I used the Bayesian Classifiers inside this tookit to compare with other strategies Liu was pinpointing. \nOne of the methods relies on tagging as pos/neg every trigrram expressing this information, and using some classifier on this data. \nOther method I tried, and worked better (around 85% accuracy in my dataset), was calculating  the sum of scores of PMI (punctual mutual information) for every word in the sentence and the words excellent/poor as seeds of pos/neg class. \n"}, "479": {"topic": "Unsupervised Sentiment Analysis", "user_name": "        user325117\r", "text": "\nI tried spotting keywords using a dictionary of affect to predict the sentiment label at sentence level. Given the generality of the vocabulary (non domain dependent), the results were just about 61%. The paper is available in my homepage.\nIn a somewhat improved version, negation adverbs were considered. The whole system, named EmoLib, is available for demo:\nhttp://dtminredis.housing.salle.url.edu:8080/EmoLib/\nRegards,\n"}, "480": {"topic": "Unsupervised Sentiment Analysis", "user_name": "Trung HuynhTrung Huynh", "text": "\nDavid, \nI'm not sure if this helps but you may want to look into Jacob Perkin's blog post on using NLTK for sentiment analysis.\n"}, "481": {"topic": "Unsupervised Sentiment Analysis", "user_name": "LuchuxLuchux", "text": "\nThere are no magic \"shortcuts\" in sentiment analysis, as with any other sort of text analysis that seeks to discover the underlying \"aboutness,\" of a chunk of text. Attempting to short cut proven text analysis methods through simplistic \"adjective\" checking or similar approaches leads to ambiguity, incorrect classification, etc., that at the end of the day give you a poor accuracy read on sentiment. The more terse the source (e.g. Twitter), the more difficult the problem.\n"}, "482": {"topic": "Where can I learn more about the Google search \"did you mean\" algorithm? [duplicate]", "user_name": "CommunityBot", "text": "\n\n\n\n\n\n\nThis question already has answers here:                    \n\n\nClosed 12 years ago.\n\n\n\n\nPossible Duplicate:\nHow do you implement a \u201cDid you mean\u201d? \n\nI am writing an application where I require functionality similar to Google's \"did you mean?\" feature used by their search engine:\n\nIs there source code available for such a thing or where can I find articles that would help me to build my own?\n"}, "483": {"topic": "Where can I learn more about the Google search \"did you mean\" algorithm? [duplicate]", "user_name": "vidhividhi", "text": "\nYou should check out Peter Norvigs article about implementing the spell checker in a few lines of python:\nHow to Write a Spelling Corrector It also has links for implementations in other languages (i.e. C#)\n"}, "484": {"topic": "Where can I learn more about the Google search \"did you mean\" algorithm? [duplicate]", "user_name": "BrokenGlassBrokenGlass", "text": "\nI attended a seminar by a Google engineer a year and a half ago, where they talked about their approach to this. The presenter was saying that (at least part of) their algorithm has little intelligence at all; but rather, utilises the huge amounts of data they have access to. They determined that if someone searches for \"Brittany Speares\", clicks on nothing, and then does another search for \"Britney Spears\", and clicks on something, we can have a fair guess about what they were searching for, and can suggest that in future.\nDisclaimer: This may have just been part of their algorithm\n"}, "485": {"topic": "Where can I learn more about the Google search \"did you mean\" algorithm? [duplicate]", "user_name": "SmasherySmashery", "text": "\nPython has a module called difflib. It provides a functionality called get_close_matches. From the Python Documentation:\n\nget_close_matches(word, possibilities[, n][, cutoff])\nReturn a list of the best \"good\n  enough\" matches.  word is a sequence\n  for which    close matches are desired\n  (typically a string), and\n  possibilities is a list of    sequences against which to match\n  word (typically a list of strings).\nOptional argument n (default\n  3) is the maximum number of close\n  matches to    return; n must be\n  greater than 0.\nOptional argument cutoff (default\n  0.6) is a float in the range [0,\n  1].    Possibilities that don't score\n  at least that similar to word are\n  ignored.\nThe best (no more than n) matches\n  among the possibilities are returned\n  in a    list, sorted by similarity\n  score, most similar first.\n\n  >>> get_close_matches('appel', ['ape', 'apple', 'peach', 'puppy'])\n  ['apple', 'ape']\n  >>> import keyword\n  >>> get_close_matches('wheel', keyword.kwlist)\n  ['while']\n  >>> get_close_matches('apple', keyword.kwlist)\n  []\n  >>> get_close_matches('accept', keyword.kwlist)\n  ['except']\n\nCould this library help you?\n"}, "486": {"topic": "Where can I learn more about the Google search \"did you mean\" algorithm? [duplicate]", "user_name": "EscualoEscualo", "text": "\nYou can use http://developer.yahoo.com/search/web/V1/spellingSuggestion.html which would give a similar functionality.\n"}, "487": {"topic": "Where can I learn more about the Google search \"did you mean\" algorithm? [duplicate]", "user_name": "OligoglotOligoglot", "text": "\nYou can check out the source code for Xapian which provides this functionality, as do a lot of other search libraries. http://xapian.org/\n"}, "488": {"topic": "Where can I learn more about the Google search \"did you mean\" algorithm? [duplicate]", "user_name": "m7dm7d", "text": "\nI am not sure if it serves your purpose but a String Edit distance Algorithm with a dictionary might suffice for a small Application.\n"}, "489": {"topic": "Where can I learn more about the Google search \"did you mean\" algorithm? [duplicate]", "user_name": "GeekGeek", "text": "\nI'd take a look at this article on google bombing. It shows that it just suggests answers based off previously entered results.\n"}, "490": {"topic": "Where can I learn more about the Google search \"did you mean\" algorithm? [duplicate]", "user_name": "KyleKyle", "text": "\nAFAIK the \"did you mean ?\" feature doesn't check the spelling. It only gives you another query based on the content parsed by google.\n"}, "491": {"topic": "Where can I learn more about the Google search \"did you mean\" algorithm? [duplicate]", "user_name": "Michael Petrotta", "text": "\nA great chapter to this topic can be found in the openly available Introduction to Information Retrieval.\n"}, "492": {"topic": "Where can I learn more about the Google search \"did you mean\" algorithm? [duplicate]", "user_name": "Colin HebertColin Hebert", "text": "\nU could use ngram for the comparisment: http://en.wikipedia.org/wiki/N-gram\nUsing python ngram module: http://packages.python.org/ngram/index.html\nimport ngram\n\nG2 = ngram.NGram([  \"iis7 configure ftp 7.5\",\n                    \"ubunto configre 8.5\",\n                    \"mac configure ftp\"])\n\nprint \"String\", \"\\t\", \"Similarity\"\nfor i in G2.search(\"iis7 configurftp 7.5\", threshold=0.1):\n    print i[0], \"\\t\", i[1]\n\nU get:\n>>> \nString  Similarity\n\"iis7 configure ftp 7.5\"    0.76\n\"mac configure ftp  0.24\"\n\"ubunto configre 8.5\"   0.19\n\n"}, "493": {"topic": "Where can I learn more about the Google search \"did you mean\" algorithm? [duplicate]", "user_name": "Fabian SteegFabian Steeg", "text": "\ntake a look at Levenshtein-Automata\n"}, "494": {"topic": "Add/remove custom stop words with spacy", "user_name": "Davide Fiocco", "text": "\nWhat is the best way to add/remove stop words with spacy? I am using token.is_stop function and would like to make some custom changes to the set. I was looking at the documentation but could not find anything regarding of stop words. Thanks!\n"}, "495": {"topic": "Add/remove custom stop words with spacy", "user_name": "E.K.E.K.", "text": "\nUsing Spacy 2.0.11, you can update its stopwords set using one of the following:\nTo add a single stopword:\nimport spacy    \nnlp = spacy.load(\"en\")\nnlp.Defaults.stop_words.add(\"my_new_stopword\")\n\nTo add several stopwords at once:\nimport spacy    \nnlp = spacy.load(\"en\")\nnlp.Defaults.stop_words |= {\"my_new_stopword1\",\"my_new_stopword2\",}\n\nTo remove a single stopword:\nimport spacy    \nnlp = spacy.load(\"en\")\nnlp.Defaults.stop_words.remove(\"whatever\")\n\nTo remove several stopwords at once:\nimport spacy    \nnlp = spacy.load(\"en\")\nnlp.Defaults.stop_words -= {\"whatever\", \"whenever\"}\n\nNote: To see the current set of stopwords, use:\nprint(nlp.Defaults.stop_words)\n\nUpdate : It was noted in the comments that this fix only affects the current execution. To update the model, you can use the methods nlp.to_disk(\"/path\") and nlp.from_disk(\"/path\") (further described at https://spacy.io/usage/saving-loading).\n"}, "496": {"topic": "Add/remove custom stop words with spacy", "user_name": "", "text": "\nYou can edit them before processing your text like this (see this post):\n>>> import spacy\n>>> nlp = spacy.load(\"en\")\n>>> nlp.vocab[\"the\"].is_stop = False\n>>> nlp.vocab[\"definitelynotastopword\"].is_stop = True\n>>> sentence = nlp(\"the word is definitelynotastopword\")\n>>> sentence[0].is_stop\nFalse\n>>> sentence[3].is_stop\nTrue\n\nNote: This seems to work <=v1.8. For newer versions, see other answers.\n"}, "497": {"topic": "Add/remove custom stop words with spacy", "user_name": "RomainRomain", "text": "\nShort answer for version 2.0 and above (just tested with 3.4+):\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nprint(STOP_WORDS) # <- set of Spacy's default stop words\n\nSTOP_WORDS.add(\"your_additional_stop_word_here\")\n\n\nThis loads all stop words as a set.\nYou can add your stop words to STOP_WORDS or use your own list in the first place.\n\nTo check if the attribute is_stop for the stop words is set to True use this:\nfor word in STOP_WORDS:\n    lexeme = nlp.vocab[word]\n    print(lexeme.text, lexeme.is_stop)\n\nIn the unlikely case that stop words for some reason aren't set to is_stop = True do this:\nfor word in STOP_WORDS:\n    lexeme = nlp.vocab[word]\n    lexeme.is_stop = True \n\n\nDetailed explanation step by step with links to documentation.\nFirst we import spacy:\nimport spacy\n\nTo instantiate class Language as nlp from scratch we need to import Vocab and Language. Documentation and example here.\nfrom spacy.vocab import Vocab\nfrom spacy.language import Language\n\n# create new Language object from scratch\nnlp = Language(Vocab())\n\nstop_words is a default attribute of class Language and can be set to customize the default language data. Documentation here. You can find spacy's GitHub repo folder with defaults for various languages here.\nFor our instance of nlp we get 0 stop words which is reasonable since we haven't set any language with defaults\nprint(f\"Language instance 'nlp' has {len(nlp.Defaults.stop_words)} default stopwords.\")\n>>> Language instance 'nlp' has 0 default stopwords.\n\nLet's import English language defaults.\nfrom spacy.lang.en import English\n\nNow we have 326 default stop words.\nprint(f\"The language default English has {len(spacy.lang.en.STOP_WORDS)} stopwords.\")\nprint(sorted(list(spacy.lang.en.STOP_WORDS))[:10])\n>>> The language default English has 326 stopwords.\n>>> [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across']\n\nLet's create a new instance of Language, now with defaults for English. We get the same result.\nnlp = English()\nprint(f\"Language instance 'nlp' now has {len(nlp.Defaults.stop_words)} default stopwords.\")\nprint(sorted(list(nlp.Defaults.stop_words))[:10])\n>>> Language instance 'nlp' now has 326 default stopwords.\n>>> [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across']\n\nTo check if all words are set to is_stop = True we iterate over the stop words, retrieve the lexeme from vocab and print out the is_stop attribute.\n[nlp.vocab[word].is_stop for word in nlp.Defaults.stop_words][:10]\n>>> [True, True, True, True, True, True, True, True, True, True]\n\nWe can add stopwords to the English language defaults.\nspacy.lang.en.STOP_WORDS.add(\"aaaahhh-new-stopword\")\nprint(len(spacy.lang.en.STOP_WORDS))\n# these propagate to our instance 'nlp' too! \nprint(len(nlp.Defaults.stop_words))\n>>> 327\n>>> 327\n\nOr we can add new stopwords to instance nlp. However, these propagate to our language defaults too!\nnlp.Defaults.stop_words.add(\"_another-new-stop-word\")\nprint(len(spacy.lang.en.STOP_WORDS))\nprint(len(nlp.Defaults.stop_words))\n>>> 328\n>>> 328\n\nThe new stop words are set to is_stop = True.\nprint(nlp.vocab[\"aaaahhh-new-stopword\"].is_stop)\nprint(nlp.vocab[\"_another-new-stop-word\"].is_stop)\n>>> True\n>>> True\n\n"}, "498": {"topic": "Add/remove custom stop words with spacy", "user_name": "", "text": "\nFor 2.0 use the following:\nfor word in nlp.Defaults.stop_words:\n    lex = nlp.vocab[word]\n    lex.is_stop = True\n\n"}, "499": {"topic": "Add/remove custom stop words with spacy", "user_name": "dantistondantiston", "text": "\nThis collects the stop words too :) \nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n"}, "500": {"topic": "Add/remove custom stop words with spacy", "user_name": "", "text": "\nIn latest version following would remove the word out of the list:\nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\nspacy_stopwords.remove('not')\n\n"}, "501": {"topic": "Add/remove custom stop words with spacy", "user_name": "petezurichpetezurich", "text": "\nFor version 2.3.0\nIf you want to replace the entire list instead of adding or removing a few stop words, you can do this:\ncustom_stop_words = set(['the','and','a'])\n\n# First override the stop words set for the language\ncls = spacy.util.get_lang_class('en')\ncls.Defaults.stop_words = custom_stop_words\n\n# Now load your model\nnlp = spacy.load('en_core_web_md')\n\n\nThe trick is to assign the stop word set for the language before loading the model. It also ensures that any upper/lower case variation of the stop words are considered stop words.\n"}, "502": {"topic": "Add/remove custom stop words with spacy", "user_name": "CommunityBot", "text": "\nSee below piece of code\n# Perform standard imports:\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\n# Print the set of spaCy's default stop words (remember that sets are unordered):\nprint(nlp.Defaults.stop_words)\n\nlen(nlp.Defaults.stop_words)\n\n# Make list of word you want to add to stop words\nlist = ['apple', 'ball', 'cat']\n\n# Iterate this in loop\n\nfor item in list:\n    # Add the word to the set of stop words. Use lowercase!\n    nlp.Defaults.stop_words.add(item)\n    \n    # Set the stop_word tag on the lexeme\n    nlp.vocab[item].is_stop = True\n\nHope this helps. You can print length before and after to confirm.\n"}, "503": {"topic": "What do spaCy's part-of-speech and dependency tags mean?", "user_name": "Mark Amery", "text": "\nspaCy tags up each of the Tokens in a Document with a part of speech (in two different formats, one stored in the pos and pos_ properties of the Token and the other stored in the tag and tag_ properties) and a syntactic dependency to its .head token (stored in the dep and dep_ properties).\nSome of these tags are self-explanatory, even to somebody like me without a linguistics background:\n>>> import spacy\n>>> en_nlp = spacy.load('en')\n>>> document = en_nlp(\"I shot a man in Reno just to watch him die.\")\n>>> document[1]\nshot\n>>> document[1].pos_\n'VERB'\n\nOthers... are not:\n>>> document[1].tag_\n'VBD'\n>>> document[2].pos_\n'DET'\n>>> document[3].dep_\n'dobj'\n\nWorse, the official docs don't contain even a list of the possible tags for most of these properties, nor the meanings of any of them. They sometimes mention what tokenization standard they use, but these claims aren't currently entirely accurate and on top of that the standards are tricky to track down.\nWhat are the possible values of the tag_, pos_, and dep_ properties, and what do they mean?\n"}, "504": {"topic": "What do spaCy's part-of-speech and dependency tags mean?", "user_name": "Mark AmeryMark Amery", "text": "\ntl;dr answer\nJust expand the lists at:\n\nhttps://spacy.io/api/annotation#pos-tagging (POS tags) and\nhttps://spacy.io/api/annotation#dependency-parsing (dependency tags)\n\nLonger answer\nThe docs have greatly improved since I first asked this question, and spaCy now documents this much better.\nPart-of-speech tags\nThe pos and tag attributes are tabulated at https://spacy.io/api/annotation#pos-tagging, and the origin of those lists of values is described. At the time of this (January 2020) edit, the docs say of the pos attribute that:\n\nspaCy maps all language-specific part-of-speech tags to a small, fixed set of word type tags following the Universal Dependencies scheme. The universal tags don\u2019t code for any morphological features and only cover the word type. They\u2019re available as the Token.pos and Token.pos_ attributes.\n\nAs for the tag attribute, the docs say:\n\nThe English part-of-speech tagger uses the OntoNotes 5 version of the Penn Treebank tag set. We also map the tags to the simpler Universal Dependencies v2 POS tag set.\n\nand\n\nThe German part-of-speech tagger uses the TIGER Treebank annotation scheme. We also map the tags to the simpler Universal Dependencies v2 POS tag set.\n\nYou thus have a choice between using a coarse-grained tag set that is consistent across languages (.pos), or a fine-grained tag set (.tag) that is specific to a particular treebank, and hence a particular language.\n.pos_ tag list\nThe docs list the following coarse-grained tags used for the pos and pos_ attributes:\n\nADJ: adjective, e.g. big, old, green, incomprehensible, first\nADP: adposition, e.g. in, to, during\nADV: adverb, e.g. very, tomorrow, down, where, there\nAUX: auxiliary, e.g. is, has (done), will (do), should (do)\nCONJ: conjunction, e.g. and, or, but\nCCONJ: coordinating conjunction, e.g. and, or, but\nDET: determiner, e.g. a, an, the\nINTJ: interjection, e.g. psst, ouch, bravo, hello\nNOUN: noun, e.g. girl, cat, tree, air, beauty\nNUM: numeral, e.g. 1, 2017, one, seventy-seven, IV, MMXIV\nPART: particle, e.g. \u2019s, not,\nPRON: pronoun, e.g I, you, he, she, myself, themselves, somebody\nPROPN: proper noun, e.g. Mary, John, London, NATO, HBO\nPUNCT: punctuation, e.g. ., (, ), ?\nSCONJ: subordinating conjunction, e.g. if, while, that\nSYM: symbol, e.g. $, %, \u00a7, \u00a9, +, \u2212, \u00d7, \u00f7, =, :), \ud83d\ude1d\nVERB: verb, e.g. run, runs, running, eat, ate, eating\nX: other, e.g. sfpksdpsxmsa\nSPACE: space, e.g.    \n\nNote that the docs are lying slightly when they say that this list follows the Universal Dependencies Scheme; there are two tags listed above that aren't part of that scheme.\nOne of those is CONJ, which used to exist in the Universal POS Tags scheme but has been split into CCONJ and SCONJ since spaCy was first written. Based on the mappings of tag->pos in the docs, it would seem that spaCy's current models don't actually use CONJ, but it still exists in spaCy's code and docs for some reason - perhaps backwards compatibility with old models.\nThe second is SPACE, which isn't part of the Universal POS Tags scheme (and never has been, as far as I know) and is used by spaCy for any spacing besides single normal ASCII spaces (which don't get their own token):\n>>> document = en_nlp(\"This\\nsentence\\thas      some weird spaces in\\n\\n\\n\\n\\t\\t   it.\")\n>>> for token in document:\n...   print('%r (%s)' % (str(token), token.pos_))\n... \n'This' (DET)\n'\\n' (SPACE)\n'sentence' (NOUN)\n'\\t' (SPACE)\n'has' (VERB)\n'     ' (SPACE)\n'some' (DET)\n'weird' (ADJ)\n'spaces' (NOUN)\n'in' (ADP)\n'\\n\\n\\n\\n\\t\\t   ' (SPACE)\n'it' (PRON)\n'.' (PUNCT)\n\nI'll omit the full list of .tag_ tags (the finer-grained ones) from this answer, since they're numerous, well-documented now, different for English and German, and probably more likely to change between releases. Instead, look at the list in the docs (e.g. https://spacy.io/api/annotation#pos-en for English) which lists every possible tag, the .pos_ value it maps to, and a description of what it means.\nDependency tokens\nThere are now three different schemes that spaCy uses for dependency tagging: one for English, one for German, and one for everything else. Once again, the list of values is huge and I won't reproduce it in full here. Every dependency has a brief definition next to it, but unfortunately, many of them - like \"appositional modifier\" or \"clausal complement\" - are terms of art that are rather alien to an everyday programmer like me. If you're not a linguist, you'll simply have to research the meanings of those terms of art to make sense of them.\nI can at least provide a starting point for that research for people working with English text, though. If you'd like to see some examples of the CLEAR dependencies (used by the English model) in real sentences, check out the 2012 work of Jinho D. Choi: either his Optimization of Natural Language Processing Components for Robustness and Scalability or his Guidelines for the CLEAR Style\nConstituent to Dependency Conversion (which seems to just be a subsection of the former paper). Both list all the CLEAR dependency labels that existed in 2012 along with definitions and example sentences. (Unfortunately, the set of CLEAR dependency labels has changed a little since 2012, so some of the modern labels are not listed or exemplified in Choi's work - but it remains a useful resource despite being slightly outdated.)\n"}, "505": {"topic": "What do spaCy's part-of-speech and dependency tags mean?", "user_name": "", "text": "\nJust a quick tip about getting the detail meaning of the short forms. You can use explain method like following:\nspacy.explain('pobj')\n\nwhich will give you output like:\n'object of preposition'\n\n"}, "506": {"topic": "What do spaCy's part-of-speech and dependency tags mean?", "user_name": "Mark AmeryMark Amery", "text": "\nThe official documentation now provides much more details for all those annotations at https://spacy.io/api/annotation (and the list of other attributes for tokens can be found at https://spacy.io/api/token).\nAs the documentation shows, their parts-of-speech (POS) and dependency tags have both Universal and specific variations for different languages and the explain() function is a very useful shortcut to get a better description of a tag's meaning without the documentation, e.g.\nspacy.explain(\"VBD\")\n\nwhich gives \"verb, past tense\".\n"}, "507": {"topic": "What do spaCy's part-of-speech and dependency tags mean?", "user_name": "Nuhil MehdyNuhil Mehdy", "text": "\nDirect links (if you don't feel like going through endless spacy documentation to get the full tables):\n\nfor .pos_ (parts of speech, English): https://universaldependencies.org/docs/en/pos/\n\nfor .dep_ (dependency relations, English): https://universaldependencies.org/docs/en/dep/\n\n\n"}, "508": {"topic": "What do spaCy's part-of-speech and dependency tags mean?", "user_name": "SilveriSilveri", "text": "\nAfter the recent update of Spacy to v3 the above links do not work.\nYou may visit this link to get the complete list.\nUniversal POS Tags\n\nEnglish POS Tags\n\n"}, "509": {"topic": "What do spaCy's part-of-speech and dependency tags mean?", "user_name": "", "text": "\nAt present, dependency parsing and tagging in SpaCy appears to be implemented only at the word level, and not at the phrase (other than noun phrase) or clause level. This means SpaCy can be used to identify things like nouns (NN, NNS), adjectives (JJ, JJR, JJS), and verbs (VB, VBD, VBG, etc.), but not adjective phrases (ADJP), adverbial phrases (ADVP), or questions (SBARQ, SQ). \nFor illustration, when you use SpaCy to parse the sentence \"Which way is the bus going?\", we get the following tree.\nBy contrast, if you use the Stanford parser you get a much more deeply structured syntax tree.\n"}, "510": {"topic": "What do spaCy's part-of-speech and dependency tags mean?", "user_name": "Ovi OpreaOvi Oprea", "text": "\nRetrieve the tags and their meaning directly from pipeline/model programmatically\nAn alternative to looking up the tags in the documentation is to retrieve these programmatically from nlp.pipe_labels.\nThis has the advantage that you get the actual labels that your trained pipeline (aka model) provides and you don't have to copy these manually.\nThe following example code uses model en_core_web_sm. Link to model card here. See Label Scheme at bottom. Adapt to the model of your choosing.\nNote: The Universal Part-of-speech Tags aren't programmatically available (at least I couldn't find a way) and can be looked up here in the documentation.\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\nfor component in nlp.pipe_names:\n    tags = nlp.pipe_labels[component]\n    if len(tags)!=0:\n        print(f\"Label mapping for component: {component}\")\n        display(dict(list(zip(tags, [spacy.explain(tag) for tag in tags]))))\n        print()\n\nOutput\nLabel mapping for component: tagger\n\n{'$': 'symbol, currency',\n \"''\": 'closing quotation mark',\n ',': 'punctuation mark, comma',\n '-LRB-': 'left round bracket',\n '-RRB-': 'right round bracket',\n '.': 'punctuation mark, sentence closer',\n ':': 'punctuation mark, colon or ellipsis',\n 'ADD': 'email',\n 'AFX': 'affix',\n 'CC': 'conjunction, coordinating',\n 'CD': 'cardinal number',\n 'DT': 'determiner',\n 'EX': 'existential there',\n 'FW': 'foreign word',\n 'HYPH': 'punctuation mark, hyphen',\n 'IN': 'conjunction, subordinating or preposition',\n 'JJ': 'adjective (English), other noun-modifier (Chinese)',\n 'JJR': 'adjective, comparative',\n 'JJS': 'adjective, superlative',\n 'LS': 'list item marker',\n 'MD': 'verb, modal auxiliary',\n 'NFP': 'superfluous punctuation',\n 'NN': 'noun, singular or mass',\n 'NNP': 'noun, proper singular',\n 'NNPS': 'noun, proper plural',\n 'NNS': 'noun, plural',\n 'PDT': 'predeterminer',\n 'POS': 'possessive ending',\n 'PRP': 'pronoun, personal',\n 'PRP$': 'pronoun, possessive',\n 'RB': 'adverb',\n 'RBR': 'adverb, comparative',\n 'RBS': 'adverb, superlative',\n 'RP': 'adverb, particle',\n 'SYM': 'symbol',\n 'TO': 'infinitival \"to\"',\n 'UH': 'interjection',\n 'VB': 'verb, base form',\n 'VBD': 'verb, past tense',\n 'VBG': 'verb, gerund or present participle',\n 'VBN': 'verb, past participle',\n 'VBP': 'verb, non-3rd person singular present',\n 'VBZ': 'verb, 3rd person singular present',\n 'WDT': 'wh-determiner',\n 'WP': 'wh-pronoun, personal',\n 'WP$': 'wh-pronoun, possessive',\n 'WRB': 'wh-adverb',\n 'XX': 'unknown',\n '_SP': 'whitespace',\n '``': 'opening quotation mark'}\n\n\nLabel mapping for component: parser\n\n{'ROOT': 'root',\n 'acl': 'clausal modifier of noun (adjectival clause)',\n 'acomp': 'adjectival complement',\n 'advcl': 'adverbial clause modifier',\n 'advmod': 'adverbial modifier',\n 'agent': 'agent',\n 'amod': 'adjectival modifier',\n 'appos': 'appositional modifier',\n 'attr': 'attribute',\n 'aux': 'auxiliary',\n 'auxpass': 'auxiliary (passive)',\n 'case': 'case marking',\n 'cc': 'coordinating conjunction',\n 'ccomp': 'clausal complement',\n 'compound': 'compound',\n 'conj': 'conjunct',\n 'csubj': 'clausal subject',\n 'csubjpass': 'clausal subject (passive)',\n 'dative': 'dative',\n 'dep': 'unclassified dependent',\n 'det': 'determiner',\n 'dobj': 'direct object',\n 'expl': 'expletive',\n 'intj': 'interjection',\n 'mark': 'marker',\n 'meta': 'meta modifier',\n 'neg': 'negation modifier',\n 'nmod': 'modifier of nominal',\n 'npadvmod': 'noun phrase as adverbial modifier',\n 'nsubj': 'nominal subject',\n 'nsubjpass': 'nominal subject (passive)',\n 'nummod': 'numeric modifier',\n 'oprd': 'object predicate',\n 'parataxis': 'parataxis',\n 'pcomp': 'complement of preposition',\n 'pobj': 'object of preposition',\n 'poss': 'possession modifier',\n 'preconj': 'pre-correlative conjunction',\n 'predet': None,\n 'prep': 'prepositional modifier',\n 'prt': 'particle',\n 'punct': 'punctuation',\n 'quantmod': 'modifier of quantifier',\n 'relcl': 'relative clause modifier',\n 'xcomp': 'open clausal complement'}\n\n\nLabel mapping for component: ner\n\n{'CARDINAL': 'Numerals that do not fall under another type',\n 'DATE': 'Absolute or relative dates or periods',\n 'EVENT': 'Named hurricanes, battles, wars, sports events, etc.',\n 'FAC': 'Buildings, airports, highways, bridges, etc.',\n 'GPE': 'Countries, cities, states',\n 'LANGUAGE': 'Any named language',\n 'LAW': 'Named documents made into laws.',\n 'LOC': 'Non-GPE locations, mountain ranges, bodies of water',\n 'MONEY': 'Monetary values, including unit',\n 'NORP': 'Nationalities or religious or political groups',\n 'ORDINAL': '\"first\", \"second\", etc.',\n 'ORG': 'Companies, agencies, institutions, etc.',\n 'PERCENT': 'Percentage, including \"%\"',\n 'PERSON': 'People, including fictional',\n 'PRODUCT': 'Objects, vehicles, foods, etc. (not services)',\n 'QUANTITY': 'Measurements, as of weight or distance',\n 'TIME': 'Times smaller than a day',\n 'WORK_OF_ART': 'Titles of books, songs, etc.'}\n\n"}, "511": {"topic": "Algorithm to determine how positive or negative a statement/text is", "user_name": "Joel Coehoorn", "text": "\nI need an algorithm to determine if a sentence, paragraph or article is negative or positive in tone... or better yet, how negative or positive.\nFor instance:\n\n\nJason is the worst SO user I have ever witnessed (-10)\nJason is an SO user (0)\nJason is the best SO user I have ever seen (+10)\nJason is the best at sucking with SO (-10)\nWhile, okay at SO, Jason is the worst at doing bad (+10)\n\n\nNot easy, huh? :)\nI don't expect somebody to explain this algorithm to me, but I assume there is already much work on something like this in academia somewhere. If you can point me to some articles or research, I would love it.\nThanks.\n"}, "512": {"topic": "Algorithm to determine how positive or negative a statement/text is", "user_name": "JasonJason", "text": "\nThere is a sub-field of natural language processing called sentiment analysis that deals specifically with this problem domain. There is a fair amount of commercial work done in the area because consumer products are so heavily reviewed in online user forums (ugc or user-generated-content). There is also a prototype platform for text analytics called GATE from the university of sheffield, and a python project called nltk. Both are considered flexible, but not very high performance. One or the other might be good for working out your own ideas.\n"}, "513": {"topic": "Algorithm to determine how positive or negative a statement/text is", "user_name": "gpmcadam", "text": "\nIn my company we have a product which does this and also performs well. I did most of the work on it. I can give a brief idea:\nYou need to split the paragraph into sentences and then split each sentence into smaller sub sentences - splitting based on commas, hyphen, semi colon, colon, 'and', 'or', etc.\nEach sub sentence will be exhibiting a totally seperate sentiment in some cases.\nSome sentences even if it is split, will have to be joined together. \nEg: The product is amazing, excellent and fantastic. \nWe have developed a comprehensive set of rules on the type of sentences which need to be split and which shouldn't be (based on the POS tags of the words)\nOn the first level, you can use a bag of words approach, meaning - have a list of positive and negative words/phrases and check in every sub sentence. While doing this, also look at the negation words like 'not', 'no', etc which will change the polarity of the sentence. \nEven then if you can't find the sentiment, you can go for a naive bayes approach. This approach is not very accurate (about 60%). But if you apply this to only sentence which fail to pass through the first set of rules - you can easily get to 80-85% accuracy. \nThe important part is the positive/negative word list and the way you split things up. If you want, you can go even a level higher by implementing HMM (Hidden Markov Model) or CRF (Conditional Random Fields). But I am not a pro in NLP and someone else may fill you in that part.\nFor the curious people, we implemented all of this is python with NLTK and the Reverend Bayes module. \nPretty simple and handles most of the sentences. You may however face problems when trying to tag content from the web. Most people don't write proper sentences on the web. Also handling sarcasm is very hard.\n"}, "514": {"topic": "Algorithm to determine how positive or negative a statement/text is", "user_name": "fawcefawce", "text": "\nThis falls under the umbrella of Natural Language Processing, and so reading about that is probably a good place to start.\nIf you don't want to get in to a very complicated problem, you can just create lists of \"positive\" and \"negative\" words (and weight them if you want) and do word counts on sections of text.  Obviously this isn't a \"smart\" solution, but it gets you some information with very little work, where doing serious NLP would be very time consuming.\nOne of your examples would potentially be marked positive when it was in fact negative using this approach (\"Jason is the best at sucking with SO\") unless you happen to weight \"sucking\" more than \"best\"....  But also this is a small text sample, if you're looking at paragraphs or more of text, then weighting becomes more reliable unless you have someone purposefully trying to fool your algorithm.\n"}, "515": {"topic": "Algorithm to determine how positive or negative a statement/text is", "user_name": "cnucnu", "text": "\nAs pointed out, this comes under sentiment analysis under natural language processing. Afaik GATE doesn't have any component that does sentiment analysis.\nIn my experience, I have implemented an algorithm which is an adaptation of the one in the paper 'Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis' by Theresa Wilson, Janyce Wiebe, Paul Hoffmann (this) as a GATE plugin, which gives reasonable good results. It could help you if you want to bootstrap the implementation.\n"}, "516": {"topic": "Algorithm to determine how positive or negative a statement/text is", "user_name": "Greg Hewgill", "text": "\nDepending on your application you could do it via a Bayesian Filtering algorithm (which is often used in spam filters). \nOne way to do it would be to have two filters. One for positive documents and another for negative documents. You would seed the positive filter with positive documents (whatever criteria you use) and the negative filter with negative documents.  The trick would be to find these documents. Maybe your could set it up so your users effectively rate documents.\nThe positive filter (once seeded) would look for positive words. Maybe it would end up with words like love, peace, etc. The negative filter would be seeded appropriately as well.\nOnce your filters are setup, then you run the test text through them to come up with positive and negative scores. Based on these scores and some weighting, you could come up with your numeric score.\nBayesian Filters, though simple, are surprisingly effective. \n"}, "517": {"topic": "Algorithm to determine how positive or negative a statement/text is", "user_name": "SoapBoxSoapBox", "text": "\nYou can do like this:\n    Jason is the worst SO user I have ever witnessed (-10)\n\nworst (-), the rest is (+). so, that would be (-) + (+) = (-)\n    Jason is an SO user (0)\n\n( ) + ( ) = ( )\n    Jason is the best SO user I have ever seen (+10)\n\nbest (+) , the rest is ( ). so, that would be (+) + ( ) = (+)\n    Jason is the best at sucking with SO (-10)\n\nbest (+), sucking (-). so, (+) + (-) = (-)\n    While, okay at SO, Jason is the worst at doing bad (+10)\n\nworst (-), doing bad (-). so, (-) + (-) = (+)\n"}, "518": {"topic": "Algorithm to determine how positive or negative a statement/text is", "user_name": "", "text": "\nThere are many machine learning approaches for this kind of Sentiment Analysis. I used most of the machine learning algorithms, which are already implemented. my case I have used \nweka classification algorithms\n\nSVM\nnaive basian\nJ48\nOnly you have to do this train the model to your context , add featured vector and rule based tune up. In my case I got some (61% accuracy). So We move into stanford core nlp ( they trained their model for movie reviews) and we used their training set and add our training set. we could achieved 80-90% accuracy.  \n\n"}, "519": {"topic": "Algorithm to determine how positive or negative a statement/text is", "user_name": "AnandAnand", "text": "\nThis is an old question, but I happened upon it looking for a tool that could analyze article tone and found Watson Tone Analyzer by IBM. It allows 1000 api calls monthly for free.\n"}, "520": {"topic": "Algorithm to determine how positive or negative a statement/text is", "user_name": "TAGTAG", "text": "\nIt's all about context, I think. If you're looking for the people who are best at sucking with SO. Sucking the best can be a positive thing. For determination what is bad or good and how much I could recommend looking into Fuzzy Logic. \nIt's a bit like being tall. Someone who's 1.95m can considered to be tall. If you place that person in a group with people all over 2.10m, he looks short. \n"}, "521": {"topic": "Algorithm to determine how positive or negative a statement/text is", "user_name": "aliv faizal muhammadaliv faizal muhammad", "text": "\nMaybe essay grading software could be used to estimate tone? WIRED article.\nPossible reference. (I couldn't read it.)\nThis report compares writing skill to the Flesch-Kincaid Grade Level needed to read it!\nPage 4 of e-rator says that they look at mispelling and such. (Maybe bad post are misspelled too!)\nSlashdot article.  \nYou could also use an email filter of some sort for negativity instead of spam-ness.\n"}, "522": {"topic": "Algorithm to determine how positive or negative a statement/text is", "user_name": "Bruce", "text": "\nHow about sarcasm:\n\nJason is the best SO user I have ever seen, NOT\nJason is the best SO user I have ever seen, right\n\n"}, "523": {"topic": "Algorithm to determine how positive or negative a statement/text is", "user_name": "Paranetharan SaravanaperumalParanetharan Saravanaperumal", "text": "\nAh, I remember one java library for this called LingPipe (commercial license) that we evaluated. It would work fine for the example corpus that is available at the site, but for real data it sucks pretty bad. \n"}, "524": {"topic": "Algorithm to determine how positive or negative a statement/text is", "user_name": "weagle08weagle08", "text": "\nMost of the sentiment analysis tools are lexicon based and none of them is perfect. Also, sentiment analysis can be described as a trinary sentiment classification or binary sentiment classification. Moreover, it is a domain specific task. Meaning that tools which work well on news dataset may not do a good job on informal and unstructured tweets. \nI would suggest using several tools and have an aggregation or vote based mechanism to decide the intensity of the sentiment. The best survey study on sentiment analysis tools that I have come across is SentiBench. You will find it helpful.  \n"}, "525": {"topic": "Algorithm to determine how positive or negative a statement/text is", "user_name": "SorskootSorskoot", "text": "\n   use Algorithm::NaiveBayes;\n     my $nb = Algorithm::NaiveBayes->new;\n\n     $nb->add_instance\n       (attributes => {foo => 1, bar => 1, baz => 3},\n        label => 'sports');\n\n     $nb->add_instance\n       (attributes => {foo => 2, blurp => 1},\n        label => ['sports', 'finance']);\n\n     ... repeat for several more instances, then:\n     $nb->train;\n\n     # Find results for unseen instances\n     my $result = $nb->predict\n       (attributes => {bar => 3, blurp => 2});\n\n"}, "526": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "David Brown", "text": "\nI assume a natural language processor would need to be used to parse the text itself, but what suggestions do you have for an algorithm to detect a user's mood based on text that they have written? I doubt it would be very accurate, but I'm still interested nonetheless.\nEDIT: I am by no means an expert on linguistics or natural language processing, so I apologize if this question is too general or stupid.\n"}, "527": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "David BrownDavid Brown", "text": "\nThis is the basis of an area of natural language processing called sentiment analysis. Although your question is general, it's certainly not stupid - this sort of research is done by Amazon on the text in product reviews for example.\nIf you are serious about this, then a simple version could be achieved by -\n\nAcquire a corpus of positive/negative sentiment. If this was a professional project you may take some time and manually annotate a corpus yourself, but if you were in a hurry or just wanted to experiment this at first then I'd suggest looking at the sentiment polarity corpus from Bo Pang and Lillian Lee's research. The issue with using that corpus is it is not tailored to your domain (specifically, the corpus uses movie reviews), but it should still be applicable.\nSplit your dataset into sentences either Positive or Negative. For the sentiment polarity corpus you could split each review into it's composite sentences and then apply the overall sentiment polarity tag (positive or negative) to all of those sentences. Split this corpus into two parts - 90% should be for training, 10% should be for test. If you're using Weka then it can handle the splitting of the corpus for you.\nApply a machine learning algorithm (such as SVM, Naive Bayes, Maximum Entropy) to the training corpus at a word level. This model is called a bag of words model, which is just representing the sentence as the words that it's composed of. This is the same model which many spam filters run on. For a nice introduction to machine learning algorithms there is an application called Weka that implements a range of these algorithms and gives you a GUI to play with them. You can then test the performance of the machine learned model from the errors made when attempting to classify your test corpus with this model.\nApply this machine learning algorithm to your user posts. For each user post, separate the post into sentences and then classify them using your machine learned model.\n\nSo yes, if you are serious about this then it is achievable - even without past experience in computational linguistics. It would be a fair amount of work, but even with word based models good results can be achieved. \nIf you need more help feel free to contact me - I'm always happy to help others interested in NLP =]\n\nSmall Notes - \n\nMerely splitting a segment of text into sentences is a field of NLP - called sentence boundary detection. There are a number of tools, OSS or free, available to do this, but for your task a simple split on whitespaces and punctuation should be fine.\nSVMlight is also another machine learner to consider, and in fact their inductive SVM does a similar task to what we're looking at - trying to classify which Reuter articles are about \"corporate acquisitions\" with 1000 positive and 1000 negative examples.\nTurning the sentences into features to classify over may take some work. In this model each word is a feature - this requires tokenizing the sentence, which means separating words and punctuation from each other. Another tip is to lowercase all the separate word tokens so that \"I HATE you\" and \"I hate YOU\" both end up being considered the same. With more data you could try and also include whether capitalization helps in classifying whether someone is angry, but I believe words should be sufficient at least for an initial effort.\n\n\nEdit\nI just discovered LingPipe that in fact has a tutorial on sentiment analysis using the Bo Pang and Lillian Lee Sentiment Polarity corpus I was talking about. If you use Java that may be an excellent tool to use, and even if not it goes through all of the steps I discussed above.\n"}, "528": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "", "text": "\nNo doubt it is possible to judge a user's mood based on the text they type but it would be no trivial thing. Things that I can think of:\n\nCapitals tends to signify agitation, annoyance or frustration and is certainly an emotional response but then again some newbies do that because they don't realize the significance so you couldn't assume that without looking at what else they've written (to make sure its not all in caps);\nCapitals are really just one form of emphasis. Others are use of certain aggressive colours (eg red) or use of bold or larger fonts;\nSome people make more spelling and grammar mistakes and typos when they're highly emotional;\nScanning for emoticons could give you a very clear picture of what the user is feeling but again something like :) could be interpreted as happy, \"I told you so\" or even have a sarcastic meaning;\nUse of expletives tends to have a clear meaning but again its not clearcut. Colloquial speech by many people will routinely contain certain four letter words. For some other people, they might not even say \"hell\", saying \"heck\" instead so any expletive (even \"sucks\") is significant;\nGroups of punctuation marks (like @#$@$@) tend to be replaced for expletives in a context when expletives aren't necessarily appropriate, so thats less likely to be colloquial;\nExclamation marks can indicate surprise, shock or exasperation.\n\nYou might want to look at Advances in written text analysis or even Determining Mood for a Blog by Combining Multiple Sources of Evidence.\nLastly it's worth noting that written text is usually perceived to be more negative than it actually is. This is a common problem with email communication in companies, just as one example.\n"}, "529": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "SmeritySmerity", "text": "\nI can't believe I'm taking this seriously...  assuming a one-dimensional mood space:\n\nIf the text contains a curse word,\n-10 mood.\nI think exclamations would tend to be negative, so -2 mood.\nWhen I get frustrated, I type in\nVery. Short. Sentences.  -5 mood.\n\nThe more I think about this, the more it's clear that a lot of these signifiers indicate extreme mood in general, but it's not always clear what kind of mood.\n"}, "530": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "cletuscletus", "text": "\nIf you support fonts, bold red text is probably an angry user. Green regular sized texts with butterfly clip art a happy one.\n"}, "531": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "Michael PetrottaMichael Petrotta", "text": "\nMy memory isn't good on this subject, but I believe I saw some research about the grammar structure of the text and the overall tone. That could be also as simple as shorter words and emotion expression words (well, expletives are pretty obvious).\nEdit: I noted that the first person to answer had substantially similar post. There could be indeed some serious idea about shorter sentences.\n"}, "532": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "AlexAlex", "text": "\nAnalysis of mood and behavior is very serious science.  Despite the other answers mocking the question law enforcement agencies have been investigating categorization of mood for years.  Uses in computers I have heard of generally had more context (timing information, voice pattern, speed in changing channels).  I think that you could--with some success--determine if a user is in a particular mood by training a Neural Network with samples from two known groups: angry and not angry.  Good luck with your efforts.\n"}, "533": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "", "text": "\nI think, my algorythm is rather straightforward, yet, why not calculating smilics through the text :) vs :(\nObviously, the text \":) :) :) :)\" resolves to a happy user, while \":( :( :(\" will surely resolve to a sad one. Enjoy!\n"}, "534": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "ilya n.ilya n.", "text": "\nI agree with ojblass that this is a serious question.\nMood categorization is currently a hot topic in the speech recognition area.  If you think about it, an interactive voice response (IVR) application needs to handle angry customers far differently than calm ones: angry people should be routed quickly to human operators with the right experience and training.  Vocal tone is a pretty reliable indicator of emotion, practical enough so that companies are eager to get this to work.  Google \"speech emotion recognition\", or read this article to find out more.\nThe situation should be no different in web-based GUIs.  Referring back to cletus's comments, the analogies between text and speech emotion detection are interesting.  If a person types CAPITALS they are said to be 'shouting', just as if his voice rose in volume and pitch using a voice interface.  Detecting typed profanities is analogous to \"keyword spotting\" of profanity in speech systems.  If a person is upset, they'll make more errors using either a GUI or a voice user interface (VUI) and can be routed to a human.\nThere's a \"multimodal\" emotion detection research area here.  Imagine a web interface that you can also speak to (along the lines of the IBM/Motorola/Opera XHTML + Voice Profile prototype implementation).  Emotion detection could be based on a combination of cues from the speech and visual input modality.\n"}, "535": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "ojblassojblass", "text": "\nYes.\nWhether or not you can do it is another story. The problem seems at first to be AI complete.\nNow then, if you had keystroke timings you should be able to figure it out.\n"}, "536": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "        SadSidoSadSido", "text": "\nFuzzy logic will do I guess.\nAny way it will be quite easy to start with several rules of determining the user's mood and then extend and combine the \"engine\" with more accurate and sophisticated ones.\n"}, "537": {"topic": "Semantic search with NLP and elasticsearch", "user_name": "Ivaylo Slavov", "text": "\nI am experimenting with elasticsearch as a search server and my task is to build a \"semantic\" search functionality. From a short text phrase like \"I have a burst pipe\" the system should infer that the user is searching for a plumber and return all plumbers indexed in elasticsearch. \nCan that be done directly in a search server like elasticsearch or do I have to use a natural language processing (NLP) tool like e.g. Maui Indexer. What is the exact terminology for my task at hand, text classification? Though the given text is very short as it is a search phrase.\n"}, "538": {"topic": "Semantic search with NLP and elasticsearch", "user_name": "user1089363user1089363", "text": "\nThere may be several approaches with different implementation complexity. \nThe easiest one is to create list of topics (like plumbing), attach bag of words (like \"pipe\"), identify search request by majority of keywords and search only in specified topic (you can add field topic to your elastic search documents and set it as mandatory with + during search). \nOf course, if you have lots of documents, manual creation of topic list and bag of words is very time expensive. You can use machine learning to automate some of tasks. Basically, it is enough to have distance measure between words and/or documents to automatically discover topics (e.g. by data clustering) and classify query to one of these topics. Mix of these techniques may also be a good choice (for example, you can manually create topics and assign initial documents to them, but use classification for query assignment). Take a look at Wikipedia's article on latent semantic analysis to better understand the idea. Also pay attention to the 2 linked articles on data clustering and document classification. And yes, Maui Indexer may become good helper tool this way. \nFinally, you can try to build an engine that \"understands\" meaning of the phrase (not just uses terms frequency) and searches appropriate topics. Most probably, this will involve natural language processing and ontology-based knowledgebases. But in fact, this field is still in active research and without previous experience it will be very hard for you to implement something like this. \n"}, "539": {"topic": "Semantic search with NLP and elasticsearch", "user_name": "ffriendffriend", "text": "\nYou may want to explore https://blog.conceptnet.io/2016/11/03/conceptnet-5-5-and-conceptnet-io/.\nIt combines semantic networks and distributional semantics.\n\nWhen most developers need word embeddings, the first and possibly only place they look is word2vec, a neural net algorithm from Google that computes word embeddings from distributional semantics. That is, it learns to predict words in a sentence from the other words around them, and the embeddings are the representation of words that make the best predictions. But even after terabytes of text, there are aspects of word meanings that you just won\u2019t learn from distributional semantics alone.\n\nSome results\n\nThe ConceptNet Numberbatch word embeddings, built into ConceptNet 5.5, solve these SAT analogies better than any previous system. It gets 56.4% of the questions correct. The best comparable previous system, Turney\u2019s SuperSim (2013), got 54.8%. And we\u2019re getting ever closer to \u201chuman-level\u201d performance on SAT analogies \u2014 while particularly smart humans can of course get a lot more questions right, the average college applicant gets 57.0%.\n\n"}, "540": {"topic": "Semantic search with NLP and elasticsearch", "user_name": "SemanticBeengSemanticBeeng", "text": "\nSemantic search is basically search with meaning. Elasticsearch uses JSON serialization by default, to apply search with meaning to JSON you would need to extend it to support edge relations via JSON-LD. You can then apply your semantic analysis over the JSON-LD schema to word disambiguate plumber entity and burst pipe contexts as a subject, predicate, object relationships. Elasticsearch has a very weak semantic search support but you can go around it using faceted searching and bag of words. You can index a thesaurus schema for plumbing terms, then do a semantic matching over the text phrases in your sentences. \n"}, "541": {"topic": "Semantic search with NLP and elasticsearch", "user_name": "        user3366107user3366107", "text": "\n\"Elasticsearch 7.3 introduced introduced text similarity search with vector fields\".\nThey describe the application of using text embeddings (e.g., word embeddings and sentence embeddings) to implement this sort of semantic similarity measure.\n"}, "542": {"topic": "Semantic search with NLP and elasticsearch", "user_name": "dfrankowdfrankow", "text": "\nA bit late to the party, but part II of this blog seems to address this through \"contextual searches\". It basically makes a two-part query to Elasticsearch in order to build a list of \"seed\" documents and then an expanded query via the more-like-this API. The result is a set of documents most contextually similar to the search query.\n"}, "543": {"topic": "Semantic search with NLP and elasticsearch", "user_name": "kd88kd88", "text": "\nit's possible. This GitHub repo shows how to integrate Elasticsearch with the current state-of-the-art on NLP for semantic representation of language: BERT (Bidirectional Encoder Representations from Transformers) https://github.com/Hironsan/bertsearch\nGood luck.\n"}, "544": {"topic": "Semantic search with NLP and elasticsearch", "user_name": "Vinicius WoloszynVinicius Woloszyn", "text": "\nMy suggestion is to use BERT embedding for your sentences and add an embedding field to your ElasticSearch, as it is described in https://www.elastic.co/blog/text-similarity-search-with-vectors-in-elasticsearch\nFor BERT embedding I suggest to use sentence-transformers from Huggingface library. You can find sample codes in https://towardsdatascience.com/how-to-build-a-semantic-search-engine-with-transformers-and-faiss-dcbea307a0e8\n"}, "545": {"topic": "Semantic search with NLP and elasticsearch", "user_name": "Ali Reza EbadatAli Reza Ebadat", "text": "\nThere are several options for that:\n\nYou can perform it in elasticsearch itself. Elasticsearch supports the indexing of Dense Embedding of docs. From there, you can write your own pipeline for search and use your preferred relevancy score formula ie. cosine similarity or something else.\n\nUse Haystack pipeline, refer to my blog which describes setting up a semantic search pipeline (end-to-end).\n\nYou can use Meta's Faiss\n\n\n"}, "546": {"topic": "Natural Language Processing in Ruby [closed]", "user_name": "CommunityBot", "text": "\n\n\n\n\n\n\nClosed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.                    \n\n\n\n\n\n\n\n\n\n\n We don\u2019t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\n\n\nClosed 6 years ago.\n\n\n\n\n\n\n\r\n                        Improve this question\r\n                    \n\n\n\nI'm looking to do some sentence analysis (mostly for twitter apps) and infer some general characteristics. Are there any good natural language processing libraries for this sort of thing in Ruby?\nSimilar to Is there a good natural language processing library but for Ruby. I'd prefer something very general, but any leads are appreciated!\n"}, "547": {"topic": "Natural Language Processing in Ruby [closed]", "user_name": "Joey RobertJoey Robert", "text": "\nThree excellent and mature NLP packages are Stanford Core NLP, Open NLP and LingPipe. There are Ruby bindings to the Stanford Core NLP tools (GPL license) as well as the OpenNLP tools (Apache License).\nOn the more experimental side of things, I maintain a Text Retrieval, Extraction and Annotation Toolkit (Treat), released under the GPL, that provides a common API for almost every NLP-related gem that exists for Ruby. The following list of Treat's features can also serve as a good reference in terms of stable natural language processing gems compatible with Ruby 1.9.\n\nText segmenters and tokenizers (punkt-segmenter, tactful_tokenizer, srx-english, scalpel)\nNatural language parsers for English, French and German and named entity extraction for English (stanford-core-nlp).\nWord inflection and conjugation (linguistics), stemming (ruby-stemmer, uea-stemmer, lingua, etc.)\nWordNet interface (rwordnet), POS taggers (rbtagger, engtagger, etc.)\nLanguage (whatlanguage), date/time (chronic, kronic, nickel), keyword (lda-ruby) extraction.\nText retrieval with indexation and full-text search (ferret).\nNamed entity extraction (stanford-core-nlp).\nBasic machine learning with decision trees (decisiontree), MLPs (ruby-fann), SVMs (rb-libsvm) and linear classification (tomz-liblinear-ruby-swig).\nText similarity metrics (levenshtein-ffi, fuzzy-string-match, tf-idf-similarity).\n\nNot included in Treat, but relevant to NLP: hotwater (string distance algorithms), yomu (binders to Apache Tiki for reading .doc, .docx, .pages, .odt, .rtf, .pdf), graph-rank (an implementation of GraphRank).\n"}, "548": {"topic": "Natural Language Processing in Ruby [closed]", "user_name": "", "text": "\nThere are some things at Ruby Linguistics and some links therefrom, though it doesn't seem anywhere close to what NLTK is for Python, yet.\n"}, "549": {"topic": "Natural Language Processing in Ruby [closed]", "user_name": "user2398029user2398029", "text": "\nYou can always use jruby and use the java libraries.\nEDIT: The ability to do ruby natively on the jvm and easily leverage java libraries is a big plus for rubyists. This is a good option that should be considered in a situation like this.\nWhich NLP toolkit to use in JAVA?\n"}, "550": {"topic": "Natural Language Processing in Ruby [closed]", "user_name": "Alex MartelliAlex Martelli", "text": "\nI found an excellent article detailing some NLP algorithms in Ruby here. This includes stemmers, date time parsers and grammar parsers.\n"}, "551": {"topic": "Natural Language Processing in Ruby [closed]", "user_name": "CommunityBot", "text": "\nTREAT \u2013 the Text REtrieval and Annotation Toolkit \u2013 is the most comprehensive toolkit I know of for Ruby: https://github.com/louismullie/treat/wiki/\n"}, "552": {"topic": "Natural Language Processing in Ruby [closed]", "user_name": "jshenjshen", "text": "\nI maintain a list of Ruby Natural Language Processing resources (libraries, APIs, and presentations) on GitHub that covers the libraries listed in the other answers here as well as some additional libraries.\n"}, "553": {"topic": "Natural Language Processing in Ruby [closed]", "user_name": "Joey RobertJoey Robert", "text": "\nAlso consider using SaaS APIs like MonkeyLearn. You can easily train text classifiers with machine learning and integrate via an API. There's a Ruby SDK available.\nBesides creating your own classifiers, you can pick pre-created modules for sentiment analysis, topic classification, language detection and more.\nWe also have extractors like keyword extraction and entities, and we'll keep adding more public modules.\nOther nice features:\n\nYou have a GUI to create/test algorithms.\nAlgorithms run really fast in our cloud computing platform.\nYou can integrate with Ruby or any other programming language.\n\n"}, "554": {"topic": "Natural Language Processing in Ruby [closed]", "user_name": "zanbrizanbri", "text": "\nTry this one \nhttps://github.com/louismullie/stanford-core-nlp\nAbout stanford-core-nlp gem\nThis gem provides high-level Ruby bindings to the Stanford Core NLP package, a set natural language processing tools for tokenization, sentence segmentation, part-of-speech tagging, lemmatization, and parsing of English, French and German. The package also provides named entity recognition and coreference resolution for English.\nhttp://nlp.stanford.edu/software/corenlp.shtml \ndemo page \nhttp://nlp.stanford.edu:8080/corenlp/\n"}, "555": {"topic": "Natural Language Processing in Ruby [closed]", "user_name": "diasks2diasks2", "text": "\nYou need to be much more specific about what these \"general characteristics\" are.\nIn NLP \"general characteristics\" of a sentence can mean a million different things - sentiment analysis (ie, the attitude of the speaker), basic part of speech tagging, use of personal pronoun, does the sentence contain active or passive verbs, what's the tense and voice of the verbs...\nI don't mind if you're vague about describing it, but if we don't know what you're asking it's highly unlikely we can be specific in helping you.\nMy general suggestion, especially for NLP, is you should get the tool best designed for the job instead of limiting yourself to a specific language. Limiting yourself to a specific language is fine for some tasks where the general tools are implemented everywhere, but NLP is not one of those.\nThe other issue in working with Twitter is a great deal of the sentences there will be half baked or compressed in strange and wonderful ways - which most NLP tools aren't trained for. To help there, the NUS SMS Corpus consists of \"about 10,000 SMS messages collected by students\". Due to the similar restrictions and usage, analysing that may be helpful in your explorations with Twitter.\nIf you're more specific I'll try and list some tools that will help.\n"}, "556": {"topic": "Natural Language Processing in Ruby [closed]", "user_name": "", "text": "\nI would check out Mark Watson's free book Practical Semantic Web and Linked Data Applications, Java, Scala, Clojure, and JRuby Edition.  He has chapters on NLP using java, clojure, ruby, and scala.  He also provides links to the resources you need.\n"}, "557": {"topic": "Natural Language Processing in Ruby [closed]", "user_name": "Raul GarretaRaul Garreta", "text": "\nFor people looking for something more lightweight and simple to implement this option worked well for me.\nhttps://github.com/yohasebe/engtagger\n"}, "558": {"topic": "Add/remove custom stop words with spacy", "user_name": "Davide Fiocco", "text": "\nWhat is the best way to add/remove stop words with spacy? I am using token.is_stop function and would like to make some custom changes to the set. I was looking at the documentation but could not find anything regarding of stop words. Thanks!\n"}, "559": {"topic": "Add/remove custom stop words with spacy", "user_name": "E.K.E.K.", "text": "\nUsing Spacy 2.0.11, you can update its stopwords set using one of the following:\nTo add a single stopword:\nimport spacy    \nnlp = spacy.load(\"en\")\nnlp.Defaults.stop_words.add(\"my_new_stopword\")\n\nTo add several stopwords at once:\nimport spacy    \nnlp = spacy.load(\"en\")\nnlp.Defaults.stop_words |= {\"my_new_stopword1\",\"my_new_stopword2\",}\n\nTo remove a single stopword:\nimport spacy    \nnlp = spacy.load(\"en\")\nnlp.Defaults.stop_words.remove(\"whatever\")\n\nTo remove several stopwords at once:\nimport spacy    \nnlp = spacy.load(\"en\")\nnlp.Defaults.stop_words -= {\"whatever\", \"whenever\"}\n\nNote: To see the current set of stopwords, use:\nprint(nlp.Defaults.stop_words)\n\nUpdate : It was noted in the comments that this fix only affects the current execution. To update the model, you can use the methods nlp.to_disk(\"/path\") and nlp.from_disk(\"/path\") (further described at https://spacy.io/usage/saving-loading).\n"}, "560": {"topic": "Add/remove custom stop words with spacy", "user_name": "", "text": "\nYou can edit them before processing your text like this (see this post):\n>>> import spacy\n>>> nlp = spacy.load(\"en\")\n>>> nlp.vocab[\"the\"].is_stop = False\n>>> nlp.vocab[\"definitelynotastopword\"].is_stop = True\n>>> sentence = nlp(\"the word is definitelynotastopword\")\n>>> sentence[0].is_stop\nFalse\n>>> sentence[3].is_stop\nTrue\n\nNote: This seems to work <=v1.8. For newer versions, see other answers.\n"}, "561": {"topic": "Add/remove custom stop words with spacy", "user_name": "RomainRomain", "text": "\nShort answer for version 2.0 and above (just tested with 3.4+):\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nprint(STOP_WORDS) # <- set of Spacy's default stop words\n\nSTOP_WORDS.add(\"your_additional_stop_word_here\")\n\n\nThis loads all stop words as a set.\nYou can add your stop words to STOP_WORDS or use your own list in the first place.\n\nTo check if the attribute is_stop for the stop words is set to True use this:\nfor word in STOP_WORDS:\n    lexeme = nlp.vocab[word]\n    print(lexeme.text, lexeme.is_stop)\n\nIn the unlikely case that stop words for some reason aren't set to is_stop = True do this:\nfor word in STOP_WORDS:\n    lexeme = nlp.vocab[word]\n    lexeme.is_stop = True \n\n\nDetailed explanation step by step with links to documentation.\nFirst we import spacy:\nimport spacy\n\nTo instantiate class Language as nlp from scratch we need to import Vocab and Language. Documentation and example here.\nfrom spacy.vocab import Vocab\nfrom spacy.language import Language\n\n# create new Language object from scratch\nnlp = Language(Vocab())\n\nstop_words is a default attribute of class Language and can be set to customize the default language data. Documentation here. You can find spacy's GitHub repo folder with defaults for various languages here.\nFor our instance of nlp we get 0 stop words which is reasonable since we haven't set any language with defaults\nprint(f\"Language instance 'nlp' has {len(nlp.Defaults.stop_words)} default stopwords.\")\n>>> Language instance 'nlp' has 0 default stopwords.\n\nLet's import English language defaults.\nfrom spacy.lang.en import English\n\nNow we have 326 default stop words.\nprint(f\"The language default English has {len(spacy.lang.en.STOP_WORDS)} stopwords.\")\nprint(sorted(list(spacy.lang.en.STOP_WORDS))[:10])\n>>> The language default English has 326 stopwords.\n>>> [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across']\n\nLet's create a new instance of Language, now with defaults for English. We get the same result.\nnlp = English()\nprint(f\"Language instance 'nlp' now has {len(nlp.Defaults.stop_words)} default stopwords.\")\nprint(sorted(list(nlp.Defaults.stop_words))[:10])\n>>> Language instance 'nlp' now has 326 default stopwords.\n>>> [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across']\n\nTo check if all words are set to is_stop = True we iterate over the stop words, retrieve the lexeme from vocab and print out the is_stop attribute.\n[nlp.vocab[word].is_stop for word in nlp.Defaults.stop_words][:10]\n>>> [True, True, True, True, True, True, True, True, True, True]\n\nWe can add stopwords to the English language defaults.\nspacy.lang.en.STOP_WORDS.add(\"aaaahhh-new-stopword\")\nprint(len(spacy.lang.en.STOP_WORDS))\n# these propagate to our instance 'nlp' too! \nprint(len(nlp.Defaults.stop_words))\n>>> 327\n>>> 327\n\nOr we can add new stopwords to instance nlp. However, these propagate to our language defaults too!\nnlp.Defaults.stop_words.add(\"_another-new-stop-word\")\nprint(len(spacy.lang.en.STOP_WORDS))\nprint(len(nlp.Defaults.stop_words))\n>>> 328\n>>> 328\n\nThe new stop words are set to is_stop = True.\nprint(nlp.vocab[\"aaaahhh-new-stopword\"].is_stop)\nprint(nlp.vocab[\"_another-new-stop-word\"].is_stop)\n>>> True\n>>> True\n\n"}, "562": {"topic": "Add/remove custom stop words with spacy", "user_name": "", "text": "\nFor 2.0 use the following:\nfor word in nlp.Defaults.stop_words:\n    lex = nlp.vocab[word]\n    lex.is_stop = True\n\n"}, "563": {"topic": "Add/remove custom stop words with spacy", "user_name": "dantistondantiston", "text": "\nThis collects the stop words too :) \nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n"}, "564": {"topic": "Add/remove custom stop words with spacy", "user_name": "", "text": "\nIn latest version following would remove the word out of the list:\nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\nspacy_stopwords.remove('not')\n\n"}, "565": {"topic": "Add/remove custom stop words with spacy", "user_name": "petezurichpetezurich", "text": "\nFor version 2.3.0\nIf you want to replace the entire list instead of adding or removing a few stop words, you can do this:\ncustom_stop_words = set(['the','and','a'])\n\n# First override the stop words set for the language\ncls = spacy.util.get_lang_class('en')\ncls.Defaults.stop_words = custom_stop_words\n\n# Now load your model\nnlp = spacy.load('en_core_web_md')\n\n\nThe trick is to assign the stop word set for the language before loading the model. It also ensures that any upper/lower case variation of the stop words are considered stop words.\n"}, "566": {"topic": "Add/remove custom stop words with spacy", "user_name": "CommunityBot", "text": "\nSee below piece of code\n# Perform standard imports:\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\n# Print the set of spaCy's default stop words (remember that sets are unordered):\nprint(nlp.Defaults.stop_words)\n\nlen(nlp.Defaults.stop_words)\n\n# Make list of word you want to add to stop words\nlist = ['apple', 'ball', 'cat']\n\n# Iterate this in loop\n\nfor item in list:\n    # Add the word to the set of stop words. Use lowercase!\n    nlp.Defaults.stop_words.add(item)\n    \n    # Set the stop_word tag on the lexeme\n    nlp.vocab[item].is_stop = True\n\nHope this helps. You can print length before and after to confirm.\n"}, "567": {"topic": "Is there an algorithm that tells the semantic similarity of two phrases", "user_name": "btw0", "text": "\ninput: phrase 1, phrase 2\noutput: semantic similarity value (between 0 and 1), or the probability these two phrases are talking about the same thing\n"}, "568": {"topic": "Is there an algorithm that tells the semantic similarity of two phrases", "user_name": "btw0btw0", "text": "\n\nYou might want to check out this paper:\nSentence similarity based on semantic nets and corpus statistics (PDF)\nI've implemented the algorithm described. Our context was very general (effectively any two English sentences) and we found the approach taken was too slow and the results, while promising, not good enough (or likely to be so without considerable, extra, effort).\nYou don't give a lot of context so I can't necessarily recommend this but reading the paper could be useful for you in understanding how to tackle the problem.\nRegards,\nMatt.\n"}, "569": {"topic": "Is there an algorithm that tells the semantic similarity of two phrases", "user_name": "\u042fegDwight", "text": "\nThere's a short and a long answer to this.\nThe short answer:\nUse the WordNet::Similarity Perl package. If Perl is not your language of choice, check the WordNet project page at Princeton, or google for a wrapper library.\nThe long answer:\nDetermining word similarity is a complicated issue, and research is still very hot in this area. To compute similarity, you need an appropriate represenation of the meaning of a word. But what would be a representation of the meaning of, say, 'chair'? In fact, what is the exact meaning of 'chair'? If you think long and hard about this, it will twist your mind, you will go slightly mad, and finally take up a research career in Philosophy or Computational Linguistics to find the truth\u2122. Both philosophers and linguists have tried to come up with an answer for literally thousands of years, and there's no end in sight.\nSo, if you're interested in exploring this problem a little more in-depth, I highly recommend reading Chapter 20.7 in Speech and Language Processing by Jurafsky and Martin, some of which is available through Google Books. It gives a very good overview of the state-of-the-art of distributional methods, which use word co-occurrence statistics to define a measure for word similarity. You are not likely to find libraries implementing these, however.\n"}, "570": {"topic": "Is there an algorithm that tells the semantic similarity of two phrases", "user_name": "Matt MowerMatt Mower", "text": "\nFor anyone just coming at this, i would suggest taking a look at SEMILAR - http://www.semanticsimilarity.org/ . They implement a lot of the modern research methods for calculating word and sentence similarity. It is written in Java.  \n\nSEMILAR API comes with various similarity methods based on Wordnet, Latent Semantic Analysis (LSA), Latent Dirichlet Allocation (LDA), BLEU, Meteor, Pointwise Mutual Information (PMI), Dependency based methods, optimized methods based on Quadratic Assignment, etc. And the similarity methods work in different granularities - word to word, sentence to sentence, or bigger texts. \n\n"}, "571": {"topic": "Is there an algorithm that tells the semantic similarity of two phrases", "user_name": "nfelgernfelger", "text": "\nYou might want to check into the WordNet project at Princeton University. One possible approach to this would be to first run each phrase through a stop-word list (to remove \"common\" words such as \"a\", \"to\", \"the\", etc.) Then for each of the remaining words in each phrase, you could compute the semantic \"similarity\" between each of the words in the other phrase using a distance measure based on WordNet. The distance measure could be something like: the number of arcs you have to pass through in WordNet to get from word1 to word2. \nSorry this is pretty high-level. I've obviously never tried this. Just a quick thought.\n"}, "572": {"topic": "Is there an algorithm that tells the semantic similarity of two phrases", "user_name": "kyreniakyrenia", "text": "\nI would look into latent semantic indexing for this. I believe you can create something similar to a vector space search index but with semantically related terms being closer together i.e. having a smaller angle between them. If I learn more I will post here.\n"}, "573": {"topic": "Is there an algorithm that tells the semantic similarity of two phrases", "user_name": "Chuck WootersChuck Wooters", "text": "\nSorry to dig up a 6 year old question, but as I just came across this post today, I'll throw in an answer in case anyone else is looking for something similar.\ncortical.io has developed a process for calculating the semantic similarity of two expressions and they have a demo of it up on their website. They offer a free API providing access to the functionality, so you can use it in your own application without having to implement the algorithm yourself.\n"}, "574": {"topic": "Is there an algorithm that tells the semantic similarity of two phrases", "user_name": "jonfmjonfm", "text": "\nOne simple solution is to use the dot product of character n-gram vectors.  This is robust over ordering changes (which many edit distance metrics are not) and captures many issues around stemming.  It also prevents the AI-complete problem of full semantic understanding.\nTo compute the n-gram vector, just pick a value of n (say, 3), and hash every 3-word sequence in the phrase into a vector.  Normalize the vector to unit length, then take the dot product of different vectors to detect similarity.\nThis approach has been described in \nJ. Mitchell and M. Lapata, \u201cComposition in Distributional Models of Semantics,\u201d Cognitive Science, vol. 34, no. 8, pp. 1388\u20131429, Nov. 2010., DOI 10.1111/j.1551-6709.2010.01106.x \n"}, "575": {"topic": "Is there an algorithm that tells the semantic similarity of two phrases", "user_name": "", "text": "\nI would have a look at statistical techniques that take into consideration the probability of each word to appear within a sentence. This will allow you to give less importance to popular words such as 'and', 'or', 'the' and give more importance to words that appear less regurarly, and that are therefore a better discriminating factor. For example, if you have two sentences:\n1) The smith-waterman algorithm gives you a similarity measure between two strings.\n2) We have reviewed the smith-waterman algorithm and we found it to be good enough for our project.\nThe fact that the two sentences share the words \"smith-waterman\" and the words \"algorithms\" (which are not as common as 'and', 'or', etc.), will allow you to say that the two sentences might indeed be talking about the same topic.\nSummarizing, I would suggest you have a look at:\n 1) String similarity measures;\n 2) Statistic methods;\nHope this helps.\n"}, "576": {"topic": "Is there an algorithm that tells the semantic similarity of two phrases", "user_name": "Hybrid SystemHybrid System", "text": "\nTry SimService, which provides a service for computing top-n similar words and phrase similarity.\n"}, "577": {"topic": "Is there an algorithm that tells the semantic similarity of two phrases", "user_name": "helt", "text": "\nThis requires your algorithm actually knows what your talking about. It can be done in some rudimentary form by just comparing words and looking for synonyms etc, but any sort of accurate result would require some form of intelligence.\n"}, "578": {"topic": "Is there an algorithm that tells the semantic similarity of two phrases", "user_name": "        Jonathan BetzJonathan Betz", "text": "\nTake a look at http://mkusner.github.io/publications/WMD.pdf This paper describes an algorithm called Word Mover distance that tries to uncover semantic similarity. It relies on the similarity scores as dictated by word2vec. Integrating this with GoogleNews-vectors-negative300 yields desirable results. \n"}, "579": {"topic": "Keras Text Preprocessing - Saving Tokenizer object to file for scoring", "user_name": "Marcin Mo\u017cejko", "text": "\nI've trained a sentiment classifier model using Keras library by following the below steps(broadly).\n\nConvert Text corpus into sequences using Tokenizer object/class\nBuild a model using the model.fit() method \nEvaluate this model\n\nNow for scoring using this model, I was able to save the model to a file and load from a file. However I've not found a way to save the Tokenizer object to file. Without this I'll have to process the corpus every time I need to score even a single sentence. Is there a way around this?\n"}, "580": {"topic": "Keras Text Preprocessing - Saving Tokenizer object to file for scoring", "user_name": "Rajkumar KaliyaperumalRajkumar Kaliyaperumal", "text": "\nThe most common way is to use either pickle or joblib. Here you have an example on how to use pickle in order to save Tokenizer:\nimport pickle\n\n# saving\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n# loading\nwith open('tokenizer.pickle', 'rb') as handle:\n    tokenizer = pickle.load(handle)\n\n"}, "581": {"topic": "Keras Text Preprocessing - Saving Tokenizer object to file for scoring", "user_name": "today", "text": "\nTokenizer class has a function to save date into JSON format:\ntokenizer_json = tokenizer.to_json()\nwith io.open('tokenizer.json', 'w', encoding='utf-8') as f:\n    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n\nThe data can be loaded using tokenizer_from_json function from keras_preprocessing.text:\nwith open('tokenizer.json') as f:\n    data = json.load(f)\n    tokenizer = tokenizer_from_json(data)\n\n"}, "582": {"topic": "Keras Text Preprocessing - Saving Tokenizer object to file for scoring", "user_name": "Marcin Mo\u017cejkoMarcin Mo\u017cejko", "text": "\nThe accepted answer clearly demonstrates how to save the tokenizer. The following is a comment on the problem of (generally) scoring after fitting or saving. Suppose that a list texts is comprised of two lists Train_text and Test_text, where the set of tokens in Test_text is a subset of the set of tokens in Train_text (an optimistic assumption). Then fit_on_texts(Train_text) gives different results for texts_to_sequences(Test_text) as compared with first calling fit_on_texts(texts) and then text_to_sequences(Test_text).\nConcrete Example:\nfrom keras.preprocessing.text import Tokenizer\n\ndocs = [\"A heart that\",\n         \"full up like\",\n         \"a landfill\",\n        \"no surprises\",\n        \"and no alarms\"\n         \"a job that slowly\"\n         \"Bruises that\",\n         \"You look so\",\n         \"tired happy\",\n         \"no alarms\",\n        \"and no surprises\"]\ndocs_train = docs[:7]\ndocs_test = docs[7:]\n# EXPERIMENT 1: FIT  TOKENIZER ONLY ON TRAIN\nT_1 = Tokenizer()\nT_1.fit_on_texts(docs_train)  # only train set\nencoded_train_1 = T_1.texts_to_sequences(docs_train)\nencoded_test_1 = T_1.texts_to_sequences(docs_test)\nprint(\"result for test 1:\\n%s\" %(encoded_test_1,))\n\n# EXPERIMENT 2: FIT TOKENIZER ON BOTH TRAIN + TEST\nT_2 = Tokenizer()\nT_2.fit_on_texts(docs)  # both train and test set\nencoded_train_2 = T_2.texts_to_sequences(docs_train)\nencoded_test_2 = T_2.texts_to_sequences(docs_test)\nprint(\"result for test 2:\\n%s\" %(encoded_test_2,))\n\nResults:\nresult for test 1:\n[[3], [10, 3, 9]]\nresult for test 2:\n[[1, 19], [5, 1, 4]]\n\nOf course, if the above optimistic assumption is not satisfied and the set of tokens in Test_text is disjoint from that of Train_test, then test 1 results in a list of empty brackets [].\n"}, "583": {"topic": "Keras Text Preprocessing - Saving Tokenizer object to file for scoring", "user_name": "", "text": "\nI've created the issue https://github.com/keras-team/keras/issues/9289 in  the keras Repo. Until the API is changed, the issue has a link to a gist that has code to demonstrate how to save and restore a tokenizer without having the original documents the tokenizer was fit on. I prefer to store all my model information in a JSON file (because reasons, but mainly mixed JS/Python environment), and this will allow for that, even with sort_keys=True \n"}, "584": {"topic": "Keras Text Preprocessing - Saving Tokenizer object to file for scoring", "user_name": "MaxMax", "text": "\nI found the following snippet provided at following link by @thusv89.\nSave objects:\nimport pickle\n\nwith open('data_objects.pickle', 'wb') as handle:\n    pickle.dump(\n        {'input_tensor': input_tensor, \n         'target_tensor': target_tensor, \n         'inp_lang': inp_lang,\n         'targ_lang': targ_lang,\n        }, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\nLoad objects:\nwith open(\"dataset_fr_en.pickle\", 'rb') as f:\n    data = pickle.load(f)\n    input_tensor = data['input_tensor']\n    target_tensor = data['target_tensor']\n    inp_lang = data['inp_lang']\n    targ_lang = data['targ_lang']\n\n"}, "585": {"topic": "Keras Text Preprocessing - Saving Tokenizer object to file for scoring", "user_name": "", "text": "\nQuite easy, because Tokenizer class has provided two funtions for save and load:\nsave \u2014\u2014 Tokenizer.to_json()\nload \u2014\u2014 keras.preprocessing.text.tokenizer_from_json\nIn to_json() method\uff0cit call \"get_config\" method which handle this:\n    json_word_counts = json.dumps(self.word_counts)\n    json_word_docs = json.dumps(self.word_docs)\n    json_index_docs = json.dumps(self.index_docs)\n    json_word_index = json.dumps(self.word_index)\n    json_index_word = json.dumps(self.index_word)\n\n    return {\n        'num_words': self.num_words,\n        'filters': self.filters,\n        'lower': self.lower,\n        'split': self.split,\n        'char_level': self.char_level,\n        'oov_token': self.oov_token,\n        'document_count': self.document_count,\n        'word_counts': json_word_counts,\n        'word_docs': json_word_docs,\n        'index_docs': json_index_docs,\n        'index_word': json_index_word,\n        'word_index': json_word_index\n    }\n\n"}, "586": {"topic": "How to check whether a sentence is correct (simple grammar check in Python)?", "user_name": "alvas", "text": "\nHow to check whether a sentence is valid in Python?\nExamples:\nI love Stackoverflow - Correct\nI Stackoverflow love - Incorrect\n\n"}, "587": {"topic": "How to check whether a sentence is correct (simple grammar check in Python)?", "user_name": "ChamingaDChamingaD", "text": "\nThere are various Web Services providing automated proofreading and grammar checking. Some have a Python library to simplify querying.\nAs far as I can tell, most of those tools (certainly After the Deadline and LanguageTool) are rule based. The checked text is compared with a large set of rules describing common errors. If a rule matches, the software calls it an error. If a rule does not match, the software does nothing (it cannot detect errors it does not have rules for).\nAfter the Deadline\nimport ATD\nATD.setDefaultKey(\"your API key\")\nerrors = ATD.checkDocument(\"Looking too the water. Fixing your writing typoss.\")\nfor error in errors:\n print \"%s error for: %s **%s**\" % (error.type, error.precontext, error.string)\n print \"some suggestions: %s\" % (\", \".join(error.suggestions),)\n\nExpected output:\ngrammar error for: Looking **too the**\nsome suggestions: to the\nspelling error for: writing **typoss**\nsome suggestions: typos\n\nIt is possible to run the server application on your own machine, 4 GB RAM is recommended.\nLanguageTool\nhttps://pypi.python.org/pypi/language-check\n>>> import language_check\n>>> tool = language_check.LanguageTool('en-US')\n>>> text = 'A sentence with a error in the Hitchhiker\u2019s Guide tot he Galaxy'\n>>> matches = tool.check(text)\n\n>>> matches[0].fromy, matches[0].fromx\n(0, 16)\n>>> matches[0].ruleId, matches[0].replacements\n('EN_A_VS_AN', ['an'])\n>>> matches[1].fromy, matches[1].fromx\n(0, 50)\n>>> matches[1].ruleId, matches[1].replacements\n('TOT_HE', ['to the'])\n\n>>> print(matches[1])\nLine 1, column 51, Rule ID: TOT_HE[1]\nMessage: Did you mean 'to the'?\nSuggestion: to the\n...\n\n>>> language_check.correct(text, matches)\n'A sentence with an error in the Hitchhiker\u2019s Guide to the Galaxy'\n\nIt is also possible to run the server side privately.\nGinger\nAdditionally, this is a hacky (screen scraping) library for Ginger, arguably one of the most polished free-to-use grammar checking options out there.\nMicrosoft Word\nIt should be possible to script Microsoft Word and use its grammar checking functionality.\nMore\nThere is a curated list of grammar checkers on Open Office website. Noted in comments by Patrick.\n"}, "588": {"topic": "How to check whether a sentence is correct (simple grammar check in Python)?", "user_name": "", "text": "\nCheck out NLTK.  They have support for grammars that you can use to parse your sentence.  You can define a grammar, or use one that is provided, along with a context-free parser.  If the sentence parses, then it has valid grammar; if not, then it doesn't.  These grammars may not have the widest coverage (eg, it might not know how to handle a word like StackOverflow), but this approach will allow you to say specifically what is valid or invalid in the grammar.  Chapter 8 of the NLTK book covers parsing and should explain what you need to know.\nAn alternative would be to write a python interface to a wide-coverage parser (like the Stanford parser or C&C).  These are statistical parsers that will be able to understand sentences even if they haven't seen all the words or all the grammatical constructions before.  The downside is that sometimes the parser will still return a parse for a sentence with bad grammar because it will use the statistics to make the best guess possible.\nSo, it really depends on exactly what your goal is.  If you want very precise control over what is considered grammatical, use a context-free parser with NLTK.  If you want robustness and wide-coverage, use a statistical parser.\n"}, "589": {"topic": "How to check whether a sentence is correct (simple grammar check in Python)?", "user_name": "user7610user7610", "text": "\nSome other answers have mentioned LanguageTool, the largest open-source grammar checker. It didn't have a reliable, up-to-date Python port until now. \nI recommend language_tool_python, a grammar checker that supports Python 3 and the latest versions of Java and LanguageTool. It's the only up-to-date, free Python grammar checker. (full disclosure, I made this library)\n"}, "590": {"topic": "How to check whether a sentence is correct (simple grammar check in Python)?", "user_name": "Ponkadoodle", "text": "\nI would suggest the language-tool-python. For example:\nimport language_tool_python\ntool = language_tool_python.LanguageTool('en-US')\n\ntext = \"Your the best but their are allso  good !\"\nmatches = tool.check(text)\nlen(matches)\n\nand we get:\n4\n\nWe can have a look at the 4 issues that it found:\n1st Issue:\nmatches[0]\nAnd we get:\nMatch({'ruleId': 'YOUR_YOU_RE', 'message': 'Did you mean \"You\\'re\"?', 'replacements': [\"You're\"], 'context': 'Your the best but their are allso  good !', 'offset': 0, 'errorLength': 4, 'category': 'TYPOS', 'ruleIssueType': 'misspelling'})\n\n2nd Issue:\nmatches[1]\nand we get:\nMatch({'ruleId': 'THEIR_IS', 'message': 'Did you mean \"there\"?', 'replacements': ['there'], 'context': 'Your the best but their are allso  good !', 'offset': 18, 'errorLength': 5, 'category': 'CONFUSED_WORDS', 'ruleIssueType': 'misspelling'})\n\n3rd Issue:\nmatches[2]\nand we get:\nMatch({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'message': 'Possible spelling mistake found.', 'replacements': ['also', 'all so'], 'context': 'Your the best but their are allso  good !', 'offset': 28, 'errorLength': 5, 'category': 'TYPOS', 'ruleIssueType': 'misspelling'})\n\n4th Issue:\nmatches[3]\nand we get:\nMatch({'ruleId': 'WHITESPACE_RULE', 'message': 'Possible typo: you repeated a whitespace', 'replacements': [' '], 'context': 'Your the best but their are allso  good!', 'offset': 33, 'errorLength': 2, 'category': 'TYPOGRAPHY', 'ruleIssueType': 'whitespace'})\n\nIf you are looking for a more detailed example you can have a look at the related post of Predictive Hacks\n"}, "591": {"topic": "How to check whether a sentence is correct (simple grammar check in Python)?", "user_name": "dhgdhg", "text": "\nStep 1\npip install Caribe\n\nStep 2\nimport Caribe as cb\nsentence=\"I is playing football\"\noutput=cb.caribe_corrector(sentence)\nprint(output)\n\n"}, "592": {"topic": "SpaCy OSError: Can't find model 'en'", "user_name": "Mona Jalal", "text": "\neven though I downloaded the model it cannot load it\n[jalal@goku entity-sentiment-analysis]$ which python\n/scratch/sjn/anaconda/bin/python\n[jalal@goku entity-sentiment-analysis]$ sudo python -m spacy download en\n[sudo] password for jalal: \nCollecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 37.4MB 9.4MB/s \nInstalling collected packages: en-core-web-sm\n  Running setup.py install for en-core-web-sm ... done\nSuccessfully installed en-core-web-sm-2.0.0\n\n    Linking successful\n    /usr/lib/python2.7/site-packages/en_core_web_sm -->\n    /usr/lib64/python2.7/site-packages/spacy/data/en\n\n    You can now load the model via spacy.load('en')\n\nimport spacy \n\nnlp = spacy.load('en')\n---------------------------------------------------------------------------\nOSError                                   Traceback (most recent call last)\n<ipython-input-2-0fcabaab8c3d> in <module>()\n      1 import spacy\n      2 \n----> 3 nlp = spacy.load('en')\n\n/scratch/sjn/anaconda/lib/python3.6/site-packages/spacy/__init__.py in load(name, **overrides)\n     17             \"to load. For example:\\nnlp = spacy.load('{}')\".format(depr_path),\n     18             'error')\n---> 19     return util.load_model(name, **overrides)\n     20 \n     21 \n\n/scratch/sjn/anaconda/lib/python3.6/site-packages/spacy/util.py in load_model(name, **overrides)\n    118     elif hasattr(name, 'exists'):  # Path or Path-like to model data\n    119         return load_model_from_path(name, **overrides)\n--> 120     raise IOError(\"Can't find model '%s'\" % name)\n    121 \n    122 \n\nOSError: Can't find model 'en'\n\nHow should I fix this?\nIf I don't use sudo for downloading the en model, I get:\nCollecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 37.4MB 9.6MB/s ta 0:00:011   62% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588            | 23.3MB 8.6MB/s eta 0:00:02\nRequirement already satisfied (use --upgrade to upgrade): en-core-web-sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz in /scratch/sjn/anaconda/lib/python3.6/site-packages\nYou are using pip version 10.0.0, however version 10.0.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n\n    Error: Couldn't link model to 'en'\n    Creating a symlink in spacy/data failed. Make sure you have the required\n    permissions and try re-running the command as admin, or use a\n    virtualenv. You can still import the model as a module and call its\n    load() method, or create the symlink manually.\n\n    /scratch/sjn/anaconda/lib/python3.6/site-packages/en_core_web_sm -->\n    /scratch/sjn/anaconda/lib/python3.6/site-packages/spacy/data/en\n\n\n    Download successful but linking failed\n    Creating a shortcut link for 'en' didn't work (maybe you don't have\n    admin permissions?), but you can still load the model via its full\n    package name:\n\n    nlp = spacy.load('en_core_web_sm')\n\n"}, "593": {"topic": "SpaCy OSError: Can't find model 'en'", "user_name": "Mona JalalMona Jalal", "text": "\n\nFINALLY CLEARED THE ERROR !!!\n\nBest Way to Install now\npip install -U pip setuptools wheel\n\npip install -U spacy\n\npython -m spacy download en_core_web_sm\n\n\nAlways Open Anaconda Prompt / Command Prompt with Admin Rights to avoid Linking errors!!!\n\n\nTried multiple options including :\npython -m spacy download en\nconda install -c conda-forge spacy\npython -m spacy download en_core_web_sm\npython -m spacy link en_core_web_sm en\n\nNone worked since im using my Company's Network. Finally this Command Worked like a Charm :-)\npip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz --no-deps\n\nUpdated with Latest Link :\n\npip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz --no-deps\n\n\nThanks to the Updated Github Links :-)\n"}, "594": {"topic": "SpaCy OSError: Can't find model 'en'", "user_name": "", "text": "\nBy using sudo python ... you install the model for a different python interpreter than your local one. In fact, it says in your log that the spaCy model is installed to /usr/lib64/python2.7/site-packages/ instead of  /scratch/sjn/anaconda/lib/python3.6/site-packages/.\nTry running python -m spacy download en and it should install the model to the correct directory.\n"}, "595": {"topic": "SpaCy OSError: Can't find model 'en'", "user_name": "Vetrivel PSVetrivel PS", "text": "\n1) Install Spacy\n$ python -m spacy download en\n\n2) Install the model en_core_web_sm\n$ python -m spacy download en_core_web_sm\n>>> import spacy\n>>> nlp = spacy.load(\"en_core_web_sm\")\n\n"}, "596": {"topic": "SpaCy OSError: Can't find model 'en'", "user_name": "Johannes GontrumJohannes Gontrum", "text": "\noh well. turns out even though my which python was showing anaconda python, when I was using python download it was linking it to python2.7 local on my machine. I fixed it using below command:\n$ sudo /scratch/sjn/anaconda/bin/python -m spacy download en\n\n"}, "597": {"topic": "SpaCy OSError: Can't find model 'en'", "user_name": "RaghuRaghu", "text": "\nI am using anaconda jupyter notebook and was getting same error. Ran below commands in anaconda prompt (run as administrator) and it resolved my issue:\n(base) C:\\WINDOWS\\system32>conda install -c conda-forge spacy\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: C:\\Users\\yadav\\Anaconda3\n\n  added / updated specs:\n    - spacy\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    cymem-2.0.3                |   py37h6538335_0          35 KB  conda-forge\n    cython-blis-0.4.1          |   py37hfa6e2cd_0         4.3 MB  conda-forge\n    murmurhash-1.0.0           |   py37h6538335_0          17 KB  conda-forge\n    plac-0.9.6                 |             py_1          18 KB  conda-forge\n    preshed-3.0.2              |   py37h6538335_1          89 KB  conda-forge\n    spacy-2.2.1                |   py37he980bc4_0         7.4 MB  conda-forge\n    srsly-0.2.0                |   py37h6538335_0         189 KB  conda-forge\n    thinc-7.1.1                |   py37he980bc4_0         1.4 MB  conda-forge\n    wasabi-0.4.0               |             py_0          19 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:        13.4 MB\n\nThe following NEW packages will be INSTALLED:\n\n  cymem              conda-forge/win-64::cymem-2.0.3-py37h6538335_0\n  cython-blis        conda-forge/win-64::cython-blis-0.4.1-py37hfa6e2cd_0\n  murmurhash         conda-forge/win-64::murmurhash-1.0.0-py37h6538335_0\n  plac               conda-forge/noarch::plac-0.9.6-py_1\n  preshed            conda-forge/win-64::preshed-3.0.2-py37h6538335_1\n  spacy              conda-forge/win-64::spacy-2.2.1-py37he980bc4_0\n  srsly              conda-forge/win-64::srsly-0.2.0-py37h6538335_0\n  thinc              conda-forge/win-64::thinc-7.1.1-py37he980bc4_0\n  wasabi             conda-forge/noarch::wasabi-0.4.0-py_0\n\n\nProceed ([y]/n)? Y\n\n\nDownloading and Extracting Packages\ncython-blis-0.4.1    | 4.3 MB    | ############################################################################ | 100%\ncymem-2.0.3          | 35 KB     | ############################################################################ | 100%\nsrsly-0.2.0          | 189 KB    | ############################################################################ | 100%\nthinc-7.1.1          | 1.4 MB    | ############################################################################ | 100%\nplac-0.9.6           | 18 KB     | ############################################################################ | 100%\nspacy-2.2.1          | 7.4 MB    | ############################################################################ | 100%\npreshed-3.0.2        | 89 KB     | ############################################################################ | 100%\nwasabi-0.4.0         | 19 KB     | ############################################################################ | 100%\nmurmurhash-1.0.0     | 17 KB     | ############################################################################ | 100%\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n\n\n(base) C:\\WINDOWS\\system32>python -m spacy download en\nCollecting en_core_web_sm==2.2.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz (12.0MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.0MB 409kB/s\nRequirement already satisfied: spacy>=2.2.0 in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from en_core_web_sm==2.2.0) (2.2.2)\nRequirement already satisfied: numpy>=1.15.0 in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.16.2)\nRequirement already satisfied: thinc<7.4.0,>=7.3.0 in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (7.3.1)\nRequirement already satisfied: wasabi<1.1.0,>=0.3.0 in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.4.0)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.21.0)\nRequirement already satisfied: setuptools in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (40.8.0)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.1.3)\nRequirement already satisfied: srsly<1.1.0,>=0.1.0 in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.2.0)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.0.3)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.23)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.0.2)\nRequirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.4.1)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (3.0.2)\nRequirement already satisfied: tqdm<5.0.0,>=4.10.0 in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (4.36.1)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (2019.3.9)\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (1.24.1)\nRequirement already satisfied: idna<2.9,>=2.5 in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (2.8)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (3.0.4)\nRequirement already satisfied: zipp>=0.5 in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy>=2.2.0->en_core_web_sm==2.2.0) (0.6.0)\nRequirement already satisfied: more-itertools in c:\\users\\yadav\\anaconda3\\lib\\site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->spacy>=2.2.0->en_core_web_sm==2.2.0) (6.0.0)\nBuilding wheels for collected packages: en-core-web-sm\n  Building wheel for en-core-web-sm (setup.py) ... done\n  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.0-cp37-none-any.whl size=12019131 sha256=f716e80f029462a80e9fb79ef353c1ac8c0f81d3754778bb6fec520d640fcc87\n  Stored in directory: C:\\Users\\yadav\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-bvy0x0eg\\wheels\\48\\5c\\1c\\15f9d02afc8221a668d2172446dd8467b20cdb9aef80a172a4\nSuccessfully built en-core-web-sm\nInstalling collected packages: en-core-web-sm\n  Found existing installation: en-core-web-sm 2.0.0\n    Uninstalling en-core-web-sm-2.0.0:\n      Successfully uninstalled en-core-web-sm-2.0.0\nSuccessfully installed en-core-web-sm-2.2.0\n\u2714 Download and installation successful\nYou can now load the model via spacy.load('en_core_web_sm')\nsymbolic link created for C:\\Users\\yadav\\Anaconda3\\lib\\site-packages\\spacy\\data\\en <<===>> C:\\Users\\yadav\\Anaconda3\\lib\\site-packages\\en_core_web_sm\n\u2714 Linking successful\nC:\\Users\\yadav\\Anaconda3\\lib\\site-packages\\en_core_web_sm -->\nC:\\Users\\yadav\\Anaconda3\\lib\\site-packages\\spacy\\data\\en\nYou can now load the model via spacy.load('en')\n\n(base) C:\\WINDOWS\\system32>\n\nThen in jupyter notebook load it like below:\nnlp = spacy.load('en',parse=True,tag=True, entity=True)\n\n"}, "598": {"topic": "SpaCy OSError: Can't find model 'en'", "user_name": "Mona JalalMona Jalal", "text": "\nIf you use other python version, you can run :\n\nsudo python3.6 -m spacy download en\n\nWith me, my version 3.6\nI hope it can help your problem!\n"}, "599": {"topic": "SpaCy OSError: Can't find model 'en'", "user_name": "dineshydvdineshydv", "text": "\nAs you are using Anaconda, open Anaconda Prompt as an Admin and execute the following command\npython -m spacy download en\n\nTo load Spacy 'en' in Jupyter Notebook use the following command\nspacy.load('en')\n\n"}, "600": {"topic": "SpaCy OSError: Can't find model 'en'", "user_name": "ThientvseThientvse", "text": "\n\nyou need to download en_core_web_sm\nif you are using anaconda , then run this command\nconda install -c conda-forge spacy-model-en_core_web_sm\nand load it as\nnlp= spacy.load('en_core_web_sm')\n\n"}, "601": {"topic": "SpaCy OSError: Can't find model 'en'", "user_name": "neelneelpurkneelneelpurk", "text": "\nBc i didnt find my error here (For everyone who uses jupyter Notebook, Alteryx, Company Network and had this error):\ni tried to create a macro with python for topic detection but got the Error that there is not a module named \"en_core_web_sm\" \nInstall following Packages at the beginning with following code:\nfrom ayx import Package \nPackage.installPackages(['pandas','numpy', 'matplotlib',\n                         'gensim', 'spacy', 'pyLDAvis',\n                         'https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz'])\nAnd if you are referencing to the module use:\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\nworked for me perf fine :))\n"}, "602": {"topic": "SpaCy OSError: Can't find model 'en'", "user_name": "Rohan DevakiRohan Devaki", "text": "\nGo to https://github.com/explosion/spacy-models\nDownload the model you want to load in SpaCy\nPaste the downloaded file in SpaCy folder present inside the Anaconda folder\nopen cmd there. Type the following command and hit enter:\npip install en_core_web_md-1.2.0.tar.gz\n\nThe above command may vary depending upon the version of the file downloaded.\nVoila! Error has gone :)\n"}, "603": {"topic": "SpaCy OSError: Can't find model 'en'", "user_name": "", "text": "\n  pip install https://github.com/explosion/spacy- \n  models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n\n  #fixes an permission error when attempting to create the symlinks on windows 10\n\n  python -m spacy link en_core_web_sm en_core_web_smc\n\n  from Jupyter notes\n  import spacy\n\n  nlp = spacy.load('en_core_web_sm')\n\n"}, "604": {"topic": "SpaCy OSError: Can't find model 'en'", "user_name": "Daweed24321Daweed24321", "text": "\nSince you are using python version 3.6, try using -\npython3 -m spacy download en\ninstead of just python -m .....\n"}, "605": {"topic": "SpaCy OSError: Can't find model 'en'", "user_name": "", "text": "\nIf you have already downloaded spacy and the language model (E.g., en_core_web_sm or en_core_web_md), then you can follow these steps:\n\nOpen Anaconda prompt as admin\n\nThen type : python -m spacy link [package name or path] [shortcut]\nFor E.g., python -m spacy link /Users/you/model en\n\n\nThis will create a symlink to the your language model. Now you can load the model using spacy.load(\"en\") in your notebooks or scripts\n"}, "606": {"topic": "SpaCy OSError: Can't find model 'en'", "user_name": "Palak JainPalak Jain", "text": "\nA quick hack to fix: Install an available model (e.g. en_core_web_sm) and then make the symlink yourself. Copy the two paths spacy says that it can't link (probably due to the virtual environment running without admin elevation) and use e.g. mklink on Windows.\nE.g. mklink /D C:\\Users\\USER\\PROJECT\\venv2\\lib\\site-packages\\spacy\\data\\en C:\\Users\\USER\\PROJECT\\venv2\\lib\\site-packages\\en_core_web_sm\n"}, "607": {"topic": "How to remove cuda completely from ubuntu?", "user_name": "marlonmarlon", "text": "\nI have ubuntu 18.04, and accidentally installed cuda 9.1 to run Tensorflow-gpu, but it seems tensorflow-gpu requires cuda 10.0, so I want to remove cuda first by executing:\nmartin@nlp-server:~$ sudo apt-get remove --auto-remove nvidia-cuda-toolkit\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nYou might want to run 'apt --fix-broken install' to correct these.\nThe following packages have unmet dependencies:\n cuda-libraries-dev-10-1 : Depends: libcublas-dev (>= 10.2.0.168) but 10.1.0.105-1 is to be installed\n cuda-samples-10-1 : Depends: libcublas-dev (>= 10.2.0.168) but 10.1.0.105-1 is to be installed\n cuda-visual-tools-10-1 : Depends: libcublas-dev (>= 10.2.0.168) but 10.1.0.105-1 is to be installed\nE: Unmet dependencies. Try 'apt --fix-broken install' with no packages (or specify a solution).\n\nThen I tried to run 'apt --fix-broken install', but got the following error:\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nCorrecting dependencies... Done\nThe following additional packages will be installed:\n  libcublas-dev\nThe following packages will be upgraded:\n  libcublas-dev\n1 upgraded, 0 newly installed, 0 to remove and 145 not upgraded.\n69 not fully installed or removed.\nNeed to get 0 B/39.1 MB of archives.\nAfter this operation, 3,458 kB disk space will be freed.\nDo you want to continue? [Y/n] y\n(Reading database ... 253408 files and directories currently installed.)\nPreparing to unpack .../libcublas-dev_10.2.0.168-1_amd64.deb ...\n\nUnpacking libcublas-dev (10.2.0.168-1) over (10.1.0.105-1) .........................................................................................................................................................................................................................................................] \ndpkg: error processing archive /var/cache/apt/archives/libcublas-dev_10.2.0.168-1_amd64.deb (--unpack):\n trying to overwrite '/usr/lib/x86_64-linux-gnu/libcublas_static.a', which is also in package nvidia-cuda-dev 9.1.85-3ubuntu1\ndpkg-deb: error: paste subprocess was killed by signal (Broken pipe)\nErrors were encountered while processing:\n /var/cache/apt/archives/libcublas-dev_10.2.0.168-1_amd64.deb\nE: Sub-process /usr/bin/dpkg returned an error code (1)\n\nHow to remove cuda completely? \n"}, "608": {"topic": "How to remove cuda completely from ubuntu?", "user_name": "Shayan Amani", "text": "\nThere are two things- nvidia drivers and cuda toolkit- which you may want to remove.\nIf you have installed using apt-get use the following to remove the packages completely from the system:\nTo remove cuda toolkit:\nsudo apt-get --purge remove \"*cublas*\" \"cuda*\" \"nsight*\" \n\nTo remove Nvidia drivers:\nsudo apt-get --purge remove \"*nvidia*\"\n\nIf you have installed via source files (assuming the default location to be /usr/local) then remove it using:\nsudo rm -rf /usr/local/cuda*\n\nFrom cuda 11.4 onwards, an uninstaller script has been provided. Use it for the uninstallation instead:\n# To uninstall cuda\nsudo /usr/local/cuda-11.4/bin/cuda-uninstaller \n# To uninstall nvidia\nsudo /usr/bin/nvidia-uninstall\n\nIf you get the problem of broken packages, it has happened since you added repo to the apt/sources.lst. Run the following to delete it:\nsudo vim /etc/apt/sources.list\n\nGo to the line containing reference to Nvidia repo and comment it by appending # in front of the line, for e.g.:\n#deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/ /\n\nThen run\nsudo apt-get update \n\nThis will fix the problem.\nReferences: Nvidia uninstallation\n"}, "609": {"topic": "How to remove cuda completely from ubuntu?", "user_name": "ManishManish", "text": "\nI solved this issue as follow :\nsudo apt-get purge nvidia*\nsudo apt-get autoremove\nsudo apt-get autoclean\nsudo rm -rf /usr/local/cuda*\n\nThis will do the job . \n"}, "610": {"topic": "How to remove cuda completely from ubuntu?", "user_name": "smerllosmerllo", "text": "\nGo to - /usr/local/cuda-10.2/bin\nRun sudo ./cuda-uninstaller \nCheck all the options and then it will be uninstalled automatical\n"}, "611": {"topic": "How to remove cuda completely from ubuntu?", "user_name": "Subham TiwariSubham Tiwari", "text": "\nTo remove CUDA Toolkit:\n$ sudo apt-get --purge remove \"*cublas*\" \"*cufft*\" \"*curand*\" \\\n \"*cusolver*\" \"*cusparse*\" \"*npp*\" \"*nvjpeg*\" \"cuda*\" \"nsight*\" \n\nTo remove NVIDIA Drivers:\n$ sudo apt-get --purge remove \"*nvidia*\"\n\nTo clean up the uninstall:\n$ sudo apt-get autoremove\n\nSource : NVIDIA installation guide\n"}, "612": {"topic": "How to remove cuda completely from ubuntu?", "user_name": "wbadrywbadry", "text": "\nRemove CUDA Toolkit:\nsudo apt-get --purge remove \"*cublas*\" \"*cufft*\" \"*curand*\" \"*cusolver*\" \"*cusparse*\" \"*npp*\" \"*nvjpeg*\" \"cuda*\" \"nsight*\" \n\n"}, "613": {"topic": "Why does the deployment package from my buildserver have additional assemblies?", "user_name": "ilivewithian", "text": "\nI've got an asp.net mvc deployment package that I'm trying to build with team city. The package builds without any problems, but the bin folder contains file that are not needed (and cause the site to fail when present). \nIf I build the same package from visual studio the additional files are not present.\nThe additional files are:\nMicrosoft.VisualBasic.Activities.Compiler.dll\nmscorlib.dll\nnormidna.nlp\nnormnfc.nlp\nnormnfd.nlp\nnormnfkc.nlp\nnormnfkd.nlp\nSystem.Data.dll\nSystem.Data.OracleClient.dll\nSystem.EnterpriseServices.dll\nSystem.EnterpriseServices.Wrapper.dll\nSystem.Transactions.dll\n\nWhat can I do to prevent these additional assemblies and .nlp files from being included in the package?\nUPDATE\nAfter a bit more digging through log files I've found that the _CopyFilesMarkedCopyLocal build task is copying the files into the bin directory. The odd thing is that the assemblies are not marked as copy local.\n"}, "614": {"topic": "Why does the deployment package from my buildserver have additional assemblies?", "user_name": "ilivewithianilivewithian", "text": "\nAfter a bunch more digging around I noticed that the build server had the .Net framework on, but not the framework SDK. After installing the SDK on the build server the additional assemblies were no longer added.\n"}, "615": {"topic": "Why does the deployment package from my buildserver have additional assemblies?", "user_name": "ilivewithianilivewithian", "text": "\nI experienced the same issue on a build server that only had 4.5.1 sdk installed. \nFix\nAdd the p:FrameworkPathOverride parameter to msbuild. For example:\nmsbuild /p:FrameworkPathOverride=\"C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework\\.NETFramework\\v4.5.1\"\n\n"}, "616": {"topic": "Why does the deployment package from my buildserver have additional assemblies?", "user_name": "Edward WildeEdward Wilde", "text": "\nEasiest solution was to copy my local C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\Framework.NETFramework up to the build server\n"}, "617": {"topic": "Why does the deployment package from my buildserver have additional assemblies?", "user_name": "matt-dot-netmatt-dot-net", "text": "\nOn a build server running Windows Server 2012 R2 I experienced a similar problem - the following task was copying unwanted files to the output directory:\n[06:47:07]_CopyFilesMarkedCopyLocal\n[06:47:07]Copy\n[...]\n[06:47:07]Copying file from \"C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319\\mscorlib.dll\" to \"bin\\Release\\mscorlib.dll\".\n[06:47:07]Copying file from \"C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319\\normidna.nlp\" to \"bin\\Release\\normidna.nlp\".\n[06:47:07]Copying file from \"C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319\\normnfc.nlp\" to \"bin\\Release\\normnfc.nlp\".\n[06:47:07]Copying file from \"C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319\\normnfd.nlp\" to \"bin\\Release\\normnfd.nlp\".\n[06:47:07]Copying file from \"C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319\\normnfkc.nlp\" to \"bin\\Release\\normnfkc.nlp\".\n[06:47:07]Copying file from \"C:\\Windows\\Microsoft.NET\\Framework\\v4.0.30319\\normnfkd.nlp\" to \"bin\\Release\\normnfkd.nlp\".\n\nSimilar to the answer of @ilivewithian a package was missing: Microsoft .NET Framework 4.5.2 Developer Pack for Windows Vista SP2, Windows 7 SP1, Windows 8, Windows 8.1, Windows Server 2008 SP2 Windows Server 2008 R2 SP1, Windows Server 2012 and Windows Server 2012 R2.\n"}, "618": {"topic": "What is a projection layer in the context of neural networks?", "user_name": "nbro", "text": "\nI am currently trying to understand the architecture behind the word2vec neural net learning algorithm, for representing words as vectors based on their context.\nAfter reading Tomas Mikolov paper I came across what he defines as a projection layer. Even though this term is widely used when referred to word2vec, I couldn't find a precise definition of what it actually is in the neural net context.\n\nMy question is, in the neural net context, what is a projection layer? Is it the name given to a hidden layer whose links to previous nodes share the same weights? Do its units actually have an activation function of some kind?\n\ufffcAnother resource that also refers more broadly to the problem can be found in this tutorial, which also refers to a projection layer around page 67.\n"}, "619": {"topic": "What is a projection layer in the context of neural networks?", "user_name": "RogerRoger", "text": "\nI find the previous answers here a bit overcomplicated - a projection layer is just a simple matrix multiplication, or in the context of NN, a regular/dense/linear layer, without the non-linear activation in the end (sigmoid/tanh/relu/etc.) The idea is to project the (e.g.) 100K-dimensions discrete vector into a 600-dimensions continuous vector (I chose the numbers here randomly, \"your mileage may vary\"). The exact matrix parameters are learned through the training process.\nWhat happens before/after already depends on the model and context, and is not what OP asks.\n(In practice you wouldn't even bother with the matrix multiplication (as you are multiplying a 1-hot vector which has 1 for the word index and 0's everywhere else), and would treat the trained matrix as a lookout table (i.e. the 6257th word in the corpus = the 6257th row/column (depends how you define it) in the projection matrix).)\n"}, "620": {"topic": "What is a projection layer in the context of neural networks?", "user_name": "", "text": "\n\nThe projection layer maps the discrete word indices of an n-gram context to a continuous vector space.\n\nAs explained in this thesis\n\nThe projection layer is shared such that for contexts containing the same word multiple times, the same set of weights is applied to form each part of the projection vector.\nThis organization effectively increases the amount of data available for training the projection layer weights since each word of each context training pattern individually contributes changes to the weight values.\n\n\nthis figure shows the trivial topology how the output of the projection layer can be efficiently assembled by copying columns from the projection layer weights matrix.\nNow, the Hidden layer:\n\nThe hidden layer processes the output of the projection layer and is also created with a\nnumber of neurons specified in the topology configuration file.\n\nEdit: An explanation of what is happening in the diagram\n\nEach neuron in the projection layer is represented by a number of weights equal to the size of the vocabulary. The projection layer differs from the hidden and output layers by not using a non-linear activation function. Its purpose is simply to provide an efficient means of projecting the given n- gram context onto a reduced continuous vector space for subsequent processing by hidden and output layers trained to classify such vectors. Given the one-or-zero nature of the input vector elements, the output for a particular word with index i is simply the ith column of the trained matrix of projection layer weights (where each row of the matrix represents the weights of a single neuron).\n\n"}, "621": {"topic": "What is a projection layer in the context of neural networks?", "user_name": "Maverick MeerkatMaverick Meerkat", "text": "\nThe continuous bag of words is used to predict a single word given its prior and future entries: thus it is a contextual result. \nThe inputs are the computed weights from the prior and future entries: and all are given new weights identically: thus the complexity / features count of this model is much smaller than many other NN architectures.\nRE: what is the projection layer: from the paper you cited\n\nthe non-linear hidden layer is removed and the projection layer is\n  shared for all words (not just the projection matrix); thus, all words\n  get projected into the same position (their vectors are averaged).\n\nSo the projection layer is a single set of shared weights and no activation function is indicated.\n\nNote that the weight matrix between the input and the projection layer\n  is shared for all word positions in the same way as in the NNLM\n\nSo the hidden layer is in fact represented by this single set of shared weights - as you correctly implied that is identical across all of the input nodes.\n"}, "622": {"topic": "What programming language is most like natural language? [closed]", "user_name": "", "text": "\n\n\n\n\n\n\n\n\n\r\nAs it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.                    \n\n\nClosed 11 years ago.\n\n\n\nI got the idea for this question from numerous situations where I don't understand what the person is talking about and when others don't understand me.\nSo, a \"smart\" solution would be to speak a computer language. :)\nI am interested how far a programming language can go to get near to (English) natural language. When I say near, I mean not just to use words and sentences, but to be able to \"do\" things a natural language can \"do\" and by \"do\" I mean that it can be used (in a very limited way) as a replacement for natural language.\nI know that this is impossible (is it?) but I think that this can be interesting.\n"}, "623": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\nThere is a programming language called Inform that, in its most recent incarnation, Inform 7, looks a lot like natural language...in particular, written language. \nInform is very specifically for creating text adventure games, but there is no inherent reason that the concepts couldn't be extended into other realms.\nHere's a small snippet of Inform 7 code, taken from the game Glass, by Emily Short.\nStage is a room. \n\nThe old lady is a woman in the Stage. Understand \"mother\" or \n\"stepmother\" as the old lady. The old lady is active. The description \nof the lady is \"She looks plucked: thin neck with folds of skin\nexposed, nose beaky, lips white. Perhaps when her fortunes are mended\nher cosmetics too will improve.\" \n\nThe Prince is a man in the Stage. The description of the prince is\n\"He's tolerably attractive, in his flightless way. It's hard not to\npity him a little.\" The prince carries a glass slipper. The glass\nslipper is wearable. Understand \"shoe\" or \"heel\" or \"toe\" or \"foot\"\nas the slipper. The description of the slipper is \"It is very small\nfor an adult woman's foot.\" \n\nComplete code can be found here.\nThis is a small simple example...it can actually handle a surprisingly robust set of ideas. \nIt should be pointed out that the code isn't really a strange cypher where the constructs have hidden meanings...this code does more or less what you would expect.  For example:\nThe old lady is a woman in the Stage. Understand \"mother\" or \n\"stepmother\" as the old lady. \n\ncreates an object that happens to be a female person, names that object \"old lady\", and places that object within the room object called the \"Stage\".  Then two aliases (\"mother\" and \"stepmother\" are created that also both reference the \"old lady\" object.\nOf course, as the examples get increasingly complex, the necessary hoops to jump through also become more complex.  English is, by its very nature, ambiguous, while computer code is most definitively not.  So we'll never get a \"perfect marriage\".\n"}, "624": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\nDepends on what circles you roll in, but LOLCODE could be considered like natural language ;)\nExample loop:\nHAI\n    CAN HAS STDIO?\n    I HAS A VAR\n    IM IN YR LOOP\n        UP VAR!!1\n        VISIBLE VAR\n        IZ VAR BIGGER THAN 10? KTHXBYE\n    IM OUTTA YR LOOP\nKTHXBYE\n\nOn a serious note, VB is a pretty natural language. It's easy for non-programmer types to learn, so the syntax must be pretty easy to understand.\n"}, "625": {"topic": "What programming language is most like natural language? [closed]", "user_name": "", "text": "\nThe language Richard Pryor used to transfer millions of dollars with in Superman III was very close:\n> TRANSFER $1,000,000 DOLLARS TO WEBSTER'S ACCOUNT.... NOW\n\n;-)\nEDIT: characters corrected ;-)\n"}, "626": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\nCOBOL reads a lot like English\n000100 IDENTIFICATION DIVISION.\n000200 PROGRAM-ID.     HELLOWORLD.\n000300\n000400*\n000500 ENVIRONMENT DIVISION.\n000600 CONFIGURATION SECTION.\n000700 SOURCE-COMPUTER. RM-COBOL.\n000800 OBJECT-COMPUTER. RM-COBOL.\n000900\n001000 DATA DIVISION.\n001100 FILE SECTION.\n001200\n100000 PROCEDURE DIVISION.\n100100\n100200 MAIN-LOGIC SECTION.\n100300 BEGIN.\n100400     DISPLAY \" \" LINE 1 POSITION 1 ERASE EOS.\n100500     DISPLAY \"Hello world!\" LINE 15 POSITION 10.\n100600     STOP RUN.\n100700 MAIN-LOGIC-EXIT.\n100800     EXIT.\n\nsource\n"}, "627": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\nLisp (of course (if you know what I mean (LOL)))\n"}, "628": {"topic": "What programming language is most like natural language? [closed]", "user_name": "", "text": "\nGood 'ol AppleScript touts its likeness to english as one of its strengths. However, it's not very fun to work with.\n"}, "629": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\nIf you're a connoisseur, the Shakespeare Programming Language is fairly natural ;)\nThere is a limit to how 'natural' you can get in programming though. Human languages are too open to interpretation - a programming language needs to be specific and precise, I don't think that meshes well with having a 'natural' programming language.\n"}, "630": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\nHyperTalk - the language behind Apple's HyperCard.\n on mouseUp\n   put \"100,100\" into pos\n   repeat with x = 1 to the number of card buttons\n     set the location of card button x to pos\n     add 15 to item 1 of pos\n   end repeat\n end mouseUp\n\nHyperTalk on Wikipedia\n"}, "631": {"topic": "What programming language is most like natural language? [closed]", "user_name": "", "text": "\nI don't know that I'd go as far as to say that VB.NET is close to the English language, but I think it's about as close as you really get.  Sure, once you've programmed it for a while, it seems like English - it does read like a book to a seasoned VB programmer, but if you stop and think about real world English:\nFor i As Integer = 1 To 10\n  Console.WriteLine(\"Hello World\")\nNext\n\nIs a long way from:\n\nWrite \"Hello World\" and move to the next line of the console 10 times.\n\nOf course, the English is ambiguous - does it want you to do the whole thing 10 times, or just write \"Hello World\" once and then move to the next line 10 times?\nI guess we need to learn to talk in a less ambiguous fashion:\n\nDo this 10 times: In the console, write \"Hello World\" and move to the next line.\n\nBut I doubt very much there's a programming language that really reads like English.  Even those Cobol fanatics that say it's like natural language - it really isn't if you stop and think about how you think about things in a real way instead of in the manner defined by the programming language.\nEven in VB you're limited to the way the framework dictates the way you do things...\n"}, "632": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\nPerl has some design principles that are based on how humans process natural languages (see http://www.wall.org/~larry/natural.html ). \nThat's a different thing from syntactical hacks to make code read like sentences in English or some other language. I'm not entirely convinced that those are useful. As an analogy, I can also make ASCII art with my code, but that doesn't mean that my language is based on principles of visual composition.\nTo give an example of where it may not be useful,suppose this does what it looks like it does in some rubyish/smalltalky language:\n3.times say \"hello!\" \n\nThat's nice, it makes my code a bit more readable, and there's a similar sort of fun in it to having a parrot that can talk, but it's only useful if I know the underlying rules of the computer language. The fact that it happens to look like English gives me no extra leverage or insight. I can't use the English grammar processing engine in my brain to generate sentences like the following:\n// The dot looks like misplaced punctuation \n// in the \"English\" above, but it's essential in \n// the computer language\n3 times say \"hello!\" // syntax error\n\n// In a natural language, a reordering might make\n// sense, but it's impossible here because the word\n// order was essential to carrying the parameters\n// to the method invocation in the right order.\nsay \"hello\" 3 times // syntax error\n\n"}, "633": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\ngherkin is a domain specific language to \ndescribe executable bdd-specifications. It is used among other by cucumber (ruby) and specflow (dotnet).\nExample\n    Feature: Browsing\n        In order to see who's been on the site\n        As a user\n        I want to be able to view the list of posts\n\n    Scenario: Navigation to homepage\n        When I navigate to /Guestbook\n        Then I should be on the guestbook page\n\n    Scenario: Viewing existing entries\n        Given I am on the guestbook page\n        Then I should see a list of guestbook entries\n            And guestbook entries have an author\n            And guestbook entries have a posted date\n            And guestbook entries have a comment\n\n    Scenario: Most recent entries are displayed first\n        Given we have the following existing entries\n            | Name      | Comment      | Posted date       |\n            | Mr. A     | I like A     | 2008-10-01 09:20  |\n            | Mrs. B    | I like B     | 2010-03-05 02:15  |\n            | Dr. C     | I like C     | 2010-02-20 12:21  |\n          And I am on the guestbook page\n        Then the guestbook entries includes the following, in this order\n            | Name      | Comment      | Posted date       |\n            | Mrs. B    | I like B     | 2010-03-05 02:15  |\n            | Dr. C     | I like C     | 2010-02-20 12:21  |\n            | Mr. A     | I like A     | 2008-10-01 09:20  |\n\n"}, "634": {"topic": "What programming language is most like natural language? [closed]", "user_name": "", "text": "\nWell, Plain English, of course!  \nTo sing the beer song:\n  Put 99 into a number.\n  Loop.\n  If the number is 0, break.\n  Format a string given the number and \"bottle\" and \"bottles\".\n  Write the string then \" of beer on the wall, \" then the string then \" of beer.\".\n  Format another string given the number minus 1 and \"bottle\" and \"bottles\".\n  Write \"Take one down and pass it around, \" then the other string then \" of beer on the wall.\".\n  Skip a line.\n  Subtract 1 from the number.\n  Repeat.\n  Write \"No more bottles of beer on the wall, no more bottles of beer.\".\n  Write \"Go to the store and buy some more, 99 bottles of beer on the wall.\".\n\nTo format a string given a number and a singular string and a plural string:\n  If the number is 0, put \"no more \" then the plural into the string; exit.\n  If the number is 1, put \"1 \" then the singular into the string; exit.\n  Put the number then \" \" then the plural into the string.\n\nI haven't actually used this - I found it here.\n"}, "635": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\nWell, Ruby and Python are supposed to be fairly close. Ruby even goes to the length of adding special keywords that simulate real life. Such as the unless keyword, etc.\nOf course, one you type real code in either of those 2 languages, it's not really like natural language, but then again what is?\n"}, "636": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\nI'd say SQL or COBOL.\n"}, "637": {"topic": "What programming language is most like natural language? [closed]", "user_name": "", "text": "\nthe syntax of VB.NET is very near to English language \n"}, "638": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\nForth is reverse-Polish based, and would work naturally for some people.\n\"Learn Forth quickly I will\" - Yoda.\n"}, "639": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\nThat is called \"pseudocode\". You use whatever means necessary to communicate the intent of the code (you have written or will later write).\nAny programming language has some features that are ambiguous to outsiders.\n"}, "640": {"topic": "What programming language is most like natural language? [closed]", "user_name": "", "text": "\nWell natural language is equivocal, and takes a bit more than a literal linear reading to understand. But that being granted, VB.NET is getting close in some constructs. Closest I've seen.\nFor Loop in VB.NET\nFor i = 0 To 2\n  'loop time!\nNext i\n\nIt's about as \"natural\" as I've seen without being too verbose.\n"}, "641": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\nAlthough not exactly what you asked for, there are languages that accomplish what you want, but from the other direction. Lojban, for example, is a language made to be used as a natural language, but without ambiguity.\n\nLojban (pronounced [\u02c8lo\u0292ban]) is a\n  constructed, syntactically unambiguous\n  human language based on predicate\n  logic.\n\n"}, "642": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\nApplescript looks like natural language.\n"}, "643": {"topic": "What programming language is most like natural language? [closed]", "user_name": "", "text": "\nI believe William Shakespeare was the world's best programmer...\nThe Shakespeare Programming Language\n"}, "644": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\nI believe your question is based on a fallacy. Programming is not mainly about translating from human to computer language. It is understanding technical problems and designing programs that is hard, typing in the code is a minor part. Learning a programming language won't make someone a programmer any more than learning musical notation will make them a composer. \nThat said, if you write at a high enough level in almost any language and spend a few minutes explaining syntax, you can communicate the gist of a piece of code to a dedicated non-programer. Conversely, a precise enough natural language specification can sometimes be translated into high level functions (although people are rarely willing to put in the effort to write such a spec.)\n"}, "645": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\nCOBOL was created with the specific intent of being like natural language (English in this case)\n"}, "646": {"topic": "What programming language is most like natural language? [closed]", "user_name": "", "text": "\nWith Ruby and Oslo (and possibly F#), you could build a very language-friendly DSL. That's at least the promise of Oslo. You can find an example of an Oslo grammar for BDD here.\n"}, "647": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\nSanskrit comes close to what you describe. \nIt has no redundancies, it was the first language to follow BNF which is the basis of all modern prog. language grammar, and it shares a common Indo-European descent with English\n"}, "648": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\nCobol is close to English as it gets\nI believe Logo is also not too far from the English language\n"}, "649": {"topic": "What programming language is most like natural language? [closed]", "user_name": "", "text": "\nI wish there was a COmmon Business Oriented Language that read like English so everyone, even non-programmers could unterstand it... Maybe we should create one! (stolen from here)\n"}, "650": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\nWhat we normally call \"pseudo-code\" is very, very close to Pascal.  That probably doesn't make it particularly close to natural English, but if it weren't simpler than most langauges, we wouldn't write pseudo-code at all (we'd just write code).\n"}, "651": {"topic": "What programming language is most like natural language? [closed]", "user_name": "\r", "text": "\nI thought of Eiffel. Quote from here:\n\nRaphael Simon, lead software engineer\n  for Eiffel Software, said the language\n  was designed so that one could use\n  natural language to write the program.\n\nSee for example the 99 bottles of beer program.\nI wouldn't say it's the \"most\" natural, but I find it rather natural.\n"}, "652": {"topic": "What programming language is most like natural language? [closed]", "user_name": "", "text": "\nFor me, It is Python. \nYMMV\n"}, "653": {"topic": "Expanding English language contractions in Python", "user_name": "Abdulrahman Bres", "text": "\nThe English language has a couple of contractions. For instance:\nyou've -> you have\nhe's -> he is\n\nThese can sometimes cause headache when you are doing natural language processing. Is there a Python library, which can expand these contractions?\n"}, "654": {"topic": "Expanding English language contractions in Python", "user_name": "MaartenMaarten", "text": "\nI made that wikipedia contraction-to-expansion page into a python dictionary (see below)\nNote, as you might expect, that you definitely want to use double quotes when querying the dictionary:\n\nAlso, I've left multiple options in as in the wikipedia page. Feel free to modify it as you wish. Note that disambiguation to the right expansion would be a tricky problem!\ncontractions = { \n\"ain't\": \"am not / are not / is not / has not / have not\",\n\"aren't\": \"are not / am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had / he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall / he will\",\n\"he'll've\": \"he shall have / he will have\",\n\"he's\": \"he has / he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has / how is / how does\",\n\"I'd\": \"I had / I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I shall / I will\",\n\"I'll've\": \"I shall have / I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had / it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall / it will\",\n\"it'll've\": \"it shall have / it will have\",\n\"it's\": \"it has / it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had / she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall / she will\",\n\"she'll've\": \"she shall have / she will have\",\n\"she's\": \"she has / she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as / so is\",\n\"that'd\": \"that would / that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has / that is\",\n\"there'd\": \"there had / there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has / there is\",\n\"they'd\": \"they had / they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall / they will\",\n\"they'll've\": \"they shall have / they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had / we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall / what will\",\n\"what'll've\": \"what shall have / what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has / what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has / when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has / where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall / who will\",\n\"who'll've\": \"who shall have / who will have\",\n\"who's\": \"who has / who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has / why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had / you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall / you will\",\n\"you'll've\": \"you shall have / you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\n"}, "655": {"topic": "Expanding English language contractions in Python", "user_name": "", "text": "\nThe answers above will work perfectly well and could be better for ambiguous contraction (although I would argue that there aren't that many ambiguous cases). I would use something more readable and easier to maintain:\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n\ntest = \"Hey I'm Yann, how're you and how's it going ? That's interesting: I'd love to hear more about it.\"\nprint(decontracted(test))\n# Hey I am Yann, how are you and how is it going ? That is interesting: I would love to hear more about it.\n\nIt might have some flaws I didn't think about though.\nReposted from my other answer\n"}, "656": {"topic": "Expanding English language contractions in Python", "user_name": "arturomparturomp", "text": "\nI have found a library for this, contractions Its very simple.\nimport contractions\nprint(contractions.fix(\"you've\"))\nprint(contractions.fix(\"he's\"))\n\nOutput:\nyou have\nhe is\n\n"}, "657": {"topic": "Expanding English language contractions in Python", "user_name": "", "text": "\nYou don't need a library, it is possible to do with reg exp for example.\n>>> import re\n>>> contractions_dict = {\n...     'didn\\'t': 'did not',\n...     'don\\'t': 'do not',\n... }\n>>> contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n>>> def expand_contractions(s, contractions_dict=contractions_dict):\n...     def replace(match):\n...         return contractions_dict[match.group(0)]\n...     return contractions_re.sub(replace, s)\n...\n>>> expand_contractions('You don\\'t need a library')\n'You do not need a library'\n\n"}, "658": {"topic": "Expanding English language contractions in Python", "user_name": "Yann DuboisYann Dubois", "text": "\nThis is a very cool and easy to use library for the purpose \nhttps://pypi.python.org/pypi/pycontractions/1.0.1.\nExample of use (detailed in link):\nfrom pycontractions import Contractions\n\n# Load your favorite word2vec model\ncont = Contractions('GoogleNews-vectors-negative300.bin')\n\n# optional, prevents loading on first expand_texts call\ncont.load_models()\n\nout = list(cont.expand_texts([\"I'd like to know how I'd done that!\",\n                            \"We're going to the zoo and I don't think I'll be home for dinner.\",\n                            \"Theyre going to the zoo and she'll be home for dinner.\"], precise=True))\nprint(out)\n\nYou will also need GoogleNews-vectors-negative300.bin, link to download in the pycontractions link above.\n*Example code in python3. \n"}, "659": {"topic": "Expanding English language contractions in Python", "user_name": "Hammad HassanHammad Hassan", "text": "\nI would like to add little to alko's answer here. If you check wikipedia, the number of English Language contractions as mentioned there are less than 100. Granted, in real scenario this number could be more than that. But still, I am pretty sure that 200-300 words are all you will have for English contraction words. Now, do you want to get a separate library for those (I don't think what you are looking for actually exists, though)?. However, you can easily solve this problem with dictionary and using regex. I would recommend using a nice tokenizer asNatural Language Toolkit and the rest you should have no problem in implementing yourself.\n"}, "660": {"topic": "Expanding English language contractions in Python", "user_name": "alkoalko", "text": "\ndef expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n    # contraction_mapping is a dictionary of words having the compact form\n    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),flags=re.IGNORECASE|re.DOTALL)\n    def expand_match(contraction):\n        match = contraction.group(0)\n        first_char = match[0]\n        expanded_contraction = contraction_mapping.get(match) \\\n                                   if contraction_mapping.get(match) \\\n                                    else contraction_mapping.get(match.lower())                       \n        expanded_contraction = first_char+expanded_contraction[1:]\n        return expanded_contraction\n        \n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(\"'\", \"\", expanded_text)\n    return expanded_text\n\n"}, "661": {"topic": "Expanding English language contractions in Python", "user_name": "", "text": "\nEven though this is an old question, I figured I might as well answer since there is still no real solution to this as far as I can see.\nI have had to work on this on a related NLP project and I decided to tackle the problem since there didn't seem to be anything here. You can check my expander github repository if you are interested.\nIt's a fairly badly optimized (I think) program based on NLTK, the Stanford Core NLP models, which you will have to download separately, and the dictionary in the previous answer. All the necessary information should be in the README and the lavishly commented code. I know commented code is dead code, but this is just how I write to keep things clear for myself.\nThe example input in expander.py are the following sentences:\n    [\"I won't let you get away with that\",  # won't ->  will not\n    \"I'm a bad person\",  # 'm -> am\n    \"It's his cat anyway\",  # 's -> is\n    \"It's not what you think\",  # 's -> is\n    \"It's a man's world\",  # 's -> is and 's possessive\n    \"Catherine's been thinking about it\",  # 's -> has\n    \"It'll be done\",  # 'll -> will\n    \"Who'd've thought!\",  # 'd -> would, 've -> have\n    \"She said she'd go.\",  # she'd -> she would\n    \"She said she'd gone.\",  # she'd -> had\n    \"Y'all'd've a great time, wouldn't it be so cold!\", # Y'all'd've -> You all would have, wouldn't -> would not\n    \" My name is Jack.\",   # No replacements.\n    \"'Tis questionable whether Ma'am should be going.\", # 'Tis -> it is, Ma'am -> madam\n    \"As history tells, 'twas the night before Christmas.\", # 'Twas -> It was\n    \"Martha, Peter and Christine've been indulging in a menage-\u00e0-trois.\"] # 've -> have\n\nTo which the output is\n    [\"I will not let you get away with that\",\n    \"I am a bad person\",\n    \"It is his cat anyway\",\n    \"It is not what you think\",\n    \"It is a man's world\",\n    \"Catherine has been thinking about it\",\n    \"It will be done\",\n    \"Who would have thought!\",\n    \"She said she would go.\",\n    \"She said she had gone.\",\n    \"You all would have a great time, would not it be so cold!\",\n    \"My name is Jack.\",\n    \"It is questionable whether Madam should be going.\",\n    \"As history tells, it was the night before Christmas.\",\n    \"Martha, Peter and Christine have been indulging in a menage-\u00e0-trois.\"]\n\nSo for this small set of test sentences, I came up with to test some edge-cases, it works well.\nSince this project has lost importance right now, I am not actively developing this anymore. Any help on this project would be appreciated. Things to be done are written in the TODO list. Or if you have any tips on how to improve my python I would also be very thankful.\n"}, "662": {"topic": "What do the BILOU tags mean in Named Entity Recognition?", "user_name": "dd.", "text": "\nTitle pretty much sums up the question.  I've noticed that in some papers people have referred to a BILOU encoding scheme for NER as opposed to the typical BIO tagging scheme (Such as this paper by Ratinov and Roth in 2009 http://cogcomp.cs.illinois.edu/page/publication_view/199)\nFrom working with the 2003 CoNLL data I know that\nB stands for 'beginning' (signifies beginning of an NE)\nI stands for 'inside' (signifies that the word is inside an NE)\nO stands for 'outside' (signifies that the word is just a regular word outside of an NE)\n\nWhile I've been told that the words in BILOU stand for\nB - 'beginning'\nI - 'inside'\nL - 'last'\nO - 'outside'\nU - 'unit'\n\nI've also seen people reference another tag \nE - 'end', use it concurrently with the 'last' tag\nS - 'singleton', use it concurrently with the 'unit' tag\n\nI'm pretty new to the NER literature, but I've been unable to find something clearly explaining these tags.  My questions in particular relates to what the difference between 'last' and 'end' tags are, and what 'unit' tag stands for.\n"}, "663": {"topic": "What do the BILOU tags mean in Named Entity Recognition?", "user_name": "GrantD71GrantD71", "text": "\nBased on an issue and a patch in Clear TK, it seems like BILOU stands for \"Beginning, Inside and Last tokens of multi-token chunks, Unit-length chunks and Outside\" (emphasis added). For instance, the chunking denoted by brackets\n(foo foo foo) (bar) no no no (bar bar)\n\ncan be encoded with BILOU as \nB-foo, I-foo, L-foo, U-bar, O, O, O, B-bar, L-bar\n\n"}, "664": {"topic": "What do the BILOU tags mean in Named Entity Recognition?", "user_name": "", "text": "\nI would like to add some experience comparing BIO and BILOU schemes. My experiment was on one dataset only and may not be representative.\nMy dataset contains around 35 thousand short utterances (2-10 tokens) and are annotated using 11 different tags. In other words, there are 11 named entities.\nThe features used include the word, left and right 2-grams, 1-5 character ngrams (except middle ones), shape features and so on. Few entities are gazetteer backed as well.\nI shuffled the dataset and split it into 80/20 parts: training and testing. This process was repeated 5 times and for each entity I recorded Precision, Recall and F1-measure. The performance was measured at entity level, not at token level as in Ratinov & Roth, 2009 paper.\nThe software I used to train a model is CRFSuite. I used L-BFGS solver with c1=0 and c2=1.\nFirst of all, the test results compared for the 5 folds are very similar. This means there is little of variability from run to run, which is good. Second, BIO scheme performed very similarly as BILOU scheme. If there is any significant difference, perhaps it is at the third or fourth digit after period in Precision, Recall and F1-measures.\nConclusion: In my experiment BILOU scheme is not better (but also not worse) than the BIO scheme.\n"}, "665": {"topic": "What do the BILOU tags mean in Named Entity Recognition?", "user_name": "mbatchkarovmbatchkarov", "text": "\nB = Beginning\nI/M = Inside / Middle\nL/E = Last / End\nO = Outside\nU/W/S = Unit-length / Whole / Singleton\n\nSo BILOU is the same with IOBES and BMEWO.\nCho et al. compares performance of different IO, IB, IE, IOB, IOBES, etc. annotation variants. https://www.academia.edu/12852833/Named_entity_recognition_with_multiple_segment_representations\nThere is also BMEWO+, which put more information about surrounding word class to Outside tokens (thus \"O plus\"). See details here https://lingpipe-blog.com/2009/10/14/coding-chunkers-as-taggers-io-bio-bmewo-and-bmewo/\n"}, "666": {"topic": "What do the BILOU tags mean in Named Entity Recognition?", "user_name": "Vladislavs DovgalecsVladislavs Dovgalecs", "text": "\nThis just gives more context to your tags saying which part of the entity.\n BILOU Method/Schema\n\n | ------|--------------------|\n | BEGIN | The first token    |\n | ------|--------------------| \n | IN    | An inner token     |\n | ------|--------------------|\n | LAST  | The final token    |\n | ------|--------------------|\n | Unit  | A single-token     |\n | ------|--------------------|\n | Out   | A non-entity token |\n | ------|--------------------|\n\nBIOES\nA more sophisticated annotation method distinguishes between the end of a named entity and single entities. This method is called BIOES for Begin, Inside, Outside, End, Single.\n\nIOB (e.g. CoNLL 2003)\nIOB (or BIO) stands for Begin, Inside and Outside. Words tagged with O are outside of named entities\n\nfor more detailed information Please go through the below link\n    URL : https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)\n\n    URL :https://towardsdatascience.com/deep-learning-for-ner-1-public-datasets-and-annotation-methods-8b1ad5e98caf\n\n"}, "667": {"topic": "What do the BILOU tags mean in Named Entity Recognition?", "user_name": "", "text": "\n\nB - 'begin'\nI - 'inside'\nL - 'last'\nO - 'outside/other'\nU - 'unigram'\n\n"}, "668": {"topic": "What do the BILOU tags mean in Named Entity Recognition?", "user_name": "bact'bact'", "text": "\nBIO is the same as BILOU except for the following points:\n\nIn BILOU, the last I tag in a particular I \"cluster\" would be converted to L.\nEg.\n\nBIO - B-foo, I-foo, I-foo, O, O, O, B-bar, I-bar\nBILOU - B-foo, I-foo, L-foo, O, O, O, B-bar, L-bar\n\n\nIn BILOU, any standalone tag is converted to a U tag.\nEg.\n\nBIO - B-foo, O, O, O, B-bar\nBILOU - U-foo, O, O, O, U-bar\n\nFollowing is a set of same tags represented in both BIO and BILOU notations:\nBIO - B-foo, I-foo, I-foo, O, O, B-bar, I-bar, O, B-bar, O\nBILOU - B-foo, I-foo, L-foo, O, O, B-bar, L-bar, O, U-bar, O\n\n"}, "669": {"topic": "Training data for sentiment analysis [closed]", "user_name": "Iterator", "text": "\n\n\n\n\n\n\nClosed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.                    \n\n\n\n\n\n\n\n\n\n\n We don\u2019t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\n\n\nClosed 7 years ago.\n\n\n\n\n\n\n\r\n                        Improve this question\r\n                    \n\n\n\nWhere can I get a corpus of documents that have already been classified as positive/negative for sentiment in the corporate domain? I want a large corpus of documents that provide reviews for companies, like reviews of companies provided by analysts and media.\nI find corpora that have reviews of products and movies. Is there a corpus for the business domain including reviews of companies, that match the language of business?\n"}, "670": {"topic": "Training data for sentiment analysis [closed]", "user_name": "London guyLondon guy", "text": "\nhttp://www.cs.cornell.edu/home/llee/data/\nhttp://mpqa.cs.pitt.edu/corpora/mpqa_corpus\nYou can use twitter, with its smileys, like this: http://web.archive.org/web/20111119181304/http://deepthoughtinc.com/wp-content/uploads/2011/01/Twitter-as-a-Corpus-for-Sentiment-Analysis-and-Opinion-Mining.pdf\nHope that gets you started.  There's more in the literature, if you're interested in specific subtasks like negation, sentiment scope, etc.\nTo get a focus on companies, you might pair a method with topic detection, or cheaply just a lot of mentions of a given company.  Or you could get your data annotated by Mechanical Turkers.\n"}, "671": {"topic": "Training data for sentiment analysis [closed]", "user_name": "        user325117\r", "text": "\nThis is a list I wrote a few weeks ago, from my blog. Some of these datasets have been recently included in the NLTK Python platform.\nLexicons\n\nOpinion Lexicon by Bing Liu\n\nURL: http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon\nPAPERS: Mining and summarizing customer reviews\nNOTES: Included in the NLTK Python platform\n\n\nMPQA Subjectivity Lexicon\n\nURL: http://mpqa.cs.pitt.edu/#subj_lexicon\nPAPERS: Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis (Theresa Wilson, Janyce Wiebe, and Paul Hoffmann, 2005).\n\n\nSentiWordNet\n\nURL: http://sentiwordnet.isti.cnr.it\nNOTES: Included in the NLTK Python platform\n\n\nHarvard General Inquirer\n\nURL: http://www.wjh.harvard.edu/~inquirer\nPAPERS: The General Inquirer: A Computer Approach to Content Analysis (Stone, Philip J; Dexter C. Dunphry; Marshall S. Smith; and Daniel M. Ogilvie. 1966)\n\n\nLinguistic Inquiry and Word Counts (LIWC)\n\nURL: http://www.liwc.net\n\n\nVader Lexicon\n\nURLs: https://github.com/cjhutto/vaderSentiment, http://comp.social.gatech.edu/papers\nPAPERS: Vader: A parsimonious rule-based model for sentiment analysis of social media text (Hutto, Gilbert.  2014)\n\n\n\n\nDatasets\n\nMPQA Datasets\n\nURL: http://mpqa.cs.pitt.edu\n\nNOTES: GNU Public License.\n\nPolitical Debate data\nProduct Debate data\nSubjectivity Sense Annotations\n\n\n\n\nSentiment140 (Tweets)\n\nURL: http://help.sentiment140.com/for-students\nPAPERS: Twitter Sent classification using Distant Supervision (Go, Alec, Richa Bhayani, and Lei Huang)\nURLs: http://help.sentiment140.com, https://groups.google.com/forum/#!forum/sentiment140\n\n\nSTS-Gold (Tweets)\n\nURL: http://www.tweenator.com/index.php?page_id=13\nPAPERS: Evaluation datasets for twitter sentiment analysis (Saif, Fernandez, He, Alani)\nNOTES: As Sentiment140, but the dataset is smaller and with human annotators. It comes with 3 files: tweets, entities (with their sentiment) and an aggregate set.\n\n\nCustomer Review Dataset (Product reviews)\n\nURL: http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets\n\nPAPERS: Mining and summarizing customer reviews\n\nNOTES: Title of review, product feature, positive/negative label with opinion strength, other info (comparisons, pronoun resolution, etc.)\nIncluded in the NLTK Python platform\n\n\n\nPros and Cons Dataset (Pros and cons sentences)\n\nURL: http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets\n\nPAPERS: Mining Opinions in Comparative Sentences (Ganapathibhotla, Liu 2008)\n\nNOTES: A list of sentences tagged <pros> or <cons>\nIncluded in the NLTK Python platform\n\n\n\nComparative Sentences (Reviews)\n\nURL: http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets\n\nPAPERS: Identifying Comparative Sentences in Text Documents (Nitin Jindal and Bing Liu), Mining Opinion Features in Customer Reviews (Minqing Hu and Bing Liu)\n\nNOTES: Sentence, POS-tagged sentence, entities, comparison type (non-equal, equative, superlative, non-gradable)\nIncluded in the NLTK Python platform\n\n\n\nSanders Analytics Twitter Sentiment Corpus (Tweets)\n\nURL: http://www.sananalytics.com/lab/twitter-sentiment\n\n\n5513 hand-classified tweets wrt 4 different topics. Because of Twitter\u2019s ToS, a small Python script is included to download all of the tweets. The sentiment classifications themselves are provided free of charge and without restrictions. They may be used for commercial products. They may be redistributed. They may be modified.\n\n\nSpanish tweets (Tweets)\n\nURL: http://www.daedalus.es/TASS2013/corpus.php\n\n\nSemEval 2014 (Tweets)\n\nURL: http://alt.qcri.org/semeval2014/task9\n\n\nYou MUST NOT re-distribute the tweets, the annotations or the corpus obtained (from the readme file)\n\n\nVarious Datasets (Reviews)\n\nURL: https://personalwebs.coloradocollege.edu/~mwhitehead/html/opinion_mining.html\nPAPERS: Building a General Purpose Cross-Domain Sentiment Mining Model (Whitehead and Yaeger), Sentiment Mining Using Ensemble Classification Models (Whitehead and Yaeger)\n\n\nVarious Datasets #2 (Reviews)\n\nURL: http://www.text-analytics101.com/2011/07/user-review-datasets_20.html\n\n\n\n\nReferences:\n\nKeenformatics - Sentiment Analysis lexicons and datasets (my blog)\nPersonal experience\n\n"}, "672": {"topic": "Training data for sentiment analysis [closed]", "user_name": "Gregory MartonGregory Marton", "text": "\nHere are a few more;\nhttp://inclass.kaggle.com/c/si650winter11\nhttp://alias-i.com/lingpipe/demos/tutorial/sentiment/read-me.html\n"}, "673": {"topic": "Training data for sentiment analysis [closed]", "user_name": "", "text": "\nIf you have some resources (media channels, blogs, etc) about the domain you want to explore, you can create your own corpus. \nI do this in python: \n\nusing Beautiful Soup http://www.crummy.com/software/BeautifulSoup/ for parsing the content that I want to classify. \nseparate those sentences meaning positive/negative opinions about companies. \nUse NLTK to process this sentences, tokenize words, POS tagging, etc. \nUse NLTK PMI to calculate bigrams or trigrams mos frequent in only one class\n\nCreating corpus is a hard work of pre-processing, checking, tagging, etc, but has the benefits of preparing a model for a specific domain many times increasing the accuracy. If you can get already prepared corpus, just go ahead with the sentiment analysis ;) \n"}, "674": {"topic": "Training data for sentiment analysis [closed]", "user_name": "Kurt BourbakiKurt Bourbaki", "text": "\nI'm not aware of any such corpus being freely available, but you could try an unsupervised method on an unlabeled dataset.\n"}, "675": {"topic": "Training data for sentiment analysis [closed]", "user_name": "y2py2p", "text": "\nYou can get a large select of online reviews from Datafiniti.  Most of the reviews come with rating data, which would provide more granularity on sentiment than positive / negative.  Here's a list of businesses with reviews, and here's a list of products with reviews.\n"}, "676": {"topic": "Best way to identify and extract dates from text Python?", "user_name": "redctredct", "text": "\nAs part of a larger personal project I'm working on, I'm attempting to separate out inline dates from a variety of text sources.\nFor example, I have a large list of strings (that usually take the form of English sentences or statements) that take a variety of forms:\n\nCentral design committee session Tuesday 10/22 6:30 pm\nTh 9/19 LAB: Serial encoding (Section 2.2)\nThere will be another one on December 15th for those who are unable to make it today.\nWorkbook 3 (Minimum Wage): due Wednesday 9/18 11:59pm\nHe will be flying in Sept. 15th.\n\nWhile these dates are in-line with natural text, none of them are in specifically natural language forms themselves (e.g., there's no \"The meeting will be two weeks from tomorrow\"\u2014it's all explicit).  \nAs someone who doesn't have too much experience with this kind of processing, what would be the best place to begin? I've looked into things like the dateutil.parser module and parsedatetime, but those seem to be for after you've isolated the date.\nBecause of this, is there any good way to extract the date and the extraneous text \ninput:  Th 9/19 LAB: Serial encoding (Section 2.2)\noutput: ['Th 9/19', 'LAB: Serial encoding (Section 2.2)']\n\nor something similar? It seems like this sort of processing is done by applications like Gmail and Apple Mail, but is it possible to implement in Python?\n"}, "677": {"topic": "Best way to identify and extract dates from text Python?", "user_name": "Renaud", "text": "\nI was also looking for a solution to this and couldn't find any, so a friend and I built a tool to do this. I thought I would come back and share incase others found it helpful.\ndatefinder -- find and extract dates inside text\nHere's an example:\nimport datefinder\n\nstring_with_dates = '''\n    Central design committee session Tuesday 10/22 6:30 pm\n    Th 9/19 LAB: Serial encoding (Section 2.2)\n    There will be another one on December 15th for those who are unable to make it today.\n    Workbook 3 (Minimum Wage): due Wednesday 9/18 11:59pm\n    He will be flying in Sept. 15th.\n    We expect to deliver this between late 2021 and early 2022.\n'''\n\nmatches = datefinder.find_dates(string_with_dates)\nfor match in matches:\n    print(match)\n\n"}, "678": {"topic": "Best way to identify and extract dates from text Python?", "user_name": "akoumjianakoumjian", "text": "\nI am surprised that there is no mention of SUTime and dateparser's search_dates method.  \nfrom sutime import SUTime\nimport os\nimport json\nfrom dateparser.search import search_dates\n\nstr1 = \"Let's meet sometime next Thursday\" \n\n# You'll get more information about these jar files from SUTime's github page\njar_files = os.path.join(os.path.dirname(__file__), 'jars')\nsutime = SUTime(jars=jar_files, mark_time_ranges=True)\n\nprint(json.dumps(sutime.parse(str1), sort_keys=True, indent=4))\n\"\"\"output: \n[\n    {\n        \"end\": 33,\n        \"start\": 20,\n        \"text\": \"next Thursday\",\n        \"type\": \"DATE\",\n        \"value\": \"2018-10-11\"\n    }\n]\n\"\"\"\n\nprint(search_dates(str1))\n#output:\n#[('Thursday', datetime.datetime(2018, 9, 27, 0, 0))]\n\nAlthough I have tried other modules like dateutil, datefinder and natty (couldn't get duckling to work with python), this two seem to give the most promising results. \nThe results from SUTime are more reliable and it's clear from the above code snippet. However, the SUTime fails in some basic scenarios like parsing a text \n\n\"I won't be available until 9/19\"\n\nor \n\n\"I won't be available between (September 18-September 20).\n\nIt gives no result for the first text and only gives month and year for the second text. \nThis is however handled quite well in the search_dates method. \nsearch_dates method is more aggressive and will give all possible dates related to any words in the input text. \nI haven't yet found a way to parse the text strictly for dates in search_methods. If I could find a way to do that, it'll be my first choice over SUTime and I would also make sure to update this answer if I find it. \n"}, "679": {"topic": "Best way to identify and extract dates from text Python?", "user_name": "Afsan Abdulali GujaratiAfsan Abdulali Gujarati", "text": "\nYou can use the dateutil module's parse method with the fuzzy option.\n>>> from dateutil.parser import parse\n>>> parse(\"Central design committee session Tuesday 10/22 6:30 pm\", fuzzy=True)\ndatetime.datetime(2018, 10, 22, 18, 30)\n>>> parse(\"There will be another one on December 15th for those who are unable to make it today.\", fuzzy=True)\ndatetime.datetime(2018, 12, 15, 0, 0)\n>>> parse(\"Workbook 3 (Minimum Wage): due Wednesday 9/18 11:59pm\", fuzzy=True)\ndatetime.datetime(2018, 3, 9, 23, 59)\n>>> parse(\"He will be flying in Sept. 15th.\", fuzzy=True)\ndatetime.datetime(2018, 9, 15, 0, 0)\n>>> parse(\"Th 9/19 LAB: Serial encoding (Section 2.2)\", fuzzy=True)\ndatetime.datetime(2002, 9, 19, 0, 0)\n\n"}, "680": {"topic": "Best way to identify and extract dates from text Python?", "user_name": "Samkit JainSamkit Jain", "text": "\nIf you can identify the segments that actually contain the date information, parsing them can be fairly simple with parsedatetime. There are a few things to consider though namely that your dates don't have years and you should pick a locale.\n>>> import parsedatetime\n>>> p = parsedatetime.Calendar()\n>>> p.parse(\"December 15th\")\n((2013, 12, 15, 0, 13, 30, 4, 319, 0), 1)\n>>> p.parse(\"9/18 11:59 pm\")\n((2014, 9, 18, 23, 59, 0, 4, 319, 0), 3)\n>>> # It chooses 2014 since that's the *next* occurence of 9/18\n\nIt doesn't always work perfectly when you have extraneous text.\n>>> p.parse(\"9/19 LAB: Serial encoding\")\n((2014, 9, 19, 0, 15, 30, 4, 319, 0), 1)\n>>> p.parse(\"9/19 LAB: Serial encoding (Section 2.2)\")\n((2014, 2, 2, 0, 15, 32, 4, 319, 0), 1)\n\nHonestly, this seems like the kind of problem that would be simple enough to parse for particular formats and pick the most likely out of each sentence. Beyond that, it would be a decent machine learning problem.\n"}, "681": {"topic": "Best way to identify and extract dates from text Python?", "user_name": "", "text": "\nNewer versions of parsedatetime lib provide search functionality. \nExample\nfrom dateparser.search import search_dates\n\ndates = search_dates('Central design committee session Tuesday 10/22 6:30 pm')\n\n"}, "682": {"topic": "Best way to identify and extract dates from text Python?", "user_name": "Kyle KelleyKyle Kelley", "text": "\nHi I'm not sure bellow approach is machine learning but you may try it:\n\nadd some context from outside text, e.g publishing time of text message, posting, now etc. (your text doesn't tell anything about year) \nextract all tokens with separator white-space and should get something like this:\n['Th','Wednesday','9:34pm','7:34','pm','am','9/18','9/','/18', '19','12']\n\nprocess them with rule-sets e.g subsisting from weekdays and/or variations of components forming time and mark them e.g. '%d:%dpm', '%d am', '%d/%d', '%d/ %d' etc.  may means time. \n Note that it may have compositions e.g. \"12 / 31\" is 3gram ('12','/','31') should be one token \"12/31\" of interest.\n\"see\" what tokens are around marked tokens like \"9:45pm\" e.g ('Th\",'9/19','9:45pm') is 3gram formed from \"interesting\" tokens and apply rules about it that may determine meaning. \nprocess for more specific analysis for example if have 31/12 so 31 > 12 means d/m, or vice verse, but if have 12/12 m,d will be available only in context build from text and/or outside.\n\nCheers\n"}, "683": {"topic": "Best way to identify and extract dates from text Python?", "user_name": "Ramtin M. SerajRamtin M. Seraj", "text": "\nThere is no any perfact solution. IT's completely depend on which type of data u are suppose to work. Quickly review and analyze data by going through certain set of data manually and prepare regex pattern and test it wheather it is working or not.\nPredefined all packages solve a date extraction problem up to some extent and it is limited one. if one will approximately find out pattern by looking to data then user can prepare regex. It will help them to prevent to iterate and loop over all rules written in packages.\n"}, "684": {"topic": "How can I split multiple joined words?", "user_name": "H H", "text": "\nI have an array of 1000 or so entries, with examples below:\nwickedweather\nliquidweather\ndriveourtrucks\ngocompact\nslimprojector\n\nI would like to be able to split these into their respective words, as:\nwicked weather\nliquid weather\ndrive our trucks\ngo compact\nslim projector\n\nI was hoping a regular expression my do the trick.  But, since there is no boundary to stop on, nor is there any sort of capitalization that I could possibly key on, I am thinking, that some sort of reference to a dictionary might be necessary?  \nI suppose it could be done by hand, but why - when it can be done with code! =)  But this has stumped me.  Any ideas?  \n"}, "685": {"topic": "How can I split multiple joined words?", "user_name": "TaptronicTaptronic", "text": "\nThe Viterbi algorithm is much faster. It computes the same scores as the recursive search in Dmitry's answer above, but in O(n) time. (Dmitry's search takes exponential time; Viterbi does it by dynamic programming.)\nimport re\nfrom collections import Counter\n\ndef viterbi_segment(text):\n    probs, lasts = [1.0], [0]\n    for i in range(1, len(text) + 1):\n        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)\n                        for j in range(max(0, i - max_word_length), i))\n        probs.append(prob_k)\n        lasts.append(k)\n    words = []\n    i = len(text)\n    while 0 < i:\n        words.append(text[lasts[i]:i])\n        i = lasts[i]\n    words.reverse()\n    return words, probs[-1]\n\ndef word_prob(word): return dictionary[word] / total\ndef words(text): return re.findall('[a-z]+', text.lower()) \ndictionary = Counter(words(open('big.txt').read()))\nmax_word_length = max(map(len, dictionary))\ntotal = float(sum(dictionary.values()))\n\nTesting it:\n>>> viterbi_segment('wickedweather')\n(['wicked', 'weather'], 5.1518198982768158e-10)\n>>> ' '.join(viterbi_segment('itseasyformetosplitlongruntogetherblocks')[0])\n'its easy for me to split long run together blocks'\n\nTo be practical you'll likely want a couple refinements:\n\nAdd logs of probabilities, don't multiply probabilities. This avoids floating-point underflow.\nYour inputs will in general use words not in your corpus. These substrings must be assigned a nonzero probability as words, or you end up with no solution or a bad solution. (That's just as true for the above exponential search algorithm.) This probability has to be siphoned off the corpus words' probabilities and distributed plausibly among all other word candidates: the general topic is known as smoothing in statistical language models. (You can get away with some pretty rough hacks, though.) This is where the O(n) Viterbi algorithm blows away the search algorithm, because considering non-corpus words blows up the branching factor.\n\n"}, "686": {"topic": "How can I split multiple joined words?", "user_name": "kumardeepakr3", "text": "\nCan a human do it?\n\nfarsidebag\nfar sidebag\nfarside bag\nfar side bag\n\nNot only do you have to use a dictionary, you might have to use a statistical approach to figure out what's most likely (or, god forbid, an actual HMM for your human language of choice...)\nFor how to do statistics that might be helpful, I turn you to Dr. Peter Norvig, who addresses a different, but related problem of spell-checking in 21 lines of code:\nhttp://norvig.com/spell-correct.html\n(he does cheat a bit by folding every for loop into a single line.. but still).\nUpdate This got stuck in my head, so I had to birth it today.  This code does a similar split to the one described by Robert Gamble, but then it orders the results based on word frequency in the provided dictionary file (which is now expected to be some text representative of your domain or English in general.  I used big.txt from Norvig, linked above, and catted a dictionary to it, to cover missing words).\nA combination of two words will most of the time beat a combination of 3 words, unless the frequency difference is enormous.\n\nI posted this code with some minor changes on my blog\nhttp://squarecog.wordpress.com/2008/10/19/splitting-words-joined-into-a-single-string/\nand also wrote a little about the underflow bug in this code.. I was tempted to just quietly fix it, but figured this may help some folks who haven't seen the log trick before:\nhttp://squarecog.wordpress.com/2009/01/10/dealing-with-underflow-in-joint-probability-calculations/\n\nOutput on your words, plus a few of my own -- notice what happens with \"orcore\":\n\nperl splitwords.pl big.txt words\nanswerveal: 2 possibilities\n -  answer veal\n -  answer ve al\n\nwickedweather: 4 possibilities\n -  wicked weather\n -  wicked we at her\n -  wick ed weather\n -  wick ed we at her\n\nliquidweather: 6 possibilities\n -  liquid weather\n -  liquid we at her\n -  li quid weather\n -  li quid we at her\n -  li qu id weather\n -  li qu id we at her\n\ndriveourtrucks: 1 possibilities\n -  drive our trucks\n\ngocompact: 1 possibilities\n -  go compact\n\nslimprojector: 2 possibilities\n -  slim projector\n -  slim project or\n\norcore: 3 possibilities\n -  or core\n -  or co re\n -  orc ore\n\n\nCode:\n#!/usr/bin/env perl\n\nuse strict;\nuse warnings;\n\nsub find_matches($);\nsub find_matches_rec($\\@\\@);\nsub find_word_seq_score(@);\nsub get_word_stats($);\nsub print_results($@);\nsub Usage();\n\nour(%DICT,$TOTAL);\n{\n  my( $dict_file, $word_file ) = @ARGV;\n  ($dict_file && $word_file) or die(Usage);\n\n  {\n    my $DICT;\n    ($DICT, $TOTAL) = get_word_stats($dict_file);\n    %DICT = %$DICT;\n  }\n\n  {\n    open( my $WORDS, '<', $word_file ) or die \"unable to open $word_file\\n\";\n\n    foreach my $word (<$WORDS>) {\n      chomp $word;\n      my $arr = find_matches($word);\n\n\n      local $_;\n      # Schwartzian Transform\n      my @sorted_arr =\n        map  { $_->[0] }\n        sort { $b->[1] <=> $a->[1] }\n        map  {\n          [ $_, find_word_seq_score(@$_) ]\n        }\n        @$arr;\n\n\n      print_results( $word, @sorted_arr );\n    }\n\n    close $WORDS;\n  }\n}\n\n\nsub find_matches($){\n    my( $string ) = @_;\n\n    my @found_parses;\n    my @words;\n    find_matches_rec( $string, @words, @found_parses );\n\n    return  @found_parses if wantarray;\n    return \\@found_parses;\n}\n\nsub find_matches_rec($\\@\\@){\n    my( $string, $words_sofar, $found_parses ) = @_;\n    my $length = length $string;\n\n    unless( $length ){\n      push @$found_parses, $words_sofar;\n\n      return @$found_parses if wantarray;\n      return  $found_parses;\n    }\n\n    foreach my $i ( 2..$length ){\n      my $prefix = substr($string, 0, $i);\n      my $suffix = substr($string, $i, $length-$i);\n\n      if( exists $DICT{$prefix} ){\n        my @words = ( @$words_sofar, $prefix );\n        find_matches_rec( $suffix, @words, @$found_parses );\n      }\n    }\n\n    return @$found_parses if wantarray;\n    return  $found_parses;\n}\n\n\n## Just a simple joint probability\n## assumes independence between words, which is obviously untrue\n## that's why this is broken out -- feel free to add better brains\nsub find_word_seq_score(@){\n    my( @words ) = @_;\n    local $_;\n\n    my $score = 1;\n    foreach ( @words ){\n        $score = $score * $DICT{$_} / $TOTAL;\n    }\n\n    return $score;\n}\n\nsub get_word_stats($){\n    my ($filename) = @_;\n\n    open(my $DICT, '<', $filename) or die \"unable to open $filename\\n\";\n\n    local $/= undef;\n    local $_;\n    my %dict;\n    my $total = 0;\n\n    while ( <$DICT> ){\n      foreach ( split(/\\b/, $_) ) {\n        $dict{$_} += 1;\n        $total++;\n      }\n    }\n\n    close $DICT;\n\n    return (\\%dict, $total);\n}\n\nsub print_results($@){\n    #( 'word', [qw'test one'], [qw'test two'], ... )\n    my ($word,  @combos) = @_;\n    local $_;\n    my $possible = scalar @combos;\n\n    print \"$word: $possible possibilities\\n\";\n    foreach (@combos) {\n      print ' -  ', join(' ', @$_), \"\\n\";\n    }\n    print \"\\n\";\n}\n\nsub Usage(){\n    return \"$0 /path/to/dictionary /path/to/your_words\";\n}\n\n"}, "687": {"topic": "How can I split multiple joined words?", "user_name": "Darius BaconDarius Bacon", "text": "\npip install wordninja\n>>> import wordninja\n>>> wordninja.split('bettergood')\n['better', 'good']\n\n"}, "688": {"topic": "How can I split multiple joined words?", "user_name": "", "text": "\nThe best tool for the job here is recursion, not regular expressions.  The basic idea is to start from the beginning of the string looking for a word, then take the remainder of the string and look for another word, and so on until the end of the string is reached.  A recursive solution is natural since backtracking needs to happen when a given remainder of the string cannot be broken into a set of words.  The solution below uses a dictionary to determine what is a word and prints out solutions as it finds them (some strings can be broken out into multiple possible sets of words, for example wickedweather could be parsed as \"wicked we at her\").  If you just want one set of words you will need to determine the rules for selecting the best set, perhaps by selecting the solution with fewest number of words or by setting a minimum word length.\n#!/usr/bin/perl\n\nuse strict;\n\nmy $WORD_FILE = '/usr/share/dict/words'; #Change as needed\nmy %words; # Hash of words in dictionary\n\n# Open dictionary, load words into hash\nopen(WORDS, $WORD_FILE) or die \"Failed to open dictionary: $!\\n\";\nwhile (<WORDS>) {\n  chomp;\n  $words{lc($_)} = 1;\n}\nclose(WORDS);\n\n# Read one line at a time from stdin, break into words\nwhile (<>) {\n  chomp;\n  my @words;\n  find_words(lc($_));\n}\n\nsub find_words {\n  # Print every way $string can be parsed into whole words\n  my $string = shift;\n  my @words = @_;\n  my $length = length $string;\n\n  foreach my $i ( 1 .. $length ) {\n    my $word = substr $string, 0, $i;\n    my $remainder = substr $string, $i, $length - $i;\n    # Some dictionaries contain each letter as a word\n    next if ($i == 1 && ($word ne \"a\" && $word ne \"i\"));\n\n    if (defined($words{$word})) {\n      push @words, $word;\n      if ($remainder eq \"\") {\n        print join(' ', @words), \"\\n\";\n        return;\n      } else {\n        find_words($remainder, @words);\n      }\n      pop @words;\n    }\n  }\n\n  return;\n}\n\n"}, "689": {"topic": "How can I split multiple joined words?", "user_name": "\r", "text": "\nI think you're right in thinking that it's not really a job for a regular expression. I would approach this using the dictionary idea - look for the longest prefix that is a word in the dictionary. When you find that, chop it off and do the same with the remainder of the string.\nThe above method is subject to ambiguity, for example \"drivereallyfast\" would first find \"driver\" and then have trouble with \"eallyfast\". So you would also have to do some backtracking if you ran into this situation. Or, since you don't have that many strings to split, just do by hand the ones that fail the automated split.\n"}, "690": {"topic": "How can I split multiple joined words?", "user_name": "\r", "text": "\nThis is related to a problem known as identifier splitting or identifier name tokenization.  In the OP's case, the inputs seem to be concatenations of ordinary words; in identifier splitting, the inputs are class names, function names or other identifiers from source code, and the problem is harder.  I realize this is an old question and the OP has either solved their problem or moved on, but in case someone else comes across this question while looking for identifier splitters (like I was, not long ago), I would like to offer Spiral (\"SPlitters for IdentifieRs: A Library\").  It is written in Python but comes with a command-line utility that can read a file of identifiers (one per line) and split each one.\nSplitting identifiers is deceptively difficult.  Programmers commonly use abbreviations, acronyms and word fragments when naming things, and they don't always use consistent conventions.  Even in when identifiers do follow some convention such as camel case, ambiguities can arise.\nSpiral implements numerous identifier splitting algorithms, including a novel algorithm called Ronin.  It uses a variety of heuristic rules, English dictionaries, and tables of token frequencies obtained from mining source code repositories.  Ronin can split identifiers that do not use camel case or other naming conventions, including cases such as splitting J2SEProjectTypeProfiler into [J2SE, Project, Type, Profiler], which requires the reader to recognize J2SE as a unit.  Here are some more examples of what Ronin can split:\n# spiral mStartCData nonnegativedecimaltype getUtf8Octets GPSmodule savefileas nbrOfbugs\nmStartCData: ['m', 'Start', 'C', 'Data']\nnonnegativedecimaltype: ['nonnegative', 'decimal', 'type']\ngetUtf8Octets: ['get', 'Utf8', 'Octets']\nGPSmodule: ['GPS', 'module']\nsavefileas: ['save', 'file', 'as']\nnbrOfbugs: ['nbr', 'Of', 'bugs']\n\nUsing the examples from the OP's question:\n# spiral wickedweather liquidweather  driveourtrucks gocompact slimprojector\nwickedweather: ['wicked', 'weather']\nliquidweather: ['liquid', 'weather']\ndriveourtrucks: ['driveourtrucks']\ngocompact: ['go', 'compact']\nslimprojector: ['slim', 'projector']\n\nAs you can see, it is not perfect.  It's worth noting that Ronin has a number of parameters and adjusting them makes it possible to split driveourtrucks too, but at the cost of worsening performance on program identifiers.\nMore information can be found in the GitHub repo for Spiral.\n"}, "691": {"topic": "How can I split multiple joined words?", "user_name": "kamran kausarkamran kausar", "text": "\nA simple solution with Python: install the wordsegment package: pip install wordsegment.\n$ echo thisisatest | python -m wordsegment\nthis is a test\n\n"}, "692": {"topic": "How can I split multiple joined words?", "user_name": "Robert GambleRobert Gamble", "text": "\nWell, the problem itself is not solvable with just a regular expression. A solution (probably not the best) would be to get a dictionary and do a regular expression match for each work in the dictionary to each word in the list, adding the space whenever successful. Certainly this would not be terribly quick, but it would be easy to program and faster than hand doing it. \n"}, "693": {"topic": "How can I split multiple joined words?", "user_name": "Greg HewgillGreg Hewgill", "text": "\nA dictionary based solution would be required. This might be simplified somewhat if you have a limited dictionary of words that can occur, otherwise words that form the prefix of other words are going to be a problem.\n"}, "694": {"topic": "How can I split multiple joined words?", "user_name": "mhuckamhucka", "text": "\nThere is python package released Santhosh thottingal called mlmorph which can be used for morphological analysis.\nhttps://pypi.org/project/mlmorph/\nExamples:\nfrom mlmorph import Analyser\nanalyser = Analyser()\nanalyser.analyse(\"\u0d15\u0d47\u0d30\u0d33\u0d24\u0d4d\u0d24\u0d3f\u0d28\u0d4d\u0d31\u0d46\")\n\nGives\n[('\u0d15\u0d47\u0d30\u0d33\u0d02<np><genitive>', 179)]\n\nHe also wrote a blog on the topic https://thottingal.in/blog/2017/11/26/towards-a-malayalam-morphology-analyser/\n"}, "695": {"topic": "How can I split multiple joined words?", "user_name": "RabashRabash", "text": "\nThis will work if the are camelCase. JavaScript!!!\nfunction spinalCase(str) {\n  let lowercase = str.trim()\n  let regEx = /\\W+|(?=[A-Z])|_/g\n  let result = lowercase.split(regEx).join(\"-\").toLowerCase()\n\n  return result;\n}\n\nspinalCase(\"AllThe-small Things\");\n\n"}, "696": {"topic": "How can I split multiple joined words?", "user_name": "Zoe GagnonZoe Gagnon", "text": "\nOne of the solutions could be with recurssion (the same can be converted into dynamic-programming):\nstatic List<String> wordBreak(\n    String input,\n    Set<String> dictionary\n) {\n\n  List<List<String>> result = new ArrayList<>();\n  List<String> r = new ArrayList<>();\n\n  helper(input, dictionary, result, \"\", 0, new Stack<>());\n\n  for (List<String> strings : result) {\n    String s = String.join(\" \", strings);\n    r.add(s);\n  }\n\n  return r;\n}\n\nstatic void helper(\n    final String input,\n    final Set<String> dictionary,\n    final List<List<String>> result,\n    String state,\n    int index,\n    Stack<String> stack\n) {\n\n  if (index == input.length()) {\n\n    // add the last word\n    stack.push(state);\n\n    for (String s : stack) {\n      if (!dictionary.contains(s)) {\n        return;\n      }\n    }\n\n    result.add((List<String>) stack.clone());\n\n    return;\n  }\n\n  if (dictionary.contains(state)) {\n    // bifurcate\n    stack.push(state);\n    helper(input, dictionary, result, \"\" + input.charAt(index),\n           index + 1, stack);\n\n    String pop = stack.pop();\n    String s = stack.pop();\n\n    helper(input, dictionary, result, s + pop.charAt(0),\n           index + 1, stack);\n\n  }\n  else {\n    helper(input, dictionary, result, state + input.charAt(index),\n           index + 1, stack);\n  }\n\n  return;\n}\n\nThe other possible solution would be the use of Tries data structure.\n"}, "697": {"topic": "How can I split multiple joined words?", "user_name": "Mitch WheatMitch Wheat", "text": "\noutput :-\n['better', 'good'] ['coffee', 'shop']\n['coffee', 'shop']\n\n    pip install wordninja\nimport wordninja\nn=wordninja.split('bettergood')\nm=wordninja.split(\"coffeeshop\")\nprint(n,m)\n\nlist=['hello','coffee','shop','better','good']\nmat='coffeeshop'\nexpected=[]\nfor i in list:\n    if i in mat:\n        expected.append(i)\nprint(expected)\n\n"}, "698": {"topic": "How can I split multiple joined words?", "user_name": "adam shamsudeenadam shamsudeen", "text": "\nSo I spent like 2 days on this answer, since I need it for my own NLP work. My answer is derived from Darius Bacon's answer, which itself was derived from the Viterbi algorithm. I also abstracted it to take each word in a message, attempt to split it, and then reassemble the message. I expanded Darius's code to make it debuggable. I also swapped out the need for \"big.txt\", and use the wordfreq library instead. Some comments stress the need to use a non-zero word frequency for non-existent words. I found that using any frequency higher than zero would cause \"itseasyformetosplitlongruntogetherblocks\" to undersplit into \"itseasyformetosplitlongruntogether blocks\". The algorithm in general tends to either oversplit or undersplit various test messages depending on how you combine word frequencies and how you handle missing word frequencies. I played around with many tweaks until it behaved well. My solution uses a 0.0 frequency for missing words. It also adds a reward for word length (otherwise it tends to split words into characters). I tried many length rewards, and the one that seems to work best for my test cases is word_frequency * (e ** word_length). There were also comments warning against multiplying word frequencies together. I tried adding them, using the harmonic mean, and using 1-freq instead of the 0.00001 form. They all tended to oversplit the test cases. Simply multiplying word frequencies together worked best. I left my debugging print statements in there, to make it easier for others to continue tweaking. Finally, there's a special case where if your whole message is a word that doesn't exist, like \"Slagle's\", then the function splits the word into individual letters. In my case, I don't want that, so I have a special return statement at the end to return the original message in those cases.\nimport numpy as np\nfrom wordfreq import get_frequency_dict\n\nword_prob = get_frequency_dict(lang='en', wordlist='large')\nmax_word_len = max(map(len, word_prob))  # 34\n\ndef viterbi_segment(text, debug=False):\n    probs, lasts = [1.0], [0]\n    for i in range(1, len(text) + 1):\n        new_probs = []\n        for j in range(max(0, i - max_word_len), i):\n            substring = text[j:i]\n            length_reward = np.exp(len(substring))\n            freq = word_prob.get(substring, 0) * length_reward\n            compounded_prob = probs[j] * freq\n            new_probs.append((compounded_prob, j))\n            \n            if debug:\n                print(f'[{j}:{i}] = \"{text[lasts[j]:j]} & {substring}\" = ({probs[j]:.8f} & {freq:.8f}) = {compounded_prob:.8f}')\n\n        prob_k, k = max(new_probs)  # max of a touple is the max across the first elements, which is the max of the compounded probabilities\n        probs.append(prob_k)\n        lasts.append(k)\n\n        if debug:\n            print(f'i = {i}, prob_k = {prob_k:.8f}, k = {k}, ({text[k:i]})\\n')\n\n\n    # when text is a word that doesn't exist, the algorithm breaks it into individual letters.\n    # in that case, return the original word instead\n    if len(set(lasts)) == len(text):\n        return text\n\n    words = []\n    k = len(text)\n    while 0 < k:\n        word = text[lasts[k]:k]\n        words.append(word)\n        k = lasts[k]\n    words.reverse()\n    return ' '.join(words)\n\ndef split_message(message):\n  new_message = ' '.join(viterbi_segment(wordmash, debug=False) for wordmash in message.split())\n  return new_message\n\nmessages = [\n    'tosplit',\n    'split',\n    'driveourtrucks',\n    \"Slagle's\",\n    \"Slagle's wickedweather liquidweather driveourtrucks gocompact slimprojector\",\n    'itseasyformetosplitlongruntogetherblocks',\n]\n\nfor message in messages:\n    print(f'{message}')\n    new_message = split_message(message)\n    print(f'{new_message}\\n')\n\ntosplit\nto split\n\nsplit\nsplit\n\ndriveourtrucks\ndrive our trucks\n\nSlagle's\nSlagle's\n\nSlagle's wickedweather liquidweather driveourtrucks gocompact slimprojector\nSlagle's wicked weather liquid weather drive our trucks go compact slim projector\n\nitseasyformetosplitlongruntogetherblocks\nits easy for me to split long run together blocks\n\n"}, "699": {"topic": "How can I split multiple joined words?", "user_name": "Naphtali DuniyaNaphtali Duniya", "text": "\nI may get downmodded for this, but have the secretary do it.\nYou'll spend more time on a dictionary solution than it would take to manually process.  Further, you won't possibly have 100% confidence in the solution, so you'll still have to give it manual attention anyway.\n"}, "700": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "David Brown", "text": "\nI assume a natural language processor would need to be used to parse the text itself, but what suggestions do you have for an algorithm to detect a user's mood based on text that they have written? I doubt it would be very accurate, but I'm still interested nonetheless.\nEDIT: I am by no means an expert on linguistics or natural language processing, so I apologize if this question is too general or stupid.\n"}, "701": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "David BrownDavid Brown", "text": "\nThis is the basis of an area of natural language processing called sentiment analysis. Although your question is general, it's certainly not stupid - this sort of research is done by Amazon on the text in product reviews for example.\nIf you are serious about this, then a simple version could be achieved by -\n\nAcquire a corpus of positive/negative sentiment. If this was a professional project you may take some time and manually annotate a corpus yourself, but if you were in a hurry or just wanted to experiment this at first then I'd suggest looking at the sentiment polarity corpus from Bo Pang and Lillian Lee's research. The issue with using that corpus is it is not tailored to your domain (specifically, the corpus uses movie reviews), but it should still be applicable.\nSplit your dataset into sentences either Positive or Negative. For the sentiment polarity corpus you could split each review into it's composite sentences and then apply the overall sentiment polarity tag (positive or negative) to all of those sentences. Split this corpus into two parts - 90% should be for training, 10% should be for test. If you're using Weka then it can handle the splitting of the corpus for you.\nApply a machine learning algorithm (such as SVM, Naive Bayes, Maximum Entropy) to the training corpus at a word level. This model is called a bag of words model, which is just representing the sentence as the words that it's composed of. This is the same model which many spam filters run on. For a nice introduction to machine learning algorithms there is an application called Weka that implements a range of these algorithms and gives you a GUI to play with them. You can then test the performance of the machine learned model from the errors made when attempting to classify your test corpus with this model.\nApply this machine learning algorithm to your user posts. For each user post, separate the post into sentences and then classify them using your machine learned model.\n\nSo yes, if you are serious about this then it is achievable - even without past experience in computational linguistics. It would be a fair amount of work, but even with word based models good results can be achieved. \nIf you need more help feel free to contact me - I'm always happy to help others interested in NLP =]\n\nSmall Notes - \n\nMerely splitting a segment of text into sentences is a field of NLP - called sentence boundary detection. There are a number of tools, OSS or free, available to do this, but for your task a simple split on whitespaces and punctuation should be fine.\nSVMlight is also another machine learner to consider, and in fact their inductive SVM does a similar task to what we're looking at - trying to classify which Reuter articles are about \"corporate acquisitions\" with 1000 positive and 1000 negative examples.\nTurning the sentences into features to classify over may take some work. In this model each word is a feature - this requires tokenizing the sentence, which means separating words and punctuation from each other. Another tip is to lowercase all the separate word tokens so that \"I HATE you\" and \"I hate YOU\" both end up being considered the same. With more data you could try and also include whether capitalization helps in classifying whether someone is angry, but I believe words should be sufficient at least for an initial effort.\n\n\nEdit\nI just discovered LingPipe that in fact has a tutorial on sentiment analysis using the Bo Pang and Lillian Lee Sentiment Polarity corpus I was talking about. If you use Java that may be an excellent tool to use, and even if not it goes through all of the steps I discussed above.\n"}, "702": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "", "text": "\nNo doubt it is possible to judge a user's mood based on the text they type but it would be no trivial thing. Things that I can think of:\n\nCapitals tends to signify agitation, annoyance or frustration and is certainly an emotional response but then again some newbies do that because they don't realize the significance so you couldn't assume that without looking at what else they've written (to make sure its not all in caps);\nCapitals are really just one form of emphasis. Others are use of certain aggressive colours (eg red) or use of bold or larger fonts;\nSome people make more spelling and grammar mistakes and typos when they're highly emotional;\nScanning for emoticons could give you a very clear picture of what the user is feeling but again something like :) could be interpreted as happy, \"I told you so\" or even have a sarcastic meaning;\nUse of expletives tends to have a clear meaning but again its not clearcut. Colloquial speech by many people will routinely contain certain four letter words. For some other people, they might not even say \"hell\", saying \"heck\" instead so any expletive (even \"sucks\") is significant;\nGroups of punctuation marks (like @#$@$@) tend to be replaced for expletives in a context when expletives aren't necessarily appropriate, so thats less likely to be colloquial;\nExclamation marks can indicate surprise, shock or exasperation.\n\nYou might want to look at Advances in written text analysis or even Determining Mood for a Blog by Combining Multiple Sources of Evidence.\nLastly it's worth noting that written text is usually perceived to be more negative than it actually is. This is a common problem with email communication in companies, just as one example.\n"}, "703": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "SmeritySmerity", "text": "\nI can't believe I'm taking this seriously...  assuming a one-dimensional mood space:\n\nIf the text contains a curse word,\n-10 mood.\nI think exclamations would tend to be negative, so -2 mood.\nWhen I get frustrated, I type in\nVery. Short. Sentences.  -5 mood.\n\nThe more I think about this, the more it's clear that a lot of these signifiers indicate extreme mood in general, but it's not always clear what kind of mood.\n"}, "704": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "cletuscletus", "text": "\nIf you support fonts, bold red text is probably an angry user. Green regular sized texts with butterfly clip art a happy one.\n"}, "705": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "Michael PetrottaMichael Petrotta", "text": "\nMy memory isn't good on this subject, but I believe I saw some research about the grammar structure of the text and the overall tone. That could be also as simple as shorter words and emotion expression words (well, expletives are pretty obvious).\nEdit: I noted that the first person to answer had substantially similar post. There could be indeed some serious idea about shorter sentences.\n"}, "706": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "AlexAlex", "text": "\nAnalysis of mood and behavior is very serious science.  Despite the other answers mocking the question law enforcement agencies have been investigating categorization of mood for years.  Uses in computers I have heard of generally had more context (timing information, voice pattern, speed in changing channels).  I think that you could--with some success--determine if a user is in a particular mood by training a Neural Network with samples from two known groups: angry and not angry.  Good luck with your efforts.\n"}, "707": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "", "text": "\nI think, my algorythm is rather straightforward, yet, why not calculating smilics through the text :) vs :(\nObviously, the text \":) :) :) :)\" resolves to a happy user, while \":( :( :(\" will surely resolve to a sad one. Enjoy!\n"}, "708": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "ilya n.ilya n.", "text": "\nI agree with ojblass that this is a serious question.\nMood categorization is currently a hot topic in the speech recognition area.  If you think about it, an interactive voice response (IVR) application needs to handle angry customers far differently than calm ones: angry people should be routed quickly to human operators with the right experience and training.  Vocal tone is a pretty reliable indicator of emotion, practical enough so that companies are eager to get this to work.  Google \"speech emotion recognition\", or read this article to find out more.\nThe situation should be no different in web-based GUIs.  Referring back to cletus's comments, the analogies between text and speech emotion detection are interesting.  If a person types CAPITALS they are said to be 'shouting', just as if his voice rose in volume and pitch using a voice interface.  Detecting typed profanities is analogous to \"keyword spotting\" of profanity in speech systems.  If a person is upset, they'll make more errors using either a GUI or a voice user interface (VUI) and can be routed to a human.\nThere's a \"multimodal\" emotion detection research area here.  Imagine a web interface that you can also speak to (along the lines of the IBM/Motorola/Opera XHTML + Voice Profile prototype implementation).  Emotion detection could be based on a combination of cues from the speech and visual input modality.\n"}, "709": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "ojblassojblass", "text": "\nYes.\nWhether or not you can do it is another story. The problem seems at first to be AI complete.\nNow then, if you had keystroke timings you should be able to figure it out.\n"}, "710": {"topic": "Is it possible to guess a user's mood based on the structure of text?", "user_name": "        SadSidoSadSido", "text": "\nFuzzy logic will do I guess.\nAny way it will be quite easy to start with several rules of determining the user's mood and then extend and combine the \"engine\" with more accurate and sophisticated ones.\n"}, "711": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\n\n\n\n\n\n\n\n\n\r\nAs it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.                    \n\n\nClosed 11 years ago.\n\n\n\nI mean, is there a coded language with human style coding?\nFor example:\nCreate an object called MyVar and initialize it to 10;\nTake MyVar and call MyMethod() with parameters. . .\n\nI know it's not so useful, but it can be interesting to create such a grammar.\n"}, "712": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nHow about LOLCODE?\nHAI\nCAN HAS STDIO?\nVISIBLE \"HAI WORLD!\"\nKTHXBYE\n\nSimplicity itself!\n"}, "713": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nCOBOL is a lot like that.\nSET MYVAR TO 10.\nEXECUTE MYMETHOD with 10, MYVAR.\n\nAnother sample from Wikipedia:\nADD YEARS TO AGE.\nMULTIPLY PRICE BY QUANTITY GIVING COST.\nSUBTRACT DISCOUNT FROM COST GIVING FINAL-COST.\n\nOddly enough though, despite its design to be readable as English, most programmers completely undermined this with bizarre naming conventions:\nSET VAR_00_MYVAR_PIC99 TO 10.\nEXECUTE PROC_10_MYMETHOD with 10, VAR_00_MYVAR_PIC99.\n\n"}, "714": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nInform 7\nInform 7 is perhaps the language I feel is most appropriately designed in a human language fashion. It is quite application specific for writing adventure games.\nIt is based on rule-based semantics, where you write a lot of rules describing the relationship between objects and their location. For instance, the section below is an Inform 7 program:\n\"Hello Deductible\" by \"I.F. Author\"\n\nThe story headline is \"An Interactive Example\".\n\nThe Living Room is a room. \"A comfortably furnished living room.\"\nThe Kitchen is north of the Living Room.\nThe Front Door is south of the Living Room.\nThe Front Door is a door. The Front Door is closed and locked.\n\nThe insurance salesman is a man in the Living Room. The description is \"An insurance salesman in a tacky polyester suit. He seems eager to speak to you.\" Understand \"man\" as the insurance salesman.\n\nA briefcase is carried by the insurance salesman. The description is \"A slightly worn, black briefcase.\"  Understand \"case\" as the briefcase.\n\nThe insurance paperwork is in the briefcase. The description is \"Page after page of small legalese.\" Understand \"papers\" or \"documents\" or \"forms\" as the paperwork.\n\nInstead of listening to the insurance salesman: \n    say \"The salesman bores you with a discussion of life  insurance policies.  From his briefcase he pulls some paperwork which he hands to you.\";\n    move the insurance paperwork to the player.\n\nExample cited from Wikipedia\n"}, "715": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nAppleScript is pretty close to that, though that is obviously platform dependent.\nHere's a script for opening iTunes and playing a playlist\ntell application \"iTunes\"\n    activate\n    play playlist \"Party Shuffle\"\nend tell\n\nSource: AppleScript Examples\n"}, "716": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\n\nProjects promoting programming in\n  \"natural language\" are intrinsically\n  doomed to fail.\n\n-- Edsger W.Dijkstra, How do we tell truths that might hurt?\n"}, "717": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nThis was \"the next big thing\" around about the early 1980s and I spent much of my first couple of years as a a coder working in \"NATURAL\", which was the supposedly the best of the new crop of 4GLs (fourth generation languages) which were designed to make data access (in this case to an ADABAS database) human readable.\nOf course it did absolutely nothing of the type.  All we ended up with was verbose badly structured code.  Both of these products are still around, but you've never heard of them, which sort of proves the what a dead end it was.\nActually at that period there appeared to be a general desire to move beyond 'programming' into some sort of 2001 inspired AI heaven.  Oracle were really keen on code generation and I remember with some interest a product called 'the last one' that was being marketed to managers as a product that would automatically generate any program you wanted and make all your programming staff redundant.  Seems not to have lived up to expectations ;-)\nIt's worth remembering to that SQL was originally marketed in some quarters as a way to allow management to directly query their data.  I was even sent on a course to learn basic SQL (in a large national transport organization that ran on rails - the steel variety) where junior management types were included because they had plans to put basic query tools in their hands.  What a disaster that was.\nMaybe it might be different in 50 years, but at the current stage of play coding demands a certain clarity of thought and implementation which is best mediated through a dedicated syntax designed for those ends, not any approximation to a natural language which is unclear and ambiguous.  The nearest approximation is possibly physics where the essence of the subject is in the mathematics used (think a programming language for physics) not verbose wordage. \nADDED\nI was forgetting, apart from COBOL there was also PL/1, sometime credited with allowing NASA to put a man on the moon it was just as verbose as COBOL and tried even harder to be 'Manager-readable'.  Which is why no-one has really heard of it now either :-)\n"}, "718": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nChef! Anyone can read recipes right? Behold hello world!\nIngredients.\n72 g haricot beans\n101 eggs\n108 g lard\n111 cups oil\n32 zucchinis\n119 ml water\n114 g red salmon\n100 g dijon mustard\n33 potatoes\n\nMethod.\nPut potatoes into the mixing bowl. Put dijon mustard into the mixing bowl. \nPut lard into the mixing bowl. Put red salmon into the mixing bowl. Put oil into the mixing bowl. \nPut water into the mixing bowl. Put zucchinis into the mixing bowl. Put oil into the mixing bowl. \nPut lard into the mixing bowl. Put lard into the mixing bowl. Put eggs into the mixing bowl. \nPut haricot beans into the mixing bowl. Liquefy contents of the mixing bowl. \nPour contents of the mixing bowl into the baking dish.\n\nSorry if it's not a serious answer, but this is way awesome. :-)\n"}, "719": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nAll languages are 'human readable'. :) How else would someone be able to create it? That being said, languages that support DSLs can be incredibly intuitive such as Boo.\n"}, "720": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nHaving a programming language read like a (verbose) normal language, would be like requiring people to converse all the time in legalese.  All the extra verbiage just gets in the way.\nAn ideal programming language should have syntax that is as transparent as possible and let the concepts behind the program stand out.  Obviously there is a trade off between having a quick learning curve and having minimal but obscure syntax (think Perl, or even K).\n"}, "721": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nBy creating a set of rules, it is possible to do logic programming in Prolog like this.  You can build a grammar (or download one) for a particular domain, create a knowledge base and then query it.  After defining your grammar you could do something like:\nbob is a parent of tim.\nmary is a parent of bob.\n\n?- X is a grandparent of tim.\nX = mary\n\n?- jim is a parent of bob.\nfalse\n\n"}, "722": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nI see the Shakespeare programming language have yet to be mentioned.\nThese programs are coded to look like shakespear plays, the individial characters in the play being variables that can hold numbers and the various phrases in the play manipulate the characters and the number they hold. For instance, \"Speak your mind\" orders a character to output his value.\n"}, "723": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nApplescript:\ntell application \"Finder\"\n set the percent_free to \u00ac\n (((the free space of the startup disk) / (the capacity of the startup disk)) * 100) div 1\nend tell\nif the percent_free is less than 10 then\n tell application (path to frontmost application as text)\n display dialog \"The startup disk has only \" & the percent_free & \u00ac\n \" percent of its capacity available.\" & return & return & \u00ac\n \"Should this script continue?\" with icon 1\n end tell\nend if\n\n"}, "724": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nI can read C. That means it's human-readable(because I'm a human). It's just too terse for the average person. The general concept of programming languages is to maximize the information about how the computer should operate in a given line.\nThis is why Ruby is so popular; it maximizes the functionality in minimal text. English(or any other other natural language) is a pretty imprecise, low-information/character language.\nIn sum, it is: (i)done before and (ii)a known weaker idea. \n"}, "725": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nThis is actually a hot topic.\nFor starters - What is Human readable?\nA Chinese-reader cannot read Russian and vice versa. \nIt you narrow your domain for example to Chinese pharmacists writing a perscription you could design a language around that. And that would be human readable.\nSuch as language would fall under a the umbrella of Domain Specific Languages.\n"}, "726": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nSQL\nSELECT name, address FROM customers WHERE region = 'Europe'\n\n"}, "727": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nYes.  It's called COBOL, and people generally detest it.\n"}, "728": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nHyperTalk and its descendant AppleScript were designed to be similar to the English language.\n"}, "729": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nInform 7 is the most successful such system I've seen. It has two advantages over the cruder systems listed in other answers here: it's for a domain particularly appropriate for natural language (interactive fiction), and it does a fancier analysis of the input code based on more computational-linguistics lore, not just a conventional programming-language grammar that happens to use English words instead of braces, etc.\n"}, "730": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nPerl, some people claim.\nprint \"hello!\" and open my $File, '<', $path or die \"Couldn't open the file after saying hello!\";\n\n"}, "731": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nDo a google search for \"natural language programming\" and you'll find lots of information (including why this is a bad idea).\n"}, "732": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nClarity of Expression is important.\nBut Clarity of Thought is far, far more important.\n"}, "733": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nVB is as close as I can think of one:\nIf MyLife.Sucks Then MyLife.End Else MyLife.Continue\n"}, "734": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nSure, Erlang.\n-module(listsort).\n-export([by_length/1]).\n\n by_length(Lists) ->\n    F = fun(A,B) when is_list(A), is_list(B) ->\n            length(A) < length(B)\n        end,\n    qsort(Lists, F).\n\n qsort([], _)-> [];\n qsort([Pivot|Rest], Smaller) ->\n     qsort([ X || X <- Rest, Smaller(X,Pivot)], Smaller)\n     ++ [Pivot] ++\n     qsort([ Y ||Y <- Rest, not(Smaller(Y, Pivot))], Smaller).\n\nI'm a human, it's a programming language, and I can read it. I don't know what any of it means, but I see a lot of English words in there, I think.\n(Tongue firmly in cheek.)\n"}, "735": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nDSLs can be very natural-looking. See this example created with MGrammar:\ntest \"Searching google for watin\"\n    goto \"http://www.google.se\"\n    type \"watin\" into \"q\"\n    click \"btnG\"\n    assert that text \"WatiN Home\" exists\n    assert that element \"res\" exists\nend\n\n"}, "736": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nCOBOL was intended to be read by managers, and has \"noise words\" to make it more readable.\nThe funny thing is, it reads a bit like a verbose DSL.\n"}, "737": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nBeing more human-readable than most was one of the early selling points of Ada. I find it a silly argument these days, as any sufficently complex task in any language is going to require a competent practicioner to understand. However, it does beat the bejeezus out of C-syntax languages. Its dominant coding styles can enhance this effect too. For example, comparing loops in an if statement:\nAda:\nif Time_To_Loop then\n   for i in Some_Array loop\n      Some_Array(i) := i;\n   end loop;\nend if;\n\nC:\nif (timeToLoop != 0) {\n   for (int i=0;i<SOME_ARRAY_LENGTH;i++) {\n      someArray[i] = i;\n   }\n}\n\nThe C code would look even worse if I used Hungarian notation like Microsoft, but I'm trying to be nice. :-)\n"}, "738": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nInteresting question. Your question can be read as \"Is there any programming language that is easily readable by humans?\", OR ELSE as \"Is there a human language that can be used for programming?\". All the answers here have focused on the former, so let me try answering the latter.\nHave you heard of Sanskrit? It is an ancient Indian language on which modern Indian languages like Hindi are based.\nwiki/Sanskrit\nI've been hearing for years that it is precise and complete enough to be used, as it is, as a high-level language on a computer. Ofcourse, you need a compiler to convert Sanskrit instructions to machine language.  I know the script & yes, it is precise (entirely phonetic so you never have to ask \"how do you spell that\"), but I don't know the grammer well enough.\nThis is completeley anecdotal, so I don't vouch for the accuracy of this. Just wanted to share what I know regarding this. :-)\n"}, "739": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nI agree with the general consensus here. \"Human readable\" general purpose programming languages are mostly a bad idea, but human readable Domain Specific Languages are very worthwhile.\nREBOL has a great system for creating DSLs.\n"}, "740": {"topic": "Is there a human readable programming language? [closed]", "user_name": "\r", "text": "\nGradStudent\nIt only has one statement: \"you - write me a program to do x\"\nIt's valid for all values of X and has the advantage that x doesn't have to be defined and can be changed after the program is written.\nA commercial dialect is available called intern: development cost is lower but it isn't guaranteed to work\n"}, "741": {"topic": "Is there a human readable programming language? [closed]", "user_name": "", "text": "\nCobol was kind of like that.\n"}, "742": {"topic": "how to check if a string looks randomized, or human generated and pronouncable?", "user_name": "IAdapter", "text": "\nFor the purpose of identifying [possible] bot-generated usernames.\nSuppose you have a username like \"bilbomoothof\" .. it may be nonsense, but it still contains pronouncable sounds and so appears human-generated.\nI accept that it could have been randomly generated from a dictionary of syllables, or word parts, but let's assume for a moment that the bot in question is a bit rubbish.\n\nSuppose you have a username like\n\"sdfgbhm342r3f\", to a human this is\nclearly a random string. But can\nthis be identified programatically?\nAre there any algorithms available\n(similar to Soundex, etc..) that can\nidentify pronounceable sounds within\na string like this?\n\nSolutions applicable in PHP/MySQL most appreciated.\n"}, "743": {"topic": "how to check if a string looks randomized, or human generated and pronouncable?", "user_name": "Tim WhitlockTim Whitlock", "text": "\nI guess you could think of something like that if you could restrict yourself to pronounceable sounds in english. For me (I am French), words like szczepan or wawrzyniec are unpronounceable and certainly have a certain randomness.\nBut they are actually Polish first names (meaning steven and lawrence)...\n"}, "744": {"topic": "how to check if a string looks randomized, or human generated and pronouncable?", "user_name": "MacMac", "text": "\nI agree with Mac. But more than that, people sometimes have user name that aren't pronouncable, like qwerty or rtfmorleave. \nWhy bother with that ?\n< obsolete and false, but i don't delete because of comments >\nBut more than that, no bots use 'zetztzgsd' as user name, they have dictionnary of realname, possible nick name, etc. so I think this would be a waster of time for you\n< / obsolete and false, but i don't delete because of comments>\n"}, "745": {"topic": "how to check if a string looks randomized, or human generated and pronouncable?", "user_name": "", "text": "\nLook up n-gram analysis. It is successfully used to automatically detect text language and works surprisingly well even on very short texts.\nThe online demo (no longer online) recognized 'bilbomoothof' as English and 'sdfgbhm342r3f' as Nepali. It probably always returns the best match, even if it's a very poor one. I think you could train it to discern between 'pronounceable' and 'random'.\n"}, "746": {"topic": "how to check if a string looks randomized, or human generated and pronouncable?", "user_name": "Clement HerremanClement Herreman", "text": "\nJust use CAPTCHA as a part of the registration process.\nYou can never distinguish real uesrnames from bot-created usernames, without severely annoying your users.\nYou will block users with bizzare, or non-English names, which will irritate them, and the bots will just keep trying until they catch a good username (from dictionary, or other sources - this is a very nice one, by the way!).\nEDIT : Looking for prevention rather than after-the-fact analysis?\nThe solution is letting somebody else manage user's identities for you. For instance, you can use a small list of OpenID providers (like SO), or facebook connect, or both.\nYou'll know for sure that the users are real, and that they have been solving at least one CAPTCHA.\nEDIT: Another Idea\nSearch the string in Google, and check the number of matches found. Shouldn't be your only tool, but it is a good indicator, too. Randomized strings, of course, should have little or no matches.\n"}, "747": {"topic": "how to check if a string looks randomized, or human generated and pronouncable?", "user_name": "", "text": "\nOff the top of my head, you could look for syllables, making use of soundex. That's the direction I would explore, based on the assumption that a pronounceable word has at least one syllable.\nEDIT: Here's a function for counting syllables:\nfunction count_syllables($word) {\n \n$subsyl = Array(\n'cial'\n,'tia'\n ,'cius'\n ,'cious'\n ,'giu'\n ,'ion'\n ,'iou'\n ,'sia$'\n ,'.ely$'\n );\n  \n $addsyl = Array(\n 'ia'\n ,'riet'\n ,'dien'\n ,'iu'\n ,'io'\n ,'ii'\n ,'[aeiouym]bl$'\n ,'[aeiou]{3}'\n ,'^mc'\n ,'ism$'\n ,'([^aeiouy])\\1l$'\n ,'[^l]lien'\n ,'^coa[dglx].'\n ,'[^gq]ua[^auieo]'\n ,'dnt$'\n );\n  \n // Based on Greg Fast's Perl module Lingua::EN::Syllables\n $word = preg_replace('/[^a-z]/is', '', strtolower($word));\n $word_parts = preg_split('/[^aeiouy]+/', $word);\n foreach ($word_parts as $key => $value) {\n if ($value <> '') {\n $valid_word_parts[] = $value;\n }\n }\n  \n $syllables = 0;\n // Thanks to Joe Kovar for correcting a bug in the following lines\n foreach ($subsyl as $syl) {\n $syllables -= preg_match('~'.$syl.'~', $word);\n }\n foreach ($addsyl as $syl) {\n $syllables += preg_match('~'.$syl.'~', $word);\n }\n if (strlen($word) == 1) {\n $syllables++;\n }\n $syllables += count($valid_word_parts);\n $syllables = ($syllables == 0) ? 1 : $syllables;\n return $syllables;\n }\n\nFrom this very interesting link:\nhttp://www.addedbytes.com/php/flesch-kincaid-function/\n"}, "748": {"topic": "how to check if a string looks randomized, or human generated and pronouncable?", "user_name": "Rafa\u0142 DowgirdRafa\u0142 Dowgird", "text": "\nReply for question #1:\nUnfortunately this cannot be done, since Kolmogorov complexity function is not computable, therefore you cannot generate such algorithm unless you will apply some rules to domain of possible user names, then you will be able to perform heuristic analysis and decide, but even then it's really hard to do.\nPS: After posted this answer, I bumped into some service which gave an idea of example for user name domain restriction, let to the users use the mail box of well known public domain as they user names.\n"}, "749": {"topic": "how to check if a string looks randomized, or human generated and pronouncable?", "user_name": "CommunityBot", "text": "\nYou could use a neural network to evaluate whether the nickname looks like a natural-language nickname.\nAssemble two data-sets: one of valid nicknames, and one of bogus-generated ones. Train a simple back-progating single hidden layer neural network with the character values as inputs. The neural network will learn to discriminate between strings like \"zrgssgbt\" and \"zargbyt\", since the latter has consonants and vowels intermingled . \nIt is important to use real-world examples to get a good discriminator.\n"}, "750": {"topic": "how to check if a string looks randomized, or human generated and pronouncable?", "user_name": "Adam MatanAdam Matan", "text": "\nI dont know of existing algorithms for this problem, but I think it can be attacked in any one of the following ways:\n\nyour bot may be rubbish, but you can keep a list of syllables, or more specifically, phonemes, that you can try finding in your given string. But this sounds a bit difficult becasuse you would need to segment the string in different places etc.\nthere are 5 vowels in the english alphabet, and 21 others. You could assume that if they were randomly generated, then approximately you would expect 5/26*W, (where W is word length) letters that are vowels, and significant deviations from this could be suspicious. (If letter are included then 5/31 and so on..) You can try building on this idea by searching for doubletons, and trying to make sure that each doubleton occurs with same probability etc. \nfurther, you can try to segment your input string around vowels, example three lettters before a vowel and three letters after a vowel, and try to find out if it make a recognizable sound by comparing with phonemes.\n\n"}, "751": {"topic": "how to check if a string looks randomized, or human generated and pronouncable?", "user_name": "CommunityBot", "text": "\nIn Russian, we have forbidden syllables, like \u0413\u0419, \u0430 \u042a or \u042c after a vowel and so on.\nHowever, spam bots just use the names database, that's why my spam inbox is full of strange names you can only meet in history books.\nI expect English to have syllable distribution histograms too (like ETAOIN SHRDLU, but for two-letter or even three-letter syllables), and having critical density of low frequency syllables in one name is certainly a sign.\n"}, "752": {"topic": "how to check if a string looks randomized, or human generated and pronouncable?", "user_name": "karim79karim79", "text": "\nNote that many large sites suggest usernames like [first init][middle init][last name][number].  The users then carry these usernames over to other sites, and the first three letters are definitely not pronounceable.\n"}, "753": {"topic": "how to check if a string looks randomized, or human generated and pronouncable?", "user_name": "", "text": "\nI've seen bot registrations where both the username and full name are strings of random Upper- and lowercase letters. They tend to be at least 10 letters long, so in this case, it's not possible to be 100% accurate, but you can get pretty close by first passing any that have a non [a-zA-Z] character (e.g., space, number, or special character).\nThen, for the few that haven't passed the test above, if there are both upper-and lowercase letters, failing those with too many uppercase letters in the full name, which normally wouldn't have more than three or four. You'll make an error with names like JoHnDoE for both the username and full name, or JohnSmithIII, but those are a pretty rare cases.\nYou can refine the algorithm by running it against a group of known valid registrations.\n"}, "754": {"topic": "gensim word2vec: Find number of words in vocabulary", "user_name": "kmario23", "text": "\nAfter training a word2vec model using python gensim, how do you find the number of words in the model's vocabulary?\n"}, "755": {"topic": "gensim word2vec: Find number of words in vocabulary", "user_name": "hlin117hlin117", "text": "\nIn recent versions, the model.wv property holds the words-and-vectors, and can itself can report a length \u2013 the number of words it contains. So if w2v_model is your Word2Vec (or Doc2Vec or FastText) model, it's enough to just do:\nvocab_len = len(w2v_model.wv)\n\nIf your model is just a raw set of word-vectors, like a KeyedVectors instance rather than a full Word2Vec/etc model, it's just:\nvocab_len = len(kv_model)\n\nOther useful internals in Gensim 4.0+ include model.wv.index_to_key, a plain list of the key (word) in each index position, and model.wv.key_to_index, a plain dict mapping keys (words) to their index positions.\nIn pre-4.0 versions, the vocabulary was in the vocab field of the Word2Vec model's wv property, as a dictionary, with the keys being each token (word). So there it was just the usual Python for getting a dictionary's length:\nlen(w2v_model.wv.vocab)\n\nIn very-old gensim versions before 0.13 vocab appeared directly on the model. So way back then you would use w2v_model.vocab instead of w2v_model.wv.vocab.\nBut if you're still using anything from before Gensim 4.0, you should definitely upgrade! There are big memory & performance improvements, and the changes required in calling code are relatively small \u2013 some renamings & moves, covered in the 4.0 Migration Notes.\n"}, "756": {"topic": "gensim word2vec: Find number of words in vocabulary", "user_name": "", "text": "\nOne more way to get the vocabulary size is from the embedding matrix itself as in:\nIn [33]: from gensim.models import Word2Vec\n\n# load the pretrained model\nIn [34]: model = Word2Vec.load(pretrained_model)\n\n# get the shape of embedding matrix    \nIn [35]: model.wv.vectors.shape\nOut[35]: (662109, 300)\n\n# `vocabulary_size` is just the number of rows (i.e. axis 0)\nIn [36]: model.wv.vectors.shape[0]\nOut[36]: 662109\n\n"}, "757": {"topic": "gensim word2vec: Find number of words in vocabulary", "user_name": "gojomogojomo", "text": "\nGojomo's answer raises an AttributeError for Gensim 4.0.0+.\nFor these versions, you can get the length of the vocabulary as follows:\nlen(w2v_model.wv.index_to_key)\n(which is slightly faster than: len(w2v_model.wv.key_to_index))\n"}, "758": {"topic": "gensim word2vec: Find number of words in vocabulary", "user_name": "kmario23kmario23", "text": "\nLatest:\nUse model.wv.key_to_index, after creating gensim model\nvocab dict became key_to_index for looking up a key's integer index, or get_vecattr() and set_vecattr() for other per-key attributes:https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#4-vocab-dict-became-key_to_index-for-looking-up-a-keys-integer-index-or-get_vecattr-and-set_vecattr-for-other-per-key-attributes\n"}, "759": {"topic": "Improving the extraction of human names with nltk [closed]", "user_name": "Palak Bansal", "text": "\n\n\n\n\n\n\nClosed. This question is opinion-based. It is not currently accepting answers.                    \n\n\n\n\n\n\n\n\n\n\nWant to improve this question? Update the question so it can be answered with facts and citations by editing this post.\n\n\nClosed 3 years ago.\n\n\n\n\n\n\n\r\n                        Improve this question\r\n                    \n\n\n\nI am trying to extract human names from text. \nDoes anyone have a method that they would recommend?\nThis is what I tried (code is below):\nI am using nltk to find everything marked as a person and then generating a list of all the NNP parts of that person. I am skipping persons where there is only one NNP which avoids grabbing a lone surname.\nI am getting decent results but was wondering if there are better ways to go about solving this problem.\nCode:\nimport nltk\nfrom nameparser.parser import HumanName\n\ndef get_human_names(text):\n    tokens = nltk.tokenize.word_tokenize(text)\n    pos = nltk.pos_tag(tokens)\n    sentt = nltk.ne_chunk(pos, binary = False)\n    person_list = []\n    person = []\n    name = \"\"\n    for subtree in sentt.subtrees(filter=lambda t: t.node == 'PERSON'):\n        for leaf in subtree.leaves():\n            person.append(leaf[0])\n        if len(person) > 1: #avoid grabbing lone surnames\n            for part in person:\n                name += part + ' '\n            if name[:-1] not in person_list:\n                person_list.append(name[:-1])\n            name = ''\n        person = []\n\n    return (person_list)\n\ntext = \"\"\"\nSome economists have responded positively to Bitcoin, including \nFrancois R. Velde, senior economist of the Federal Reserve in Chicago \nwho described it as \"an elegant solution to the problem of creating a \ndigital currency.\" In November 2013 Richard Branson announced that \nVirgin Galactic would accept Bitcoin as payment, saying that he had invested \nin Bitcoin and found it \"fascinating how a whole new global currency \nhas been created\", encouraging others to also invest in Bitcoin.\nOther economists commenting on Bitcoin have been critical. \nEconomist Paul Krugman has suggested that the structure of the currency \nincentivizes hoarding and that its value derives from the expectation that \nothers will accept it as payment. Economist Larry Summers has expressed \na \"wait and see\" attitude when it comes to Bitcoin. Nick Colas, a market \nstrategist for ConvergEx Group, has remarked on the effect of increasing \nuse of Bitcoin and its restricted supply, noting, \"When incremental \nadoption meets relatively fixed supply, it should be no surprise that \nprices go up. And that\u2019s exactly what is happening to BTC prices.\"\n\"\"\"\n\nnames = get_human_names(text)\nprint \"LAST, FIRST\"\nfor name in names: \n    last_first = HumanName(name).last + ', ' + HumanName(name).first\n        print last_first\n\nOutput:\nLAST, FIRST\nVelde, Francois\nBranson, Richard\nGalactic, Virgin\nKrugman, Paul\nSummers, Larry\nColas, Nick\n\nApart from Virgin Galactic, this is all valid output. Of course, knowing that Virgin Galactic isn't a human name in the context of this article is the hard (maybe impossible) part.\n"}, "760": {"topic": "Improving the extraction of human names with nltk [closed]", "user_name": "e he h", "text": "\nMust agree with the suggestion that \"make my code better\" isn't well suited for this site, but I can give you some way where you can try to dig in.\nDisclaimer: This answer is ~7 years old. Definitely, it needs to be updated to newer Python and NLTK versions. Please, try to do it yourself, and if it works, share your know-how with us.\nTake a look at Stanford Named Entity Recognizer (NER). Its binding has been included in NLTK v 2.0, but you must download some core files. Here is script which can do all of that for you.\nI wrote this script:\nimport nltk\nfrom nltk.tag.stanford import NERTagger\nst = NERTagger('stanford-ner/all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')\ntext = \"\"\"YOUR TEXT GOES HERE\"\"\"\n\nfor sent in nltk.sent_tokenize(text):\n    tokens = nltk.tokenize.word_tokenize(sent)\n    tags = st.tag(tokens)\n    for tag in tags:\n        if tag[1]=='PERSON': print tag\n\nand got not so bad output:\n\n('Francois', 'PERSON')\n('R.', 'PERSON')\n('Velde', 'PERSON')\n('Richard', 'PERSON')\n('Branson', 'PERSON')\n('Virgin', 'PERSON')\n('Galactic', 'PERSON')\n('Bitcoin', 'PERSON')\n('Bitcoin', 'PERSON')\n('Paul', 'PERSON')\n('Krugman', 'PERSON')\n('Larry', 'PERSON')\n('Summers', 'PERSON')\n('Bitcoin', 'PERSON')\n('Nick', 'PERSON')\n('Colas', 'PERSON')\n\nHope this is helpful.\n"}, "761": {"topic": "Improving the extraction of human names with nltk [closed]", "user_name": "", "text": "\nFor anyone else looking, I found this article to be useful: http://timmcnamara.co.nz/post/2650550090/extracting-names-with-6-lines-of-python-code\n>>> import nltk\n>>> def extract_entities(text):\n...     for sent in nltk.sent_tokenize(text):\n...         for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n...             if hasattr(chunk, 'node'):\n...                 print chunk.node, ' '.join(c[0] for c in chunk.leaves())\n...\n\n"}, "762": {"topic": "Improving the extraction of human names with nltk [closed]", "user_name": "trotro", "text": "\nI actually wanted to extract only the person name, so, thought to check all the names that come as an output against wordnet( A large lexical database of English).\nMore Information on Wordnet can be found here: http://www.nltk.org/howto/wordnet.html\nimport nltk\nfrom nameparser.parser import HumanName\nfrom nltk.corpus import wordnet\n\n\nperson_list = []\nperson_names=person_list\ndef get_human_names(text):\n    tokens = nltk.tokenize.word_tokenize(text)\n    pos = nltk.pos_tag(tokens)\n    sentt = nltk.ne_chunk(pos, binary = False)\n\n    person = []\n    name = \"\"\n    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n        for leaf in subtree.leaves():\n            person.append(leaf[0])\n        if len(person) > 1: #avoid grabbing lone surnames\n            for part in person:\n                name += part + ' '\n            if name[:-1] not in person_list:\n                person_list.append(name[:-1])\n            name = ''\n        person = []\n#     print (person_list)\n\ntext = \"\"\"\n\nSome economists have responded positively to Bitcoin, including \nFrancois R. Velde, senior economist of the Federal Reserve in Chicago \nwho described it as \"an elegant solution to the problem of creating a \ndigital currency.\" In November 2013 Richard Branson announced that \nVirgin Galactic would accept Bitcoin as payment, saying that he had invested \nin Bitcoin and found it \"fascinating how a whole new global currency \nhas been created\", encouraging others to also invest in Bitcoin.\nOther economists commenting on Bitcoin have been critical. \nEconomist Paul Krugman has suggested that the structure of the currency \nincentivizes hoarding and that its value derives from the expectation that \nothers will accept it as payment. Economist Larry Summers has expressed \na \"wait and see\" attitude when it comes to Bitcoin. Nick Colas, a market \nstrategist for ConvergEx Group, has remarked on the effect of increasing \nuse of Bitcoin and its restricted supply, noting, \"When incremental \nadoption meets relatively fixed supply, it should be no surprise that \nprices go up. And that\u2019s exactly what is happening to BTC prices.\"\n\"\"\"\n\nnames = get_human_names(text)\nfor person in person_list:\n    person_split = person.split(\" \")\n    for name in person_split:\n        if wordnet.synsets(name):\n            if(name in person):\n                person_names.remove(person)\n                break\n\nprint(person_names)\n\nOUTPUT\n['Francois R. Velde', 'Richard Branson', 'Economist Paul Krugman', 'Nick Colas']\n\nApart from Larry Summers all the names are correct and that is because of the last name \"Summers\". \n"}, "763": {"topic": "Improving the extraction of human names with nltk [closed]", "user_name": "Curtis MattoonCurtis Mattoon", "text": "\nThe answer of @trojane didn't quite work for me, but helped a lot for this one.\nPrerequesites\nCreate a folder stanford-ner and download the following two files to it:\n\nenglish.all.3class.distsim.crf.ser.gz\nstanford-ner.jar (Look for download and extract the archive)\n\nScript\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport nltk\nfrom nltk.tag.stanford import StanfordNERTagger\n\ntext = u\"\"\"\nSome economists have responded positively to Bitcoin, including\nFrancois R. Velde, senior economist of the Federal Reserve in Chicago\nwho described it as \"an elegant solution to the problem of creating a\ndigital currency.\" In November 2013 Richard Branson announced that\nVirgin Galactic would accept Bitcoin as payment, saying that he had invested\nin Bitcoin and found it \"fascinating how a whole new global currency\nhas been created\", encouraging others to also invest in Bitcoin.\nOther economists commenting on Bitcoin have been critical.\nEconomist Paul Krugman has suggested that the structure of the currency\nincentivizes hoarding and that its value derives from the expectation that\nothers will accept it as payment. Economist Larry Summers has expressed\na \"wait and see\" attitude when it comes to Bitcoin. Nick Colas, a market\nstrategist for ConvergEx Group, has remarked on the effect of increasing\nuse of Bitcoin and its restricted supply, noting, \"When incremental\nadoption meets relatively fixed supply, it should be no surprise that\nprices go up. And that\u2019s exactly what is happening to BTC prices.\n\"\"\"\n\nst = StanfordNERTagger('stanford-ner/english.all.3class.distsim.crf.ser.gz',\n                       'stanford-ner/stanford-ner.jar')\n\nfor sent in nltk.sent_tokenize(text):\n    tokens = nltk.tokenize.word_tokenize(sent)\n    tags = st.tag(tokens)\n    for tag in tags:\n        if tag[1] in [\"PERSON\", \"LOCATION\", \"ORGANIZATION\"]:\n            print(tag)\n\nResults\n('Bitcoin', 'LOCATION')       # wrong\n('Francois', 'PERSON')\n('R.', 'PERSON')\n('Velde', 'PERSON')\n('Federal', 'ORGANIZATION')\n('Reserve', 'ORGANIZATION')\n('Chicago', 'LOCATION')\n('Richard', 'PERSON')\n('Branson', 'PERSON')\n('Virgin', 'PERSON')         # Wrong\n('Galactic', 'PERSON')       # Wrong\n('Bitcoin', 'PERSON')        # Wrong\n('Bitcoin', 'LOCATION')      # Wrong\n('Bitcoin', 'LOCATION')      # Wrong\n('Paul', 'PERSON')\n('Krugman', 'PERSON')\n('Larry', 'PERSON')\n('Summers', 'PERSON')\n('Bitcoin', 'PERSON')        # Wrong\n('Nick', 'PERSON')\n('Colas', 'PERSON')\n('ConvergEx', 'ORGANIZATION')\n('Group', 'ORGANIZATION')     \n('Bitcoin', 'LOCATION')       # Wrong\n('BTC', 'ORGANIZATION')       # Wrong\n\n"}, "764": {"topic": "Improving the extraction of human names with nltk [closed]", "user_name": "", "text": "\nYou can try to do resolution of the found names, and check if you can find them in a database such as freebase.com. Get the data locally and query it (it's in RDF), or use google's api: https://developers.google.com/freebase/v1/getting-started. Most big companies, geographical locations, etc. (that would be caught by your snippet) could be then discarded based on the freebase data. \n"}, "765": {"topic": "Improving the extraction of human names with nltk [closed]", "user_name": "Shivansh bhandariShivansh bhandari", "text": "\nI would like to post a brutal and greedy solution here to solve the problem cast by @Enthusiast: get the full name of a person if possible.\nThe capitalization of the first character in each name is used as a criterion for recognizing PERSON in Spacy. For example, 'jim hoffman' itself won't be recognized as a named entity, while 'Jim Hoffman' will be. \nTherefore, if our task is simply picking out persons from a script, we may simply first capitalize the first letter of each word, and then dump it to spacy.\nimport spacy\n\ndef capitalizeWords(text):\n\n  newText = ''\n\n  for sentence in text.split('.'):\n    newSentence = ''\n    for word in sentence.split():\n      newSentence += word+' '\n    newText += newSentence+'\\n'\n\n  return newText\n\nnlp = spacy.load('en_core_web_md')\n\ndoc = nlp(capitalizeWords(rawText))\n\n#......\n\n\nNote that this approach covers full names at the cost of the increasing of false positives.\n"}, "766": {"topic": "Improving the extraction of human names with nltk [closed]", "user_name": "", "text": "\nThis worked pretty well for me. I just had to change one line in order for it to run. \n    for subtree in sentt.subtrees(filter=lambda t: t.node == 'PERSON'):\n\nneeds to be\n    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n\nThere were imperfections in the output (for example it identified \"Money Laundering\" as a person), but with my data a name database may not be dependable. \n"}, "767": {"topic": "Looking for Java spell checker library [closed]", "user_name": "Wolfgang Fahl", "text": "\n\n\n\n\n\n\n\n\n\r\nAs it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.                    \n\n\nClosed 10 years ago.\n\n\n\nI am looking for an open source Java spell checking library which has dictionaries for at least the following languages: French, German, Spanish, and Czech. Any suggestion?\n"}, "768": {"topic": "Looking for Java spell checker library [closed]", "user_name": "avernetavernet", "text": "\nAnother good library is JLanguageTool http://www.languagetool.org/usage/\nIt has a pretty simple api and does both spelling and grammar checking/suggestions.\nJLanguageTool langTool = new JLanguageTool(Language.AMERICAN_ENGLISH);\nlangTool.activateDefaultPatternRules();\n\nList<RuleMatch> matches = langTool.check(\"Hitchhiker's Guide tot he Galaxy\");\nfor (RuleMatch match : matches) {\n    System.out.println(\"Potential error at line \" +\n        match.getEndLine() + \", column \" +\n        match.getColumn() + \": \" + match.getMessage());\n    System.out.println(\"Suggested correction: \" +\n        match.getSuggestedReplacements());\n}\n\nYou can also use it to host your own spelling and grammar web service. \n"}, "769": {"topic": "Looking for Java spell checker library [closed]", "user_name": "budi", "text": "\nYou should check out Jazzy its used in some high profile Java applications. Two problems with it:\n\nIt has not been updated since 2005.\nThere is only English dictionary on their SourceForge page.\n\nThere are some third party dictionaries floating around. I had one for French, last time I used jazzy.\n"}, "770": {"topic": "Looking for Java spell checker library [closed]", "user_name": "pfranzapfranza", "text": "\nCheck out JSpell by Page Scholar, http://www.jspell.com.\n"}, "771": {"topic": "Looking for Java spell checker library [closed]", "user_name": "Joyal Augustine", "text": "\nAnother possible alternative is JOrtho http://jortho.sourceforge.net\nI haven't used it yet, but I'm evaluating the current Java Open Source spellcheckers to figure out which one to use.\n"}, "772": {"topic": "Looking for Java spell checker library [closed]", "user_name": "InfamyInfamy", "text": "\nHunspell looks like it could be of use.  It is written in C++ but a java interface according to the home page.  Tri-licensed under GPL, LGPL and MPL so you shouldn't have a problem with it.\n"}, "773": {"topic": "Looking for Java spell checker library [closed]", "user_name": "Big Red DogBig Red Dog", "text": "\nHave a look at JaSpell. It comes with an internal spell checking engine or you can use aspell. Since the source is available, you can also attach aspell-like engines easily (like Hunspell).\nIt comes with filters for TeX and XML and it has support for suggestion engines like keyboard distance, common misspellings (where you can define words and their replacements for common typos), Levenshtein distance, and phonetic distance.\n"}, "774": {"topic": "Looking for Java spell checker library [closed]", "user_name": "MikeMike", "text": "\nLook at this: http://code.google.com/p/google-api-spelling-java/\nThis is a simple Java API that makes it very easy to call Google's spell checker service from Java applications. \nI tried it and it works very well.\n"}, "775": {"topic": "Looking for Java spell checker library [closed]", "user_name": "EvanEvan", "text": "\nYou can try Suggester. It is  open source, free, and supports all of the above listed languages.\n"}, "776": {"topic": "Feature Selection and Reduction for Text Classification", "user_name": "StatguyUser", "text": "\nI am currently working on a project, a simple sentiment analyzer such that there will be 2 and 3 classes in separate cases. I am using a corpus that is pretty rich in the means of unique words (around 200.000). I used bag-of-words method for feature selection and to reduce the number of unique features, an elimination is done due to a threshold value of frequency of occurrence. The final set of features includes around 20.000 features, which is actually a 90% decrease, but not enough for intended accuracy of test-prediction. I am using LibSVM and SVM-light in turn for training and prediction (both linear and RBF kernel) and also Python and Bash in general.\nThe highest accuracy observed so far is around 75% and I need at least 90%. This is the case for binary classification. For multi-class training, the accuracy falls to ~60%. I need at least 90% at both cases and can not figure how to increase it: via optimizing training parameters or via optimizing feature selection?\nI have read articles about feature selection in text classification and what I found is that three different methods are used, which have actually a clear correlation among each other. These methods are as follows:\n\nFrequency approach of bag-of-words (BOW)\nInformation Gain (IG)\nX^2 Statistic (CHI)\n\nThe first method is already the one I use, but I use it very simply and need guidance for a better use of it in order to obtain high enough accuracy. I am also lacking knowledge about practical implementations of IG and CHI and looking for any help to guide me in that way.\nThanks a lot, and if you need any additional info for help, just let me know.\n\n\n@larsmans: Frequency Threshold: I am looking for the occurrences of unique words in examples, such that if a word is occurring in different examples frequently enough, it is included in the feature set as a unique feature.   \n@TheManWithNoName: First of all thanks for your effort in explaining the general concerns of document classification. I examined and experimented all the methods you bring forward and others. I found Proportional Difference (PD) method the best for feature selection, where features are uni-grams and Term Presence (TP) for the weighting (I didn't understand why you tagged Term-Frequency-Inverse-Document-Frequency (TF-IDF) as an indexing method, I rather consider it as a feature weighting approach).  Pre-processing is also an important aspect for this task as you mentioned. I used certain types of string elimination for refining the data as well as morphological parsing and stemming. Also note that I am working on Turkish, which has different characteristics compared to English. Finally, I managed to reach ~88% accuracy (f-measure) for binary classification and ~84% for multi-class. These values are solid proofs of the success of the model I used. This is what I have done so far. Now working on clustering and reduction models, have tried LDA and LSI and moving on to moVMF and maybe spherical models (LDA + moVMF), which seems to work better on corpus those have objective nature, like news corpus. If you have any information and guidance on these issues, I will appreciate. I need info especially to setup an interface (python oriented, open-source) between feature space dimension reduction methods (LDA, LSI, moVMF etc.) and clustering methods (k-means, hierarchical etc.).\n\n"}, "777": {"topic": "Feature Selection and Reduction for Text Classification", "user_name": "clanculariusclancularius", "text": "\nThis is probably a bit late to the table, but...\nAs Bee points out and you are already aware, the use of SVM as a classifier is wasted if you have already lost the information in the stages prior to classification. However, the process of text classification requires much more that just a couple of stages and each stage has significant effects on the result. Therefore, before looking into more complicated feature selection measures there are a number of much simpler possibilities that will typically require much lower resource consumption.\nDo you pre-process the documents before performing tokensiation/representation into the bag-of-words format? Simply removing stop words or punctuation may improve accuracy considerably.\nHave you considered altering your bag-of-words representation to use, for example, word pairs or n-grams instead? You may find that you have more dimensions to begin with but that they condense down a lot further and contain more useful information.\nIts also worth noting that dimension reduction is feature selection/feature extraction. The difference is that feature selection reduces the dimensions in a univariate manner, i.e. it removes terms on an individual basis as they currently appear without altering them, whereas feature extraction (which I think Ben Allison is referring to) is multivaritate, combining one or more single terms together to produce higher orthangonal terms that (hopefully) contain more information and reduce the feature space.\nRegarding your use of document frequency, are you merely using the probability/percentage of documents that contain a term or are you using the term densities found within the documents? If category one has only 10 douments and they each contain a term once, then category one is indeed associated with the document. However, if category two has only 10 documents that each contain the same term a hundred times each, then obviously category two has a much higher relation to that term than category one. If term densities are not taken into account this information is lost and the fewer categories you have the more impact this loss with have. On a similar note, it is not always prudent to only retain terms that have high frequencies, as they may not actually be providing any useful information. For example if a term appears a hundred times in every document, then it is considered a noise term and, while it looks important, there is no practical value in keeping it in your feature set.\nAlso how do you index the data, are you using the Vector Space Model with simple boolean indexing or a more complicated measure such as TF-IDF? Considering the low number of categories in your scenario a more complex measure will be beneficial as they can account for term importance for each category in relation to its importance throughout the entire dataset.\nPersonally I would experiment with some of the above possibilities first and then consider tweaking the feature selection/extraction with a (or a combination of) complex equations if you need an additional performance boost.\n\nAdditional\nBased on the new information, it sounds as though you are on the right track and 84%+ accuracy (F1 or BEP - precision and recall based for multi-class problems) is generally considered very good for most datasets. It might be that you have successfully acquired all information rich features from the data already, or that a few are still being pruned.\nHaving said that, something that can be used as a predictor of how good aggressive dimension reduction may be for a particular dataset is 'Outlier Count' analysis, which uses the decline of Information Gain in outlying features to determine how likely it is that information will be lost during feature selection. You can use it on the raw and/or processed data to give an estimate of how aggressively you should aim to prune features (or unprune them as the case may be). A paper describing it can be found here:\nPaper with Outlier Count information\nWith regards to describing TF-IDF as an indexing method, you are correct in it being a feature weighting measure, but I consider it to be used mostly as part of the indexing process (though it can also be used for dimension reduction). The reasoning for this is that some measures are better aimed toward feature selection/extraction, while others are preferable for feature weighting specifically in your document vectors (i.e. the indexed data). This is generally due to dimension reduction measures being determined on a per category basis, whereas index weighting measures tend to be more document orientated to give superior vector representation.\nIn respect to LDA, LSI and moVMF, I'm afraid I have too little experience of them to provide any guidance. Unfortunately I've also not worked with Turkish datasets or the python language. \n"}, "778": {"topic": "Feature Selection and Reduction for Text Classification", "user_name": "", "text": "\nI would recommend dimensionality reduction instead of feature selection. Consider either singular value decomposition, principal component analysis, or even better considering it's tailored for bag-of-words representations, Latent Dirichlet Allocation. This will allow you to notionally retain representations that include all words, but to collapse them to fewer dimensions by exploiting similarity (or even synonymy-type) relations between them.\nAll these methods have fairly standard implementations that you can get access to and run---if you let us know which language you're using, I or someone else will be able to point you in the right direction.\n"}, "779": {"topic": "Feature Selection and Reduction for Text Classification", "user_name": "TheManWithNoNameTheManWithNoName", "text": "\nThere's a python library for feature selection\nTextFeatureSelection.  This library provides discriminatory power in the form of score for each word token, bigram, trigram etc.\nThose who are aware of feature selection methods in machine learning, it is based on filter method and provides ML engineers required tools to improve the classification accuracy in their NLP and deep learning models. It has 4 methods namely Chi-square, Mutual information, Proportional difference\u00a0and Information gain to help select words as features before being fed into machine learning classifiers.\nfrom TextFeatureSelection import TextFeatureSelection\n\n#Multiclass classification problem\ninput_doc_list=['i am very happy','i just had an awesome weekend','this is a very difficult terrain to trek. i wish i stayed back at home.','i just had lunch','Do you want chips?']\ntarget=['Positive','Positive','Negative','Neutral','Neutral']\nfsOBJ=TextFeatureSelection(target=target,input_doc_list=input_doc_list)\nresult_df=fsOBJ.getScore()\nprint(result_df)\n\n#Binary classification\ninput_doc_list=['i am content with this location','i am having the time of my life','you cannot learn machine learning without linear algebra','i want to go to mars']\ntarget=[1,1,0,1]\nfsOBJ=TextFeatureSelection(target=target,input_doc_list=input_doc_list)\nresult_df=fsOBJ.getScore()\nprint(result_df)\n\nEdit:\nIt now has genetic algorithm for feature selection as well.\nfrom TextFeatureSelection import TextFeatureSelectionGA\n#Input documents: doc_list\n#Input labels: label_list\ngetGAobj=TextFeatureSelectionGA(percentage_of_token=60)\nbest_vocabulary=getGAobj.getGeneticFeatures(doc_list=doc_list,label_list=label_list)\n\nEdit2\nThere is another method nowTextFeatureSelectionEnsemble, which combines feature selection while ensembling. It does feature selection for base models through document frequency thresholds. At ensemble layer, it uses genetic algorithm to identify best combination of base models and keeps only those.\nfrom TextFeatureSelection import TextFeatureSelectionEnsemble \n\nimdb_data=pd.read_csv('../input/IMDB Dataset.csv')\nle = LabelEncoder()\nimdb_data['labels'] = le.fit_transform(imdb_data['sentiment'].values)\n\n#convert raw text and labels to python list\ndoc_list=imdb_data['review'].tolist()\nlabel_list=imdb_data['labels'].tolist()\n\n#Initialize parameter for TextFeatureSelectionEnsemble and start training\ngaObj=TextFeatureSelectionEnsemble(doc_list,label_list,n_crossvalidation=2,pickle_path='/home/user/folder/',average='micro',base_model_list=['LogisticRegression','RandomForestClassifier','ExtraTreesClassifier','KNeighborsClassifier'])\nbest_columns=gaObj.doTFSE()`\n\nCheck the project for details: https://pypi.org/project/TextFeatureSelection/\n"}, "780": {"topic": "Feature Selection and Reduction for Text Classification", "user_name": "Ben AllisonBen Allison", "text": "\nLinear svm is recommended for high dimensional features. Based on my experience the ultimate limitation of SVM accuracy depends on the positive and negative \"features\". You can do a grid search (or in the case of linear svm you can just search for the best cost value) to find the optimal parameters for maximum accuracy, but in the end you are limited by the separability of your feature-sets. The fact that you are not getting 90% means that you still have some work to do finding better features to describe your members of the classes.\n"}, "781": {"topic": "Feature Selection and Reduction for Text Classification", "user_name": "", "text": "\nI'm sure this is way too late to be of use to the poster, but perhaps it will be useful to someone else. The chi-squared approach to feature reduction is pretty simple to implement. Assuming BoW binary classification into classes C1 and C2, for each feature f in candidate_features calculate the freq of f in C1; calculate total words C1; repeat calculations for C2; Calculate a chi-sqaure determine filter candidate_features based on whether p-value is below a certain threshold (e.g. p < 0.05). A tutorial using Python and nltk can been seen here: http://streamhacker.com/2010/06/16/text-classification-sentiment-analysis-eliminate-low-information-features/ (though if I remember correctly, I believe the author incorrectly applies this technique to his test data, which biases the reported results).\n"}, "782": {"topic": "How can a sentence or a document be converted to a vector?", "user_name": "SahilSahil", "text": "\nWe have models for converting words to vectors (for example the word2vec model). Do similar models exist which convert sentences/documents into vectors, using perhaps the vectors learnt for the individual words?\n"}, "783": {"topic": "How can a sentence or a document be converted to a vector?", "user_name": "AzraelAzrael", "text": "\n1) Skip gram method: paper here and the tool that uses it, google word2vec\n2) Using LSTM-RNN to form semantic representations of sentences.\n3) Representations of sentences and documents. The Paragraph vector is introduced in this paper. It is basically an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents.\n4) Though this paper does not form sentence/paragraph vectors, it is simple enough to do that. One can just plug in the individual word vectors(Glove word vectors are found to give the best performance) and then can form a vector representation of the whole sentence/paragraph. \n5) Using a CNN to summarize documents.\n"}, "784": {"topic": "How can a sentence or a document be converted to a vector?", "user_name": "", "text": "\nIt all depends on:\n\nwhich vector model you're using\nwhat is the purpose of the model\nyour creativity in combining word vectors into a document vector\n\nIf you've generated the model using Word2Vec, you can either try:\n\nDoc2Vec: https://radimrehurek.com/gensim/models/doc2vec.html\nWiki2Vec: https://github.com/idio/wiki2vec\n\nOr you can do what some people do, i.e. sum all content words in the documents and divide by the content words, e.g. https://github.com/alvations/oque/blob/master/o.py#L13 (note: line 17-18 is a hack to reduce noise):\ndef sent_vectorizer(sent, model):\n    sent_vec = np.zeros(400)\n    numw = 0\n    for w in sent:\n        try:\n            sent_vec = np.add(sent_vec, model[w])\n            numw+=1\n        except:\n            pass\n    return sent_vec / np.sqrt(sent_vec.dot(sent_vec))\n\n"}, "785": {"topic": "How can a sentence or a document be converted to a vector?", "user_name": "alvasalvas", "text": "\nA solution that is slightly less off the shelf, but probably hard to beat in terms of accuracy if you have a specific thing you're trying to do:\nBuild an RNN (with LSTM or GRU memory cells, comparison here) and optimize the error function of the actual task you're trying to accomplish. You feed it your sentence, and train it to produce the output you want. The activations of the network after being fed your sentence is a representation of the sentence (although you might only care about the networks output). \nYou can represent the sentence as a sequence of one-hot encoded characters, as a sequence of one-hot encoded words, or as a sequence of word vectors (e.g. GloVe or word2vec). If you use word vectors, you can keep backpropagating into the word vectors, updating their weights, so you also get custom word vectors tweaked specifically for the task you're doing. \n"}, "786": {"topic": "How can a sentence or a document be converted to a vector?", "user_name": "", "text": "\nThere are a lot of ways to answer this question. The answer depends on your interpretation of phrases and sentences.\nThese distributional models such as word2vec which provide vector representation for each word can only show how a word usually is used in a window-base context in relation with other words. Based on this interpretation of context-word relations, you can take average vector of all words in a sentence as vector representation of the sentence. For example, in this sentence:\n\nvegetarians eat vegetables .\n\n\nWe can take the normalised vector as vector representation:\n\nThe problem is in compositional nature of sentences. If you take the average word vectors as above, these two sentences have the same vector representation:\n\nvegetables eat vegetarians .\n\nThere are a lot of researches in distributional fashion to learn tree structures through corpus processing. For example: Parsing With Compositional Vector Grammars. This video also explain this method.\nAgain I want to emphasise on interpretation. These sentence vectors probably have their own meanings in your application. For instance, in sentiment analysis in this project in Stanford, the meaning that they are seeking is the positive/negative sentiment of a sentence. Even if you find a perfect vector representation for a sentence, there are philosophical debates that these are not actual meanings of sentences if you cannot judge the truth condition (David Lewis \"General Semantics\" 1970). That's why there are lines of works focusing on computer vision (this paper or this paper). My point is that it can completely depend on your application and interpretation of vectors.\n"}, "787": {"topic": "How can a sentence or a document be converted to a vector?", "user_name": "larsparslarspars", "text": "\nHope you welcome an implementation. I faced the similar problem in converting the movie plots for analysis, after trying many other solutions I sticked to an implementation that made my job easier. The code snippet is attached below.\nInstall 'spaCy' from the following link. \nimport spacy\nnlp = spacy.load('en')\ndoc = nlp(YOUR_DOC_HERE)\nvec = doc.vector\n\nHope this helps. \n"}, "788": {"topic": "How is WordPiece tokenization helpful to effectively deal with rare words problem in NLP?", "user_name": "HarmanHarman", "text": "\nI have seen that NLP models such as BERT utilize WordPiece for tokenization. In WordPiece, we split the tokens like playing to play and ##ing. It is mentioned that it covers a wider spectrum of Out-Of-Vocabulary (OOV) words. Can someone please help me explain how WordPiece tokenization is actually done, and how it handles effectively helps to rare/OOV words? \n"}, "789": {"topic": "How is WordPiece tokenization helpful to effectively deal with rare words problem in NLP?", "user_name": "ProyagProyag", "text": "\nWordPiece and BPE are two similar and commonly used techniques to segment words into subword-level in NLP tasks.\nIn both cases, the vocabulary is initialized with all the individual characters in the language, and then the most frequent/likely combinations of the symbols in the vocabulary are iteratively added to the vocabulary.\nConsider the WordPiece algorithm from the original paper (wording slightly modified by me):\n\n\nInitialize the word unit inventory with all the characters in the text.\nBuild a language model on the training data using the inventory from 1.\nGenerate a new word unit by combining two units out of the current word inventory to increment the word unit inventory by one. Choose the new word unit out of all the possible ones that increases the likelihood on the training data the most when added to the model.\nGoto 2 until a predefined limit of word units is reached or the likelihood increase falls below a certain threshold.\n\n\nThe BPE algorithm only differs in Step 3, where it simply chooses the new word unit as the combination of the next most frequently occurring pair among the current set of subword units.\nExample\nInput text: she walked . he is a dog walker . i walk\nFirst 3 BPE Merges:\n\nw a = wa\nl k = lk\nwa lk = walk\n\nSo at this stage, your vocabulary includes all the initial characters, along with wa, lk, and walk. You usually do this for a fixed number of merge operations.\nHow does it handle rare/OOV words?\nQuite simply, OOV words are impossible if you use such a segmentation method. Any word which does not occur in the vocabulary will be broken down into subword units. Similarly, for rare words, given that the number of subword merges we used is limited, the word will not occur in the vocabulary, so it will be split into more frequent subwords.\nHow does this help?\nImagine that the model sees the word walking. Unless this word occurs at least a few times in the training corpus, the model can't learn to deal with this word very well. However, it may have the words walked, walker, walks, each occurring only a few times. Without subword segmentation, all these words are treated as completely different words by the model.\nHowever, if these get segmented as walk@@ ing, walk@@ ed, etc., notice that all of them will now have walk@@ in common, which will occur much frequently while training, and the model might be able to learn more about it.\n"}, "790": {"topic": "How is WordPiece tokenization helpful to effectively deal with rare words problem in NLP?", "user_name": "Abhi25tAbhi25t", "text": "\nWordPiece is very similar to BPE.\nAs an example, let\u2019s assume that after pre-tokenization, the following set of words including their frequency has been determined:\n(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n\nConsequently, the base vocabulary is [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]. Splitting all words into symbols of the base vocabulary, we obtain:\n(\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)\n\nBPE then counts the frequency of each possible symbol pair and picks the symbol pair that occurs most frequently. In the example above \"h\" followed by \"u\" is present 10 + 5 = 15 times (10 times in the 10 occurrences of \"hug\", 5 times in the 5 occurrences of \"hugs\"). However, the most frequent symbol pair is \"u\" followed by \"g\", occurring 10 + 5 + 5 = 20 times in total. Thus, the first merge rule the tokenizer learns is to group all \"u\" symbols followed by a \"g\" symbol together. Next, \"ug\" is added to the vocabulary. The set of words then becomes\n(\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)\n\nBPE then identifies the next most common symbol pair. It\u2019s \"u\" followed by \"n\", which occurs 16 times. \"u\", \"n\" is merged to \"un\" and added to the vocabulary. The next most frequent symbol pair is \"h\" followed by \"ug\", occurring 15 times. Again the pair is merged and \"hug\" can be added to the vocabulary.\nAt this stage, the vocabulary is [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"] and our set of unique words is represented as\n(\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)\n\nAssuming, that the Byte-Pair Encoding training would stop at this point, the learned merge rules would then be applied to new words (as long as those new words do not include symbols that were not in the base vocabulary). For instance, the word \"bug\" would be tokenized to [\"b\", \"ug\"] but \"mug\" would be tokenized as [\"\", \"ug\"] since the symbol \"m\" is not in the base vocabulary. In general, single letters such as \"m\" are not replaced by the \"<unk>\" symbol because the training data usually includes at least one occurrence of each letter, but it is likely to happen for very special characters like emojis.\nThe base vocabulary size + the number of merges, is a hyper-parameter to choose. For instance GPT has a vocabulary size of 40,478 since they have 478 base characters and chose to stop training after 40,000 merges.\nWordPiece Vs BPE\nWordPiece first initializes the vocabulary to include every character present in the training data and progressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent symbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary. Referring to the previous example, maximizing the likelihood of the training data is equivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by its second symbol is the greatest among all symbol pairs. E.g. \"u\", followed by \"g\" would have only been merged if the probability of \"ug\" divided by \"u\", \"g\" would have been greater than for any other symbol pair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it loses by merging two symbols to make ensure it\u2019s worth it.\nAlso, BPE places the @@ at the end of tokens while wordpieces place the ## at the beginning.\n"}, "791": {"topic": "What Is the Difference Between POS Tagging and Shallow Parsing?", "user_name": "bertzziebertzzie", "text": "\nI'm currently taking a Natural Language Processing course at my University and still confused with some basic concept. I get the definition of POS Tagging from the Foundations of Statistical Natural Language Processing book:\n\nTagging is the task of labeling (or tagging) each word in a sentence\n  with its appropriate part of speech. We decide whether each word is a\n  noun, verb, adjective, or whatever.\n\nBut I can't find a definition of Shallow Parsing in the book since it also describe shallow parsing as one of the utilities of POS Tagging. So I began to search the web and found no direct explanation of shallow parsing, but in Wikipedia:\n\nShallow parsing (also chunking, \"light parsing\") is an analysis of a sentence which identifies the constituents (noun groups, verbs, verb groups, etc.), but does not specify their internal structure, nor their role in the main sentence.\n\nI frankly don't see the difference, but it may be because of my English or just me not understanding simple basic concept. Can anyone please explain the difference between shallow parsing and POS Tagging? Is shallow parsing often also called Shallow Semantic Parsing?\nThanks before.\n"}, "792": {"topic": "What Is the Difference Between POS Tagging and Shallow Parsing?", "user_name": "Aditya MukherjiAditya Mukherji", "text": "\nPOS tagging would give a POS tag to each and every word in the input sentence.  \nParsing the sentence (using the stanford pcfg for example) would convert the sentence into a tree whose leaves will hold POS tags (which correspond to words in the sentence), but the rest of the tree would tell you how exactly these these words are joining together to make the overall sentence. For example an adjective and a noun might combine to be a 'Noun Phrase', which might combine with another adjective to form another Noun Phrase (e.g. quick brown fox) (the exact way the pieces combine depends on the parser in question).\nYou can see how parser output looks like at http://nlp.stanford.edu:8080/parser/index.jsp \nA shallow parser or 'chunker' comes somewhere in between these two. A plain POS tagger is really fast but does not give you enough information and a full blown parser is slow and gives you too much. A POS tagger can be thought of as a parser which only returns the bottom-most tier of the parse tree to you. A chunker might be thought of as a parser that returns some other tier of the parse tree to you instead. Sometimes you just need to know that a bunch of words together form a Noun Phrase but don't care about the sub-structure of the tree within those words (i.e. which words are adjectives, determiners, nouns, etc and how do they combine). In such cases you can use a chunker to get exactly the information you need instead of wasting time generating the full parse tree for the sentence.\n"}, "793": {"topic": "What Is the Difference Between POS Tagging and Shallow Parsing?", "user_name": "KhairulKhairul", "text": "\nPOS tagging is a process deciding what is the type of every token from a text, e.g. NOUN, VERB, DETERMINER, etc. Token can be word or punctuation.\nMeanwhile shallow parsing or chunking is a process dividing a text into syntactically related group.  \nPos Tagging output\n\nMy/PRP$ dog/NN likes/VBZ his/PRP$ food/NN ./.\n\nChunking output\n\n[NP My Dog] [VP likes] [NP his food]\n\n"}, "794": {"topic": "What Is the Difference Between POS Tagging and Shallow Parsing?", "user_name": "", "text": "\nThe Constraint Grammar framework is illustrative. In its simplest, crudest form, it takes as input POS-tagged text, and adds what you could call Part of Clause tags. For an adjective, for example, it could add @NN> to indicate that it is part of an NP whose head word is to the right.\n"}, "795": {"topic": "What Is the Difference Between POS Tagging and Shallow Parsing?", "user_name": "tripleeetripleee", "text": "\nIn POS_tagger, we tag words using a \"tagset\" like {noun, verb, adj, adv, prob...}\nwhile shallow parser try to define sub-components such as Name Entity and phrases in the sentence like \n\"I'm currently (taking a Natural (Language Processing course) at (my University)) and (still confused with some basic concept.)\"\n"}, "796": {"topic": "What Is the Difference Between POS Tagging and Shallow Parsing?", "user_name": "        user1149501user1149501", "text": "\nD. Jurafsky and J. H. Martin say in their book, that shallow parse (partial parse) is a parse that doesn't extract all the possible information from the sentence, but just extract valuable in the specific case information.\nChunking is just a one of the approaches to shallow parsing. As it was mentioned, it extracts only information about basic non-recursive phrases (e.g. verb phrases or noun phrases).\nOther approaches, for example, produce flatted parse trees. These trees may contain information about part-of-speech tags, but defer decisions that may require semantic or contextual factors, such as PP attachments, coordination ambiguities, and nominal compound analyses. \nSo, shallow parse is the parse that produce a partial parse tree. Chunking is an example of such parsing.  \n"}, "797": {"topic": "tag generation from a text content", "user_name": "dmcer", "text": "\nI am curious if there is an algorithm/method exists to generate keywords/tags from a given text, by using some weight calculations, occurrence ratio or other tools.\nAdditionally, I will be grateful if you point any Python based solution / library for this.\nThanks\n"}, "798": {"topic": "tag generation from a text content", "user_name": "HellnarHellnar", "text": "\nOne way to do this would be to extract words that occur more frequently in a document than you would expect them to by chance. For example, say in a larger collection of documents the term 'Markov' is almost never seen. However, in a particular document from the same collection Markov shows up very frequently. This would suggest that Markov might be a good keyword or tag to associate with the document.\nTo identify keywords like this, you could use the point-wise mutual information of the keyword and the document. This is given by PMI(term, doc) = log [ P(term, doc) / (P(term)*P(doc)) ]. This will roughly tell you how much less (or more) surprised you are to come across the term in the specific document as appose to coming across it in the larger collection.\nTo identify the 5 best keywords to associate with a document, you would just sort the terms by their PMI score with the document and pick the 5 with the highest score. \nIf you want to extract multiword tags, see the StackOverflow question How to extract common / significant phrases from a series of text entries. \nBorrowing from my answer to that question, the NLTK collocations how-to covers how to do \nextract interesting multiword expressions using n-gram PMI in a about 7 lines of code, e.g.:\nimport nltk\nfrom nltk.collocations import *\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n\n# change this to read in your data\nfinder = BigramCollocationFinder.from_words(\n   nltk.corpus.genesis.words('english-web.txt'))\n\n# only bigrams that appear 3+ times\nfinder.apply_freq_filter(3) \n\n# return the 5 n-grams with the highest PMI\nfinder.nbest(bigram_measures.pmi, 5)  \n\n"}, "799": {"topic": "tag generation from a text content", "user_name": "CommunityBot", "text": "\nFirst, the key python library for computational linguistics is NLTK (\"Natural Language Toolkit\"). This is a stable, mature library created and maintained by professional computational linguists. It also has an extensive collection of tutorials, FAQs, etc. I recommend it highly.\nBelow is a simple template, in python code, for the problem raised in your Question; although it's a template it runs--supply any text as a string (as i've done) and it will return a list of word frequencies as well as a ranked list of those words in order of 'importance' (or suitability as keywords) according to a very simple heuristic.\nKeywords for a given document are (obviously) chosen from among important words in a document--ie, those words that are likely to distinguish it from another document. If you had no a priori knowledge of the text's subject matter, a common technique is to infer the importance or weight of a given word/term from its frequency, or importance = 1/frequency.\ntext = \"\"\" The intensity of the feeling makes up for the disproportion of the objects.  Things are equal to the imagination, which have the power of affecting the mind with an equal degree of terror, admiration, delight, or love.  When Lear calls upon the heavens to avenge his cause, \"for they are old like him,\" there is nothing extravagant or impious in this sublime identification of his age with theirs; for there is no other image which could do justice to the agonising sense of his wrongs and his despair! \"\"\"\n\nBAD_CHARS = \".!?,\\'\\\"\"\n\n# transform text into a list words--removing punctuation and filtering small words\nwords = [ word.strip(BAD_CHARS) for word in text.strip().split() if len(word) > 4 ]\n\nword_freq = {}\n\n# generate a 'word histogram' for the text--ie, a list of the frequencies of each word\nfor word in words :\n  word_freq[word] = word_freq.get(word, 0) + 1\n\n# sort the word list by frequency \n# (just a DSU sort, there's a python built-in for this, but i can't remember it)\ntx = [ (v, k) for (k, v) in word_freq.items()]\ntx.sort(reverse=True)\nword_freq_sorted = [ (k, v) for (v, k) in tx ]\n\n# eg, what are the most common words in that text?\nprint(word_freq_sorted)\n# returns: [('which', 4), ('other', 4), ('like', 4), ('what', 3), ('upon', 3)]\n# obviously using a text larger than 50 or so words will give you more meaningful results\n\nterm_importance = lambda word : 1.0/word_freq[word]\n\n# select document keywords from the words at/near the top of this list:\nmap(term_importance, word_freq.keys())\n\n"}, "800": {"topic": "tag generation from a text content", "user_name": "dmcerdmcer", "text": "\nhttp://en.wikipedia.org/wiki/Latent_Dirichlet_allocation tries to represent each document in a training corpus as mixture of topics, which in turn are distributions mapping words to probabilities.  \nI had used it once to dissect a corpus of product reviews into the latent ideas that were being spoken about across all the documents such as 'customer service', 'product usability', etc.. The basic model does not advocate a way to convert the topic models into a single word describing what a topic is about.. but people have come up with all kinds of heuristics to do that once their model is trained.  \nI recommend you try playing with http://mallet.cs.umass.edu/ and seeing if this model fits your needs..  \nLDA is a completely unsupervised algorithm meaning it doesn't require you to hand annotate anything which is great, but on the flip side, might not deliver you the topics you were expecting it to give.\n"}, "801": {"topic": "tag generation from a text content", "user_name": "John Paulett", "text": "\nA very simple solution to the problem would be:\n\ncount the occurences of each word in the text\nconsider the most frequent terms as the key phrases\nhave a black-list of 'stop words' to remove common words like the, and, it, is etc\n\nI'm sure there are cleverer, stats based solutions though.\nIf you need a solution to use in a larger project rather than for interests sake, Yahoo BOSS has a key term extraction method.\n"}, "802": {"topic": "tag generation from a text content", "user_name": "dougdoug", "text": "\nLatent Dirichlet allocation or Hierarchical Dirichlet Process can be used to generate tags for individual texts within a greater corpus (body of texts) by extracting the most important words from the derived topics.\nA basic example would be if we were to run LDA over a corpus and define it to have two topics, and that we find further that a text in the corpus is 70% one topic, and 30% another. The top 70% of the words that define the first topic and 30% that define the second (without duplication) could then be considered as tags for the given text. This method provides strong results where tags generally represent the broader themes of the given texts.\nWith a general reference for preprocessing needed for these codes being found here, we can find tags through the following process using gensim.\nA heuristic way of deriving the optimal number of topics for LDA is found in this answer. Although HDP does not require the number of topics as an input, the standard in such cases is still to use LDA with a derived topic number, as HDP can be problematic. Assume here that the corpus is found to have 10 topics, and we want 5 tags per text:\nfrom gensim.models import LdaModel, HdpModel\nfrom gensim import corpora\n\nnum_topics = 10\nnum_tags = 5\n\nAssume further that we have a variable corpus, which is a preprocessed list of lists, with the subslist entries being word tokens. Initialize a Dirichlet dictionary and create a bag of words where texts are converted to their indexes for their component tokens (words):\ndirichlet_dict = corpora.Dictionary(corpus)\nbow_corpus = [dirichlet_dict.doc2bow(text) for text in corpus]\n\nCreate an LDA or HDP model:\ndirichlet_model = LdaModel(corpus=bow_corpus,\n                           id2word=dirichlet_dict,\n                           num_topics=num_topics,\n                           update_every=1,\n                           chunksize=len(bow_corpus),\n                           passes=20,\n                           alpha='auto')\n\n# dirichlet_model = HdpModel(corpus=bow_corpus, \n#                            id2word=dirichlet_dict,\n#                            chunksize=len(bow_corpus))\n\nThe following code produces ordered lists for the most important words per topic (note that here is where num_tags defines the desired tags per text):\nshown_topics = dirichlet_model.show_topics(num_topics=num_topics, \n                                           num_words=num_tags,\n                                           formatted=False)\nmodel_topics = [[word[0] for word in topic[1]] for topic in shown_topics]\n\nThen find the coherence of the topics across the texts:\ntopic_corpus = dirichlet_model.__getitem__(bow=bow_corpus, eps=0) # cutoff probability to 0 \ntopics_per_text = [text for text in topic_corpus]\n\nFrom here we have the percentage that each text coheres to a given topic, and the words associated with each topic, so we can combine them for tags with the following:\ncorpus_tags = []\n\nfor i in range(len(bow_corpus)):\n    # The complexity here is to make sure that it works with HDP\n    significant_topics = list(set([t[0] for t in topics_per_text[i]]))\n    topic_indexes_by_coherence = [tup[0] for tup in sorted(enumerate(topics_per_text[i]), key=lambda x:x[1])]\n    significant_topics_by_coherence = [significant_topics[i] for i in topic_indexes_by_coherence]\n\n    ordered_topics = [model_topics[i] for i in significant_topics_by_coherence][:num_topics] # subset for HDP\n    ordered_topic_coherences = [topics_per_text[i] for i in topic_indexes_by_coherence][:num_topics] # subset for HDP\n\n    text_tags = []\n    for i in range(num_topics):\n        # Find the number of indexes to select, which can later be extended if the word has already been selected\n        selection_indexes = list(range(int(round(num_tags * ordered_topic_coherences[i]))))\n        if selection_indexes == [] and len(text_tags) < num_tags: \n            # Fix potential rounding error by giving this topic one selection\n            selection_indexes = [0]\n              \n        for s_i in selection_indexes:\n            # ignore_words is a list of words should not be included\n            if ordered_topics[i][s_i] not in text_tags and ordered_topics[i][s_i] not in ignore_words:\n                text_tags.append(ordered_topics[i][s_i])\n            else:\n                selection_indexes.append(selection_indexes[-1] + 1)\n\n    # Fix for if too many were selected\n    text_tags = text_tags[:num_tags]\n\n    corpus_tags.append(text_tags)\n\ncorpus_tags will be a list of tags for each text based on how coherent the text is to the derived topics.\nSee this answer for a similar version of this that generates tags for a whole text corpus.\n"}, "803": {"topic": "How to read values from numbers written as words?", "user_name": "", "text": "\nAs we all know numbers can be written either in numerics, or called by their names. While there are a lot of examples to be found that convert 123 into one hundred twenty three, I could not find good examples of how to convert it the other way around.\nSome of the caveats:\n\ncardinal/nominal or ordinal: \"one\" and \"first\"\ncommon spelling mistakes: \"forty\"/\"fourty\"\nhundreds/thousands: 2100 -> \"twenty one hundred\" and also \"two thousand and one hundred\"\nseparators: \"eleven hundred fifty two\", but also \"elevenhundred fiftytwo\" or \"eleven-hundred fifty-two\" and whatnot\ncolloquialisms: \"thirty-something\"\nfractions: 'one third', 'two fifths'\ncommon names: 'a dozen', 'half'\n\nAnd there are probably more caveats possible that are not yet listed.\nSuppose the algorithm needs to be very robust, and even understand spelling mistakes.\nWhat fields/papers/studies/algorithms should I read to learn how to write all this?\nWhere is the information?\n\nPS: My final parser should actually understand 3 different languages, English, Russian and Hebrew. And maybe at a later stage more languages will be added. Hebrew also has male/female numbers, like \"one man\" and \"one woman\" have a different \"one\" \u2014 \"ehad\" and \"ahat\". Russian also has some of its own complexities.\n\nGoogle does a great job at this. For example:\nhttp://www.google.com/search?q=two+thousand+and+one+hundred+plus+five+dozen+and+four+fifths+in+decimal\n(the reverse is also possible http://www.google.com/search?q=999999999999+in+english)\n"}, "804": {"topic": "How to read values from numbers written as words?", "user_name": "\r", "text": "\nI was playing around with a PEG parser to do what you wanted (and may post that as a separate answer later) when I noticed that there's a very simple algorithm that does a remarkably good job with common forms of numbers in English, Spanish, and German, at the very least.\nWorking with English for example, you need a dictionary that maps words to values in the obvious way:\n\"one\" -> 1, \"two\" -> 2, ... \"twenty\" -> 20,\n\"dozen\" -> 12, \"score\" -> 20, ...\n\"hundred\" -> 100, \"thousand\" -> 1000, \"million\" -> 1000000\n\n...and so forth\nThe algorithm is just:\ntotal = 0\nprior = null\nfor each word w\n    v <- value(w) or next if no value defined\n    prior <- case\n        when prior is null:       v\n        when prior > v:     prior+v\n        else                prior*v\n        else\n    if w in {thousand,million,billion,trillion...}\n        total <- total + prior\n        prior <- null\ntotal = total + prior unless prior is null\n\nFor example, this progresses as follows:\ntotal    prior      v     unconsumed string\n    0      _              four score and seven \n                    4     score and seven \n    0      4              \n                   20     and seven \n    0     80      \n                    _     seven \n    0     80      \n                    7 \n    0     87      \n   87\n\ntotal    prior      v     unconsumed string\n    0        _            two million four hundred twelve thousand eight hundred seven\n                    2     million four hundred twelve thousand eight hundred seven\n    0        2\n                  1000000 four hundred twelve thousand eight hundred seven\n2000000      _\n                    4     hundred twelve thousand eight hundred seven\n2000000      4\n                    100   twelve thousand eight hundred seven\n2000000    400\n                    12    thousand eight hundred seven\n2000000    412\n                    1000  eight hundred seven\n2000000  412000\n                    1000  eight hundred seven\n2412000     _\n                      8   hundred seven\n2412000     8\n                     100  seven\n2412000   800\n                     7\n2412000   807\n2412807\n\nAnd so on.  I'm not saying it's perfect, but for a quick and dirty it does quite well.\n\nAddressing your specific list on edit:\n\ncardinal/nominal or ordinal: \"one\" and \"first\" -- just put them in the dictionary\nenglish/british: \"fourty\"/\"forty\" -- ditto\nhundreds/thousands: \n  2100 -> \"twenty one hundred\" and also \"two thousand and one hundred\" -- works as is\nseparators: \"eleven hundred fifty two\", but also \"elevenhundred fiftytwo\" or \"eleven-hundred fifty-two\" and whatnot -- just define \"next word\" to be the longest prefix that matches a defined word, or up to the next non-word if none do, for a start\ncolloqialisms: \"thirty-something\" -- works\nfragments: 'one third', 'two fifths' -- uh, not yet...\ncommon names: 'a dozen', 'half' -- works; you can even do things like \"a half dozen\"\n\nNumber 6 is the only one I don't have a ready answer for, and that's because of the ambiguity between ordinals and fractions (in English at least) added to the fact that my last cup of coffee was many hours ago.\n"}, "805": {"topic": "How to read values from numbers written as words?", "user_name": "\r", "text": "\nIt's not an easy issue, and I know of no library to do it. I might sit down and try to write something like this sometime. I'd do it in either Prolog, Java or Haskell, though. As far as I can see, there are several issues:\n\nTokenization: sometimes, numbers are written eleven hundred fifty two, but I've seen elevenhundred fiftytwo or eleven-hundred-fifty-two and whatnot. One would have to conduct a survey on what forms are actually in use. This might be especially tricky for Hebrew.\nSpelling mistakes: that's not so hard. You have a limited amount of words, and a bit of Levenshtein-distance magic should do the trick.\nAlternate forms, like you already mentioned, exist. This includes ordinal/cardinal numbers, as well as forty/fourty and...\n... common names or commonly used phrases and NEs (named entities). Would you want to extract 30 from the Thirty Years War or 2 from World War II?\nRoman numerals, too?\nColloqialisms, such as \"thirty-something\" and \"three Euro and shrapnel\", which I wouldn't know how to treat.\n\nIf you are interested in this, I could give it a shot this weekend. My idea is probably using UIMA and tokenizing with it, then going on to further tokenize/disambiguate and finally translate. There might be more issues, let's see if I can come up with some more interesting things.\nSorry, this is not a real answer yet, just an extension to your question. I'll let you know if I find/write something.\nBy the way, if you are interested in the semantics of numerals, I just found an interesting paper by Friederike Moltmann, discussing some issues regarding the logic interpretation of numerals.\n"}, "806": {"topic": "How to read values from numbers written as words?", "user_name": "", "text": "\nI have some code I wrote a while ago: text2num. This does some of what you want, except it does not handle ordinal numbers. I haven't actually used this code for anything, so it's largely untested!\n"}, "807": {"topic": "How to read values from numbers written as words?", "user_name": "MarkusQMarkusQ", "text": "\nUse the Python pattern-en library: \n>>> from pattern.en import number\n>>> number('two thousand fifty and a half') => 2050.5\n\n"}, "808": {"topic": "How to read values from numbers written as words?", "user_name": "Aleksandar DimitrovAleksandar Dimitrov", "text": "\nYou should keep in mind that Europe and America count differently.\nEuropean standard:\nOne Thousand\nOne Million\nOne Thousand Millions (British also use Milliard)\nOne Billion\nOne Thousand Billions\nOne Trillion\nOne Thousand Trillions\n\nHere is a small reference on it.\n\nA simple way to see the difference is the following:\n(American counting Trillion) == (European counting Billion)\n\n"}, "809": {"topic": "How to read values from numbers written as words?", "user_name": "", "text": "\nOrdinal numbers are not applicable because they cant be joined in meaningful ways with other numbers in language (...at least in English)\ne.g. one hundred and first, eleven second, etc...\nHowever, there is another English/American caveat with the word 'and'\ni.e.\none hundred and one (English)\none hundred one (American)\nAlso, the use of 'a' to mean one in English\na thousand = one thousand \n...On a side note Google's calculator does an amazing job of this.\none hundred and three thousand times the speed of light\nAnd even...\ntwo thousand and one hundred plus a dozen\n...wtf?!? a score plus a dozen in roman numerals\n"}, "810": {"topic": "How to read values from numbers written as words?", "user_name": "Greg HewgillGreg Hewgill", "text": "\nHere is an extremely robust solution in Clojure.\nAFAIK it is a unique implementation approach.\n;----------------------------------------------------------------------\n; numbers.clj\n; written by: Mike Mattie codermattie@gmail.com\n;----------------------------------------------------------------------\n(ns operator.numbers\n  (:use compojure.core)\n\n  (:require\n    [clojure.string     :as string] ))\n\n(def number-word-table {\n  \"zero\"          0\n  \"one\"           1\n  \"two\"           2\n  \"three\"         3\n  \"four\"          4\n  \"five\"          5\n  \"six\"           6\n  \"seven\"         7\n  \"eight\"         8\n  \"nine\"          9\n  \"ten\"           10\n  \"eleven\"        11\n  \"twelve\"        12\n  \"thirteen\"      13\n  \"fourteen\"      14\n  \"fifteen\"       15\n  \"sixteen\"       16\n  \"seventeen\"     17\n  \"eighteen\"      18\n  \"nineteen\"      19\n  \"twenty\"        20\n  \"thirty\"        30\n  \"fourty\"        40\n  \"fifty\"         50\n  \"sixty\"         60\n  \"seventy\"       70\n  \"eighty\"        80\n  \"ninety\"        90\n})\n\n(def multiplier-word-table {\n  \"hundred\"       100\n  \"thousand\"      1000\n})\n\n(defn sum-words-to-number [ words ]\n  (apply + (map (fn [ word ] (number-word-table word)) words)) )\n\n; are you down with the sickness ?\n(defn words-to-number [ words ]\n  (let\n    [ n           (count words)\n\n      multipliers (filter (fn [x] (not (false? x))) (map-indexed\n                                                      (fn [ i word ]\n                                                        (if (contains? multiplier-word-table word)\n                                                          (vector i (multiplier-word-table word))\n                                                          false))\n                                                      words) )\n\n      x           (ref 0) ]\n\n    (loop [ indices (reverse (conj (reverse multipliers) (vector n 1)))\n            left    0\n            combine + ]\n      (let\n        [ right (first indices) ]\n\n        (dosync (alter x combine (* (if (> (- (first right) left) 0)\n                                      (sum-words-to-number (subvec words left (first right)))\n                                      1)\n                                    (second right)) ))\n\n        (when (> (count (rest indices)) 0)\n          (recur (rest indices) (inc (first right))\n            (if (= (inc (first right)) (first (second indices)))\n              *\n              +))) ) )\n    @x ))\n\nHere are some examples\n(operator.numbers/words-to-number [\"six\" \"thousand\" \"five\" \"hundred\" \"twenty\" \"two\"])\n(operator.numbers/words-to-number [\"fifty\" \"seven\" \"hundred\"])\n(operator.numbers/words-to-number [\"hundred\"])\n\n"}, "811": {"topic": "How to read values from numbers written as words?", "user_name": "", "text": "\nMy LPC implementation of some of your requirements (American English only):\ninternal mapping inordinal = ([]);\ninternal mapping number = ([]);\n\n#define Numbers ([\\\n    \"zero\"        : 0, \\\n    \"one\"         : 1, \\\n    \"two\"         : 2, \\\n    \"three\"       : 3, \\\n    \"four\"        : 4, \\\n    \"five\"        : 5, \\\n    \"six\"         : 6, \\\n    \"seven\"       : 7, \\\n    \"eight\"       : 8, \\\n    \"nine\"        : 9, \\\n    \"ten\"         : 10, \\\n    \"eleven\"      : 11, \\\n    \"twelve\"      : 12, \\\n    \"thirteen\"    : 13, \\\n    \"fourteen\"    : 14, \\\n    \"fifteen\"     : 15, \\\n    \"sixteen\"     : 16, \\\n    \"seventeen\"   : 17, \\\n    \"eighteen\"    : 18, \\\n    \"nineteen\"    : 19, \\\n    \"twenty\"      : 20, \\\n    \"thirty\"      : 30, \\\n    \"forty\"       : 40, \\\n    \"fifty\"       : 50, \\\n    \"sixty\"       : 60, \\\n    \"seventy\"     : 70, \\\n    \"eighty\"      : 80, \\\n    \"ninety\"      : 90, \\\n    \"hundred\"     : 100, \\\n    \"thousand\"    : 1000, \\\n    \"million\"     : 1000000, \\\n    \"billion\"     : 1000000000, \\\n])\n\n#define Ordinals ([\\\n    \"zeroth\"        : 0, \\\n    \"first\"         : 1, \\\n    \"second\"        : 2, \\\n    \"third\"         : 3, \\\n    \"fourth\"        : 4, \\\n    \"fifth\"         : 5, \\\n    \"sixth\"         : 6, \\\n    \"seventh\"       : 7, \\\n    \"eighth\"        : 8, \\\n    \"ninth\"         : 9, \\\n    \"tenth\"         : 10, \\\n    \"eleventh\"      : 11, \\\n    \"twelfth\"       : 12, \\\n    \"thirteenth\"    : 13, \\\n    \"fourteenth\"    : 14, \\\n    \"fifteenth\"     : 15, \\\n    \"sixteenth\"     : 16, \\\n    \"seventeenth\"   : 17, \\\n    \"eighteenth\"    : 18, \\\n    \"nineteenth\"    : 19, \\\n    \"twentieth\"     : 20, \\\n    \"thirtieth\"     : 30, \\\n    \"fortieth\"      : 40, \\\n    \"fiftieth\"      : 50, \\\n    \"sixtieth\"      : 60, \\\n    \"seventieth\"    : 70, \\\n    \"eightieth\"     : 80, \\\n    \"ninetieth\"     : 90, \\\n    \"hundredth\"     : 100, \\\n    \"thousandth\"    : 1000, \\\n    \"millionth\"     : 1000000, \\\n    \"billionth\"     : 1000000000, \\\n])\n\nvarargs int denumerical(string num, status ordinal) {\n    if(ordinal) {\n        if(member(inordinal, num))\n            return inordinal[num];\n    } else {\n        if(member(number, num))\n            return number[num];\n    }\n    int sign = 1;\n    int total = 0;\n    int sub = 0;\n    int value;\n    string array parts = regexplode(num, \" |-\");\n    if(sizeof(parts) >= 2 && parts[0] == \"\" && parts[1] == \"-\")\n        sign = -1;\n    for(int ix = 0, int iix = sizeof(parts); ix < iix; ix++) {\n        string part = parts[ix];\n        switch(part) {\n        case \"negative\" :\n        case \"minus\"    :\n            sign = -1;\n            continue;\n        case \"\"         :\n            continue;\n        }\n        if(ordinal && ix == iix - 1) {\n            if(part[0] >= '0' && part[0] <= '9' && ends_with(part, \"th\"))\n                value = to_int(part[..<3]);\n            else if(member(Ordinals, part))\n                value = Ordinals[part];\n            else\n                continue;\n        } else {\n            if(part[0] >= '0' && part[0] <= '9')\n                value = to_int(part);\n            else if(member(Numbers, part))\n                value = Numbers[part];\n            else\n                continue;\n        }\n        if(value < 0) {\n            sign = -1;\n            value = - value;\n        }\n        if(value < 10) {\n            if(sub >= 1000) {\n                total += sub;\n                sub = value;\n            } else {\n                sub += value;\n            }\n        } else if(value < 100) {\n            if(sub < 10) {\n                sub = 100 * sub + value;\n            } else if(sub >= 1000) {\n                total += sub;\n                sub = value;\n            } else {\n                sub *= value;\n            }\n        } else if(value < sub) {\n            total += sub;\n            sub = value;\n        } else if(sub == 0) {\n            sub = value;\n        } else {\n            sub *= value;\n        }\n    }\n    total += sub;\n    return sign * total;\n}\n\n"}, "812": {"topic": "How to read values from numbers written as words?", "user_name": "\r", "text": "\nWell, I was too late on the answer for this question, but I was working a little test scenario that seems to have worked very well for me.  I used a (simple, but ugly, and large) regular expression to locate all the words for me.  The expression is as follows:\n(?<Value>(?:zero)|(?:one|first)|(?:two|second)|(?:three|third)|(?:four|fourth)|\n(?:five|fifth)|(?:six|sixth)|(?:seven|seventh)|(?:eight|eighth)|(?:nine|ninth)|\n(?:ten|tenth)|(?:eleven|eleventh)|(?:twelve|twelfth)|(?:thirteen|thirteenth)|\n(?:fourteen|fourteenth)|(?:fifteen|fifteenth)|(?:sixteen|sixteenth)|\n(?:seventeen|seventeenth)|(?:eighteen|eighteenth)|(?:nineteen|nineteenth)|\n(?:twenty|twentieth)|(?:thirty|thirtieth)|(?:forty|fortieth)|(?:fifty|fiftieth)|\n(?:sixty|sixtieth)|(?:seventy|seventieth)|(?:eighty|eightieth)|(?:ninety|ninetieth)|\n(?<Magnitude>(?:hundred|hundredth)|(?:thousand|thousandth)|(?:million|millionth)|\n(?:billion|billionth)))\n\nShown here with line breaks for formatting purposes..\nAnyways, my method was to execute this RegEx with a library like PCRE, and then read back the named matches.  And it worked on all of the different examples listed in this question, minus the \"One Half\", types, as I didn't add them in, but as you can see, it wouldn't be hard to do so.  This addresses a lot of issues.  For example, it addresses the following items in the original question and other answers:\n\ncardinal/nominal or ordinal: \"one\" and \"first\"\ncommon spelling mistakes: \"forty\"/\"fourty\" (Note that it does not EXPLICITLY address this, that would be something you'd want to do before you passed the string to this    parser.  This parser sees this example as \"FOUR\"...)\nhundreds/thousands: 2100 -> \"twenty one hundred\" and also \"two thousand and one hundred\"\nseparators: \"eleven hundred fifty two\", but also \"elevenhundred fiftytwo\" or \"eleven-hundred fifty-two\" and whatnot\ncolloqialisms: \"thirty-something\" (This also is not TOTALLY addressed, as what IS \"something\"?  Well, this code finds this number as simply \"30\").**\n\nNow, rather than store this monster of a regular expression in your source, I was considering building this RegEx at runtime, using something like the following:\nchar *ones[] = {\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"eleven\", \"twelve\",\n  \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\"};\nchar *tens[] = {\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"};\nchar *ordinalones[] = { \"\", \"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"\", \"\", \"\", \"\", \"\", \"\", \"twelfth\" };\nchar *ordinaltens[] = { \"\", \"\", \"twentieth\", \"thirtieth\", \"fortieth\", \"fiftieth\", \"sixtieth\", \"seventieth\", \"eightieth\", \"ninetieth\" };\nand so on...\n\nThe easy part here is we are only storing the words that matter.  In the case of SIXTH, you'll notice that there isn't an entry for it, because it's just it's normal number with TH tacked on...  But ones like TWELVE need different attention.\nOk, so now we have the code to build our (ugly) RegEx, now we just execute it on our number strings.\nOne thing I would recommend, is to filter, or eat the word \"AND\".  It's not necessary, and only leads to other issues.\nSo, what you are going to want to do is setup a function that passes the named matches for \"Magnitude\" into a function that looks at all the possible magnitude values, and multiplies your current result by that value of magnitude.  Then, you create a function that looks at the \"Value\" named matches, and returns an int (or whatever you are using), based on the value discovered there.\nAll VALUE matches are ADDED to your result, while magnitutde matches multiply the result by the mag value.  So, Two Hundred Fifty Thousand becomes \"2\", then \"2 * 100\", then \"200 + 50\", then \"250 * 1000\", ending up with 250000...\nJust for fun, I wrote a vbScript version of this and it worked great with all the examples provided.  Now, it doesn't support named matches, so I had to work a little harder getting the correct result, but I got it.  Bottom line is, if it's a \"VALUE\" match, add it your accumulator.  If it's a magnitude match, multiply your accumulator by 100, 1000, 1000000, 1000000000, etc...  This will provide you with some pretty amazing results, and all you have to do to adjust for things like \"one half\" is add them to your RegEx, put in a code marker for them, and handle them.\nWell, I hope this post helps SOMEONE out there.  If anyone want, I can post by vbScript pseudo code that I used to test this with, however, it's not pretty code, and NOT production code.\nIf I may..  What is the final language this will be written in?  C++, or something like a scripted language?  Greg Hewgill's source will go a long way in helping understand how all of this comes together.\nLet me know if I can be of any other help.  Sorry, I only know English/American, so I can't help you with the other languages.\n"}, "813": {"topic": "How to read values from numbers written as words?", "user_name": "\r", "text": "\nI was converting ordinal edition statements from early modern books (e.g. \"2nd edition\", \"Editio quarta\") to integers and needed support for ordinals 1-100 in English and ordinals 1-10 in a few Romance languages. Here's what I came up with in Python:\ndef get_data_mapping():\n  data_mapping = {\n    \"1st\": 1,\n    \"2nd\": 2,\n    \"3rd\": 3,\n\n    \"tenth\": 10,\n    \"eleventh\": 11,\n    \"twelfth\": 12,\n    \"thirteenth\": 13,\n    \"fourteenth\": 14,\n    \"fifteenth\": 15,\n    \"sixteenth\": 16,\n    \"seventeenth\": 17,\n    \"eighteenth\": 18,\n    \"nineteenth\": 19,\n    \"twentieth\": 20,\n\n    \"new\": 2,\n    \"newly\": 2,\n    \"nova\": 2,\n    \"nouvelle\": 2,\n    \"altera\": 2,\n    \"andere\": 2,\n\n    # latin\n    \"primus\": 1,\n    \"secunda\": 2,\n    \"tertia\": 3,\n    \"quarta\": 4,\n    \"quinta\": 5,\n    \"sexta\": 6,\n    \"septima\": 7,\n    \"octava\": 8,\n    \"nona\": 9,\n    \"decima\": 10,\n\n    # italian\n    \"primo\": 1,\n    \"secondo\": 2,\n    \"terzo\": 3,\n    \"quarto\": 4,\n    \"quinto\": 5,\n    \"sesto\": 6,\n    \"settimo\": 7,\n    \"ottavo\": 8,\n    \"nono\": 9,\n    \"decimo\": 10,\n\n    # french\n    \"premier\": 1,\n    \"deuxi\u00e8me\": 2,\n    \"troisi\u00e8me\": 3,\n    \"quatri\u00e8me\": 4,\n    \"cinqui\u00e8me\": 5,\n    \"sixi\u00e8me\": 6,\n    \"septi\u00e8me\": 7,\n    \"huiti\u00e8me\": 8,\n    \"neuvi\u00e8me\": 9,\n    \"dixi\u00e8me\": 10,\n\n    # spanish\n    \"primero\": 1,\n    \"segundo\": 2,\n    \"tercero\": 3,\n    \"cuarto\": 4,\n    \"quinto\": 5,\n    \"sexto\": 6,\n    \"septimo\": 7,\n    \"octavo\": 8,\n    \"noveno\": 9,\n    \"decimo\": 10\n  }\n\n  # create 4th, 5th, ... 20th\n  for i in xrange(16):\n    data_mapping[str(4+i) + \"th\"] = 4+i\n\n  # create 21st, 22nd, ... 99th\n  for i in xrange(79):\n    last_char = str(i)[-1]\n\n    if last_char == \"0\":\n      data_mapping[str(20+i) + \"th\"] = 20+i\n\n    elif last_char == \"1\":\n      data_mapping[str(20+i) + \"st\"] = 20+i\n\n    elif last_char == \"2\":\n      data_mapping[str(20+i) + \"nd\"] = 20+i\n\n    elif last_char == \"3\":\n      data_mapping[str(20+i) + \"rd\"] = 20+i\n\n    else:\n      data_mapping[str(20+i) + \"th\"] = 20+i\n\n  ordinals = [\n    \"first\", \"second\", \"third\", \n    \"fourth\", \"fifth\", \"sixth\", \n    \"seventh\", \"eighth\", \"ninth\"\n  ]\n\n  # create first, second ... ninth\n  for c, i in enumerate(ordinals):\n    data_mapping[i] = c+1\n\n  # create twenty-first, twenty-second ... ninty-ninth\n  for ci, i in enumerate([\n    \"twenty\", \"thirty\", \"forty\", \n    \"fifty\", \"sixty\", \"seventy\", \n    \"eighty\", \"ninety\"\n  ]):\n    for cj, j in enumerate(ordinals):\n      data_mapping[i + \"-\" + j] = 20 + (ci*10) + (cj+1)\n    data_mapping[i.replace(\"y\", \"ieth\")] = 20 + (ci*10)\n\n  return data_mapping\n\n"}, "814": {"topic": "How to read values from numbers written as words?", "user_name": "", "text": "\nOne place to start looking is the gnu get_date lib, which can parse just about any English textual date into a timestamp. While not exactly what you're looking for, their solution to a similar problem could provide a lot of useful clues.\n"}, "815": {"topic": "How to read values from numbers written as words?", "user_name": "\r", "text": "\nTry\n\nOpen an HTTP Request to \"http://www.google.com/search?q=\" + number + \"+in+decimal\".\nParse the result for your number.\nCache the number / result pairs to lesson the requests over time.\n\n"}, "816": {"topic": "egit - not authorized", "user_name": "Donal Fellows", "text": "\nI just connected to GIT from Eclipse Juno using EGit, and successfully cloned a certain remote repository. During the clone I entered my Github username and password, but chose not to save them.\nThen I tried to \"Fetch from Upstream\". I got this error:\nhttps://github.com/biunlp/nlp-lab.git: not authorized\nI had no chance of entering my username and password...\nThis is strange since I connected to this repository in order to clone... \n"}, "817": {"topic": "egit - not authorized", "user_name": "Erel Segal-HaleviErel Segal-Halevi", "text": "\nUpdate 2022: In what follows, always use:\n\na App password, not your account password (see blog post).\nSo create your app password.\na BitBucket Cloud, since BitBucket server will be  discontinued in Feb. 2024.\n\n\nA. To specify credentials individually for each remote\n\nOpen Git repositories view,\nopen \"Remotes > origin > <your push url>\"\nclick \"Change Credentials...\"\n\n\n(From User Guide - Resource Context Menu)\nB. To specify credentials per server\nIf you want to access multiple repositories on the same server without providing the same credentials multiple times, you may use .netrc. With this, eGit will use the configuration you provide.\n\nCreate a text file called .netrc (_netrc in Windows) in the user home directory.\nAdd content to the file in this form:\n\n\n    machine my.server1.com\n    login yourUserName\n    password yourPassword\n\n    machine my.server2.com\n    login yourUserName\n    password yourPassword\n\nThe Stash documentation contains more information about .netrc\nSecurity issue The problem with using .netrc this way is that the password is visible in plain text. Refer to this answer in Stackoverflow to solve that problem.\n\nMore secure option (2022): EGit (from issue 441198) can be made (with an extension) to recognize a native Git credential helper, using a secure encrypted cache:\n\ninstall a native Git\n\ninstall the GCM (Git Credential Manager), which is cross-platform, and already package with Git For Windows for instance.\n\ninstruct EGit to look for credentials in the GCM: gitflow-incremental-builder/gitflow-incremental-builder\n\nregister your password or token in said GCM\nprintf \"Host=my.server1.com\\nprotocol=https\\nusername=yourUsername1\\npassword=passwd1\" | \\\ngit credential-manager-core store  \n# and:  \nprintf \"Host=my.server1.com\\nprotocol=https\\nusername=yourUsername1\\npassword=passwd1\" | \\\ngit credential-manager-core store\n\n\n\nLook for executable git-credential-manager-core, and add its folder to your %PATH%/$PATH.\n"}, "818": {"topic": "egit - not authorized", "user_name": "", "text": "\nYou can try:\n\neclipse/myeclipse > menu\nwindow > preferences > general > security >\ncontent > click \"delete\" > ok\n\n"}, "819": {"topic": "egit - not authorized", "user_name": "VonCVonC", "text": "\nIf you're using Two Factor Authentication on GitHub, the \"not authorized\" error can be returned even if you are using the correct username and password. This can be resolved by generating a personal access token.\nAfter generating the secure access token, we'll use this instead of a password. Make sure not to leave the page before you're done, because once you leave the page, you'll never see it again (thankfully it can be regenerated, but anything using the previously generated token will cease to authenticate).\nThis assumes that you've successfully installed EGit and that you've successfully cloned a repository.\n\nGo to your GitHub.com settings, and in the left hand pane click Personal access tokens.\nClick Generate new token. Select the scopes that you'd like this token to be able to use, and generate it.\nCopy the token. It should look something like this:  9731f5cf519e9abb53e6ba9f5134075438944888 (don't worry, this is invalid).\nBack in Eclipse (Juno, since that's OP's version), click Window > Show View > Other.... Under Git, select Git Repositories.\nA new pane appears, from which you can open (repository name) > Remotes > origin.\nRight click on a node and choose Change Credentials.... Enter your username for User, and your secure access token for the Password.\n\n"}, "820": {"topic": "egit - not authorized", "user_name": "CommunityBot", "text": "\nI had a similar problem when I changed my password on the remote repository.\nHere is how I fixed it on Eclipse on Mac:\nImportant Note: These instructions have the side effect of clearing all passwords and other secure information.  I was fine with that, but you will want to consider that before you follow these instructions.\n\nClick Eclipse -> Preferences on menu.\nExpect a popup window called Preferences. \nExpand the \"General\" tree item.\nDouble click the \"Security\" tree item.\nExpect the main pane of the popup to change to \"See 'Secure Storage' for...\"\nClick 'Secure Storage'.\nExpect the main pane to change to tabbed window.\nClick the \"Contents\" tab.\nClick \"Delete\".\nAccept a warning that all your secure information is deleted.\nClick \"Cancel\" or \"Apply\" to exit the popup window.\nClick the 'Git Pull' icon and expect to be prompted for username and password.\n\n"}, "821": {"topic": "egit - not authorized", "user_name": "qingshanqingshan", "text": "\nThis worked for me:\nGo to Git profile-> Settings -> developer settings-> personal access tokens\ndelete if any existing token and generate a new token [provide note, specify no.of days and repo checked] ->Copy the new token generated\nGoto eclipse and now when the same login prompt appears, try providing the personal token generated as password instead of Git password.\n"}, "822": {"topic": "egit - not authorized", "user_name": "", "text": "\nBitbucket Cloud recently stopped supporting account passwords for Git authentication. From march 2022.\nSo use app password. Please read more information on below links.\n\nBitbuckect Announcement\nBitbuckect blog\nApp password\n\n"}, "823": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "user1502248user1502248", "text": "\nI'm trying to extract named entities from my text using NLTK. I find that NLTK NER is not very accurate for my purpose and I want to add some more tags of my own as well. I've been trying to find a way to train my own NER, but I don't seem to be able to find the right resources. \nI have a couple of questions regarding NLTK-\n\nCan I use my own data to train an Named Entity Recognizer in NLTK?\nIf I can train using my own data, is the named_entity.py the file to be modified?\nDoes the input file format have to be in IOB eg. Eric NNP B-PERSON ?\nAre there any resources - apart from the nltk cookbook and nlp with python that I can use?\n\nI would really appreciate help in this regard\n"}, "824": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "jjdubsjjdubs", "text": "\nAre you committed to using NLTK/Python?  I ran into the same problems as you, and had much better results using Stanford's named-entity recognizer: http://nlp.stanford.edu/software/CRF-NER.shtml.  The process for training the classifier using your own data is very well-documented in the FAQ.  \nIf you really need to use NLTK, I'd hit up the mailing list for some advice from other users: http://groups.google.com/group/nltk-users.  \nHope this helps!\n"}, "825": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "Rohan AmruteRohan Amrute", "text": "\nYou can easily use the Stanford NER alongwith nltk.\nThe python script is like\nfrom nltk.tag.stanford import NERTagger\nimport os\njava_path = \"/Java/jdk1.8.0_45/bin/java.exe\"\nos.environ['JAVAHOME'] = java_path\nst = NERTagger('../ner-model.ser.gz','../stanford-ner.jar')\ntagging = st.tag(text.split())   \n\nTo train your own data and to create a model you can refer to the first question on Stanford NER FAQ.\nThe link is http://nlp.stanford.edu/software/crf-faq.shtml\n"}, "826": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "aroparop", "text": "\nI also had this issue, but I managed to work it out. \nYou can use your own training data. I documented the main requirements/steps for this in my github repository.\nI used NLTK-trainer, so basicly you have to get the training data in the right format (token NNP B-tag), and run the training script. Check my repository for more info.\n"}, "827": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "xjixji", "text": "\nThere are some functions in the nltk.chunk.named_entity module that train a NER tagger. However, they were specifically written for ACE corpus and not totally cleaned up, so one will need to write their own training procedures with those as a reference.\nThere are also two relatively recent guides (1 2) online detailing the process of using NLTK to train the GMB corpus.\nHowever, as mentioned in answers above, now that many tools are available, one really should not need to resort to NLTK if streamlined training process is desired. Toolkits such as CoreNLP and spaCy do a much better job. As using NLTK is not that much different to writing your own training code from scratch, there is not that much value in doing so. NLTK and OpenNLP can be regarded as somehow belonging to a past era before the explosion of recent progress in NLP.\n"}, "828": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "Thang PhamThang Pham", "text": "\n\n\nAre there any resources - apart from the nltk cookbook and nlp with python that I can use?\n\n\nYou can consider using spaCy to train your own custom data for NER task. Here is an example from this thread to train a model on a custom training set to detect a new entity ANIMAL. The code was fixed and updated for easier reading.\nimport random\nimport spacy\nfrom spacy.training import Example\n\nLABEL = 'ANIMAL'\nTRAIN_DATA = [\n    (\"Horses are too tall and they pretend to care about your feelings\", {'entities': [(0, 6, LABEL)]}),\n    (\"Do they bite?\", {'entities': []}),\n    (\"horses are too tall and they pretend to care about your feelings\", {'entities': [(0, 6, LABEL)]}),\n    (\"horses pretend to care about your feelings\", {'entities': [(0, 6, LABEL)]}),\n    (\"they pretend to care about your feelings, those horses\", {'entities': [(48, 54, LABEL)]}),\n    (\"horses?\", {'entities': [(0, 6, LABEL)]})\n]\nnlp = spacy.load('en_core_web_sm')  # load existing spaCy model\nner = nlp.get_pipe('ner')\nner.add_label(LABEL)\n\noptimizer = nlp.create_optimizer()\n\n# get names of other pipes to disable them during training\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\nwith nlp.disable_pipes(*other_pipes):  # only train NER\n    for itn in range(20):\n        random.shuffle(TRAIN_DATA)\n        losses = {}\n        for text, annotations in TRAIN_DATA:\n            doc = nlp.make_doc(text)\n            example = Example.from_dict(doc, annotations)\n            nlp.update([example], drop=0.35, sgd=optimizer, losses=losses)\n        print(losses)\n\n# test the trained model\ntest_text = 'Do you like horses?'\ndoc = nlp(test_text)\nprint(\"Entities in '%s'\" % test_text)\nfor ent in doc.ents:\n    print(ent.label_, \" -- \", ent.text)\n\nHere are the outputs:\n{'ner': 9.60289144264557}\n{'ner': 8.875474230820478}\n{'ner': 6.370401408220459}\n{'ner': 6.687456469517201}\n... \n{'ner': 1.3796682589133492e-05}\n{'ner': 1.7709562613218738e-05}\n\nEntities in 'Do you like horses?'\nANIMAL  --  horses\n\n"}, "829": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "iEriiiiEriii", "text": "\nTo complete the answer by @Thang M. Pham, you need to label your data before training. To do so, you can use the spacy-annotator.\nHere is an example taken from another answer:\nTrain Spacy NER on Indian Names\n"}, "830": {"topic": "Load Pretrained glove vectors in python", "user_name": "user", "text": "\nI have downloaded pretrained glove vector file from the internet. It is a .txt file. I am unable to load and access it. It is easy to load and access a word vector binary file using gensim but I don't know how to do it when it is a text file format.\n"}, "831": {"topic": "Load Pretrained glove vectors in python", "user_name": "SameSame", "text": "\nglove model files are in a word - vector format. You can open the textfile to verify this. Here is a small snippet of code you can use to load a pretrained glove file:\nimport numpy as np\n\ndef load_glove_model(File):\n    print(\"Loading Glove Model\")\n    glove_model = {}\n    with open(File,'r') as f:\n        for line in f:\n            split_line = line.split()\n            word = split_line[0]\n            embedding = np.array(split_line[1:], dtype=np.float64)\n            glove_model[word] = embedding\n    print(f\"{len(glove_model)} words loaded!\")\n    return glove_model\n\nYou can then access the word vectors by simply using the gloveModel variable.\nprint(gloveModel['hello'])\n"}, "832": {"topic": "Load Pretrained glove vectors in python", "user_name": "Jules G.M.", "text": "\nYou can do it much faster with pandas:\nimport pandas as pd\nimport csv\n\nwords = pd.read_table(glove_data_file, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n\nThen to get the vector for a word:\ndef vec(w):\n  return words.loc[w].as_matrix()\n\nAnd to find the closest word to a vector:\nwords_matrix = words.as_matrix()\n\ndef find_closest_word(v):\n  diff = words_matrix - v\n  delta = np.sum(diff * diff, axis=1)\n  i = np.argmin(delta)\n  return words.iloc[i].name\n\n"}, "833": {"topic": "Load Pretrained glove vectors in python", "user_name": "Karishma MalkanKarishma Malkan", "text": "\nI suggest using gensim to do everything. You can read the file, and also benefit from having a lot of methods already implemented on this great package.\nSuppose you generated GloVe vectors using the C++ program and that your \"-save-file\" parameter is \"vectors\". Glove executable will generate you two files, \"vectors.bin\" and \"vectors.txt\".\nUse glove2word2vec to convert GloVe vectors in text format into the word2vec text format:\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nglove2word2vec(glove_input_file=\"vectors.txt\", word2vec_output_file=\"gensim_glove_vectors.txt\")\n\nFinally, read the word2vec txt to a gensim model using KeyedVectors:\nfrom gensim.models.keyedvectors import KeyedVectors\nglove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)\n\nNow you can use gensim word2vec methods (for example, similarity) as you'd like.\n"}, "834": {"topic": "Load Pretrained glove vectors in python", "user_name": "Steven", "text": "\nI found this approach faster. \nimport pandas as pd\n\ndf = pd.read_csv('glove.840B.300d.txt', sep=\" \", quoting=3, header=None, index_col=0)\nglove = {key: val.values for key, val in df.T.items()}\n\nSave the dictionary:\nimport pickle\nwith open('glove.840B.300d.pkl', 'wb') as fp:\n    pickle.dump(glove, fp)\n\n"}, "835": {"topic": "Load Pretrained glove vectors in python", "user_name": "PetterPetter", "text": "\nHere's a one liner if all you want is the embedding matrix\nnp.loadtxt(path, usecols=range(1, dim+1), comments=None)\nwhere path is path to your downloaded GloVe file and dim is the dimension of the word embedding.\nIf you want both the words and corresponding vectors you can do\nglove = np.loadtxt(path, dtype='str', comments=None)\nand seperate the words and vectors as follows\nwords = glove[:, 0]\nvectors = glove[:, 1:].astype('float')\n\n"}, "836": {"topic": "Load Pretrained glove vectors in python", "user_name": "BenBen", "text": "\nLoading word embedding from a text file (in my case the glove.42B.300d embeddings) takes a bit long (147.2s on my machine).\nWhat helps is converting the text file first into two new files: a text file that contains the words only (e.g. embeddings.vocab) and a binary file which contains the embedding vectors as numpy-structure (e.g. embeddings.npy).\nOnce converted, it takes me only 4.96s to load the same embeddings into the memory. This approach ends a up with exactly the same dictionary as if you load it from the text file. It is as efficient in access time and does not require any additional frameworks, but a lot faster in loading time.\nWith this code you convert your embedding text file to the two new files: \ndef convert_to_binary(embedding_path):\n    f = codecs.open(embedding_path + \".txt\", 'r', encoding='utf-8')\n    wv = []\n\n    with codecs.open(embedding_path + \".vocab\", \"w\", encoding='utf-8') as vocab_write:\n        count = 0\n        for line in f:\n            splitlines = line.split()\n            vocab_write.write(splitlines[0].strip())\n            vocab_write.write(\"\\n\")\n            wv.append([float(val) for val in splitlines[1:]])\n        count += 1\n\n    np.save(embedding_path + \".npy\", np.array(wv))\n\nAnd with this method you load it efficiently into your memory:\ndef load_word_emb_binary(embedding_file_name_w_o_suffix):\n    print(\"Loading binary word embedding from {0}.vocab and {0}.npy\".format(embedding_file_name_w_o_suffix))\n\n    with codecs.open(embedding_file_name_w_o_suffix + '.vocab', 'r', 'utf-8') as f_in:\n        index2word = [line.strip() for line in f_in]\n\n    wv = np.load(embedding_file_name_w_o_suffix + '.npy')\n    word_embedding_map = {}\n    for i, w in enumerate(index2word):\n        word_embedding_map[w] = wv[i]\n\n    return word_embedding_map\n\nDisclaimer: This code is shamelessly stolen from https://blog.ekbana.com/loading-glove-pre-trained-word-embedding-model-from-text-file-faster-5d3e8f2b8455. But it might help in this thread.\n"}, "837": {"topic": "Load Pretrained glove vectors in python", "user_name": "IndraIndra", "text": "\nPython3 version which also handles bigrams and trigrams:\nimport numpy as np\n\n\ndef load_glove_model(glove_file):\n    print(\"Loading Glove Model\")\n    f = open(glove_file, 'r')\n    model = {}\n    vector_size = 300\n    for line in f:\n        split_line = line.split()\n        word = \" \".join(split_line[0:len(split_line) - vector_size])\n        embedding = np.array([float(val) for val in split_line[-vector_size:]])\n        model[word] = embedding\n    print(\"Done.\\n\" + str(len(model)) + \" words loaded!\")\n    return model\n\n"}, "838": {"topic": "Load Pretrained glove vectors in python", "user_name": "Abhai KollaraAbhai Kollara", "text": "\nimport os\nimport numpy as np\n\n# store all the pre-trained word vectors\nprint('Loading word vectors...')\nword2vec = {}\nwith open(os.path.join('glove/glove.6B.%sd.txt' % EMBEDDING_DIM)) as f: #enter the path where you unzipped the glove file\n  # is just a space-separated text file in the format:\n  # word vec[0] vec[1] vec[2] ...\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float32')\n        word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))\n\n"}, "839": {"topic": "Load Pretrained glove vectors in python", "user_name": "Ursin BrunnerUrsin Brunner", "text": "\nThis code takes some time to store glove embeddings on shelf, but loading it is quite faster as compared to other approaches.\nimport os\nimport numpy as np\nfrom contextlib import closing\nimport shelve\n\ndef store_glove_to_shelf(glove_file_path,shelf):\n    print('Loading Glove')\n    with open(os.path.join(glove_file_path)) as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vec = np.asarray(values[1:], dtype='float32')\n            shelf[word] = vec\n\nshelf_file_name = \"glove_embeddings\"\nglove_file_path = \"glove/glove.840B.300d.txt\"\n\n# Storing glove embeddings to shelf for faster load\nwith closing(shelve.open(shelf_file_name + '.shelf', 'c')) as shelf:\n    store_glove_to_shelf(glove_file_path,shelf)\n    print(\"Stored glove embeddings from {} to {}\".format(glove_file_path,shelf_file_name+'.shelf'))\n\n# To reuse the glove embeddings stored in shelf\nwith closing(shelve.open(shelf_file_name + '.shelf')) as embeddings_index:\n    # USE embeddings_index here , which is a dictionary\n    print(\"Loaded glove embeddings from {}\".format(shelf_file_name+'.shelf'))\n    print(\"Found glove embeddings with {} words\".format(len(embeddings_index)))\n\n"}, "840": {"topic": "Load Pretrained glove vectors in python", "user_name": "alabroskialabroski", "text": "\nEach corpus need to start with a line containing the vocab size and the vector size in that order.\nOpen the .txt file of the glove model and enter the dimension of the vector at the first line by pressing Enter first:\nExample, for glove.6B.50d.txt, just add 400000 50 in the first line.\nThen use gensim to transform that raw .txt vector file to gensim vector format:\nimport gensim\n\nword_vectors = gensim.models.KeyedVectors.load_word2vec_format('path/glove.6B.50d.txt', binary=False)\nword_vectors.save('path/glove_gensim.txt')\n\n"}, "841": {"topic": "Load Pretrained glove vectors in python", "user_name": "Ankan DattaAnkan Datta", "text": "\nSome of the other approaches here required more storage space (e.g. to split files) or were quite slow to run on my personal laptop. I tried shelf db but it seemed to blow up in storage size. Here's an \"in-place\" approach with one-time file-read time cost and very low additional storage cost. We treat the original text file as a database and just store the position location for each of the words. This works really well when you're, e.g., investigating properties of word vectors.\n# First create a map from words to position in the file\ndef get_db_mapping(fname):\n    char_ct = 0    # cumulative position in file\n    pos_map = dict()\n\n    with open(fname + \".txt\", 'r', encoding='utf-8') as f:\n        for line in tqdm(f):\n            new_len = len(line)     # len of line\n\n            # get the word\n            splitlines = line.split()\n            word = splitlines[0].strip()\n\n            # store and increment counter\n            pos_map[word] = char_ct\n            char_ct += new_len\n\n    # write dict\n    with open(fname + '.db', 'wb') as handle:\n        pickle.dump(pos_map, handle)\n\n\nclass Embedding:\n\"\"\"Small wrapper so that we can use [] notation to fetch word vectors.\nIt would be better to just have the file pointer and the pos_map as part\nof this class, but that's not how I wrote it initially.\"\"\"\n    def __init__(self, emb_fn):\n        self.emb_fn = emb_fn\n\n    def __getitem__(self, item):\n        return self.emb_fn(item)\n\n\ndef load_db_mapping(fname, cache_size=1000) -> Embedding:\n    \"\"\"Creates a function closure that wraps access to the db mapping\n    and the text file that functions as db. Returns them as an\n    Embedding object\"\"\"\n    # get the two state objects: mapping and file pointer\n    with open(fname + '.db', 'rb') as handle:\n        pos_map = pickle.load(handle)\n    f = open(fname + \".txt\", 'r', encoding='utf-8')\n\n    @lru_cache(maxsize=cache_size)\n    def get_vector(word: str):\n        pos = pos_map[word]\n        f.seek(pos, 0)\n\n        # special logic needed because of small count errors\n        fail_ct = 0\n        read_word = \"\"\n        while fail_ct < 5 and read_word != word:\n            fail_ct += 1\n            l = f.readline()\n            try:\n                splitlines = l.split()\n                read_word = splitlines[0].strip()\n            except:\n                continue\n        if read_word != word:\n            raise ValueError('word not found')\n\n        # actually return\n        return np.array([float(val) for val in splitlines[1:]])\n\n    return Embedding(get_vector)\n\n# to run\nk_glove_vector_name = 'glove.42B.300d'   # omit .txt\nget_db_mapping(k_glove_vector_name)      # run only once; creates .db\nword_embedding = load_db_mapping(k_glove_vector_name)\nword_embedding['hello']\n\n"}, "842": {"topic": "Load Pretrained glove vectors in python", "user_name": "Rudra DesaiRudra Desai", "text": "\na tool with an easy implementation of GloVe is zeulgma\nhttps://pypi.org/project/zeugma/\nfrom zeugma.embeddings import EmbeddingTransformer\nglove = EmbeddingTransformer('glove')\n\nthe implementation is really very easy\n"}, "843": {"topic": "Load Pretrained glove vectors in python", "user_name": "Dharman\u2666", "text": "\ndef create_embedding_matrix(word_to_index):\n# word_to_index is dictionary containing \"word:token\" pairs\nnb_words = len(word_to_index)+1\n\nembeddings_index = {}\nwith open('C:/Users/jayde/Desktop/IISc/DLNLP/Assignment1/glove.840B.300d/glove.840B.300d.txt', encoding=\"utf-8\", errors='ignore') as f:\n    for line in f:\n        values = line.split()\n        word = ''.join(values[:-300])\n        coefs = np.asarray(values[-300:], dtype='float32')\n        embeddings_index[word] = coefs\n\nembedding_matrix = np.zeros((nb_words, 300))\n\nfor word, i in word_to_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\nreturn embedding_matrix\n\nemb_matrix = create_embedding_matrix(vocab_to_int)\n"}, "844": {"topic": "Load Pretrained glove vectors in python", "user_name": "StrayhornStrayhorn", "text": "\nEMBEDDING_LIFE = 'path/to/your/glove.txt'\n\ndef get_coefs(word,*arr): \n      return word, np.asarray(arr, dtype='float32')\n\nembeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n\nfor word, i in word_index.items():\nif i >= max_features: continue\nembedding_vector = embeddings_index.get(word)\nif embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\n"}, "845": {"topic": "Unsupervised Sentiment Analysis", "user_name": "Holger Just", "text": "\nI've been reading a lot of articles that explain the need for an initial set of texts that are classified as either 'positive' or 'negative' before a sentiment analysis system will really work.\nMy question is: Has anyone attempted just doing a rudimentary check of 'positive' adjectives vs 'negative' adjectives, taking into account any simple negators to avoid classing 'not happy' as positive? If so, are there any articles that discuss just why this strategy isn't realistic?\n"}, "846": {"topic": "Unsupervised Sentiment Analysis", "user_name": "TrindazTrindaz", "text": "\nA classic paper by Peter Turney (2002) explains a method to do unsupervised sentiment analysis (positive/negative classification) using only the words excellent and poor as a seed set. Turney uses the mutual information of other words with these two adjectives to achieve an accuracy of 74%.\n"}, "847": {"topic": "Unsupervised Sentiment Analysis", "user_name": "ruben_pants", "text": "\nI haven't tried doing untrained sentiment analysis such as you are describing, but off the top of my head I'd say you're oversimplifying the problem.  Simply analyzing adjectives is not enough to get a good grasp of the sentiment of a text; for example, consider the word 'stupid.'  Alone, you would classify that as negative, but if a product review were to have '... [x] product makes their competitors look stupid for not thinking of this feature first...' then the sentiment in there would definitely be positive. The greater context in which words appear definitely matters in something like this.  This is why an untrained bag-of-words approach alone (let alone an even more limited bag-of-adjectives) is not enough to tackle this problem adequately.\nThe pre-classified data ('training data') helps in that the problem shifts from trying to determine whether a text is of positive or negative sentiment from scratch, to trying to determine if the text is more similar to positive texts or negative texts, and classify it that way.  The other big point is that textual analyses such as sentiment analysis are often affected greatly by the differences of the characteristics of texts depending on domain.  This is why having a good set of data to train on (that is, accurate data from within the domain in which you are working, and is hopefully representative of the texts you are going to have to classify) is as important as building a good system to classify with.\nNot exactly an article, but hope that helps.\n"}, "848": {"topic": "Unsupervised Sentiment Analysis", "user_name": "Fred FooFred Foo", "text": "\nThe paper of Turney (2002) mentioned by larsmans is a good basic one. In a newer research, Li and He [2009] introduce an approach using Latent Dirichlet Allocation (LDA) to train a model that can classify an article's overall sentiment and topic simultaneously in a totally unsupervised manner. The accuracy they achieve is 84.6%.\n"}, "849": {"topic": "Unsupervised Sentiment Analysis", "user_name": "waffle paradoxwaffle paradox", "text": "\nI tried several methods of Sentiment Analysis for opinion mining in Reviews. \nWhat worked the best for me is the method described in Liu book: http://www.cs.uic.edu/~liub/WebMiningBook.html In this Book Liu and others, compared many strategies and discussed different papers on Sentiment Analysis and Opinion Mining.\nAlthough my main goal was to extract features in the opinions, I implemented a sentiment classifier to detect positive and negative classification of this features. \nI used NLTK for the pre-processing (Word tokenization, POS tagging) and the trigrams creation. Then also I used the Bayesian Classifiers inside this tookit to compare with other strategies Liu was pinpointing. \nOne of the methods relies on tagging as pos/neg every trigrram expressing this information, and using some classifier on this data. \nOther method I tried, and worked better (around 85% accuracy in my dataset), was calculating  the sum of scores of PMI (punctual mutual information) for every word in the sentence and the words excellent/poor as seeds of pos/neg class. \n"}, "850": {"topic": "Unsupervised Sentiment Analysis", "user_name": "        user325117\r", "text": "\nI tried spotting keywords using a dictionary of affect to predict the sentiment label at sentence level. Given the generality of the vocabulary (non domain dependent), the results were just about 61%. The paper is available in my homepage.\nIn a somewhat improved version, negation adverbs were considered. The whole system, named EmoLib, is available for demo:\nhttp://dtminredis.housing.salle.url.edu:8080/EmoLib/\nRegards,\n"}, "851": {"topic": "Unsupervised Sentiment Analysis", "user_name": "Trung HuynhTrung Huynh", "text": "\nDavid, \nI'm not sure if this helps but you may want to look into Jacob Perkin's blog post on using NLTK for sentiment analysis.\n"}, "852": {"topic": "Unsupervised Sentiment Analysis", "user_name": "LuchuxLuchux", "text": "\nThere are no magic \"shortcuts\" in sentiment analysis, as with any other sort of text analysis that seeks to discover the underlying \"aboutness,\" of a chunk of text. Attempting to short cut proven text analysis methods through simplistic \"adjective\" checking or similar approaches leads to ambiguity, incorrect classification, etc., that at the end of the day give you a poor accuracy read on sentiment. The more terse the source (e.g. Twitter), the more difficult the problem.\n"}, "853": {"topic": "Scikit Learn TfidfVectorizer : How to get top n terms with highest tf-idf score", "user_name": "Tommaso Di Noto", "text": "\nI am working on keyword extraction problem. Consider the very general case\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n\nt = \"\"\"Two Travellers, walking in the noonday sun, sought the shade of a widespreading tree to rest. As they lay looking up among the pleasant leaves, they saw that it was a Plane Tree.\n\n\"How useless is the Plane!\" said one of them. \"It bears no fruit whatever, and only serves to litter the ground with leaves.\"\n\n\"Ungrateful creatures!\" said a voice from the Plane Tree. \"You lie here in my cooling shade, and yet you say I am useless! Thus ungratefully, O Jupiter, do men receive their blessings!\"\n\nOur best blessings are often the least appreciated.\"\"\"\n\ntfs = tfidf.fit_transform(t.split(\" \"))\nstr = 'tree cat travellers fruit jupiter'\nresponse = tfidf.transform([str])\nfeature_names = tfidf.get_feature_names()\n\nfor col in response.nonzero()[1]:\n    print(feature_names[col], ' - ', response[0, col])\n\nand this gives me\n  (0, 28)   0.443509712811\n  (0, 27)   0.517461475101\n  (0, 8)    0.517461475101\n  (0, 6)    0.517461475101\ntree  -  0.443509712811\ntravellers  -  0.517461475101\njupiter  -  0.517461475101\nfruit  -  0.517461475101\n\nwhich is good. For any new document that comes in, is there a way to get the top n terms with the highest tfidf score?\n"}, "854": {"topic": "Scikit Learn TfidfVectorizer : How to get top n terms with highest tf-idf score", "user_name": "AbtPstAbtPst", "text": "\nYou have to do a little bit of a song and dance to get the matrices as numpy arrays instead, but this should do what you're looking for:\nfeature_array = np.array(tfidf.get_feature_names())\ntfidf_sorting = np.argsort(response.toarray()).flatten()[::-1]\n\nn = 3\ntop_n = feature_array[tfidf_sorting][:n]\n\nThis gives me:\narray([u'fruit', u'travellers', u'jupiter'], \n  dtype='<U13')\n\nThe argsort call is really the useful one, here are the docs for it. We have to do [::-1] because argsort only supports sorting small to large. We call flatten to reduce the dimensions to 1d so that the sorted indices can be used to index the 1d feature array. Note that including the call to flatten will only work if you're testing one document at at time.\nAlso, on another note, did you mean something like tfs = tfidf.fit_transform(t.split(\"\\n\\n\"))? Otherwise, each term in the multiline string is being treated as a \"document\". Using \\n\\n instead means that we are actually looking at 4 documents (one for each line), which makes more sense when you think about tfidf.\n"}, "855": {"topic": "Scikit Learn TfidfVectorizer : How to get top n terms with highest tf-idf score", "user_name": "humehume", "text": "\nSolution using sparse matrix itself (without .toarray())!\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(stop_words='english')\ncorpus = [\n    'I would like to check this document',\n    'How about one more document',\n    'Aim is to capture the key words from the corpus',\n    'frequency of words in a document is called term frequency'\n]\n\nX = tfidf.fit_transform(corpus)\nfeature_names = np.array(tfidf.get_feature_names())\n\n\nnew_doc = ['can key words in this new document be identified?',\n           'idf is the inverse document frequency caculcated for each of the words']\nresponses = tfidf.transform(new_doc)\n\n\ndef get_top_tf_idf_words(response, top_n=2):\n    sorted_nzs = np.argsort(response.data)[:-(top_n+1):-1]\n    return feature_names[response.indices[sorted_nzs]]\n  \nprint([get_top_tf_idf_words(response,2) for response in responses])\n\n#[array(['key', 'words'], dtype='<U9'),\n array(['frequency', 'words'], dtype='<U9')]\n\n"}, "856": {"topic": "Scikit Learn TfidfVectorizer : How to get top n terms with highest tf-idf score", "user_name": "", "text": "\nHere is a quick code for that:\n(documents is a list)\ndef get_tfidf_top_features(documents,n_top=10):\n  tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,  stop_words='english')\n  tfidf = tfidf_vectorizer.fit_transform(documents)\n  importance = np.argsort(np.asarray(tfidf.sum(axis=0)).ravel())[::-1]\n  tfidf_feature_names = np.array(tfidf_vectorizer.get_feature_names())\n  return tfidf_feature_names[importance[:n_top]]\n\n"}, "857": {"topic": "How to use Gensim doc2vec with pre-trained word vectors?", "user_name": "Savrige", "text": "\nI recently came across the doc2vec addition to Gensim. How can I use pre-trained word vectors (e.g. found in word2vec original website) with doc2vec?\nOr is doc2vec getting the word vectors from the same sentences it uses for paragraph-vector training?\nThanks.\n"}, "858": {"topic": "How to use Gensim doc2vec with pre-trained word vectors?", "user_name": "StergiosStergios", "text": "\nNote that the \"DBOW\" (dm=0) training mode doesn't require or even create word-vectors as part of the training. It merely learns document vectors that are good at predicting each word in turn (much like the word2vec skip-gram training mode). \n(Before gensim 0.12.0, there was the parameter train_words mentioned in another comment, which some documentation suggested will co-train words. However, I don't believe this ever actually worked. Starting in gensim 0.12.0, there is the parameter dbow_words, which works to skip-gram train words simultaneous with DBOW doc-vectors. Note that this makes training take longer \u2013 by a factor related to window. So if you don't need word-vectors, you may still leave this off.)\nIn the \"DM\" training method (dm=1), word-vectors are inherently trained during the process along with doc-vectors, and are likely to also affect the quality of the doc-vectors. It's theoretically possible to pre-initialize the word-vectors from prior data. But I don't know any strong theoretical or experimental reason to be confident this would improve the doc-vectors. \nOne fragmentary experiment I ran along these lines suggested the doc-vector training got off to a faster start \u2013 better predictive qualities after the first few passes \u2013 but this advantage faded with more passes. Whether you hold the word vectors constant or let them continue to adjust with the new training is also likely an important consideration... but which choice is better may depend on your goals, data set, and the quality/relevance of the pre-existing word-vectors. \n(You could repeat my experiment with the intersect_word2vec_format() method available in gensim 0.12.0, and try different levels of making pre-loaded vectors resistant-to-new-training via the syn0_lockf values. But remember this is experimental territory: the basic doc2vec results don't rely on, or even necessarily improve with, reused word vectors.) \n"}, "859": {"topic": "How to use Gensim doc2vec with pre-trained word vectors?", "user_name": "Simon Hessner", "text": "\nWell, I am recently using Doc2Vec too. And I was thinking of using LDA result as word vector and fix those word vectors to get a document vector. The result isn't very interesting though. Maybe it's just my data set isn't that good. \nThe code is below. Doc2Vec saves word vectors and document vectors together in dictionary doc2vecmodel.syn0. You can direct change the vector values. The only problem may be that you need to find out which position in syn0 represents which word or document. The vectors are stored in random order in dictionary syn0.\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\nfrom gensim import corpora, models, similarities\nimport gensim\nfrom sklearn import svm, metrics\nimport numpy\n\n#Read in texts into div_texts(for LDA and Doc2Vec)\ndiv_texts = []\nf = open(\"clean_ad_nonad.txt\")\nlines = f.readlines()\nf.close()\nfor line in lines:\n    div_texts.append(line.strip().split(\" \"))\n\n#Set up dictionary and MMcorpus\ndictionary = corpora.Dictionary(div_texts)\ndictionary.save(\"ad_nonad_lda_deeplearning.dict\")\n#dictionary = corpora.Dictionary.load(\"ad_nonad_lda_deeplearning.dict\")\nprint dictionary.token2id[\"junk\"]\ncorpus = [dictionary.doc2bow(text) for text in div_texts]\ncorpora.MmCorpus.serialize(\"ad_nonad_lda_deeplearning.mm\", corpus)\n\n#LDA training\nid2token = {}\ntoken2id = dictionary.token2id\nfor onemap in dictionary.token2id:\n    id2token[token2id[onemap]] = onemap\n#ldamodel = models.LdaModel(corpus, num_topics = 100, passes = 1000, id2word = id2token)\n#ldamodel.save(\"ldamodel1000pass.lda\")\n#ldamodel = models.LdaModel(corpus, num_topics = 100, id2word = id2token)\nldamodel = models.LdaModel.load(\"ldamodel1000pass.lda\")\nldatopics = ldamodel.show_topics(num_topics = 100, num_words = len(dictionary), formatted = False)\nprint ldatopics[10][1]\nprint ldatopics[10][1][1]\nldawordindex = {}\nfor i in range(len(dictionary)):\n    ldawordindex[ldatopics[0][i][1]] = i\n\n#Doc2Vec initialize\nsentences = []\nfor i in range(len(div_texts)):\n    string = \"SENT_\" + str(i)\n    sentence = models.doc2vec.LabeledSentence(div_texts[i], labels = [string])\n    sentences.append(sentence)\ndoc2vecmodel = models.Doc2Vec(sentences, size = 100, window = 5, min_count = 0, dm = 1)\nprint \"Initial word vector for word junk:\"\nprint doc2vecmodel[\"junk\"]\n\n#Replace the word vector with word vectors from LDA\nprint len(doc2vecmodel.syn0)\nindex2wordcollection = doc2vecmodel.index2word\nprint index2wordcollection\nfor i in range(len(doc2vecmodel.syn0)):\n    if index2wordcollection[i].startswith(\"SENT_\"):\n        continue\n    wordindex = ldawordindex[index2wordcollection[i]]\n    wordvectorfromlda = [ldatopics[j][wordindex][0] for j in range(100)]\n    doc2vecmodel.syn0[i] = wordvectorfromlda\n#print doc2vecmodel.index2word[26841]\n#doc2vecmodel.syn0[0] = [0 for i in range(100)]\nprint \"Changed word vector for word junk:\"\nprint doc2vecmodel[\"junk\"]\n\n#Train Doc2Vec\ndoc2vecmodel.train_words = False \nprint \"Initial doc vector for 1st document\"\nprint doc2vecmodel[\"SENT_0\"]\nfor i in range(50):\n    print \"Round: \" + str(i)\n    doc2vecmodel.train(sentences)\nprint \"Trained doc vector for 1st document\"\nprint doc2vecmodel[\"SENT_0\"]\n\n#Using SVM to do classification\nresultlist = []\nfor i in range(4143):\n    string = \"SENT_\" + str(i)\n    resultlist.append(doc2vecmodel[string])\nsvm_x_train = []\nfor i in range(1000):\n    svm_x_train.append(resultlist[i])\nfor i in range(2210,3210):\n    svm_x_train.append(resultlist[i])\nprint len(svm_x_train)\n\nsvm_x_test = []\nfor i in range(1000,2210):\n    svm_x_test.append(resultlist[i])\nfor i in range(3210,4143):\n    svm_x_test.append(resultlist[i])\nprint len(svm_x_test)\n\nsvm_y_train = numpy.array([0 for i in range(2000)])\nfor i in range(1000,2000):\n    svm_y_train[i] = 1\nprint svm_y_train\n\nsvm_y_test = numpy.array([0 for i in range(2143)])\nfor i in range(1210,2143):\n    svm_y_test[i] = 1\nprint svm_y_test\n\n\nsvc = svm.SVC(kernel='linear')\nsvc.fit(svm_x_train, svm_y_train)\n\nexpected = svm_y_test\npredicted = svc.predict(svm_x_test)\n\nprint(\"Classification report for classifier %s:\\n%s\\n\"\n      % (svc, metrics.classification_report(expected, predicted)))\nprint(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n\nprint doc2vecmodel[\"junk\"]\n\n"}, "860": {"topic": "How to use Gensim doc2vec with pre-trained word vectors?", "user_name": "gojomogojomo", "text": "\nThis forked version of gensim allows loading pre-trained word vectors for training doc2vec. Here you have an example on how to use it. The word vectors must be in the C-word2vec tool text format: one line per word vector where first comes a string representing the word and then space-separated float values, one for each dimension of the embedding.\nThis work belongs to a paper in which they claim that using pre-trained word embeddings actually helps building the document vectors. However I am getting almost the same results no matter I load the pre-trained embeddings or not.\nEdit: actually there is one remarkable difference in my experiments. When I loaded the pretrained embeddings I trained doc2vec for half of the iterations to get almost the same results (training longer than that produced worse results in my task).\n"}, "861": {"topic": "How to use Gensim doc2vec with pre-trained word vectors?", "user_name": "duhaime", "text": "\nRadim just posted a tutorial on the doc2vec features of gensim (yesterday, I believe - your question is timely!).\nGensim supports loading pre-trained vectors from the C implementation, as described in the gensim models.word2vec API documentation.\n"}, "862": {"topic": "Using NLTK and WordNet; how do I convert simple tense verb into its present, past or past participle form?", "user_name": "Peter Mortensen", "text": "\nUsing NLTK and WordNet, how do I convert simple tense verb into its present, past or past participle form?\nFor example:\nI want to write a function which would give me verb in expected form as follows.\nv = 'go'\npresent = present_tense(v)\nprint present # prints \"going\"\n\npast = past_tense(v)\nprint past # prints \"went\"\n\n"}, "863": {"topic": "Using NLTK and WordNet; how do I convert simple tense verb into its present, past or past participle form?", "user_name": "Software EnthusiasticSoftware Enthusiastic", "text": "\nWith the help of NLTK this can also be done. It can give the base form of the verb. But not the exact tense, but it still can be useful. Try the following code.\nfrom nltk.stem.wordnet import WordNetLemmatizer\nwords = ['gave','went','going','dating']\nfor word in words:\n    print word+\"-->\"+WordNetLemmatizer().lemmatize(word,'v')\n\nThe output is:\ngave-->give\nwent-->go\ngoing-->go\ndating-->date\n\nHave a look at Stack Overflow question NLTK WordNet Lemmatizer: Shouldn't it lemmatize all inflections of a word?.\n"}, "864": {"topic": "Using NLTK and WordNet; how do I convert simple tense verb into its present, past or past participle form?", "user_name": "CommunityBot", "text": "\nI think what you're looking for is the NodeBox::Linguistics library. It does exactly that:\nprint en.verb.present(\"gave\")\n>>> give\n\n"}, "865": {"topic": "Using NLTK and WordNet; how do I convert simple tense verb into its present, past or past participle form?", "user_name": "GunjanGunjan", "text": "\nFor Python3:\npip install pattern\n\nthen\nfrom pattern.en import conjugate, lemma, lexeme, PRESENT, SG\nprint (lemma('gave'))\nprint (lexeme('gave'))\nprint (conjugate(verb='give',tense=PRESENT,number=SG)) # he / she / it\n\nyields\ngive \n['give', 'gives', 'giving', 'gave', 'given'] \ngives\n\nthnks to @Agargara for pointing & authors of Pattern for their beautiful work, go support them ;-)\nPS. To use most of pattern's functionality in python 3.7+, you might want to use the trick described here\n"}, "866": {"topic": "Using NLTK and WordNet; how do I convert simple tense verb into its present, past or past participle form?", "user_name": "msbmsbmsbmsb", "text": "\nJWI (the WordNet library by MIT) also has a stemmer (WordNetStemmer) which converts different morphological forms of a word like (\"written\", \"writes\", \"wrote\") to their base form. It seems it works only for nouns (like plurals) and verbs though. \nWord Stemming in Java with WordNet and JWNL also shows how to do this kind of stemming using JWNL, another Java-based Wordnet library:\n"}, "867": {"topic": "TFIDF for Large Dataset", "user_name": "apurva.nandanapurva.nandan", "text": "\nI have a corpus which has around 8 million news articles, I need to get the TFIDF representation of them as a sparse matrix. I have been able to do that using scikit-learn for relatively lower number of samples, but I believe it can't be used for such a huge dataset as it loads the input matrix into memory first and that's an expensive process.\nDoes anyone know, what would be the best way to extract out the TFIDF vectors for large datasets?\n"}, "868": {"topic": "TFIDF for Large Dataset", "user_name": "Venkatachalam", "text": "\nGensim has an efficient tf-idf model and does not need to have everything in memory at once.\nYour corpus simply needs to be an iterable, so it does not need to have the whole corpus in memory at a time.\nThe make_wiki script runs over Wikipedia in about 50m on a laptop according to the comments.\n"}, "869": {"topic": "TFIDF for Large Dataset", "user_name": "Jonathan Villemaire-KrajdenJonathan Villemaire-Krajden", "text": "\nI believe you can use a HashingVectorizer to get a smallish csr_matrix out of your text data and then use a TfidfTransformer on that. Storing a sparse matrix of 8M rows and several tens of thousands of columns isn't such a big deal. Another option would be not to use TF-IDF at all- it could be the case that your system works reasonably well without it. \nIn practice you may have to subsample your data set- sometimes a system will do just as well by just learning from 10% of all available data. This is an empirical question, there is not way to tell in advance what strategy would be best for your task. I wouldn't worry about scaling to 8M document until I am convinced I need them (i.e. until I have seen a learning curve showing a clear upwards trend). \nBelow is something I was working on this morning as an example. You can see the performance of the system tends to improve as I add more documents, but it is already at a stage where it seems to make little difference. Given how long it takes to train, I don't think training it on 500 files is worth my time.\n\n"}, "870": {"topic": "TFIDF for Large Dataset", "user_name": "", "text": "\nI solve that problem using sklearn and pandas. \nIterate in your dataset once using pandas iterator and create a set of all words, after that use it in CountVectorizer vocabulary. With that the Count Vectorizer will generate a list of sparse matrix all of them with the same shape. Now is just use vstack to group them. The sparse matrix resulted have the same information (but the words in another order) as CountVectorizer object and fitted with all your data.\nThat solution is not the best if you consider the time complexity but is good for memory complexity. I use that in a dataset with 20GB +,\nI wrote a python code (NOT THE COMPLETE SOLUTION) that show the properties, write a generator or use pandas chunks for iterate in your dataset.\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom scipy.sparse import vstack\n\n\n# each string is a sample\ntext_test = [\n    'good people beauty wrong',\n    'wrong smile people wrong',\n    'idea beauty good good',\n]\n\n# scikit-learn basic usage\n\nvectorizer = CountVectorizer()\n\nresult1 = vectorizer.fit_transform(text_test)\nprint(vectorizer.inverse_transform(result1))\nprint(f\"First approach:\\n {result1}\")\n\n# Another solution is\n\nvocabulary = set()\n\nfor text in text_test:\n    for word in text.split():\n        vocabulary.add(word)\n\nvectorizer = CountVectorizer(vocabulary=vocabulary)\n\noutputs = [] \nfor text in text_test: # use a generator\n    outputs.append(vectorizer.fit_transform([text]))\n\n\nresult2 = vstack(outputs)\nprint(vectorizer.inverse_transform(result2))\n\nprint(f\"Second approach:\\n {result2}\")\n\nFinally, use TfidfTransformer.\n"}, "871": {"topic": "TFIDF for Large Dataset", "user_name": "mbatchkarovmbatchkarov", "text": "\nThe lengths of the documents The number of terms in common Whether the terms are common or unusual How many times each term appears\n"}, "872": {"topic": "Algorithms to detect phrases and keywords from text", "user_name": "mjv", "text": "\nI have around 100 megabytes of text, without any markup, divided to approximately 10,000 entries. I would like to automatically generate a 'tag' list. The problem is that there are word groups (i.e. phrases) that only make sense when they are grouped together.\nIf I just count the words, I get a large number of really common words (is, the, for, in, am, etc.). I have counted the words and the number of other words that are before and after it, but now I really cannot figure out what to do next The information relating to the 2 and 3 word phrases is present, but how do I extract this data?\n"}, "873": {"topic": "Algorithms to detect phrases and keywords from text", "user_name": "KimvaisKimvais", "text": "\nBefore anything, try to preserve the info about \"boundaries\" which comes in the input text.\n(if such info has not readily be lost, your question implies that maybe the tokenization has readily been done)\nDuring the tokenization (word parsing, in this case) process, look for patterns that may define expression boundaries (such as punctuation, particularly periods, and also multiple LF/CR separation,  use these.  Also words like \"the\", can often be used as boundaries.   Such expression boundaries are typically \"negative\", in a sense that they separate two token instances which are sure to not be included in the same expression.  A few positive boundaries are quotes, particularly double quotes.   This type of info may be useful to filter-out some of the n-grams (see next paragraph).  Also word sequencces such as \"for example\" or \"in lieu of\" or \"need to\" can be used as expression boundaries as well (but using such info is edging on using \"priors\" which I discuss later).\nWithout using external data (other than the input text), you can have a relative success with this by running statistics on the text's digrams and trigrams (sequence of 2 and 3 consecutive words).  Then [most] the sequences with a significant (*) number of instances will likely be the type of \"expression/phrases\" you are looking for.\nThis somewhat crude method will yield a few false positive, but on the whole may be workable.  Having filtered the n-grams known to cross \"boundaries\" as hinted in the first paragraph, may help significantly because in natural languages sentence ending and sentence starts tend to draw from a limited subset of the message space and hence produce combinations of token that may appear to be statistically well represented, but which are typically not semantically related. \nBetter methods (possibly more expensive, processing-wise, and design/investment-wise), will make the use of extra \"priors\" relevant to the domain and/or national languages of the input text. \n\nPOS (Part-Of-Speech) tagging is quite useful, in several ways (provides additional, more objective expression boundaries, and also \"noise\" words classes, for example all articles, even when used in the context of entities are typically of little in tag clouds such that the OP wants to produce.\nDictionaries, lexicons and the like can be quite useful too.  In particular, these which identify \"entities\" (aka instances in WordNet lingo) and their alternative forms.  Entities are very important for tag clouds (though they are not the only class of words found in them), and by identifying them, it is also possible to normalize them  (the many different expressions which can be used for say,\"Senator T. Kennedy\"), hence eliminate duplicates, but also increase the frequency of the underlying entities.\nif the corpus is structured as a document collection, it may be useful to use various tricks related to the TF (Term Frequency) and IDF (Inverse Document Frequency)\n\n[Sorry, gotta go, for now (plus would like more detail from your specific goals etc.).  I'll try and provide more detail and pointes later]\n[BTW, I want to plug here Jonathan Feinberg and Dervin Thunk responses from this post, as they provide excellent pointers, in terms of methods and tools for the kind of task at hand.  In particular, NTLK and Python-at-large provide an excellent framework for experimenting]\n"}, "874": {"topic": "Algorithms to detect phrases and keywords from text", "user_name": "", "text": "\nI'd start with a wonderful chapter, by Peter Norvig, in the O'Reilly book Beautiful Data. He provides the ngram data you'll need, along with beautiful Python code (which may solve your problems as-is, or with some modification) on his personal web site.\n"}, "875": {"topic": "Algorithms to detect phrases and keywords from text", "user_name": "mjvmjv", "text": "\nIt sounds like you're looking for collocation extraction. Manning and Sch\u00fctze devote a chapter to the topic, explaining and evaluating the 'proposed formulas' mentioned in the Wikipedia article I linked to.\nI can't fit the whole chapter into this response; hopefully some of their links will help. (NSP sounds particularly apposite.) nltk has a collocations module too, not mentioned by Manning and Sch\u00fctze since their book predates it.\nThe other responses posted so far deal with statistical language processing and n-grams more generally; collocations are a specific subtopic.\n"}, "876": {"topic": "Algorithms to detect phrases and keywords from text", "user_name": "Jonathan FeinbergJonathan Feinberg", "text": "\nDo a matrix for words. Then if there are two consecutive words then add one to that appropriate cell.\nFor example you have this sentence.\n\nmat['for']['example'] ++;\nmat['example']['you'] ++;\nmat['you']['have'] ++;\nmat['have']['this'] ++;\nmat['this']['sentence'] ++;\n\nThis will give you values for two consecutive words.\nYou can do this word three words also. Beware this requires O(n^3) memory.\nYou can also use a heap for storing the data like:\nheap['for example']++;\nheap['example you']++;\n\n"}, "877": {"topic": "Algorithms to detect phrases and keywords from text", "user_name": "", "text": "\nOne way would be to build yourself an automaton. most likely a Nondeterministic Finite Automaton(NFA).\nNFA\nAnother more simple way would be to create a file that has contains the words and/or word groups that you want to ignore, find, compare, etc. and store them in memory when the program starts and then you can compare the file you are parsing with the word/word groups that are contained in the file.\n"}, "878": {"topic": "Machine Learning and Natural Language Processing [closed]", "user_name": "Stephano", "text": "\n\n\n\n\n\n\nClosed. This question does not meet Stack Overflow guidelines. It is not currently accepting answers.                    \n\n\n\n\n\n\n\n\n\n\n Questions asking us to recommend or find a tool, library or favorite off-site resource are off-topic for Stack Overflow as they tend to attract opinionated answers and spam. Instead, describe the problem and what has been done so far to solve it.\n\n\nClosed 8 years ago.\n\n\n\n\n\n\n\r\n                        Improve this question\r\n                    \n\n\n\nAssume you know a student who wants to study Machine Learning and Natural Language Processing.\nWhat specific computer science subjects should they focus on and which programming languages are specifically designed to solve these types of problems?\nI am not looking for your favorite subjects and tools, but rather industry standards.\nExample: I'm guessing that knowing Prolog and Matlab might help them.  They also might want to study Discrete Structures*, Calculus, and Statistics.\n*Graphs and trees. Functions: properties, recursive definitions, solving recurrences. Relations: properties, equivalence, partial order. Proof techniques, inductive proof. Counting techniques and discrete probability.  Logic: propositional calculus, first-order predicate calculus. Formal reasoning: natural deduction, resolution. Applications to program correctness and automatic reasoning. Introduction to algebraic structures in computing.\n"}, "879": {"topic": "Machine Learning and Natural Language Processing [closed]", "user_name": "StephanoStephano", "text": "\nThis related stackoverflow question has some nice answers: What are good starting points for someone interested in natural language processing?\nThis is a very big field.  The prerequisites mostly consist of probability/statistics, linear algebra, and basic computer science, although Natural Language Processing requires a more intensive computer science background to start with (frequently covering some basic AI).  Regarding specific langauges: Lisp was created \"as an afterthought\" for doing AI research, while Prolog (with it's roots in formal logic) is especially aimed at Natural Language Processing, and many courses will use Prolog, Scheme, Matlab, R, or another functional language (e.g. OCaml is used for this course at Cornell) as they are very suited to this kind of analysis.  \nHere are some more specific pointers:\nFor Machine Learning, Stanford CS 229: Machine Learning is great: it includes everything, including full videos of the lectures (also up on iTunes), course notes, problem sets, etc., and it was very well taught by Andrew Ng.\nNote the prerequisites:\n\nStudents are expected to have the following background: Knowledge of\n  basic computer science principles and skills, at a level sufficient to write\n  a reasonably non-trivial computer program. Familiarity with the basic probability theory. \n  Familiarity with the basic linear algebra.\n\nThe course uses Matlab and/or Octave.  It also recommends the following readings (although the course notes themselves are very complete):\n\nChristopher Bishop, Pattern Recognition and Machine Learning. Springer, 2006.\nRichard Duda, Peter Hart and David Stork, Pattern Classification, 2nd ed. John Wiley & Sons, 2001.\nTom Mitchell, Machine Learning. McGraw-Hill, 1997.\nRichard Sutton and Andrew Barto, Reinforcement Learning: An introduction. MIT Press, 1998\n\nFor Natural Language Processing, the NLP group at Stanford provides many good resources.  The introductory course Stanford CS 224: Natural Language Processing includes all the lectures online and has the following prerequisites:\n\nAdequate experience with programming\n  and formal structures.  Programming\n  projects will be written in Java 1.5,\n  so knowledge of Java (or a willingness\n  to learn on your own) is required. \n  Knowledge of standard concepts in\n  artificial intelligence and/or\n  computational linguistics.  Basic\n  familiarity with logic, vector spaces,\n  and probability.\n\nSome recommended texts are:\n\nDaniel Jurafsky and James H. Martin. 2008. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition. Second Edition. Prentice Hall. \nChristopher D. Manning and Hinrich Sch\u00fctze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.\nJames Allen. 1995. Natural Language Understanding. Benjamin/Cummings, 2ed.\nGerald Gazdar and Chris Mellish. 1989. Natural Language Processing in Prolog. Addison-Wesley. (this is available online for free)\nFrederick Jelinek. 1998. Statistical Methods for Speech Recognition. MIT Press.\n\nThe prerequisite computational linguistics course requires basic computer programming and data structures knowledge, and uses the same text books.  The required articificial intelligence course is also available online along with all the lecture notes and uses:\n\nS. Russell and P. Norvig, Artificial Intelligence: A Modern Approach. Second Edition\n\nThis is the standard Artificial Intelligence text and is also worth reading.\nI use R for machine learning myself and really recommend it.  For this, I would suggest looking at The Elements of Statistical Learning, for which the full text is available online for free.  You may want to refer to the Machine Learning and Natural Language Processing views on CRAN for specific functionality.  \n"}, "880": {"topic": "Machine Learning and Natural Language Processing [closed]", "user_name": "CommunityBot", "text": "\nMy recommendation would be either or all (depending on his amount and area of interest) of these:\nThe Oxford Handbook of Computational Linguistics:\n\n(source: oup.com) \nFoundations of Statistical Natural Language Processing:\n\nIntroduction to Information Retrieval:\n\n"}, "881": {"topic": "Machine Learning and Natural Language Processing [closed]", "user_name": "ShaneShane", "text": "\nString algorithms, including suffix trees. Calculus and linear algebra. Varying varieties of statistics. Artificial intelligence optimization algorithms. Data clustering techniques... and a million other things. This is a very active field right now, depending on what you intend to do.\nIt doesn't really matter what language you choose to operate in. Python, for instance has the NLTK, which is a pretty nice free package for tinkering with computational linguistics.\n"}, "882": {"topic": "Machine Learning and Natural Language Processing [closed]", "user_name": "Glorfindel", "text": "\nI would say probabily & statistics is the most important prerequisite. Especially Gaussian Mixture Models (GMMs) and Hidden Markov Models (HMMs) are very important both in machine learning and natural language processing (of course these subjects may be part of the course if it is introductory).\nThen, I would say basic CS knowledge is also helpful, for example Algorithms, Formal Languages and basic Complexity theory.\n"}, "883": {"topic": "Machine Learning and Natural Language Processing [closed]", "user_name": "Fabian SteegFabian Steeg", "text": "\nStanford CS 224: Natural Language Processing course that was mentioned already includes also videos online (in addition to other course materials). The videos aren't linked to on the course website, so many people may not notice them.\n"}, "884": {"topic": "Machine Learning and Natural Language Processing [closed]", "user_name": "San JacintoSan Jacinto", "text": "\nJurafsky and Martin's Speech and Language Processing http://www.amazon.com/Speech-Language-Processing-Daniel-Jurafsky/dp/0131873210/ is very good. Unfortunately the draft second edition chapters are no longer free online now that it's been published :(\nAlso, if you're a decent programmer it's never too early to toy around with NLP programs. NLTK comes to mind (Python). It has a book you can read free online that was published (by OReilly I think). \n"}, "885": {"topic": "Machine Learning and Natural Language Processing [closed]", "user_name": "", "text": "\nHow about Markdown and an Introduction to Parsing Expression Grammars (PEG) posted by cletus on his site cforcoding?\nANTLR seems like a good place to start for natural language processing. I'm no expert though.\n"}, "886": {"topic": "Machine Learning and Natural Language Processing [closed]", "user_name": "3lectrologos3lectrologos", "text": "\nBroad question, but I certainly think that a knowledge of finite state automata and hidden Markov models would be useful. That requires knowledge of statistical learning, Bayesian parameter estimation, and entropy.\nLatent semantic indexing is a commonly yet recently used tool in many machine learning problems. Some of the methods are rather easy to understand. There are a bunch of potential basic projects. \n\nFind co-occurrences in text corpora for document/paragraph/sentence clustering.\nClassify the mood of a text corpus.\nAutomatically annotate or summarize a document.\nFind relationships among separate documents to automatically generate a \"graph\" among the documents.\n\nEDIT: Nonnegative matrix factorization (NMF) is a tool that has grown considerably in popularity due to its simplicity and effectiveness. It's easy to understand. I currently research the use of NMF for music information retrieval; NMF has shown to be useful for latent semantic indexing of text corpora, as well. Here is one paper. PDF\n"}, "887": {"topic": "Machine Learning and Natural Language Processing [closed]", "user_name": "michaumichau", "text": "\nProlog will only help them academically it is also limited for logic constraints and semantic NLP based work. Prolog is not yet an industry friendly language so not yet practical in real-world. And, matlab also is an academic based tool unless they are doing a lot of scientific or quants based work they wouldn't really have much need for it. To start of they might want to pick up the 'Norvig' book and enter the world of AI get a grounding in all the areas. Understand some basic probability, statistics, databases, os, datastructures, and most likely an understanding and experience with a programming language. They need to be able to prove to themselves why AI techniques work and where they don't. Then look to specific areas like machine learning and NLP in further detail. In fact, the norvig book sources references after every chapter so they already have a lot of further reading available. There are a lot of reference material available for them over internet, books, journal papers for guidance. Don't just read the book try to build tools in a programming language then extrapolate 'meaningful' results. Did the learning algorithm actually learn as expected, if it didn't why was this the case, how could it be fixed.\n"}, "888": {"topic": "How to detect language of user entered text? [closed]", "user_name": "CommunityBot", "text": "\n\n\n\n\n\n\nClosed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.                    \n\n\n\n\n\n\n\n\n\n\n We don\u2019t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\n\n\nClosed 6 years ago.\n\n\n\n\n\n\n\r\n                        Improve this question\r\n                    \n\n\n\nI am dealing with an application that is accepting user input in different languages (currently 3 languages fixed). The requirement is that users can enter text and dont bother to select the language via a provided checkbox in the UI.\nIs there an existing Java library to detect the language of a text?\nI want something like this:\ntext = \"To be or not to be thats the question.\"\n\n// returns ISO 639 Alpha-2 code\nlanguage = detect(text);\n\nprint(language);\n\nresult:\nEN\n\nI dont want to know how to create a language detector by myself (i have seen plenty of blogs trying to do that). The library should provide a simple APi and also work completely offline. Open-source or commercial closed doesn't matter.\ni also found this questions on SO (and a few more):\nHow to detect language\nHow to detect language of text?\n"}, "889": {"topic": "How to detect language of user entered text? [closed]", "user_name": "ManBugraManBugra", "text": "\nThis Language Detection Library for Java should give more than 99% accuracy for 53 languages. \nAlternatively, there is Apache Tika, a library for content analysis that offers much more than just language detection. \n"}, "890": {"topic": "How to detect language of user entered text? [closed]", "user_name": "yvespeirsmanyvespeirsman", "text": "\nHere are two options \n\nLanguageIdentifier\nRosette Language Identifier\n\n"}, "891": {"topic": "How to detect language of user entered text? [closed]", "user_name": "bmargulies", "text": "\nGoogle offers an API that can do this for you. I just stumbled across this yesterday and didn't keep a link, but if you, umm, Google for it you should manage to find it.\nThis was somewhere near the description of their translation API, which will translate text for you into any language you like. There's another call just for guessing the input language.\nGoogle is among the world's leaders in mechanical translation; they base their stuff on extremely large corpuses of text (most of the Internet, kinda) and a statistical approach that usually \"gets\" it right simply by virtue of having a huge sample space.\nEDIT: Here's the link: http://code.google.com/apis/ajaxlanguage/\nEDIT 2: If you insist on \"offline\": A well upvoted answer was the suggestion of Guess-Language. It's a C++ library and handles about 60 languages.\n"}, "892": {"topic": "How to detect language of user entered text? [closed]", "user_name": "Jay AskrenJay Askren", "text": "\nAn alternative is the JLangDetect but it's not very robust and has a limited language base.  Good thing is it's an Apache license, if it satisfies your requirements, you can use it.  I'm guessing here, but do you release the space key between the single and double jump event?\nIn version 0.4 it is very robust. I have been using this in many projects of my own and never had any major problems. Also, when it comes to speed it is comparable to very specialized language detectors (e.g., few languages only).\n"}, "893": {"topic": "How to detect language of user entered text? [closed]", "user_name": "", "text": "\nDetect Language API also provides Java client.\nExample:\nList<Result> results = DetectLanguage.detect(\"Hello world\");\n\nResult result = results.get(0);\n\nSystem.out.println(\"Language: \" + result.language);\nSystem.out.println(\"Is reliable: \" + result.reliable);\nSystem.out.println(\"Confidence: \" + result.confidence);\n\n"}, "894": {"topic": "How to detect language of user entered text? [closed]", "user_name": "Carl SmotriczCarl Smotricz", "text": "\nhere is another option : Language Detection Library for Java\nthis is a library in Java. \n"}, "895": {"topic": "How to detect language of user entered text? [closed]", "user_name": "James Daily", "text": "\nJust a working code from already available solution from cybozu labs:\n\npackage com.et.generate;\n\nimport java.util.ArrayList;\nimport com.cybozu.labs.langdetect.Detector;\nimport com.cybozu.labs.langdetect.DetectorFactory;\nimport com.cybozu.labs.langdetect.LangDetectException;\nimport com.cybozu.labs.langdetect.Language;\n\npublic class LanguageCodeDetection {\n\n    public void init(String profileDirectory) throws LangDetectException {\n        DetectorFactory.loadProfile(profileDirectory);\n    }\n    public String detect(String text) throws LangDetectException {\n        Detector detector = DetectorFactory.create();\n        detector.append(text);\n        return detector.detect();\n    }\n    public ArrayList<Language> detectLangs(String text) throws LangDetectException {\n        Detector detector = DetectorFactory.create();\n        detector.append(text);\n        return detector.getProbabilities();\n    }\n    public static void main(String args[]) {\n        try {\n            LanguageCodeDetection ld = new  LanguageCodeDetection();\n\n            String profileDirectory = \"C:/profiles/\";\n            ld.init(profileDirectory);\n            String text = \"\u041a\u0440\u0435\u043c\u043b\u044c \u0440\u043e\u0441\u0441\u0438\u0439\";\n            System.out.println(ld.detectLangs(text));\n            System.out.println(ld.detect(text));\n        } catch (LangDetectException e) {\n            e.printStackTrace();\n        }\n    }\n\n}\n\nOutput:\n[ru:0.9999983255911719]\nru\n\nProfiles can be downloaded from:\nhttps://language-detection.googlecode.com/files/langdetect-09-13-2011.zip\n"}, "896": {"topic": "Entity Extraction/Recognition with free tools while feeding Lucene Index", "user_name": "CommunityBot", "text": "\nI'm currently investigating the options to extract person names, locations, tech words and categories from text (a lot articles from the web) which will then feeded into a Lucene/ElasticSearch index. The additional information is then added as metadata and should increase precision of the search. \nE.g. when someone queries 'wicket' he should be able to decide whether he means the cricket sport or the Apache project. I tried to implement this on my own with minor success so far. Now I found a lot tools, but I'm not sure if they are suited for this task and which of them integrates good with Lucene or if precision of entity extraction is high enough.\n\nDbpedia Spotlight, the demo looks very promising\nOpenNLP requires training. Which training data to use?\nOpenNLP tools\nStanbol\nNLTK\nbalie\nUIMA\nGATE -> example code\nApache Mahout\nStanford CRF-NER\nmaui-indexer\nMallet\nIllinois Named Entity Tagger Not open source but free\nwikipedianer data\n\nMy questions:\n\nDoes anyone have experience with some of the listed tools above and its precision/recall? Or if there is training data required + available.\nAre there articles or tutorials where I can get started with entity extraction(NER) for each and every tool?\nHow can they be integrated with Lucene?\n\nHere are some questions related to that subject:\n\nDoes an algorithm exist to help detect the \"primary topic\" of an English sentence?\nNamed Entity Recognition Libraries for Java\nNamed entity recognition with Java\n\n"}, "897": {"topic": "Entity Extraction/Recognition with free tools while feeding Lucene Index", "user_name": "KarussellKarussell", "text": "\nThe problem you are facing in the 'wicket' example is called entity disambiguation, not entity extraction/recognition (NER). NER can be useful but only when the categories are specific enough. Most NER systems doesn't have enough granularity to distinguish between a sport and a software project (both types would fall outside the typically recognized types: person, org, location).\nFor disambiguation, you need a knowledge base against which entities are being disambiguated. DBpedia is a typical choice due to its broad coverage. See my answer for How to use DBPedia to extract Tags/Keywords from content? where I provide more explanation, and mentions several tools for disambiguation including:\n\nZemanta \nMaui-indexer \nDbpedia Spotlight\nExtractiv (my company)\n\nThese tools often use a language-independent API like REST, and I do not know that they directly provide Lucene support, but I hope my answer has been beneficial for the problem you are trying to solve.\n"}, "898": {"topic": "Entity Extraction/Recognition with free tools while feeding Lucene Index", "user_name": "CommunityBot", "text": "\nYou can use OpenNLP to extract names of people, places, organisations without training. You just use pre-exisiting models which can be downloaded from here: http://opennlp.sourceforge.net/models-1.5/\nFor an example on how to use one of these model see: http://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html#tools.namefind\n"}, "899": {"topic": "Entity Extraction/Recognition with free tools while feeding Lucene Index", "user_name": "John LehmannJohn Lehmann", "text": "\nRosoka is a commercial product that provides a computation of \"Salience\" which measures the importance of the term or entity to the document. Salience is based on the linguistic usage and not the frequency. Using the salience values you can determine the primary topic of the document as a whole. \n The output is in your choice of XML or JSON which makes it very easy to use with Lucene.\n It is written in java. \n There is an Amazon Cloud version available at https://aws.amazon.com/marketplace/pp/B00E6FGJZ0. The cost to try it out is $0.99/hour. The Rosoka Cloud version does not have all of the Java API features available to it that the full Rosoka does.\n Yes both versions perform entity and term disambiguation based on the linguistic usage.\nThe disambiguation, whether human or software requires that there is enough contextual information to be able to determine the difference. The context may be contained within the document, within a corpus constraint, or within the context of the users.  The former being more specific, and the later having the greater potential ambiguity. I.e. typing in the key word \"wicket\" into a Google search, could refer to either cricket, Apache software or the Star Wars Ewok character (i.e. an Entity). The general The sentence \"The wicket is guarded by the batsman\" has contextual clues within the sentence to interpret it as an object. \"Wicket Wystri Warrick was a male Ewok scout\" should enterpret \"Wicket\" as the given name of the person entity \"Wicket Wystri Warrick\". \"Welcome to Apache Wicket\" has the contextual clues that \"Wicket\" is part of a place name, etc. \n"}, "900": {"topic": "Entity Extraction/Recognition with free tools while feeding Lucene Index", "user_name": "Abul FayesAbul Fayes", "text": "\nLately I have been fiddling with stanford crf ner. They have released quite a few versions http://nlp.stanford.edu/software/CRF-NER.shtml\nThe good thing is you can train your own classifier. You should follow the link which has the guidelines on how to train your own NER.  http://nlp.stanford.edu/software/crf-faq.shtml#a\nUnfortunately, in my case, the named entities are not efficiently extracted from the document. Most of the entities go undetected. \nJust in case you find it useful. \n"}, "901": {"topic": "Use of PunktSentenceTokenizer in NLTK", "user_name": "arqam", "text": "\nI am learning Natural Language Processing using NLTK.\nI came across the code using PunktSentenceTokenizer whose actual use I cannot understand in the given code. The code is given :\nimport nltk\nfrom nltk.corpus import state_union\nfrom nltk.tokenize import PunktSentenceTokenizer\n\ntrain_text = state_union.raw(\"2005-GWBush.txt\")\nsample_text = state_union.raw(\"2006-GWBush.txt\")\n\ncustom_sent_tokenizer = PunktSentenceTokenizer(train_text) #A\n\ntokenized = custom_sent_tokenizer.tokenize(sample_text)   #B\n\ndef process_content():\ntry:\n    for i in tokenized[:5]:\n        words = nltk.word_tokenize(i)\n        tagged = nltk.pos_tag(words)\n        print(tagged)\n\nexcept Exception as e:\n    print(str(e))\n\n\nprocess_content()\n\nSo, why do we use PunktSentenceTokenizer.  And what is going on in the line marked A and B. I mean there is a training text and the other a sample text, but what is the need for two data sets to get the Part of Speech tagging.\nLine marked as A and B is which I am not able to understand.\nPS : I did try to look in the NLTK book but could not understand what is the real use of PunktSentenceTokenizer\n"}, "902": {"topic": "Use of PunktSentenceTokenizer in NLTK", "user_name": "arqamarqam", "text": "\nPunktSentenceTokenizer is the abstract class for the default sentence tokenizer, i.e. sent_tokenize(), provided in NLTK. It is an implmentation of Unsupervised Multilingual Sentence\nBoundary Detection (Kiss and Strunk (2005). See https://github.com/nltk/nltk/blob/develop/nltk/tokenize/init.py#L79\nGiven a paragraph with multiple sentence, e.g:\n>>> from nltk.corpus import state_union\n>>> train_text = state_union.raw(\"2005-GWBush.txt\").split('\\n')\n>>> train_text[11]\nu'Two weeks ago, I stood on the steps of this Capitol and renewed the commitment of our nation to the guiding ideal of liberty for all. This evening I will set forth policies to advance that ideal at home and around the world. '\n\nYou can use the sent_tokenize():\n>>> sent_tokenize(train_text[11])\n[u'Two weeks ago, I stood on the steps of this Capitol and renewed the commitment of our nation to the guiding ideal of liberty for all.', u'This evening I will set forth policies to advance that ideal at home and around the world. ']\n>>> for sent in sent_tokenize(train_text[11]):\n...     print sent\n...     print '--------'\n... \nTwo weeks ago, I stood on the steps of this Capitol and renewed the commitment of our nation to the guiding ideal of liberty for all.\n--------\nThis evening I will set forth policies to advance that ideal at home and around the world. \n--------\n\nThe sent_tokenize() uses a pre-trained model from nltk_data/tokenizers/punkt/english.pickle. You can also specify other languages, the list of available languages with pre-trained models in NLTK are:\nalvas@ubi:~/nltk_data/tokenizers/punkt$ ls\nczech.pickle     finnish.pickle  norwegian.pickle   slovene.pickle\ndanish.pickle    french.pickle   polish.pickle      spanish.pickle\ndutch.pickle     german.pickle   portuguese.pickle  swedish.pickle\nenglish.pickle   greek.pickle    PY3                turkish.pickle\nestonian.pickle  italian.pickle  README\n\nGiven a text in another language, do this:\n>>> german_text = u\"Die Orgellandschaft S\u00fcdniedersachsen umfasst das Gebiet der Landkreise Goslar, G\u00f6ttingen, Hameln-Pyrmont, Hildesheim, Holzminden, Northeim und Osterode am Harz sowie die Stadt Salzgitter. \u00dcber 70 historische Orgeln vom 17. bis 19. Jahrhundert sind in der s\u00fcdnieders\u00e4chsischen Orgellandschaft vollst\u00e4ndig oder in Teilen erhalten. \"\n\n>>> for sent in sent_tokenize(german_text, language='german'):\n...     print sent\n...     print '---------'\n... \nDie Orgellandschaft S\u00fcdniedersachsen umfasst das Gebiet der Landkreise Goslar, G\u00f6ttingen, Hameln-Pyrmont, Hildesheim, Holzminden, Northeim und Osterode am Harz sowie die Stadt Salzgitter.\n---------\n\u00dcber 70 historische Orgeln vom 17. bis 19. Jahrhundert sind in der s\u00fcdnieders\u00e4chsischen Orgellandschaft vollst\u00e4ndig oder in Teilen erhalten. \n---------\n\nTo train your own punkt model, see https://github.com/nltk/nltk/blob/develop/nltk/tokenize/punkt.py and training data format for nltk punkt\n"}, "903": {"topic": "Use of PunktSentenceTokenizer in NLTK", "user_name": "CommunityBot", "text": "\nPunktSentenceTokenizer is an sentence boundary detection algorithm that must be trained to be used [1]. NLTK already includes a pre-trained version of the PunktSentenceTokenizer. \nSo if you use initialize the tokenizer without any arguments, it will default to the pre-trained version:\nIn [1]: import nltk\nIn [2]: tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\nIn [3]: txt = \"\"\" This is one sentence. This is another sentence.\"\"\"\nIn [4]: tokenizer.tokenize(txt)\nOut[4]: [' This is one sentence.', 'This is another sentence.']\n\nYou can also provide your own training data to train the tokenizer before using it. Punkt tokenizer uses an unsupervised algorithm, meaning you just train it with regular text.\ncustom_sent_tokenizer = PunktSentenceTokenizer(train_text)\nFor most of the cases, it is totally fine to use the pre-trained version. So you can simply initialize the tokenizer without providing any arguments.\nSo \"what all this has to do with POS tagging\"? The NLTK POS tagger works with tokenized sentences, so you need to break your text into sentences and word tokens before you can POS tag.\nNLTK's documentation.\n[1] Kiss and Strunk, \"\nUnsupervised Multilingual Sentence Boundary Detection\"  \n"}, "904": {"topic": "Use of PunktSentenceTokenizer in NLTK", "user_name": "alvasalvas", "text": "\nYou can refer below link to get more insight on usage of PunktSentenceTokenizer.\nIt vividly explains why PunktSentenceTokenizer is used instead of sent-tokenize() with regard to your case.\nhttp://nlpforhackers.io/splitting-text-into-sentences/\n"}, "905": {"topic": "Use of PunktSentenceTokenizer in NLTK", "user_name": "", "text": "\ndef process_content(corpus):\n\n    tokenized = PunktSentenceTokenizer().tokenize(corpus)\n\n    try:\n        for sent in tokenized:\n            words = nltk.word_tokenize(sent)\n            tagged = nltk.pos_tag(words)\n            print(tagged)\n    except Exception as e:\n        print(str(e))\n\nprocess_content(train_text)\n\nWithout even training it on other text data it works the same as it is pre-trained.\n"}, "906": {"topic": "What are good starting points for someone interested in natural language processing? [closed]", "user_name": "CommunityBot", "text": "\n\n\n\n\n\n\n\n\n\r\nAs it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.                    \n\n\nClosed 9 years ago.\n\n\n\nQuestion\nSo I've recently came up with some new possible projects that would have to deal with deriving 'meaning' from text submitted and generated by users.\nNatural language processing is the field that deals with these kinds of issues, and after some initial research I found the OpenNLP Hub and university collaborations like the attempto project. And stackoverflow has this.\nIf anyone could link me to some good resources, from reseach papers and introductionary texts to apis, I'd be happier than a 6 year-old kid opening his christmas presents!\nUpdate\nThrough one of your recommendations I've found opencyc ('the world's largest and most complete general knowledge base and commonsense reasoning engine'). Even more amazing still, there's a project that is a distilled version of opencyc called UMBEL. It features semantic data in rdf/owl/skos n3 syntax.\nI've also stumbled upon antlr, a parser generator for 'constructing recognizers, interpreters, compilers, and translators from grammatical descriptions'.\nAnd there's a question on here by me, that lists tons of free and open data.\nThanks stackoverflow community!\n"}, "907": {"topic": "What are good starting points for someone interested in natural language processing? [closed]", "user_name": "kitsunekitsune", "text": "\nTough call, NLP is a much wider field than most people think it is. Basically, language can be split up into several categories, which will require you to learn totally different things.\nBefore I start, let me tell you that I doubt you'll have any notable success (as a professional, at least) without having a degree in some (closely related) field. There is a lot of theory involved, most of it is dry stuff and hard to learn. You'll need a lot of endurance and most of all: time.\nIf you're interested in the meaning of text, well, that's the Next Big Thing. Semantic search engines are predicted as initiating Web 3.0, but we're far from 'there' yet. Extracting logic from a text is dependant on several steps:\n\nTokenization, Chunking\nDisambiguation on a lexical level (Time flies like an arrow, but fruit flies like a banana.)\nSyntactic Parsing\nMorphological analysis (tense, aspect, case, number, whatnot)\n\nA small list, off the top of my head. There's more :-), and many more details to each point. For example, when I say \"parsing\", what is this? There are many different parsing algorithms, and there are just as many parsing formalisms. Among the most powerful are Tree-adjoining grammar and Head-driven phrase structure grammar. But both of them are hardly used in the field (for now). Usually, you'll be dealing with some half-baked generative approach, and will have to conduct morphological analysis yourself.\nGoing from there to semantics is a big step. A Syntax/Semantics interface is dependant both, on the syntactic and semantic framework employed, and there is no single working solution yet. On the semantic side, there's classic generative semantics, then there is Discourse Representation Theory, dynamic semantics, and many more. Even the logical formalism everything is based on is still not well-defined. Some say one should use first-order logic, but that hardly seems sufficient; then there is intensional logic, as used by Montague, but that seems overly complex, and computationally unfeasible. There also is dynamic logic (Groenendijk and Stokhof have pioneered this stuff. Great stuff!) and very recently, this summer actually, Jeroen Groenendijk presented a new formalism, Inquisitive Semantics, also very interesting.\nIf you want to get started on a very simple level, read Blackburn and Bos (2005), it's great stuff, and the de-facto introduction to Computational Semantics! I recently extended their system to cover the partition-theory of questions (question answering is a beast!), as proposed by Groenendijk and Stokhof (1982), but unfortunately, the theory has a complexity of O(n\u00b2) over the domain of individuals. While doing so, I found B&B's implementation to be a bit, erhm\u2026 hackish, at places. Still, it is going to really, really help you dive into computational semantics, and it is still a very impressive showcase of what can be done. Also, they deserve extra cool-points for implementing a grammar that is settled in Pulp Fiction (the movie).\nAnd while I'm at it, pick up Prolog. A lot of research in computational semantics is based on Prolog. Learn Prolog Now! is a good intro. I can also recommend \"The Art of Prolog\" and Covington's \"Prolog Programming in Depth\" and \"Natural Language Processing for Prolog Programmers\", the former of which is available for free online.\n"}, "908": {"topic": "What are good starting points for someone interested in natural language processing? [closed]", "user_name": "Aleksandar DimitrovAleksandar Dimitrov", "text": "\nChomsky is totally the wrong source to look to for NLP (and he'd say as much himself, emphatically)--see: \"Statistical Methods and Linguistics\" by Abney.\nJurafsky and Martin, mentioned above, is a standard reference, but I myself prefer Manning and Sch\u00fctze. If you're serious about NLP you'll probably want to read both. There are videos of one of Manning's courses available online.\n"}, "909": {"topic": "What are good starting points for someone interested in natural language processing? [closed]", "user_name": "        user18015user18015", "text": "\nIf you get through Prolog until the DCG chapter in Learn Prolog Now! mentioned by Mr. Dimitrov above, you'll have a good beginning at getting some semantics into your system, since Prolog gives you a very simple way of maintaining a database of knowledge and belief, which can be updated through question-answering.\nAs regards the literature, I have one major recommendation for you: run out and buy Speech and Language Processing by Jurafsky & Martin. It is pretty much the book on NLP (the first chapter is available online); used in a frillion university courses but also very readable for the non-linguist and practically oriented, while at the same time going fairly deep into the linguistics problems. I really cannot recommend it enough. Chapters 17, 18 and 21 seem to be what you're looking for (14, 15 and 18 in the first edition); they show you simple lambda notation which translates pretty well to Prolog DCG's with features.\nOh, btw, on getting the masters in linguistics; if NL semantics is what you're into, I'd rather recommend taking all the AI-related courses you can find (although any courses on \"plain\" linguistic semantics, logic, logical semantics, DRT, LFG/HPSG/CCG, NL parsing, formal linguistic theory, etc. wouldn't hurt...) \nReading Chomsky's original literature is not really useful; as far as I know there are no current implementations that directly correspond to his theories, all the useful stuff of his is pretty much subsumed by other theories (and anyone who stays near linguists for any matter of time will absorb knowledge of Chomsky by osmosis).\n"}, "910": {"topic": "What are good starting points for someone interested in natural language processing? [closed]", "user_name": "", "text": "\nI'd highly recommend playing around with the NLTK and reading the NLTK Book.  The NLTK is very powerful and easy to get into.\n"}, "911": {"topic": "What are good starting points for someone interested in natural language processing? [closed]", "user_name": "unhammerunhammer", "text": "\nYou could try reading up a bit on phrase structured grammers, which is basically the mathematics behind much language processessing.  It's actually not that heavy, being largely based on set and graph theory.  I studied it many moons ago as part of a discrete math course, and I guess there are many good references available at this stage.\nEdit:Not as much as I expected on google, although this one looks like a good learning source.\n"}, "912": {"topic": "What are good starting points for someone interested in natural language processing? [closed]", "user_name": "theycallmemortytheycallmemorty", "text": "\nOne of the early explorers into NLP is Noam Chomsky; he wrote small books on the subject in the 50s through the 70s. You may find that engaging reading.\n"}, "913": {"topic": "What are good starting points for someone interested in natural language processing? [closed]", "user_name": "SmacLSmacL", "text": "\nCycorp have a short description of how their Cyc knowledge base derives meaning from sentences.\nBy utilising a massive knowledge base of common facts, the system can determine the most logical parse of a sentence.\n"}, "914": {"topic": "What are good starting points for someone interested in natural language processing? [closed]", "user_name": "Paul NathanPaul Nathan", "text": "\nA simpler place to begin with the building blocks is the look at the documentation for a package that attempts to do it.  I'd recommend the Python [Natural Language Toolkit (NLTK)1, particularly because of their well-written, free book, which is filled with examples.  It won't get you all the way to what you want (which is an AI-hard problem), but it will give you a good footing.   NLTK has parsers, chunkers, context-free grammars, and more.  \n"}, "915": {"topic": "What are good starting points for someone interested in natural language processing? [closed]", "user_name": "Richard PooleRichard Poole", "text": "\nThis is really hard stuff.  I'd start off by getting at least a Masters in Linguistics, and then work towards my PhD in computer science, concentrating on NLP.  \nThe problem is that most of us don't have the understanding of what language is.  And without that understanding, it's bloody tough to implement a solution.\nOther comments give some readings, which are probably fine if you want to get started playing around with a small subset of the problem, but in order to come up with a really robust solution, then there are no shortcuts.  You need the academic background in both disciplines.\n"}, "916": {"topic": "What are good starting points for someone interested in natural language processing? [closed]", "user_name": "Gregg LindGregg Lind", "text": "\nA very enjoyable readable introduction is The Language Instinct by Steven Pinker. It goes into the Chomsky stuff and also tells interesting stories from the evolutionary biology angle. Might be worth starting with something like that before diving into Chomsky's papers and related work, if you're new to the subject.\n"}, "917": {"topic": "Restore original text from Keras\u2019s imdb dataset", "user_name": "Marcin Mo\u017cejko", "text": "\nRestore original text from Keras\u2019s imdb dataset\nI want to restore imdb\u2019s original text from Keras\u2019s imdb dataset.\nFirst, when I load Keras\u2019s imdb dataset, it returned sequence of word index.\n\n>>> (X_train, y_train), (X_test, y_test) = imdb.load_data()\n>>> X_train[0]\n[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n\nI found imdb.get_word_index method(), it returns word index dictionary like {\u2018create\u2019: 984, \u2018make\u2019: 94,\u2026}. For converting, I create index word dictionary.\n\n\n>>> word_index = imdb.get_word_index()\n>>> index_word = {v:k for k,v in word_index.items()}\n\nThen, I tried to restore original text like following.\n\n>>> ' '.join(index_word.get(w) for w in X_train[5])\n\"the effort still been that usually makes for of finished sucking ended cbc's an because before if just though something know novel female i i slowly lot of above freshened with connect in of script their that out end his deceptively i i\"\n\nI\u2019m not good at English, but I know this sentence is something strange.\nWhy is this happened? How can I restore original text?\n"}, "918": {"topic": "Restore original text from Keras\u2019s imdb dataset", "user_name": "HironsanHironsan", "text": "\nYour example is coming out as gibberish, it's much worse than just some missing stop words.\nIf you re-read the docs for the start_char, oov_char, and index_from parameters of the [keras.datasets.imdb.load_data](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification\n) method they explain what is happening:\nstart_char: int. The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character.\noov_char: int. words that were cut out because of the num_words or skip_top limit will be replaced with this character.\nindex_from: int. Index actual words with this index and higher.\nThat dictionary you inverted assumes the word indices start from 1.\nBut the indices returned my keras have <START> and <UNKNOWN> as indexes 1 and 2. (And it assumes you will use 0 for <PADDING>).\nThis works for me:\nimport keras\nNUM_WORDS=1000 # only use top 1000 words\nINDEX_FROM=3   # word index offset\n\ntrain,test = keras.datasets.imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\ntrain_x,train_y = train\ntest_x,test_y = test\n\nword_to_id = keras.datasets.imdb.get_word_index()\nword_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\nword_to_id[\"<PAD>\"] = 0\nword_to_id[\"<START>\"] = 1\nword_to_id[\"<UNK>\"] = 2\nword_to_id[\"<UNUSED>\"] = 3\n\nid_to_word = {value:key for key,value in word_to_id.items()}\nprint(' '.join(id_to_word[id] for id in train_x[0] ))\n\nThe punctuation is missing, but that's all:\n\"<START> this film was just brilliant casting <UNK> <UNK> story\n direction <UNK> really <UNK> the part they played and you could just\n imagine being there robert <UNK> is an amazing actor ...\"\n\n"}, "919": {"topic": "Restore original text from Keras\u2019s imdb dataset", "user_name": "", "text": "\nYou can get the original dataset without stop words removed using get_file from  keras.utils.data_utils:\npath = get_file('imdb_full.pkl',\n               origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n                md5_hash='d091312047c43cf9e4e38fef92437263')\nf = open(path, 'rb')\n(training_data, training_labels), (test_data, test_labels) = pickle.load(f)\n\nCredit - Jeremy Howards fast.ai course lesson 5\n"}, "920": {"topic": "Restore original text from Keras\u2019s imdb dataset", "user_name": "mdaoustmdaoust", "text": "\nThis happened because of a basic NLP data preparation. Loads of the so called stop words were removed from text in order to make learning feasible. Usually - also the most of puntuation and less frequent words are removed from text during preprocessing. I think that the only way to restore original text is to find the most matching texts at IMDB using e.g. a Google's browser API.\n"}, "921": {"topic": "Restore original text from Keras\u2019s imdb dataset", "user_name": "kmakkmak", "text": "\nThis encoding will work along with the labels:\nfrom keras.datasets import imdb\n(x_train,y_train),(x_test,y_test) = imdb.load_data()\nword_index = imdb.get_word_index() # get {word : index}\nindex_word = {v : k for k,v in word_index.items()} # get {index : word}\n\nindex = 1\nprint(\" \".join([index_word[idx] for idx in x_train[index]]))\nprint(\"positve\" if y_train[index]==1 else \"negetive\")\n\nUpvote if helps. :)\n"}, "922": {"topic": "Restore original text from Keras\u2019s imdb dataset", "user_name": "Marcin Mo\u017cejkoMarcin Mo\u017cejko", "text": "\nThe indices are offset by 3 because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\" and \"unknown\". The following should work.\nimdb = tf.keras.datasets.imdb\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n\nword_index = imdb.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\nreview = [reverse_word_index.get(i-3, \"?\") for i in train_data[0]]\n\n"}, "923": {"topic": "Restore original text from Keras\u2019s imdb dataset", "user_name": "HayatHayat", "text": "\nThis works for me:\nword_index = imdb.get_word_index()                                    \nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])            \ndecoded_review = ' '.join([reverse_word_index.get(i - 3, \"\") for i in train_data[0]])\n\n"}, "924": {"topic": "Restore original text from Keras\u2019s imdb dataset", "user_name": "Andreas GomposAndreas Gompos", "text": "\nTo get an equivalent array of all the reviews:\ndef decode_imdb_reviews(text_data):\n    result = [0 for x in range(len(text_data))]\n    word_index = imdb.get_word_index()\n    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n    for review in range(0,len(text_data)):\n        for index in enumerate(text_data[review]):\n            decoded_review = ' '.join([reverse_word_index.get(index - 3, '#') for index in text_data[review]])\n        result[review] = decoded_review\n    return result\n\ntext_data = []\ntext_data = decode_imdb_reviews(train_data)\n\n"}, "925": {"topic": "Restore original text from Keras\u2019s imdb dataset", "user_name": "Pedram", "text": "\nTry below code. this code worked.\n# load dataset \nfrom tensorflow.keras.datasets import imdb \n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=500)\n\n\n# vec 2 num \nimdb_vec2num = imdb.get_word_index(path=\"imdb_word_index.json\")\n# num 2 vec \nimdb_num2vec = {value+3:key for key, value in imdb_vec2num.items()}\nimdb_num2vec[0] = \"<PAD>\"\nimdb_num2vec[1] = \"<START>\"\nimdb_num2vec[2] = \"<UNK>\"\nimdb_num2vec[3] = \"<UNUSED>\"\n\n# index-word table\nimdb_num2vec = {value:key for key, value in imdb_index.items()}\n\n# change encoded sentences to sentences\ndef restore_sentences(num2word, encoded_sentences): \n    sentences = []\n    for encoded_sentence in encoded_sentences:\n        sentences.append([num2word[ele] for ele in encoded_sentence])\n    return sentences\n\n# example \nsentences = restore_sentences(imdb_num2vec, x_train)\n\n"}, "926": {"topic": "Restore original text from Keras\u2019s imdb dataset", "user_name": "RevolverRakkRevolverRakk", "text": "\nfrom keras.datasets import imdb\nNUM_WORDS=1000 # only use top 1000 words\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=NUM_WORDS)\n\nGet the index details:\nword_to_id = keras.datasets.imdb.get_word_index()\n\nBuild the Key-Value pair:\nid_to_word = {value:key for key,value in word_to_id.items()}\nprint(' '.join(id_to_word[id] for id in x_train[0] ))\n\n"}, "927": {"topic": "NLTK WordNet Lemmatizer: Shouldn't it lemmatize all inflections of a word?", "user_name": "Peter Mortensen", "text": "\nI'm using the NLTK WordNet Lemmatizer for a Part-of-Speech tagging project by first modifying each word in the training corpus to its stem (in place modification), and then training only on the new corpus. However, I found that the lemmatizer is not functioning as I expected it to.\nFor example, the word loves is lemmatized to love which is correct, but the word loving remains loving even after lemmatization. Here loving is as in the sentence \"I'm loving it\".\nIsn't love the stem of the inflected word loving? Similarly, many other 'ing' forms remain as they are after lemmatization. Is this the correct behavior?\nWhat are some other lemmatizers that are accurate? (need not be in NLTK) Are there morphology analyzers or lemmatizers that also take into account a word's Part Of Speech tag, in deciding the word stem? For example, the word killing should have kill as the stem if killing is used as a verb, but it should have killing as the stem if it is used as a noun (as in the killing was done by xyz).\n"}, "928": {"topic": "NLTK WordNet Lemmatizer: Shouldn't it lemmatize all inflections of a word?", "user_name": "sanjeev mksanjeev mk", "text": "\nThe WordNet lemmatizer does take the POS tag into account, but it doesn't magically determine it:\n>>> nltk.stem.WordNetLemmatizer().lemmatize('loving')\n'loving'\n>>> nltk.stem.WordNetLemmatizer().lemmatize('loving', 'v')\nu'love'\n\nWithout a POS tag, it assumes everything you feed it is a noun. So here it thinks you're passing it the noun \"loving\" (as in \"sweet loving\").\n"}, "929": {"topic": "NLTK WordNet Lemmatizer: Shouldn't it lemmatize all inflections of a word?", "user_name": "", "text": "\nThe best way to troubleshoot this is to actually look in Wordnet. Take a look here: Loving in wordnet. As you can see, there is actually an adjective \"loving\" present in Wordnet. As a matter of fact, there is even the adverb \"lovingly\": lovingly in Wordnet. Because wordnet doesn't actually know what part of speech you actually want, it defaults to noun ('n' in Wordnet). If you are using Penn Treebank tag set, here's some handy function for transforming Penn to WN tags:\nfrom nltk.corpus import wordnet as wn\n\ndef is_noun(tag):\n    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n\n\ndef is_verb(tag):\n    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n\n\ndef is_adverb(tag):\n    return tag in ['RB', 'RBR', 'RBS']\n\n\ndef is_adjective(tag):\n    return tag in ['JJ', 'JJR', 'JJS']\n\n\ndef penn_to_wn(tag):\n    if is_adjective(tag):\n        return wn.ADJ\n    elif is_noun(tag):\n        return wn.NOUN\n    elif is_adverb(tag):\n        return wn.ADV\n    elif is_verb(tag):\n        return wn.VERB\n    return None\n\nHope this helps.\n"}, "930": {"topic": "NLTK WordNet Lemmatizer: Shouldn't it lemmatize all inflections of a word?", "user_name": "Fred FooFred Foo", "text": "\nit's clearer and more effective than enumeration\uff1a\nfrom nltk.corpus import wordnet\n\ndef get_wordnet_pos(self, treebank_tag):\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return ''\n\ndef penn_to_wn(tag):\n    return get_wordnet_pos(tag)\n\n"}, "931": {"topic": "NLTK WordNet Lemmatizer: Shouldn't it lemmatize all inflections of a word?", "user_name": "bogsbogs", "text": "\nAs an extension to the accepted answer from @Fred Foo above;\nfrom nltk import WordNetLemmatizer, pos_tag, word_tokenize\n\nlem = WordNetLemmatizer()\nword = input(\"Enter word:\\t\")\n\n# Get the single character pos constant from pos_tag like this:\npos_label = (pos_tag(word_tokenize(word))[0][1][0]).lower()\n\n# pos_refs = {'n': ['NN', 'NNS', 'NNP', 'NNPS'],\n#            'v': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n#            'r': ['RB', 'RBR', 'RBS'],\n#            'a': ['JJ', 'JJR', 'JJS']}\n\nif pos_label == 'j': pos_label = 'a'    # 'j' <--> 'a' reassignment\n\nif pos_label in ['r']:  # For adverbs it's a bit different\n    print(wordnet.synset(word+'.r.1').lemmas()[0].pertainyms()[0].name())\nelif pos_label in ['a', 's', 'v']: # For adjectives and verbs\n    print(lem.lemmatize(word, pos=pos_label))\nelse:   # For nouns and everything else as it is the default kwarg\n    print(lem.lemmatize(word))\n\n"}, "932": {"topic": "Real world typo statistics? [closed]", "user_name": "Brian Tompsett - \u6c64\u83b1\u6069", "text": "\n\n\n\n\n\n\n\n\n\r\nAs it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.                    \n\n\nClosed 10 years ago.\n\n\n\nWhere can I find some real world typo statistics?  \nI'm trying to match people's input text to internal objects, and people tend to make spelling mistakes.\nThere are 2 kinds of mistakes:  \n\ntypos - \"Helllo\" instead of \"Hello\" / \"Satudray\" instead of \"Saturday\" etc.  \nSpelling - \"Shikago\" instead of \"Chicago\" \n\nI use  Damerau-Levenshtein distance for the typos and Double Metaphone for spelling (Python implementations here and here).\nI want to focus on the Damerau-Levenshtein (or simply edit-distance). The textbook implementations always use '1' for the weight of deletions, insertions substitutions and transpositions. While this is simple and allows for nice algorithms it doesn't match \"reality\" / \"real-world probabilities\".  \nExamples:   \n\nI'm sure the likelihood of \"Helllo\" (\"Hello\") is greater than \"Helzlo\", yet they are both 1 edit distance away.\n\"Gello\" is closer than \"Qello\" to \"Hello\" on a QWERTY keyboard.\nUnicode transliterations: What is the \"real\" distance between \"M\u00fcnchen\" and \"Munchen\"?\n\nWhat should the \"real world\" weights be for deletions, insertions, substitutions, and transpositions?  \nEven Norvig's very cool spell corrector uses non-weighted edit distance.\nBTW- I'm sure the weights need to be functions and not simple floats (per the above \nexamples)...\nI can adjust the algorithm, but where can I \"learn\" these weights? I don't have access to Google-scale data...  \nShould I just guess them?\nEDIT - trying to answer user questions:\n\nMy current non-weighted algorithm fails often when faced with typos for the above reasons. \"Return on Tursday\": every \"real person\" can easily tell Thursday is more likely than Tuesday, yet they are both 1-edit-distance away! (Yes, I do log and measure my performance).\nI'm developing an NLP Travel Search engine, so my dictionary contains ~25K destinations (expected to grow to 100K), Time Expressions ~200 (expected 1K), People expressions ~100 (expected 300), Money Expressions ~100 (expected 500), \"glue logic words\" (\"from\", \"beautiful\", \"apartment\") ~2K (expected 10K) and so on...\nUsage of the edit distance is different for each of the above word-groups. I try to \"auto-correct when obvious\", e.g. 1 edit distance away from only 1 other word in the dictionary. I have many other hand-tuned rules, e.g. Double Metaphone fix which is not more than 2 edit distance away from a dictionary word with a length > 4... The list of rules continues to grow as I learn from real world input.\n\"How many pairs of dictionary entries are within your threshold?\": well, that depends on the \"fancy weighting system\" and on real world (future) input, doesn't it? Anyway, I have extensive unit tests so that every change I make to the system only makes it better (based on past inputs, of course). Most sub-6 letter words are within 1 edit distance from a word that is 1 edit distance away from another dictionary entry.\nToday when there are 2 dictionary entries at the same distance from the input I try to apply various statistics to better guess which the user meant (e.g. Paris, France is more likely to show up in my search than P\u0101r\u012bz, Iran).\nThe cost of choosing a wrong word is returning semi-random (often ridiculous) results  to the end-user and potentially losing a customer. The cost of not understanding is slightly less expensive: the user will be asked to rephrase.\nIs the cost of complexity worth it? Yes, I'm sure it is. You would not believe the amount of typos people throw at the system and expect it to understand, and I could sure use the boost in Precision and Recall.\n\n"}, "933": {"topic": "Real world typo statistics? [closed]", "user_name": "Tal WeissTal Weiss", "text": "\nPossible source for real world typo statistics would be in the Wikipedia's complete edit history. \nhttp://download.wikimedia.org/\nAlso, you might be interested in the AWB's RegExTypoFix\nhttp://en.wikipedia.org/wiki/Wikipedia:AWB/T\n"}, "934": {"topic": "Real world typo statistics? [closed]", "user_name": "tszmingtszming", "text": "\nI would advise you to check the trigram alogrithm. In my opinion it works better for finding typos then edit distance algorithm. It should work faster as well and if you keep dictionary in postgres database you can make use of index.\nYou may find useful stackoverflow topic about google \"Did you mean\"\n"}, "935": {"topic": "Real world typo statistics? [closed]", "user_name": "CommunityBot", "text": "\nProbability Scoring for Spelling Correction by Church and Gale might help.  In that paper, the authors model typos as a noisy channel between the author and the computer.  The appendix has tables for typos seen in a corpus of Associated Press publications.  There is a table for each of the following kinds of typos:\n\ndeletion\ninsertion\nsubstitution\ntransposition\n\nFor example, examining the insertion table, we can see that l was incorrectly inserted after l  128 times (the highest number in that column).  Using these tables, you can generate the probabilities you're looking for.\n"}, "936": {"topic": "Real world typo statistics? [closed]", "user_name": "jethrojethro", "text": "\nIf the research is your interest I think continuing with that algorithm, trying to find decent weights would be fruitful.\nI can't help you with typo stats, but I think you should also play with python's difflib.  Specifically, the ratio() method of SequenceMatcher.  It uses an algorithm which the docs http://docs.python.org/library/difflib.html claim is well suited to matches that 'look right', and may be useful to augment or test what you're doing.\nFor python programmers just looking for typos it is a good place to start.  One of my coworkers has used both Levenshtein edit distance and SequenceMatcher's ratio() and got much better results from ratio().  \n"}, "937": {"topic": "Real world typo statistics? [closed]", "user_name": "mndrixmndrix", "text": "\nSome questions for you, to help you determine whether you should be asking your \"where do I find real-world weights\" question:\nHave you actually measured the effectiveness of the uniform weighting implementation? How?\nHow many different \"internal objects\" do you have -- i.e. what is the size of your dictionary?\nHow are you actually using the edit distance e.g. John/Joan, Marmaduke/Marmeduke, Featherstonehaugh/Featherstonhaugh: is that \"all 1 error\" or is it 25% / 11.1% / 5.9% difference? What threshold are you using?\nHow many pairs of dictionary entries are within your threshold (e.g. John vs Joan, Joan vs Juan, etc)? If you introduced a fancy weighting system, how many pairs of dictionary entries would migrate (a) from inside the threshold to outside (b) vice versa?\nWhat do you do if both John and Juan are in your dictionary and the user types Joan?\nWhat are the penalties/costs of (1) choosing the wrong dictionary word (not the one that the user meant) (2) failing to recognise the user's input?\nWill introducing a complicated weighting system actually reduce the probabilities of the above two error types by sufficient margin to make the complication and slower speed worthwhile? \nBTW, how do you know what keyboard the user was using?\nUpdate:\n\"\"\"My current non-weighted algorithm fails often when faced with typos for the above reasons. \"Return on Tursday\": every \"real person\" can easily tell Thursday is more likely than Tuesday, yet they are both 1-edit-distance away! (Yes, I do log and measure my performance).\"\"\"\nYes, Thursday -> Tursday by omitting an \"h\", but Tuesday -> Tursday by substituting \"r\" instead of \"e\". E and R are next to each other on qwERty and azERty keyboards. Every \"real person\" can easily guess that Thursday is more likely than Tuesday. Even if statistics as well as guesses point to Thursday being more likely than Tuesday (perhaps omitting h will cost 0.5 and e->r will cost 0.75), will the difference (perhaps 0.25) be significant enough to always pick Thursday? Can/will your system ask \"Did you mean Tuesday?\" or does/will it just plough ahead with Thursday?\n"}, "938": {"topic": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`", "user_name": "talonmies", "text": "\nI got the following error when I ran my pytorch deep learning model in colab\n/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in linear(input, weight, bias)\n   1370         ret = torch.addmm(bias, input, weight.t())\n   1371     else:\n-> 1372         output = input.matmul(weight.t())\n   1373         if bias is not None:\n   1374             output += bias\n\nRuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`\n\nI even reduced batch size from 128 to 64 i.e., reduced to half,  but still, I got this error. Earlier, I ran the same code with a batch size of 128 but didn't get any error like this.\n"}, "939": {"topic": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`", "user_name": "Mr. NLPMr. NLP", "text": "\nNo, batch size does not matter in this case\nThe most likely reason is that there is an inconsistency between number of labels and number of output units.\n\nTry printing the size of the final output in the forward pass and check the size of the output\n\n\nprint(model.fc1(x).size())\nHere fc1 would be replaced by the name of your model's last linear layer before returning\n\n\nMake sure that label.size()  is equal to prediction.size() before calculating the loss\n\nAnd even after fixing that problem, you'll have to restart the GPU runtime (I needed to do this in my case when using a colab GPU)\nThis answer might also be helpful\n"}, "940": {"topic": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`", "user_name": "Mayukh DebMayukh Deb", "text": "\nThis error can actually be due to different reasons. It is recommended to debug CUDA errors by running the code on the CPU, if possible. If that\u2019s not possible, try to execute the script via:\nCUDA_LAUNCH_BLOCKING=1 python [YOUR_PROGRAM]\n\nThis will help you get the right line of code which raised the error in the stack trace so that you can resolve it.\n"}, "941": {"topic": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`", "user_name": "HLebHLeb", "text": "\nReducing batch size works for me and the training proceeds as planned.\n"}, "942": {"topic": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`", "user_name": "Frank PukFrank Puk", "text": "\nFirst, try running the same on your CPU to check if everything is fine with your tensors' shapes.\nIn my case everything was fine. And since this error means \"Resource allocation failed inside the cuBLAS library\", I tried decreasing the batch size and it solved the issue. You said you increased to 64 and it didn't help. Can you try 32, 8, 1?\n"}, "943": {"topic": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`", "user_name": "SerhiySerhiy", "text": "\nI encountered this problem when the number of label is not equaled with the number of network's output channel, i.e the number of classes predicted.\n"}, "944": {"topic": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`", "user_name": "Peter PackPeter Pack", "text": "\nI had the same problem while I don't know the reason to be exactly I know the cause,\nmy last line of the NN.module was\n self.fc3 = nn.Linear(84, num_classes) \n\nI changed my real num_classes to be 2 times as much\nbut it did not change the value of the variable num_classes, this probably made a mistake when I was outputting the results somewhere.\nafter I fixed the value of num_classes it just worked out\ni recommend going over the numbers in your model again\n"}, "945": {"topic": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`", "user_name": "ntg7 gamerntg7 gamer", "text": "\nI was facing CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasCreate(handle) on colab\nUpdating the pytorch to 1.8.1 fixed the issue.\n"}, "946": {"topic": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`", "user_name": "realsarmrealsarm", "text": "\nI ran into this issue because I was passing parameters in the wrong order to the BCELoss function. This became apparent only after switching to CPU.\n"}, "947": {"topic": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`", "user_name": "Jose SolorzanoJose Solorzano", "text": "\nGood chance that there is a layer mismatch.  Double check to make sure all the dimensions are consistent at each layer.\n"}, "948": {"topic": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`", "user_name": "LaptopProfileLaptopProfile", "text": "\nThe accurate error message can be obtained by switching to CPU. In my case I had 8 class placeholders at the input of torch.nn.CrossEntropyLoss, but there are 9 different labels (0~8).\n"}, "949": {"topic": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`", "user_name": "user433709user433709", "text": "\nMy model is to classify two classes with only one neuron in the last layer. I had this problem when the last layer is nn.Linear(512,1) in pytorch environment. But my label is just [0] or [1]. I solved this problem by adding the layer: nn.sigmoid()\n"}, "950": {"topic": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`", "user_name": "YeallyYeally", "text": "\nFor a large-scale dataset, just delete the temple variables\nfor batch_idx, (x, target) in enumerate(train_dataloader):\n    ...\n    del x,target,loss,outputs\n\n\n"}, "951": {"topic": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`", "user_name": "shenci zengshenci zeng", "text": "\nReducing the batch size worked for me.\n"}, "952": {"topic": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`", "user_name": "SoheilSoheil", "text": "\nReducing the batch size didn't work for me. I have defined num_classes in main.py and also in model structure. I forgot to change the num_classes in model structure therefore i got an error. After changing, training process has been started\n"}, "953": {"topic": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`", "user_name": "Khawar IslamKhawar Islam", "text": "\nThis is probably a mismatch of dimensions or indexes. You can have a more clear feedback about the error by running your model on cpu. You can reduce the datasets size if needed, in my case, as it was a simple prediction, I just switched for cpu and found out it was a token outside my model's vocabulary range.\n"}, "954": {"topic": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`", "user_name": "Andre GolFeAndre GolFe", "text": "\nReducing the maximum sequence length for a model that has a limit (e.g. BERT) solves the error for me.\nAlso, I faced the same issue when I resized the embedding layer of a model: model.resize_token_embeddings(NEW_SIZE), trained, and saved it.\nAt prediction time, when I loaded the model, I needed to resize the embedding layer again!\n"}, "955": {"topic": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`", "user_name": "", "text": "\nI got that same issue in google colab with GPU runtime and so i changed from GPU to TPU.\nThen it got resolved.\nI recommend u trying the same.\n"}, "956": {"topic": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`", "user_name": "MinionsMinions", "text": "\nI faced this problem when I set wrong value for num_embeddings of Embedding Layer.\n"}, "957": {"topic": "How to tweak the NLTK sentence tokenizer", "user_name": "Chris Wilson", "text": "\nI'm using NLTK to analyze a few classic texts and I'm running in to trouble tokenizing the text by sentence. For example, here's what I get for a snippet from Moby Dick:\nimport nltk\nsent_tokenize = nltk.data.load('tokenizers/punkt/english.pickle')\n\n'''\n(Chapter 16)\nA clam for supper? a cold clam; is THAT what you mean, Mrs. Hussey?\" says I, \"but\nthat's a rather cold and clammy reception in the winter time, ain't it, Mrs. Hussey?\"\n'''\nsample = 'A clam for supper? a cold clam; is THAT what you mean, Mrs. Hussey?\" says I, \"but that\\'s a rather cold and clammy reception in the winter time, ain\\'t it, Mrs. Hussey?\"'\n\nprint \"\\n-----\\n\".join(sent_tokenize.tokenize(sample))\n'''\nOUTPUT\n\"A clam for supper?\n-----\na cold clam; is THAT what you mean, Mrs.\n-----\nHussey?\n-----\n\" says I, \"but that\\'s a rather cold and clammy reception in the winter time, ain\\'t it, Mrs.\n-----\nHussey?\n-----\n\"\n'''\n\nI don't expect perfection here, considering that Melville's syntax is a bit dated, but NLTK ought to be able to handle terminal double quotes and titles like \"Mrs.\" Since the tokenizer is the result of an unsupervised training algo, however, I can't figure out how to tinker with it.\nAnyone have recommendations for a better sentence tokenizer? I'd prefer a simple heuristic that I can hack rather than having to train my own parser. \n"}, "958": {"topic": "How to tweak the NLTK sentence tokenizer", "user_name": "Chris WilsonChris Wilson", "text": "\nYou need to supply a list of abbreviations to the tokenizer, like so:\nfrom nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\npunkt_param = PunktParameters()\npunkt_param.abbrev_types = set(['dr', 'vs', 'mr', 'mrs', 'prof', 'inc'])\nsentence_splitter = PunktSentenceTokenizer(punkt_param)\ntext = \"is THAT what you mean, Mrs. Hussey?\"\nsentences = sentence_splitter.tokenize(text)\n\nsentences is now:\n['is THAT what you mean, Mrs. Hussey?']\n\nUpdate: This does not work if the last word of the sentence has an apostrophe or a quotation mark attached to it (like Hussey?'). So a quick-and-dirty way around this is to put spaces in front of apostrophes and quotes that follow sentence-end symbols (.!?):\ntext = text.replace('?\"', '? \"').replace('!\"', '! \"').replace('.\"', '. \"')\n\n"}, "959": {"topic": "How to tweak the NLTK sentence tokenizer", "user_name": "", "text": "\nYou can modify the NLTK's pre-trained English sentence tokenizer to recognize more abbreviations by adding them to the set _params.abbrev_types. For example: \nextra_abbreviations = ['dr', 'vs', 'mr', 'mrs', 'prof', 'inc', 'i.e']\nsentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\nsentence_tokenizer._params.abbrev_types.update(extra_abbreviations)\n\nNote that the abbreviations must be specified without the final period, but do include any internal periods, as in 'i.e' above. For details about the other tokenizer parameters, refer to the relevant documentation.\n"}, "960": {"topic": "How to tweak the NLTK sentence tokenizer", "user_name": "vpekarvpekar", "text": "\nYou can tell the PunktSentenceTokenizer.tokenize method to include \"terminal\" double quotes with the rest of the sentence by setting the realign_boundaries parameter to True. See the code below for an example.\nI do not know a clean way to prevent text like Mrs. Hussey from being split into two sentences. However, here is a hack which \n\nmangles all occurrences of Mrs. Hussey to Mrs._Hussey,\nthen splits the text into sentences with sent_tokenize.tokenize,\nthen for each sentence, unmangles Mrs._Hussey back to Mrs. Hussey\n\nI wish I knew a better way, but this might work in a pinch.\n\nimport nltk\nimport re\nimport functools\n\nmangle = functools.partial(re.sub, r'([MD]rs?[.]) ([A-Z])', r'\\1_\\2')\nunmangle = functools.partial(re.sub, r'([MD]rs?[.])_([A-Z])', r'\\1 \\2')\n\nsent_tokenize = nltk.data.load('tokenizers/punkt/english.pickle')\n\nsample = '''\"A clam for supper? a cold clam; is THAT what you mean, Mrs. Hussey?\" says I, \"but that\\'s a rather cold and clammy reception in the winter time, ain\\'t it, Mrs. Hussey?\"'''    \n\nsample = mangle(sample)\nsentences = [unmangle(sent) for sent in sent_tokenize.tokenize(\n    sample, realign_boundaries = True)]    \n\nprint u\"\\n-----\\n\".join(sentences)\n\nyields\n\"A clam for supper?\n-----\na cold clam; is THAT what you mean, Mrs. Hussey?\"\n-----\nsays I, \"but that's a rather cold and clammy reception in the winter time, ain't it, Mrs. Hussey?\"\n\n"}, "961": {"topic": "How to tweak the NLTK sentence tokenizer", "user_name": "bjmcbjmc", "text": "\nSo I had a similar issue and tried out vpekar's solution above.\nPerhaps mine is some sort of edge case but I observed the same behavior after applying the replacements, however, when I tried replacing the punctuation with the quotations placed before them, I got the output I was looking for. Presumably lack of adherence to MLA is less important than retaining the original quote as a single sentence.\nTo be more clear:\ntext = text.replace('?\"', '\"?').replace('!\"', '\"!').replace('.\"', '\".')\n\nIf MLA is important though you could always go back and reverse these changes wherever it counts.\n"}, "962": {"topic": "why elasticsearch won't run on Ubuntu 14.04?", "user_name": "Eric", "text": "\nI'm trying to determine if the elasticsearch instance is running, but it doesn't appear to be:\nubuntu@ubuntu:~$ sudo service elasticsearch status\n * elasticsearch is not running\nubuntu@ubuntu:~$ sudo service elasticsearch start\n * Starting Elasticsearch Server  [ OK ] \nubuntu@ubuntu:~$ sudo service elasticsearch status\n * elasticsearch is not running\nand\n\nubuntu@ubuntu:~$ sudo /etc/init.d/elasticsearch status\n * elasticsearch is not running\nubuntu@ubuntu:~$ sudo /etc/init.d/elasticsearch start\n * Starting Elasticsearch Server  [ OK ] \nubuntu@ubuntu:~$ sudo /etc/init.d/elasticsearch status\n * elasticsearch is not running\nubuntu@ubuntu:/etc/elasticsearch# sudo service elasticsearch restart\n * Stopping Elasticsearch Server  [ OK ] \n * Starting Elasticsearch Server  [ OK ] \nubuntu@ubuntu:/etc/elasticsearch# sudo service elasticsearch status\n * elasticsearch is not running\n\nand\nubuntu@ubuntu:~$ curl -XGET localhost:9200/_nodes/_all/process?pretty\ncurl: (7) Failed to connect to localhost port 9200: Connection refused\n\nand\nubuntu@ubuntu:/etc/elasticsearch$ sudo netstat -nlp\ntcp6       0      0 :::9300                 :::*                    LISTEN      4413/java       \n\nUPD\nMy elasticsearch.log:\n[2014-12-03 00:00:02,161][INFO ][cluster.metadata         ] [Zero] [logstash-2014.12.03] creating index, cause [auto(bulk api)], shards [5]/[1], mappings [_default_]\n[2014-12-03 00:00:02,617][INFO ][cluster.metadata         ] [Zero] [logstash-2014.12.03] update_mapping [logs] (dynamic)\n[2014-12-03 00:00:12,737][INFO ][cluster.metadata         ] [Zero] [logstash-2014.12.03] update_mapping [logs] (dynamic)\n[2014-12-03 00:00:17,587][INFO ][cluster.metadata         ] [Zero] [logstash-2014.12.03] update_mapping [logs] (dynamic)\n[2014-12-03 00:00:18,842][INFO ][cluster.metadata         ] [Zero] [logstash-2014.12.03] update_mapping [logs] (dynamic)\n[2014-12-03 01:00:01,430][INFO ][cluster.metadata         ] [Zero] [logstash-2014.11.25] deleting index\n[2014-12-03 09:46:57,461][INFO ][cluster.metadata         ] [Zero] [logstash-2014.12.03] update_mapping [logs] (dynamic)\n\n"}, "963": {"topic": "why elasticsearch won't run on Ubuntu 14.04?", "user_name": "henbhenb", "text": "\nElasticsearch service init script doesn't print any error information on console or log file when it fails to startup, instead it ridiculously shows [OK].\nYou have to run elaticsearch manually with the same user and same parameters as what the init script does to check what's going wrong. The error message will be printed on console.\nOn my Ubuntu 14.10 with elasticsearch-1.4.1.deb installed, without any path changed, the command to run elastisearch is:\nsudo -u elasticsearch /usr/share/elasticsearch/bin/elasticsearch -d -p /var/run/elasticsearch.pid --default.config=/etc/elasticsearch/elasticsearch.yml --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/var/lib/elasticsearch --default.path.work=/tmp/elasticsearch --default.path.conf=/etc/elasticsearch\n\nI just added a line into /etc/init.d/elasticsearch to print out the above command:\n# Start Daemon\nlog_daemon_msg \"sudo -u $ES_USER $DAEMON $DAEMON_OPTS\"    # <-- Add this line\nstart-stop-daemon --start -b --user \"$ES_USER\" -c \"$ES_USER\" --pidfile \"$PID_FILE\" --exec $DAEMON -- $DAEMON_OPTS\nlog_end_msg $?\n\n"}, "964": {"topic": "why elasticsearch won't run on Ubuntu 14.04?", "user_name": "aleungaleung", "text": "\nThe elasticsearch user cannot write the PID file because it has no permissions to create a file in /var/run/:\nFileNotFoundException[/var/run/elasticsearch.pid (Keine Berechtigung)]\n\nThe fix: create the directory /var/run/elasticsearch/, change its ownership to elasticsearch:elasticsearch, and change the PID file location to this directory in the init script.\nmkdir -p /var/run/elasticsearch\nchown elasticsearch: /var/run/elasticsearch\nsed -e 's|^PID_FILE=.*$|PID_FILE=/var/run/$NAME/$NAME.pid|g' -i /etc/init.d/elasticsearch\n\nOnce you get that far, here's the next error you might see:\nElasticsearchIllegalStateException[Failed to obtain node lock, is the following location writable?: [/var/lib/elasticsearch/elasticsearch]]\n\nAgain a resource does not have the correct permissions for the elasticsearch user.\nchown -R elasticsearch: /var/lib/elasticsearch\n\nNot done yet. Now you have to edit /etc/init.d/elasticsearch and remove this line:\ntest \"$START_DAEMON\" == true || exit 0\n\nThis line is utter garbage and is guaranteed to cause an exit.\nNow it should finally start.\n"}, "965": {"topic": "why elasticsearch won't run on Ubuntu 14.04?", "user_name": "Carlos KonstanskiCarlos Konstanski", "text": "\n\nIf you are using Elasticsearch > 5.0\n\nMin/max heap size requirements for Elasticsearch 5.0 are now both defaulted to 2gb\nCheck the ls /tmp/hs_err_pid*.log files, in the logs you'll see that JVM failed to start ES because of insufficient memory.\nYou can adjust the heap size settings in /etc/elasticsearch/jvm.options. Adjust the lines -Xms2g and -Xmx2g to -Xms1g and -Xmx1g respectively, if you're on a box with 2 GB of RAM. If you're going to use a box with 1 GB of RAM I would recommend using -Xms512m and -Xmx512m.\nReference\n"}, "966": {"topic": "why elasticsearch won't run on Ubuntu 14.04?", "user_name": "", "text": "\nWhile the accepted answer command worked for me using Elasticsearch 1.7.3, with some changes in Elasticsearch >2.0 running the accepted answer command would produce\nes.default.config is no longer supported. elasticsearch.yml \nmust be placed in the config directory and cannot be renamed\n\nGithub Issue\nThe command as specified in the above Github issue would now be:\nsudo -u elasticsearch /usr/share/elasticsearch/bin/elasticsearch -d -p /var/run/elasticsearch.pid --path.conf=/etc/elasticsearch --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/var/lib/elasticsearch --default.path.work=/tmp/elasticsearch --default.path.conf=/etc/elasticsearch\n\n"}, "967": {"topic": "why elasticsearch won't run on Ubuntu 14.04?", "user_name": "Ronak JainRonak Jain", "text": "\nI ran into the same issue this morning.  After much digging, we found out  it was caused by an unsuccessful Java 8 installation.  All was fine after Java 8 installation had been fixed.\n"}, "968": {"topic": "why elasticsearch won't run on Ubuntu 14.04?", "user_name": "Khaliq GantKhaliq Gant", "text": "\nThe command line parameters that @aleung refers to can be set in the configuration file. By default, the parameters are commented out.\nSet the following in /etc/default/elasticsearch\n################################\n# Elasticsearch\n################################\n\n# Elasticsearch home directory\nES_HOME=/usr/share/elasticsearch\n\n# Elasticsearch configuration directory\nCONF_DIR=/etc/elasticsearch\n\n# Elasticsearch data directory\nDATA_DIR=/var/lib/elasticsearch\n\n# Elasticsearch logs directory\nLOG_DIR=/var/log/elasticsearch\n\n# Elasticsearch PID directory\nPID_DIR=/var/run/elasticsearch\n\n"}, "969": {"topic": "why elasticsearch won't run on Ubuntu 14.04?", "user_name": "Bo GuoBo Guo", "text": "\nThis command resolved my issue:\nsudo chown -R elasticsearch:elasticsearch /var/lib/elasticsearch/\n\nReferenced from @imsaar github\n"}, "970": {"topic": "why elasticsearch won't run on Ubuntu 14.04?", "user_name": "CommunityBot", "text": "\nFor me, this problem was caused by the Elasticsearch Data and/or Logs directory being at 100% disk usage. Run df -h to see whether the directory your Elasticsearch process is using for data and logs has free space or not.\n"}, "971": {"topic": "why elasticsearch won't run on Ubuntu 14.04?", "user_name": "KevinOKevinO", "text": "\nI got to the same point after I've done apt-get dist-upgrade - JAVA got updated to \"Java(TM) SE Runtime Environment (build 1.8.0_91-b14)\nJava HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)\" version.\nMy ES did not want to start. I found the cause here (tail -n100 /var/log/elasticsearch/elasticsearch.log):\njava.lang.IllegalArgumentException: Plugin [license] is incompatible with Elasticsearch [2.3.2]. Was designed for version [2.3.1]\n\nI just removed the plugin (bin/plugin remove license) and started ES successfully.\nI hope this will help others.\n"}, "972": {"topic": "Code Golf: Number to Words", "user_name": "", "text": "\n\n\n\n\n\n\n\n\n\nLocked. This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions.                    \n\n\n\n\n\nThe code golf series seem to be fairly popular.  I ran across some code that converts a number to its word representation.  Some examples would be (powers of 2 for programming fun):\n\n2 -> Two\n1024 -> One Thousand Twenty Four\n1048576 -> One Million Forty Eight Thousand Five Hundred Seventy Six\n\nThe algorithm my co-worker came up was almost two hundred lines long.  Seems like there would be a more concise way to do it.\nCurrent guidelines:\n\nSubmissions in any programming language welcome (I apologize to\nPhiLho for the initial lack of clarity on this one)\nMax input of 2^64 (see following link for words, thanks mmeyers)\nShort scale with English output preferred, but any algorithm is welcome.  Just comment along with the programming language as to the method used.\n\n"}, "973": {"topic": "Code Golf: Number to Words", "user_name": "\r", "text": "\nOk, I think it's time for my own implementation in Windows BATCH script (should work on Windows 2000 or later).\nHere is the code:\n@echo off\n\nset zero_to_nineteen=Zero One Two Three Four Five Six Seven Eight Nine Ten Eleven Twelve Thirteen Fourteen Fifteen Sixteen Seventeen Eighteen Nineteen\nset twenty_to_ninety=ignore ignore Twenty Thirty Forty Fifty Sixty Seventy Eighty Ninety\nset big_numbers=ignore Thousand Million Billion Trillion Quadrillion Quintillion Sextillion Septillion Octillion Nonillion Decillion Undecillion Duodecillion Tredecillion Quattuordecillion Quindecillion Sexdecillion Septendecillion Octodecillion Novemdecillion Vigintillion\nrem             10^0   10^3     10^6    10^9    10^12    10^15       10^18       10^21      10^24      10^27     10^30     10^33     10^36       10^39        10^42        10^45             10^48         10^51        10^54           10^57         10^60          10^63\n\ncall :parse_numbers %*\n\nexit /B 0\n\n:parse_numbers\n    :parse_numbers_loop\n        if \"$%~1\" == \"$\" goto parse_numbers_end\n        call :parse_number %~1\n        echo %~1 -^> %parse_number_result%\n        shift\n        goto parse_numbers_loop\n    :parse_numbers_end\n    exit /B 0\n\n:parse_number\n    call :get_sign %~1\n    set number_sign=%get_sign_result%\n    call :remove_groups %get_sign_result_number%\n    call :trim_leading_zeros %remove_groups_result%\n    set number=%trim_leading_zeros_result%\n    if \"$%number%\" == \"$0\" (\n        set parse_number_result=Zero\n        exit /B 0\n    )\n    set counter=0\n    set parse_number_result=\n    :parse_number_loop\n        set last_three=%number:~-3%\n        set number=%number:~0,-3%\n        call :parse_three %last_three%\n        call :get_from %counter% %big_numbers%\n        if \"$%get_from_result%\" == \"$\" (\n            set parse_number_result=* ERR: the number is too big! Even wikipedia doesn't know how it's called!\n            exit /B 0\n        )\n        if not \"$%parse_three_result%\" == \"$Zero\" (\n            if %counter% == 0 (\n                set parse_number_result=%parse_three_result%\n            ) else (\n                if not \"$%parse_number_result%\" == \"$\" (\n                    set parse_number_result=%parse_three_result% %get_from_result% %parse_number_result%\n                ) else (\n                    set parse_number_result=%parse_three_result% %get_from_result%\n                )\n            )\n        )\n        set /A counter+=1\n        if not \"$%number%\" == \"$\" goto parse_number_loop\n    if \"$%parse_number_result%\" == \"$\" (\n        set parse_number_result=Zero\n        exit /B 0\n    ) else if not \"$%number_sign%\" == \"$\" (\n        set parse_number_result=%number_sign% %parse_number_result%\n    )\n    exit /B 0\n\n:parse_three\n    call :trim_leading_zeros %~1\n    set three=%trim_leading_zeros_result%\n    set /A three=%three% %% 1000\n    set /A two=%three% %% 100\n    call :parse_two %two%\n    set parse_three_result=\n    set /A digit=%three% / 100\n    if not \"$%digit%\" == \"$0\" (\n        call :get_from %digit% %zero_to_nineteen%\n    )\n    if not \"$%digit%\" == \"$0\" (\n        if not \"$%get_from_result%\" == \"$Zero\" (\n            set parse_three_result=%get_from_result% Hundred\n        )\n    )\n    if \"$%parse_two_result%\" == \"$Zero\" (\n        if \"$%parse_three_result%\" == \"$\" (\n            set parse_three_result=Zero\n        )\n    ) else (\n        if \"$%parse_three_result%\" == \"$\" (\n            set parse_three_result=%parse_two_result%\n        ) else (\n            set parse_three_result=%parse_three_result% %parse_two_result%\n        )\n    )\n    exit /B 0\n\n:parse_two\n    call :trim_leading_zeros %~1\n    set two=%trim_leading_zeros_result%\n    set /A two=%two% %% 100\n    call :get_from %two% %zero_to_nineteen%\n    if not \"$%get_from_result%\" == \"$\" (\n        set parse_two_result=%get_from_result%\n        goto parse_two_20_end\n    )\n    set /A digit=%two% %% 10\n    call :get_from %digit% %zero_to_nineteen%\n    set parse_two_result=%get_from_result%\n    set /A digit=%two% / 10\n    call :get_from %digit% %twenty_to_ninety%\n    if not \"$%parse_two_result%\" == \"$Zero\" (\n        set parse_two_result=%get_from_result% %parse_two_result%\n    ) else (\n        set parse_two_result=%get_from_result%\n    )\n    goto parse_two_20_end\n    :parse_two_20_end\n    exit /B 0\n\n:get_from\n    call :trim_leading_zeros %~1\n    set idx=%trim_leading_zeros_result%\n    set /A idx=0+%~1\n    shift\n    :get_from_loop\n        if \"$%idx%\" == \"$0\" goto get_from_loop_end\n        set /A idx-=1\n        shift\n        goto get_from_loop\n    :get_from_loop_end\n    set get_from_result=%~1\n    exit /B 0\n\n:trim_leading_zeros\n    set str=%~1\n    set trim_leading_zeros_result=\n    :trim_leading_zeros_loop\n        if not \"$%str:~0,1%\" == \"$0\" (\n            set trim_leading_zeros_result=%trim_leading_zeros_result%%str%\n            exit /B 0\n        )\n        set str=%str:~1%\n        if not \"$%str%\" == \"$\" goto trim_leading_zeros_loop\n    if \"$%trim_leading_zeros_result%\" == \"$\" set trim_leading_zeros_result=0\n    exit /B 0\n\n:get_sign\n    set str=%~1\n    set sign=%str:~0,1%\n    set get_sign_result=\n    if \"$%sign%\" == \"$-\" (\n        set get_sign_result=Minus\n        set get_sign_result_number=%str:~1%\n    ) else if \"$%sign%\" == \"$+\" (\n        set get_sign_result_number=%str:~1%\n    ) else (\n        set get_sign_result_number=%str%\n    )\n    exit /B 0\n\n:remove_groups\n    set str=%~1\n    set remove_groups_result=%str:'=%\n    exit /B 0\n\nThis is the test script I used:\n@echo off\nrem 10^x:x= 66  63  60  57  54  51  48  45  42  39  36  33  30  27  24  21  18  15  12   9   6   3   0\ncall number                                                                                          0\ncall number                                                                                          2\ncall number                                                                                        -17\ncall number                                                                                         30\ncall number                                                                                         48\ncall number                                                                                       -256\ncall number                                                                                        500\ncall number                                                                                        874\ncall number                                                                                      1'024\ncall number                                                                                    -17'001\ncall number                                                                                    999'999\ncall number                                                                                  1'048'576\ncall number                                                                         -1'000'001'000'000\ncall number                                                                    912'345'014'587'957'003\ncall number                                                       -999'912'345'014'587'124'337'999'999\ncall number                                        111'222'333'444'555'666'777'888'999'000'000'000'001\ncall number                               -912'345'014'587'912'345'014'587'124'912'345'014'587'124'337\ncall number    999'999'999'999'999'999'999'999'999'999'999'999'999'999'999'999'999'999'999'999'999'999\ncall number  1'000'000'000'000'000'000'000'000'000'000'000'000'000'000'000'000'000'000'000'000'000'000\nrem 10^x:x= 66  63  60  57  54  51  48  45  42  39  36  33  30  27  24  21  18  15  12   9   6   3   0\n\nAnd this is the output I got from my test script:\n0 -> Zero\n2 -> Two\n-17 -> Minus Seventeen\n30 -> Thirty\n48 -> Forty Eight\n-256 -> Minus Two Hundred Fifty Six\n500 -> Five Hundred\n874 -> Eight Hundred Seventy Four\n1'024 -> One Thousand Twenty Four\n-17'001 -> Minus Seventeen Thousand One\n999'999 -> Nine Hundred Ninety Nine Thousand Nine Hundred Ninety Nine\n1'048'576 -> One Million Forty Eight Thousand Five Hundred Seventy Six\n-1'000'001'000'000 -> Minus One Trillion One Million\n912'345'014'587'957'003 -> Nine Hundred Twelve Quadrillion Three Hundred Forty Five Trillion Fourteen Billion Five Hundred Eighty Seven Million Nine Hundred Fifty Seven Thousand Three\n-999'912'345'014'587'124'337'999'999 -> Minus Nine Hundred Ninety Nine Septillion Nine Hundred Twelve Sextillion Three Hundred Forty Five Quintillion Fourteen Quadrillion Five Hundred Eighty Seven Trillion One Hundred Twenty Four Billion Three Hundred Thirty Seven Million Nine Hundred Ninety Nine Thousand Nine Hundred Ninety Nine\n111'222'333'444'555'666'777'888'999'000'000'000'001 -> One Hundred Eleven Undecillion Two Hundred Twenty Two Decillion Three Hundred Thirty Three Nonillion Four Hundred Forty Four Octillion Five Hundred Fifty Five Septillion Six Hundred Sixty Six Sextillion Seven Hundred Seventy Seven Quintillion Eight Hundred Eighty Eight Quadrillion Nine Hundred Ninety Nine Trillion One\n-912'345'014'587'912'345'014'587'124'912'345'014'587'124'337 -> Minus Nine Hundred Twelve Tredecillion Three Hundred Forty Five Duodecillion Fourteen Undecillion Five Hundred Eighty Seven Decillion Nine Hundred Twelve Nonillion Three Hundred Forty Five Octillion Fourteen Septillion Five Hundred Eighty Seven Sextillion One Hundred Twenty Four Quintillion Nine Hundred Twelve Quadrillion Three Hundred Forty Five Trillion Fourteen Billion Five Hundred Eighty Seven Million One Hundred Twenty Four Thousand Three Hundred Thirty Seven\n999'999'999'999'999'999'999'999'999'999'999'999'999'999'999'999'999'999'999'999'999'999 -> Nine Hundred Ninety Nine Vigintillion Nine Hundred Ninety Nine Novemdecillion Nine Hundred Ninety Nine Octodecillion Nine Hundred Ninety Nine Septendecillion Nine Hundred Ninety Nine Sexdecillion Nine Hundred Ninety Nine Quindecillion Nine Hundred Ninety Nine Quattuordecillion Nine Hundred Ninety Nine Tredecillion Nine Hundred Ninety Nine Duodecillion Nine Hundred Ninety Nine Undecillion Nine Hundred Ninety Nine Decillion Nine Hundred Ninety Nine Nonillion Nine Hundred Ninety Nine Octillion Nine Hundred Ninety Nine Septillion Nine Hundred Ninety Nine Sextillion Nine Hundred Ninety Nine Quintillion Nine Hundred Ninety Nine Quadrillion Nine Hundred Ninety Nine Trillion Nine Hundred Ninety Nine Billion Nine Hundred Ninety Nine Million Nine Hundred Ninety Nine Thousand Nine Hundred Ninety Nine\n1'000'000'000'000'000'000'000'000'000'000'000'000'000'000'000'000'000'000'000'000'000'000 -> * ERR: the number is too big! Even wikipedia doesn't know how it's called!\n\nIf I could find some more names of large numbers, the script would support even bigger numbers. Currently, though, the script will work with all numbers from -(10^66-1) to (10^66-1).\nI have to mention, that I had a lot of fun solving this in BATCH. :)\n"}, "974": {"topic": "Code Golf: Number to Words", "user_name": "\r", "text": "\nC# - 30 lines incl. method declaration and { }s:\nTakes into account all the previously aforementioned commas, ands and hyphens.  I've only included up to octillion because decimal.MaxValue is only in the octillions.  For bigger integers you would need to add corresponding items to the thou[] array and perhaps pass the number in as a string, modifying the line to extract the block by using the last 3 chars instead of using modulo as I have here.\n    static string wordify(decimal v)\n    {\n        if (v == 0) return \"zero\";\n        var units = \" one two three four five six seven eight nine\".Split();\n        var teens = \" eleven twelve thir# four# fif# six# seven# eigh# nine#\".Replace(\"#\", \"teen\").Split();\n        var tens = \" ten twenty thirty forty fifty sixty seventy eighty ninety\".Split();\n        var thou = \" thousand m# b# tr# quadr# quint# sext# sept# oct#\".Replace(\"#\", \"illion\").Split();\n        var g = (v < 0) ? \"minus \" : \"\";\n        var w = \"\";\n        var p = 0;\n        v = Math.Abs(v);\n        while (v > 0)\n        {\n            int b = (int)(v % 1000);\n            if (b > 0)\n            {\n                var h = (b / 100);\n                var t = (b - h * 100) / 10;\n                var u = (b - h * 100 - t * 10);\n                var s = ((h > 0) ? units[h] + \" hundred\" + ((t > 0 | u > 0) ? \" and \" : \"\") : \"\")\n                      + ((t > 0) ? (t == 1 && u > 0) ? teens[u] : tens[t] + ((u > 0) ? \"-\" : \"\") : \"\")\n                      + ((t != 1) ? units[u] : \"\");\n                s = (((v > 1000) && (h == 0) && (p == 0)) ? \" and \" : (v > 1000) ? \", \" : \"\") + s;\n                w = s + \" \" + thou[p] + w;\n            }\n            v = v / 1000;\n            p++;\n        }\n        return g + w;\n    }\n\nCalled using:\nstatic void Main(string[] args)\n{\n  Console.WriteLine(wordify(decimal.MaxValue));\n}\n\nOutput: \n\nseventy-nine octillion, two hundred\n  and twenty-eight septillion, one\n  hundred and sixty-two sextillion, five\n  hundred and fourteen quintillion, two\n  hundred and sixty-four quadrillion,\n  three hundred and thirty-seven\n  trillion, five hundred and\n  ninety-three billion, five hundred and\n  forty-three million, nine hundred and\n  fifty thousand, three hundred and\n  thirty-five\n\n"}, "975": {"topic": "Code Golf: Number to Words", "user_name": "", "text": "\nIn A86 assember - assembles to a .COM executable:\ndd 0ba02c6bfh, 0b8bd10c1h, 0e808b512h, 0ea870100h, 08700e9e8h, 010273eah\ndd 0e0e8c2h, 06b51872h, 0c000ebe8h, 0b3c02e8h, 03368067dh, 0b2e901h\ndd 0baaa5004h, 0fd8110c1h, 0cd7c1630h, 0bf3031bbh, 0a0571000h, 0ec880080h\ndd 0c581c589h, 023c0081h, 0e7f087ch, 0823e38h, 027b00875h, 0e901d068h\ndd 0b6400080h, 04f6f603h, 080d08a1ch, 0b60f80c4h, 07f06c7f4h, 088303000h\ndd 0ac00813eh, 087ef828h, 0b00056e8h, 051e81dh, 0d83850adh, 0e7f157ch\ndd 0a74fc38h, 0262ce088h, 0e901a368h, 01d2c003bh, 0580036e8h, 0b7efc38h\ndd 0774d838h, 0f828e088h, 0800026e8h, 0127e1dfah, 0afd448ah, 0440afe44h\ndd 074f838ffh, 0e8c28a05h, 0cafe000fh, 0ab7cee39h, 05a2405c6h, 021cd09b4h\ndd 05e856c3h, 020b05e00h, 0c5bec3aah, 074c00a02h, 03c80460ah, 0fefa755bh\ndd 046f675c8h, 0745b3cach, 0f8ebaae8h, 0eec1d689h, 08a3c8a03h, 07e180cah\ndd 0cfd2c1feh, 0ebe8c342h, 0fed8d0ffh, 0c3f775cdh, 01e581e8fh, 0303c5ea8h\ndd 0df6f652ah, 078bde03ch, 05e027500h, 01ec1603ch, 07d40793dh, 0603c8080h\ndd 09f6f2838h, 040f17a3dh, 080f17a22h, 0403d7264h, 0793cdee1h, 0140740f1h\ndd 01e2f7d32h, 02f488948h, 0a7c43b05h, 0a257af9bh, 0be297b6ch, 04609e30ah\ndd 0b8f902abh, 07c21e13eh, 09a077d9eh, 054f82ab5h, 0fabe2af3h, 08a6534cdh\ndd 0d32b4c97h, 035c7c8ceh, 082bcc833h, 0f87f154fh, 0650ff7eah, 02f143fdfh\ndd 0a1fd687fh, 0c3e687fdh, 0c6d50fe0h, 075f13574h, 0898c335bh, 0e748ce85h\ndd 08769676fh, 0ad2cedd3h, 0928c77c7h, 077e2d18eh, 01a77e8f6h\ndb 0bah, 01bh\n\nThat's a 454 byte executable.\nHere's the (slightly smaller) code. Since A86 is an 8086 only assembler, I've had to hand code the 32bit extensions:\n    mov di,strings\n    mov dx,tree_data * 8 + 1\n    mov bp,code_data * 8\nl1:\n    mov ch,8\n    call extract_bits\n    xchg dx,bp\n    call extract_bit\n    xchg dx,bp\n    jnc l2\n    add dx,ax\nl2:\n    call extract_bit\n    jc l3\n    mov ch,6\n    call extract_bits\n    shr al,2\n    cmp al,11\n    push l27\n    jl get_string\nl25:\n    add al,48+32\n    stosb\nl27:\n    mov dx,tree_data * 8 + 1\nl3:\n    cmp bp,end_data * 8\n    jl l1\n\nconvert:\n    mov bx,'01'\n    mov di,01000h\n    push di\n\n    mov al,[80h]\n    mov ah,ch\n    mov bp,ax\n    add bp,81h\n    cmp al,2\n    jl zero\n    jg l90\n    cmp byte ptr [82h],bh\n    jne l90\nzero:   \n    mov al,39\n    push done\n\nget_string:\n    mov si,strings-1\n    or al,al\n    je l36\nl35:\n    inc si\n    cmp byte ptr [si],';'+32\n    jne l35\n    dec al\n    jnz l35\nl36:\n    inc si\nl37:\n    lodsb\n    cmp al,';'+32\n    je ret\n    stosb\n    jmp l37\n\n\nl90:\n    inc ax\n    mov dh,3\n    div dh\n    add al,28\n    mov dl,al\n    add ah,80h\n    db 0fh, 0b6h, 0f4h ; movzx si,ah\n    mov word ptr [80h],'00'\n\nl95:    \n    lodsb\n\n    sub al,bh\n    jle l100\n    call get_string2\n    mov al,29\n    call get_string2\n\nl100:\n    lodsw\n    push ax\n    cmp al,bl\n    jl l150\n    jg l140\n    cmp ah,bh\n    je l140\n\n    mov al,ah\n    sub al,'0'-10\n    push l150\n\nget_string2:\n    push si\n    call get_string\n    pop si\n    mov al,' '\n    stosb\n    ret\n\nl140:\n    sub al,'0'-19\n    call get_string2\n\nl150:\n    pop ax\n    cmp ah,bh\n    jle l200\n    cmp al,bl\n    je l200\n    mov al,ah\n    sub al,bh\n    call get_string2\n\nl200:\n    cmp dl,29\n    jle l300\n\n    mov al,[si-3]\n    or al,[si-2]\n    or al,[si-1]\n    cmp al,bh\n    je l300\n\n    mov al,dl\n    call get_string2\n\nl300:\n    dec dl\n    cmp si,bp\n    jl l95\n\ndone:   \n    mov byte ptr [di],'$'\n    pop dx\n    mov ah,9\n    int 21h \n    int 20h\n\nl41:\n    rcr al,1\n    dec ch\n    jz ret\n\nextract_bits:\n    push l41\nextract_bit:\n    mov si,dx\n    shr si,3\n    mov bh,[si]\n    mov cl,dl\n    and cl,7\n    inc cl\n    ror bh,cl\n    inc dx\n    ret\n\ntree_data:\n    dw 01e8fh, 01e58h, 05ea8h, 0303ch, 0652ah, 0df6fh, 0e03ch, 078bdh\n    dw 07500h, 05e02h, 0603ch, 01ec1h, 0793dh, 07d40h, 08080h, 0603ch\n    dw 02838h, 09f6fh, 07a3dh, 040f1h, 07a22h, 080f1h, 07264h, 0403dh\n    dw 0dee1h, 0793ch, 040f1h, 01407h, 07d32h, 01e2fh, 08948h\n    db 048h\ncode_data:\n    dw 052fh, 0c43bh, 09ba7h, 057afh, 06ca2h, 0297bh, 0abeh, 09e3h\n    dw 0ab46h, 0f902h, 03eb8h, 021e1h, 09e7ch, 077dh, 0b59ah, 0f82ah\n    dw 0f354h, 0be2ah, 0cdfah, 06534h, 0978ah, 02b4ch, 0ced3h, 0c7c8h\n    dw 03335h, 0bcc8h, 04f82h, 07f15h, 0eaf8h, 0ff7h, 0df65h, 0143fh\n    dw 07f2fh, 0fd68h, 0fda1h, 0e687h, 0e0c3h, 0d50fh, 074c6h, 0f135h\n    dw 05b75h, 08c33h, 08589h, 048ceh, 06fe7h, 06967h, 0d387h, 02cedh\n    dw 0c7adh, 08c77h, 08e92h, 0e2d1h, 0f677h, 077e8h, 0ba1ah\n    db 01bh\nend_data:\n\nstrings:\n\nThe text is stored using Huffman encoding. The command line is passed as a string so converting it is simple - split the string into groups of three and parse each group (hundreds, tens and units) following each with the current multiplier (millions, thousands, etc).\n"}, "976": {"topic": "Code Golf: Number to Words", "user_name": "\r", "text": "\nLisp, using only standard functions:\n(format nil \"~r\" 1234) ==> \"one thousand two hundred thirty-four\"\n\nBonus:\n(format nil \"~@r\" 1234)  ==> \"MCCXXXIV\"\n\n"}, "977": {"topic": "Code Golf: Number to Words", "user_name": "\r", "text": "\nC++, 15 lines:\n#include <string>\nusing namespace std;\n\nstring Thousands[] = { \"zero\", \"thousand\", \"million\", \"billion\", \"trillion\", \"quadrillion\", \"quintillion\", \"sexillion\", \"septillion\", \"octillion\", \"nonillion\", \"decillion\" };\nstring Ones[] = { \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\" };\nstring Tens[] = { \"zero\", \"ten\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\" };\nstring concat(bool cond1, string first, bool cond2, string second) { return (cond1 ? first : \"\") + (cond1 && cond2 ? \" \" : \"\") + (cond2 ? second : \"\"); }\n\nstring toStringBelowThousand(unsigned long long n) {\n  return concat(n >= 100, Ones[n / 100] + \" hundred\", n % 100 != 0, (n % 100 < 20 ? Ones[n % 100] : Tens[(n % 100) / 10] + (n % 10 > 0 ? \" \" + Ones[n % 10] : \"\")));\n}\n\nstring toString(unsigned long long n, int push = 0) {\n  return n == 0 ? \"zero\" : concat(n >= 1000, toString(n / 1000, push + 1), n % 1000 != 0, concat(true, toStringBelowThousand(n % 1000), push > 0, Thousands[push]));\n}\n\nUsage:\ncout << toString(51351);   // => fifty one thousand three hundred fifty one\n\n"}, "978": {"topic": "Code Golf: Number to Words", "user_name": "", "text": "\nIs this cheating?  \nperl -MNumber::Spell -e 'print spell_number(2);'\n\n"}, "979": {"topic": "Code Golf: Number to Words", "user_name": "\r", "text": "\nPaul Fischer and Darius:  You guys have some great ideas, but I hate to see them implemented in such an overly verbose fashion. :)  Just kidding, your solution is awesome, but I squeezed 14 30 more bytes out, while staying inside of 79 columns and maintaining python 3 compatibility.\nSo here's my 416 byte python within 79 columns: (thanks guys, I'm standing on your shoulders)\nw=lambda n:_(n,[\"\",\"thousand \"]+p(\"m b tr quadr quint\",\"illion\"))[:-1]or\"zero\"\n_=lambda n,S:n*\"x\"and _(n//M,S[1:])+(Z[n%M//C]+\"hundred \")*(n%M//C>0)+(n%C>19\nand p(\"twen thir fo\"+R,\"ty\")[n%C//10-2]+Z[n%10]or Z[n%C])+S[0]*(n%M>0)\np=lambda a,b=\"\":[i+b+\" \"for i in a.split()]\nR=\"r fif six seven eigh nine\"\nM=1000\nC=100\nZ=[\"\"]+p(\"one two three four five%st nine ten eleven twelve\"%R[5:20])+p(\n\"thir fou\"+R,\"teen\")\n\nAnd the tests:\nif __name__ == \"__main__\":\n    import sys\n    assert(w(0)==\"zero\")\n    assert(w(100)==\"one hundred\")\n    assert(w(1000000)==\"one million\")\n    assert(w(1024)==\"one thousand twenty four\")\n    assert(w(1048576)==\"one million forty eight thousand five hundred seventy six\")\n\n"}, "980": {"topic": "Code Golf: Number to Words", "user_name": "\r", "text": "\nSee recursive's better answer. It's way betterer.\nMad props to Darius for inspiration on this one.  Your big-W (now my p) was especially clever.\nw=lambda n:[\"zero\",\" \".join(_(n,0))][n>0]\n_=lambda n,l:_(n//M,l+1)+[E,Z[n%M//C]+[\"hundred\"]][n%M//C>0]+\\\n(p(\"twen thir fo\"+R,\"ty\")[n%C//10-2]+Z[n%10]if n%C>19 else Z[n%C])+\\\n[E,([E,[\"thousand\"]]+p(\"m b tr quadr quint\",\"illion\"))[l]][n%M>0]if n else E\np=lambda a,b:[[i+b]for i in a.split()]\nE=[];R=\"r fif six seven eigh nine\";M=1000;C=100\nZ=[E]+p(\"one two three four five six seven eight nine ten eleven twelve\",\"\")+\\\np(\"thir fou\"+R,\"teen\")\n\nI test it with this:\nif __name__ == \"__main__\":\n    import sys\n    print w(int(sys.argv[1]))\n    assert(w(100)==\"one hundred\")\n    assert(w(1000000)==\"one million\")\n    assert(w(1024)==\"one thousand twenty four\")\n    assert(w(1048576)==\"one million forty eight thousand five hundred seventy six\")\n\nAt this point, this is a tweak of Darius' current solution, which is in turn a tweak of my older one, which was inspired by his, and he gave some bug hints in the comments.  It is also a crime against Python.\nSpoilers below, rot13'd for your protection, because half the fun of golf \nfiguring out how.  I highly recommend the mnenhy Firefox\nextension to decode this (and other simple encoding schemes) inline.\nPbafgnagf (V eranzrq gurz guvf erivfvba gb ubcrshyyl znxr gurz pyrnere.)\n\nR: Gur rzcgl frg.\nE: Gung juvpu vf va pbzzba orgjrra pbhagvat va gur \"grraf\" (egrra,\nsvsgrra, fvkgrra...) naq va gur graf (egl, svsgl, fvkgl....)\nZ, P: Jung gurl ner va Ebzna ahzrenyf.\nM: Nyy gur ahzoref sebz bar gb gjragl.\n\nShapgvbaf (fbzr nyfb eranzrq guvf ebhaq)\n\nj: Gur choyvp-snpvat shapgvba, juvpu gheaf n ahzore vagb jbeqf.\n_: Erphefviryl gheaf gur ahzore vagb jbeqf, gubhfnaq-ol-gubhfnaq.  a vf\ngur ahzore, y vf ubj sne guebhtu gur cbjref bs 1000 jr ner.  Ergheaf n\nyvfg bs fvatyrgba yvfgf bs rnpu jbeq va gur ahzore, r.t.\n[['bar'],['gubhfnaq'],['gjragl'],['sbhe']].\nc: sbe rnpu jbeq va gur fcnpr-frcnengrq jbeq yvfg n, nccraqf o nf n\nfhssvk naq chgf gurz rnpu vagb n fvatyrgba yvfg.  Sbe rknzcyr,\nc(\"z o ge\",\"vyyvba\") == [['zvyyvba'],['ovyyvba'],['gevyyvba']].\n\n"}, "981": {"topic": "Code Golf: Number to Words", "user_name": "", "text": "\nPython, 446 bytes. All lines under 80 columns, dammit. This is Paul Fisher's solution with coding tweaks on almost every line, down from his 488-byte version; he's since squeezed out several more bytes, and I concede. Go vote for his answer!\ng=lambda n:[\"zero\",\" \".join(w(n,0))][n>0]\nw=lambda n,l:w(n//m,l+1)+[e,z[n%m//100]+[\"hundred\"]][n%m//100>0]+\\\n(p(\"twen thir fo\"+r,\"ty\")[n%100//10-2]+z[n%10]if n%100>19 else z[n%100])+\\\n[e,k[l]][n%m>0]if n else e\np=lambda a,b:[[i+b]for i in a.split()]\ne=[];r=\"r fif six seven eigh nine\";m=1000\nk=[e,[\"thousand\"]]+p(\"m b tr quadr quint\",\"illion\")\nz=[e]+p(\"one two three four five six seven eight nine ten eleven twelve\",\"\")+\\\np(\"thir fou\"+r,\"teen\")\n\nThe history has gotten complicated. I started with the unobfuscated code below, which supports negative numbers and range-checking, plus dashes in some numbers for better English:\n>>> n2w(2**20)\n'one million forty-eight thousand five hundred seventy-six'\n\ndef n2w(n):\n    if n < 0:  return 'minus ' + n2w(-n)\n    if n < 10: return W('zero one two three four five six seven eight nine')[n]\n    if n < 20: return W('ten eleven twelve',\n                        'thir four fif six seven eigh nine',\n                        'teen')[n-10]\n    if n < 100: \n        tens = W('', 'twen thir for fif six seven eigh nine', 'ty')[n//10-2]\n        return abut(tens, '-', n2w(n % 10))\n    if n < 1000:\n        return combine(n, 100, 'hundred')\n    for i, word in enumerate(W('thousand', 'm b tr quadr quint', 'illion')):\n        if n < 10**(3*(i+2)):\n            return combine(n, 10**(3*(i+1)), word)\n    assert False\n\ndef W(b, s='', suff=''): return b.split() + [s1 + suff for s1 in s.split()]\ndef combine(n, m, term): return abut(n2w(n // m) + ' ' + term, ' ', n2w(n % m))\ndef abut(w10, sep, w1):  return w10 if w1 == 'zero' else w10 + sep + w1\n\nThen I squeezed it to about 540 bytes via obfuscation (new to me), and Paul Fisher found a shorter algorithm (dropping the dashes) along with some marvelously horrible Python coding tricks. I stole the coding tricks to get down to 508 (which still did not win). I tried restarting fresh with a new algorithm, which was unable to beat Fisher's. Finally here's the tweak of his code. Respect!\nThe obfuscated code has been tested against the clean code, which was checked by eyeball on a bunch of cases.\n"}, "982": {"topic": "Code Golf: Number to Words", "user_name": "\r", "text": "\nOk, here's F#, trying to stay readable, at about 830 bytes:\n#light\nlet thou=[|\"\";\"thousand\";\"million\";\"billion\";\"trillion\";\"quadrillion\";\"quintillion\"|]\nlet ones=[|\"\";\"one\";\"two\";\"three\";\"four\";\"five\";\"six\";\"seven\";\"eight\";\"nine\";\"ten\";\"eleven\";\n  \"twelve\";\"thirteen\";\"fourteen\";\"fifteen\";\"sixteen\";\"seventeen\";\"eighteen\";\"nineteen\"|]\nlet tens=[|\"\";\"\";\"twenty\";\"thirty\";\"forty\";\"fifty\";\"sixty\";\"seventy\";\"eighty\";\"ninety\"|]\nlet (^-) x y = if y=\"\" then x else x^\"-\"^y\nlet (^+) x y = if y=\"\" then x else x^\" \"^y\nlet (^?) x y = if x=\"\" then x else x^+y\nlet (+^+) x y = if x=\"\" then y else x^+y\nlet Tiny n = if n < 20 then ones.[n] else tens.[n/10] ^- ones.[n%10]\nlet Small n = (ones.[n/100] ^? \"hundred\") +^+ Tiny(n%100)\nlet rec Big n t = if n = 0UL then \"\" else\n  (Big (n/1000UL) (t+1)) +^+ (Small(n%1000UL|>int) ^? thou.[t])\nlet Convert n = if n = 0UL then \"zero\" else Big n 0\n\nand here are the unit tests\nlet Show n = \n    printfn \"%20u -> \\\"%s\\\"\" n (Convert n)\n\nlet tinyTests = [0; 1; 10; 11; 19; 20; 21; 30; 99] |> List.map uint64\nlet smallTests = tinyTests @ (tinyTests |> List.map (fun n -> n + 200UL))\nlet MakeTests t1 t2 = \n    List.map (fun n -> n * (pown 1000UL t1)) smallTests\n    |> List.map_concat (fun n -> List.map (fun x -> x * (pown 1000UL t2) + n) smallTests)\nfor n in smallTests do\n    Show n\nfor n in MakeTests 1 0 do\n    Show n\nfor n in MakeTests 5 2 do\n    Show n            \nShow 1000001000678000001UL\nShow 17999999999999999999UL\n\n"}, "983": {"topic": "Code Golf: Number to Words", "user_name": "\r", "text": "\nHere's a relatively straightforward implementation in C (52 lines).\nNOTE: this does not perform any bounds checking; the caller must ensure that the calling buffer is large enough.\n#include <stdio.h>\n#include <string.h>\n\nconst char *zero_to_nineteen[20] = {\"\", \"One \", \"Two \", \"Three \", \"Four \", \"Five \", \"Six \", \"Seven \", \"Eight \", \"Nine \", \"Ten \", \"Eleven \", \"Twelve \", \"Thirteen \", \"Fourteen \", \"Fifteen \", \"Sixteen \", \"Seventeen \", \"Eighteen \", \"Nineteen \"};\n\nconst char *twenty_to_ninety[8] = {\"Twenty \", \"Thirty \", \"Forty \", \"Fifty \", \"Sixty \", \"Seventy \", \"Eighty \", \"Ninety \"};\n\nconst char *big_numbers[7] = {\"\", \"Thousand \", \"Million \", \"Billion \", \"Trillion \", \"Quadrillion \", \"Quintillion \"};\n\nvoid num_to_word(char *buf, unsigned long long num)\n{\n  unsigned long long power_of_1000 = 1000000000000000000ull;\n  int power_index = 6;\n\n  if(num == 0)\n  {\n    strcpy(buf, \"Zero\");\n    return;\n  }\n\n  buf[0] = 0;\n\n  while(power_of_1000 > 0)\n  {\n    int group = num / power_of_1000;\n    if(group >= 100)\n    {\n      strcat(buf, zero_to_nineteen[group / 100]);\n      strcat(buf, \"Hundred \");\n      group %= 100;\n    }\n\n    if(group >= 20)\n    {\n      strcat(buf, twenty_to_ninety[group / 10 - 2]);\n      group %= 10;\n    }\n\n    if(group > 0)\n      strcat(buf, zero_to_nineteen[group]);\n\n    if(num >= power_of_1000)\n      strcat(buf, big_numbers[power_index]);\n\n    num %= power_of_1000;\n    power_of_1000 /= 1000;\n    power_index--;\n  }\n\n  buf[strlen(buf) - 1] = 0;\n}\n\nAnd here's a much more obfuscated version of that (682 characters).  It could probably be pared down a little more if I really tried.\n#include <string.h>\n#define C strcat(b,\n#define U unsigned long long\nchar*z[]={\"\",\"One\",\"Two\",\"Three\",\"Four\",\"Five\",\"Six\",\"Seven\",\"Eight\",\"Nine\",\"Ten\",\"Eleven\",\"Twelve\",\"Thirteen\",\"Fourteen\",\"Fifteen\",\"Sixteen\",\"Seventeen\",\"Eighteen\",\"Nineteen\"},*t[]={\"Twenty \",\"Thirty \",\"Forty \",\"Fifty \",\"Sixty \",\"Seventy \",\"Eighty \",\"Ninety \"},*q[]={\"\",\"Thousand \",\"Million \",\"Billion \",\"Trillion \",\"Quadrillion \",\"Quintillion \"};\nvoid W(char*b,U n){U p=1000000000000000000ull;int i=6;*b=0;if(!n)strcpy(b,\"Zero \");else while(p){int g=n/p;if(g>99){C z[g/100]);C \" \");C \"Hundred \");g%=100;}if(g>19){C t[g/10-2]);g%=10;}if(g)C z[g]),C \" \");if(n>=p)C q[i]);n%=p;p/=1000;i--;}b[strlen(b)-1]=0;}\n\n"}, "984": {"topic": "Code Golf: Number to Words", "user_name": "", "text": "\nA T-SQL (SQL Server 2005) function, including test cases:\nif exists (select 1 from sys.objects where object_id = object_id(N'dbo.fnGetNumberString'))\n    drop function fnGetNumberString\ngo\n\n/*\nTests:\ndeclare @tests table ( testValue bigint )\ninsert into @tests select -43213 union select -5 union select 0 union select 2 union select 15 union select 33 union select 100 union select 456 union select 1024 union select 10343 union select 12345678901234 union select -3434343434343\n\nselect testValue, dbo.fnGetNumberString(testValue) as textValue\nfrom @tests\n*/\n\ncreate function dbo.fnGetNumberString\n(\n    @value bigint\n)\nreturns nvarchar(1024)\nas\nbegin\n    if @value = 0 return 'zero' -- lets me avoid special-casing this later\n\n    declare @isNegative bit\n    set @isNegative = 0\n\n    if @value < 0\n        select @isNegative = 1, @value = @value * -1\n\n    declare @groupNames table ( groupOrder int, groupName nvarchar(15) )\n    insert into @groupNames select 1, '' union select 2, 'thousand' union select 3, 'million' union select 4, 'billion' union select 5, 'trillion' union select 6, 'quadrillion' union select 7, 'quintillion' union select 8, 'sextillion'\n\n    declare @digitNames table ( digit tinyint, digitName nvarchar(10) )\n    insert into @digitNames select 0, '' union select 1, 'one' union select 2, 'two' union select 3, 'three' union select 4, 'four' union select 5, 'five' union select 6, 'six' union select 7, 'seven' union select 8, 'eight' union select 9, 'nine' union select 10, 'ten' union select 11, 'eleven' union select 12, 'twelve' union select 13, 'thirteen' union select 14, 'fourteen' union select 15, 'fifteen' union select 16, 'sixteen' union select 17, 'seventeen' union select 18, 'eighteen' union select 19, 'nineteen'\n\n    declare @tensGroups table ( digit tinyint, groupName nvarchar(10) )\n    insert into @tensGroups select 2, 'twenty' union select 3, 'thirty' union select 4, 'forty' union select 5, 'fifty' union select 6, 'sixty' union select 7, 'seventy' union select 8, 'eighty' union select 9, 'ninety'\n\n    declare @groups table ( groupOrder int identity, groupValue int )\n\n    declare @convertedValue varchar(50)\n\n    while @value > 0\n    begin\n        insert into @groups (groupValue) select @value % 1000\n\n        set @value = @value / 1000\n    end\n\n    declare @returnValue nvarchar(1024)\n    set @returnValue = ''\n\n    if @isNegative = 1 set @returnValue = 'negative'\n\n    select @returnValue = @returnValue +\n        case when len(h.digitName) > 0 then ' ' + h.digitName + ' hundred' else '' end +\n        case when len(isnull(t.groupName, '')) > 0 then ' ' + t.groupName + case when len(isnull(o.digitName, '')) > 0 then '-' else '' end + isnull(o.digitName, '') else case when len(isnull(o.digitName, '')) > 0 then ' ' + o.digitName else '' end end +\n        case when len(n.groupName) > 0 then ' ' + n.groupName else '' end\n    from @groups g\n        join @groupNames n on n.groupOrder = g.groupOrder\n        join @digitNames h on h.digit = (g.groupValue / 100)\n        left join @tensGroups t on t.digit = ((g.groupValue % 100) / 10)\n        left join @digitNames o on o.digit = case when (g.groupValue % 100) < 20 then g.groupValue % 100 else g.groupValue % 10 end\n    order by g.groupOrder desc\n\n    return @returnValue\nend\ngo\n\n"}, "985": {"topic": "Code Golf: Number to Words", "user_name": "\r", "text": "\nHere's a Scala solution. I'm not happy about trying to make it look short -- I sacrificed a bit of readability :(\n\nobject NumSpeller {\n  val digits = Array(\"\",\"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\"seven\",\"eight\",\"nine\")\n  val teens = Array(\"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\")\n  val tens = Array(\"\", \"ten\", \"twenty\", \"thirty\", \"fourty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\")\n  val thousands = Array(\"\", \"thousand\", \"million\", \"billion\", \"trillion\", \"quadrillion\", \"quintillion\")\n\n  def spellGroup(num:Int) = {\n    val (v3, v2, v1) = ((num / 100) % 10, (num / 10) % 10, num % 10)\n    val hs = v3 match { case 0 => \"\"; case d => digits(d) + \" hundred \" }\n    val ts = v2 match {\n      case 0 => digits(v1)\n      case 1 => teens(v1)\n      case _ => v3 match { case 0 => tens(v2); case _ => tens(v2) + \"-\" + digits(v1) }\n    }\n    hs + ts\n  }\n\n  def numberGroups(num:Long) = {\n    def _numberGroups(num:Long, factor:Int):List[(Double,Int)] = factor match {\n      case 0 => List((num % 1000,0))\n      case _ => ((num / Math.pow(1000, factor)) % 1000, factor) :: _numberGroups(num, factor - 1)\n    }\n    val ints = _numberGroups(num, 6) map (x => (x._1.asInstanceOf[Int],x._2))\n    ints dropWhile (x => x._1 == 0.0)\n  }\n\n  def spell(num:Long) = num match { case 0 => \"zero\"; case _ => (numberGroups(num) map { x => spellGroup(x._1) + \" \" + thousands(x._2) + \" \" }).mkString.trim  }\n}\n\nUsage is:\nNumSpeller.spell(458582)\n\n"}, "986": {"topic": "Code Golf: Number to Words", "user_name": "\r", "text": "\nPerl 5.10\nmy %expo=(0,'',\n  qw'1 thousand 2 million 3 billion 4 trillion 5 quadrillion 6 quintillion\n  7 sextillion 8 septillion 9 octillion 10 nonillion 11 decillion 12 undecillion\n  13 duodecillion 14 tredecillion 15 quattuordecillion 16 quindecillion\n  17 sexdecillion 18 septendecillion 19 octodecillion 20 novemdecillion\n  21 vigintillion'\n);\n\nmy %digit=(0,'',\n  qw'1 one 2 two 3 three 4 four 5 five 6 six 7 seven 8 eight 9 nine 10 ten\n  11 eleven 12 twelve 13 thirteen 14 fourteen 15 fifteen 16 sixteen 17 seventeen\n  18 eighteen 19 nineteen 2* twenty 3* thirty 4* forty 5* fifty 6* sixty\n  7* seventy 8* eighty 9* ninety'\n);\n\nsub spell_number(_){\n  local($_)=@_;\n  ($_,@_)=split/(?=(?:.{3})*+$)/;\n  $_=0 x(3-length).$_;\n  unshift@_,$_;\n  my @o;\n  my $c=@_;\n  for(@_){\n    my $o='';\n    /(.)(.)(.)/;\n    $o.=$1?$digit{$1}.' hundred':'';\n    $o.=$2==1?\n      ' '.$digit{$2.$3}\n    :\n      ($2?' '.$digit{\"$2*\"}:'').\n      ($2&&$3?' ':'').\n      $digit{$3}\n    ;\n    $o.=--$c?($o?' '.$expo{$c}.', ':''):'';\n    push@o,$o;\n  }\n  my $o;\n  $o.=$_ for@o;\n  $o=~/^\\s*+(.*?)(, )?$/;\n  $o?$1:'zero';\n}\n\nNotes:\n\nThis almost works on earlier Perls, it's that first split() that seems to be the main problem.  As it sits now the strings take up the bulk of the characters.\nI could have shortened it, by removing the my's, and the local, as well as putting it all on one line.\nI used Number::Spell as a starting point.\nWorks under strict and warnings.\n\n"}, "987": {"topic": "Code Golf: Number to Words", "user_name": "", "text": "\nMmm, you might have put the bar a bit high, both on the limit (18,446,744,073,709,552,000, I don't even know how to write that!) and on the goal (the other code golfs resulted in short code, this one will be long at least for the data (words)).\nAnyway, for the record, I give an well known solution (not mine!) for French, in PHP: \u00c9criture des nombres en fran\u00e7ais. :-)\nNote the ambiguity (voluntary or not) of your wording: \"Submissions in any language welcome\"\nI first took it as \"natural language\", before understand you probably meant \"programming language...\nThe algorithm is probably simpler in English (and with less regional variants...).\n"}, "988": {"topic": "Code Golf: Number to Words", "user_name": "\r", "text": "\nIn the D programming language\nstring Number(ulong i)\n{\n    static string[] names = [\n      \"\"[],\n      \" thousand\",\n      \" million\",\n      \" billion\",\n      \" trillion\",\n      \" quadrillion\",\n      ];\n    string ret = null;\n    foreach(mult; names)\n    {\n       if(i%1000 != 0)\n       {\n           if(ret != null) ret = ret ~ \", \"\n           ret = Cent(i%1000) ~ mult ~ ret;\n       }\n       i /= 1000;\n    }\n    return ret;\n}\n\nstring Cent(int i)\n{\n   static string[] v = \n        [\"\"[], \"one\", \"two\", \"three\", \"four\", \n        \"five\", \"six\", \"seven\", \"eight\", \"nine\"];\n\n   static string[] tens = \n        [\"!\"[], \"!\", \"twenty\", \"thirty\", \"forty\", \n        \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"];\n\n   string p1, p2, p3 = \"\";\n\n\n   if(i >= 100)\n   {\n      p1 = v[i/100] ~ \" hundred\";\n      p3 = (i % 100 != 0) ? \" and \" : \"\"; //optional\n   }\n   else\n      p1 = \"\";\n\n   i %= 100;\n   switch(i)\n   {\n       case 0, 1, 2, 3, 4, 5, 6, 7, 8, 9:\n          p2 = v[i];\n          break;\n\n       case 10: p2 = \"ten\"; break;\n       case 11: p2 = \"eleven\"; break;\n       case 12: p2 = \"twelve\"; break;\n       case 13: p2 = \"thirteen\"; break;\n       case 14: p2 = \"fourteen\"; break;\n       case 15: p2 = \"fifteen\"; break;\n       case 16: p2 = \"sixteen\"; break;\n       case 17: p2 = \"seventeen\"; break;\n       case 18: p2 = \"eighteen\"; break;\n       case 19: p2 = \"nineteen\"; break;\n\n       default:\n           p2 = tens[i/10] ~ \"-\" ~ v[i%10];\n           break;\n\n   }\n\n   return p1 ~ p3 ~ p2;\n}\n\nimport std.stdio;\nvoid main()\n{\n  writef(\"%s\\n\", Number(8_000_400_213));\n}\n\nTry it out here\n"}, "989": {"topic": "Code Golf: Number to Words", "user_name": "\r", "text": "\nDoes anyone plan on adding the appropriate commas and 'and' any time soon? Or hyphenating twenty-one through ninety-nine? Not much point otherwise, IMHO :)\n'Nine Hundred Ninety Nine Thousand Nine Hundred Ninety Nine'\nvs\n'Nine hundred and ninety-nine thousand, nine hundred and ninety-nine'\n(And no, mine doesn't work. Yet.)\n"}, "990": {"topic": "Code Golf: Number to Words", "user_name": "", "text": "\n#!/usr/bin/env perl\nmy %symbols = (\n1 => \"One\", 2 => \"Two\", 3 => \"Three\", 4 => \"Four\", 5 => \"Five\",\n6 => \"Six\", 7 => \"Seven\", 8 => \"Eight\", 9 => \"Nine\", 10 => \"Ten\",\n11 => \"Eleven\", 12 => \"Twelve\", 13 => \"Thirteen\", 14 => \"Fourteen\",\n15 => \"Fifteen\", 16 => \"Sixteen\", 17 => \"Seventeen\", 18 => \"Eighteen\",\n19 => \"Nineteen\", 20 => \"Twenty\", 30 => \"Thirty\", 40 => \"Forty\",\n50 => \"Fifty\", 60 => \"Sixty\", 70 => \"Seventy\", 80 => \"Eighty\",\n90 => \"Ninety\", 100 => \"Hundred\");\n\nmy %three_symbols = (1 => \"Thousand\", 2 => \"Million\", 3 => \"Billion\" );\n\nsub babo {\nmy ($input) = @_;\nmy @threes = split(undef, $input);\nmy $counter = ($#threes + 1);\nmy $remainder = $counter % 3;\nmy @result;\n\nwhile ($counter > 0){\n    my $digits = \"\";\n    my $three;\n    my $full_match = 0;\n\n    if ($remainder > 0){\n        while ($remainder > 0) {\n            $digits .= shift(@threes);\n            $remainder--;\n            $counter--;\n        }\n    }\n    else {\n        $digits = join('',@threes[0,1,2]);\n        splice(@threes, 0, 3);\n        $counter -= 3;\n    }\n    if (exists($symbols{$digits})){\n        $three = $symbols{$digits};\n        $full_match = 1;\n    }\n    elsif (length($digits) == 3) {\n        $three = $symbols{substr($digits,0,1)};\n        $three .= \" Hundred\";\n        $digits = substr($digits,1,2);\n        if (exists($symbols{$digits})){\n            $three .= \" \" . $symbols{$digits};\n            $full_match = 1;\n        }\n    }\n    if ($full_match == 0){\n        $three .= \" \" . $symbols{substr($digits,0,1).\"0\"};\n        $three .= \" \" . $symbols{substr($digits,1,1)};\n    }\n    push(@result, $three);\n    if ($counter > 0){\n        push(@result, \"Thousand\");\n    }\n}\nmy $three_counter = 0;\nmy @r = map {$_ eq \"Thousand\" ? $three_symbols{++$three_counter}:$_ }\n    reverse @result;\nreturn join(\" \", reverse @r);\n}\nprint babo(1) . \"\\n\";\nprint babo(12) . \"\\n\";\nprint babo(120) . \"\\n\";\nprint babo(1234) . \"\\n\";\nprint babo(12345) . \"\\n\";\nprint babo(123456) . \"\\n\";\nprint babo(1234567) . \"\\n\";\nprint babo(1234567890) . \"\\n\";\n\n"}, "991": {"topic": "Code Golf: Number to Words", "user_name": "\r", "text": "\nI can't find the file now, but this was an Intro to Programming problem (late in the term) where I went to school.  We had to be able to turn a float into a valid written number for use on a check.\nAfter the assignment was completed the professor showed some C++ code that solved the problem using only concepts we'd already covered.  It ran just 43 lines, and was well-documented.\n"}, "992": {"topic": "Code Golf: Number to Words", "user_name": "\r", "text": "\nPerl & CPAN working together:\n#!/usr/bin/perl\n\nuse strict;\nuse warnings;\n\nuse Lingua::EN::Numbers qw(num2en);\n\nprint num2en($_), \"\\n\" for 2, 1024, 1024*1024;\n\n\nC:\\Temp> n.pl\ntwo\none thousand and twenty-four\none million, forty-eight thousand, five hundred and seventy-six\n"}, "993": {"topic": "Code Golf: Number to Words", "user_name": "", "text": "\nHere's one in PHP, from Convert Numbers to Words:\nconvert_number(2850)\n\nreturns\n\nTwo Thousand Eight Hundred and Fifty\n\nand if you want an even more awesome one that handles commas and numbers up to  vigintillion check out zac hesters work at Language Display Functions:\nfunction convert_number($number)\n{\n    if (($number < 0) || ($number > 999999999))\n    {\n        throw new Exception(\"Number is out of range\");\n    }\n\n    $Gn = floor($number / 1000000);  /* Millions (giga) */\n    $number -= $Gn * 1000000;\n    $kn = floor($number / 1000);     /* Thousands (kilo) */\n    $number -= $kn * 1000;\n    $Hn = floor($number / 100);      /* Hundreds (hecto) */\n    $number -= $Hn * 100;\n    $Dn = floor($number / 10);       /* Tens (deca) */\n    $n = $number % 10;               /* Ones */\n\n    $res = \"\";\n\n    if ($Gn)\n    {\n        $res .= convert_number($Gn) . \" Million\";\n    }\n\n    if ($kn)\n    {\n        $res .= (empty($res) ? \"\" : \" \") .\n            convert_number($kn) . \" Thousand\";\n    }\n\n    if ($Hn)\n    {\n        $res .= (empty($res) ? \"\" : \" \") .\n            convert_number($Hn) . \" Hundred\";\n    }\n\n    $ones = array(\"\", \"One\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\",\n        \"Seven\", \"Eight\", \"Nine\", \"Ten\", \"Eleven\", \"Twelve\", \"Thirteen\",\n        \"Fourteen\", \"Fifteen\", \"Sixteen\", \"Seventeen\", \"Eightteen\",\n        \"Nineteen\");\n    $tens = array(\"\", \"\", \"Twenty\", \"Thirty\", \"Fourty\", \"Fifty\", \"Sixty\",\n        \"Seventy\", \"Eigthy\", \"Ninety\");\n\n    if ($Dn || $n)\n    {\n        if (!empty($res))\n        {\n            $res .= \" and \";\n        }\n\n        if ($Dn < 2)\n        {\n            $res .= $ones[$Dn * 10 + $n];\n        }\n        else\n        {\n            $res .= $tens[$Dn];\n\n            if ($n)\n            {\n                $res .= \"-\" . $ones[$n];\n            }\n        }\n    }\n\n    if (empty($res))\n    {\n        $res = \"zero\";\n    }\n\n    return $res;\n}\n\n"}, "994": {"topic": "Code Golf: Number to Words", "user_name": "\r", "text": "\nA few years ago I created this in C# for multi language applications.\nThis one is base class:\npublic abstract class ValueSource\n{\n    public abstract object Value { get; }\n}\n\nThis one is for wordify..\npublic abstract class NumberTextValueSource:ValueSource\n{\n    public abstract decimal Number { get; }\n    public abstract string Format { get; }\n    public abstract string Negative { get; }\n    public abstract bool UseValueIfZero { get; }\n    public abstract string N0 { get; }\n    public abstract string N1 { get; }\n    public abstract string N2 { get; }\n    public abstract string N3 { get; }\n    public abstract string N4 { get; }\n    public abstract string N5 { get; }\n    public abstract string N6 { get; }\n    public abstract string N7 { get; }\n    public abstract string N8 { get; }\n    public abstract string N9 { get; }\n    public abstract string N10 { get; }\n    public abstract string N11 { get; }\n    public abstract string N12 { get; }\n    public abstract string N13 { get; }\n    public abstract string N14 { get; }\n    public abstract string N15 { get; }\n    public abstract string N16 { get; }\n    public abstract string N17 { get; }\n    public abstract string N18 { get; }\n    public abstract string N19 { get; }\n    public abstract string N20 { get; }\n    public abstract string N30 { get; }\n    public abstract string N40 { get; }\n    public abstract string N50 { get; }\n    public abstract string N60 { get; }\n    public abstract string N70 { get; }\n    public abstract string N80 { get; }\n    public abstract string N90 { get; }\n    public abstract string N100 { get; }\n    public abstract string NHundred { get; }\n    public abstract string N1000 { get; }\n    public abstract string NThousand { get; }\n    public abstract string NMillion { get; }\n    public abstract string NBillion { get; }\n    public abstract string NTrillion { get; }\n    public abstract string NQuadrillion { get; }\n\n\n    string getOne(Type t, string v)\n    {\n        if (v[0] == '0' && !UseValueIfZero)\n            return \"\";\n        return (string)t.GetProperty(\"N\" + v[0].ToString()).GetValue(this, null);\n    }\n\n\n    string getTwo(Type t, string v)\n    {\n        if (v[0] == '0')\n            if (v[1] != '0')\n                return getOne(t, v.Substring(1));\n            else\n                return \"\";\n\n        if (v[1] == '0' || v[0] == '1')\n            return (string)t.GetProperty(\"N\" + v).GetValue(this, null);\n\n        return (string)t.GetProperty(\"N\" + v[0].ToString() + \"0\").GetValue(this, null) +\n               getOne(t, v.Substring(1));\n    }\n\n\n    string getThree(Type t, string v)\n    {\n        if(v[0] == '0')\n            return getTwo(t,v.Substring(1));\n\n        if (v[0] == '1')\n            return\n                N100 +\n                getTwo(t, v.Substring(1));\n        return\n            getOne(t, v[0].ToString()) +\n            NHundred +\n            getTwo(t, v.Substring(1));\n    }\n\n\n    string getFour(Type t, string v)\n    {\n        if (v[0] == '0')\n            return getThree(t, v.Substring(1));\n        if (v[0] == '1')\n            return\n                N1000 +\n                getThree(t, v.Substring(1));\n        return\n            getOne(t, v[0].ToString()) +\n            NThousand +\n            getThree(t, v.Substring(1));\n    }\n\n\n    string getFive(Type t, string v)\n    {\n        if (v[0] == '0')\n            return getFour(t, v.Substring(1));\n        return\n            getTwo(t, v.Substring(0, 2)) +\n            NThousand +\n            getThree(t, v.Substring(2));\n    }\n\n\n    string getSix(Type t, string v)\n    {\n        if (v[0] == '0')\n            return getFive(t, v.Substring(1));\n        return\n            getThree(t, v.Substring(0, 3)) +\n            NThousand +\n            getThree(t, v.Substring(3));\n    }\n\n\n    string getSeven(Type t, string v)\n    {\n        if (v[0] == '0')\n            return getSix(t, v.Substring(1));\n        return\n            getOne(t, v[0].ToString()) +\n            NMillion +\n            getSix(t, v.Substring(3));\n    }\n\n\n    string getEight(Type t, string v)\n    {\n        if (v[0] == '0')\n            return getSeven(t, v.Substring(1));\n        return\n            getTwo(t, v.Substring(0, 2)) +\n            NMillion +\n            getSix(t, v.Substring(2));\n    }\n\n\n    string getNine(Type t, string v)\n    {\n        if (v[0] == '0')\n            return getEight(t, v.Substring(1));\n        return\n            getThree(t, v.Substring(0, 3)) +\n            NMillion +\n            getSix(t, v.Substring(3));\n    }\n\n\n    string getTen(Type t, string v)\n    {\n        if (v[0] == '0')\n            return getNine(t, v.Substring(1));\n        return\n            getOne(t, v.Substring(0, 1)) +\n            NBillion +\n            getNine(t, v.Substring(1));\n    }\n\n\n    string getEleven(Type t, string v)\n    {\n        if (v[0] == '0')\n            return getTen(t, v.Substring(1));\n        return\n            getTwo(t, v.Substring(0, 2)) +\n            NBillion +\n            getNine(t, v.Substring(2));\n    }\n\n\n    string getTwelve(Type t, string v)\n    {\n        if (v[0] == '0')\n            return getEleven(t, v.Substring(1));\n        return\n            getThree(t, v.Substring(0, 3)) +\n            NBillion +\n            getNine(t, v.Substring(3));\n    }\n\n\n    string getThirteen(Type t, string v)\n    {\n        if (v[0] == '0')\n            return getTwelve(t, v.Substring(1));\n        return\n            getOne(t, v.Substring(0, 1)) +\n            NTrillion +\n            getTwelve(t, v.Substring(1));\n    }\n\n\n    string getForteen(Type t, string v)\n    {\n        if (v[0] == '0')\n            return getThirteen(t, v.Substring(1));\n        return\n            getTwo(t, v.Substring(0, 2)) +\n            NTrillion +\n            getTwelve(t, v.Substring(2));\n    }\n\n\n    string getFifteen(Type t, string v)\n    {\n        if (v[0] == '0')\n            return getForteen(t, v.Substring(1));\n        return\n            getThree(t, v.Substring(0, 3)) +\n            NTrillion +\n            getTwelve(t, v.Substring(3));\n    }\n\n\n    string getSixteen(Type t, string v)\n    {\n        if (v[0] == '0')\n            return getFifteen(t, v.Substring(1));\n        return\n            getOne(t, v.Substring(0, 1)) +\n            NQuadrillion +\n            getFifteen(t, v.Substring(1));\n    }\n\n\n    string getSeventeen(Type t, string v)\n    {\n        if (v[0] == '0')\n            return getSixteen(t, v.Substring(1));\n        return\n            getTwo(t, v.Substring(0, 2)) +\n            NQuadrillion +\n            getFifteen(t, v.Substring(2));\n    }\n\n\n    string getEighteen(Type t, string v)\n    {\n        if (v[0] == '0')\n            return getSeventeen(t, v.Substring(1));\n        return\n            getThree(t, v.Substring(0, 3)) +\n            NQuadrillion +\n            getFifteen(t, v.Substring(3));\n    }\n\n\n    string convert(Type t, string hp)\n    {\n        switch (hp.Length)\n        {\n            case 1:\n                return getOne(t, hp);\n            case 2:\n                return getTwo(t, hp);\n            case 3:\n                return getThree(t, hp);\n            case 4:\n                return getFour(t, hp);\n            case 5:\n                return getFive(t, hp);\n            case 6:\n                return getSix(t, hp);\n            case 7:\n                return getSeven(t, hp);\n            case 8:\n                return getEight(t, hp);\n            case 9:\n                return getNine(t, hp);\n            case 10:\n                return getTen(t, hp);\n            case 11:\n                return getEleven(t, hp);\n            case 12:\n                return getTwelve(t, hp);\n            case 13:\n                return getThirteen(t, hp);\n            case 14:\n                return getForteen(t, hp);\n            case 15:\n                return getFifteen(t, hp);\n            case 16:\n                return getSixteen(t, hp);\n            case 17:\n                return getSeventeen(t, hp);\n            case 18:\n                return getEighteen(t, hp);\n        }\n        return \"\";\n    }\n\n\n    public override object Value\n    {\n        get\n        {\n            decimal d = Number;\n            decimal highPoint, lowPoint;\n            bool isNeg = d < 0;\n            d = Math.Abs(d);\n            highPoint = Math.Floor(d);\n            lowPoint = d - highPoint;\n            Type t = this.GetType();\n\n            string strHigh = convert(t, highPoint.ToString()),\n                    strLow =\n                       lowPoint > 0 ?\n                       convert(t, lowPoint.ToString().Substring(2)) :\n                       UseValueIfZero ? N0 : \"\";\n            if (isNeg) strHigh = Negative + \" \" + strHigh;\n            return string.Format(Format, strHigh, strLow);\n        }\n    }\n}\n\nAnd this one is for Turkish Lera (TRY):\npublic class TRYNumberTextValueSource:NumberTextValueSource\n{\n    decimal num;\n    public TRYNumberTextValueSource(decimal value)\n    {\n        num = Math.Round(value, 2);\n    }\n    public override decimal Number\n    {\n        get { return num; }\n    }\n\n    public override string Format\n    {\n        get\n        {\n            if (num == 0)\n                return N0 + \" YTL\";\n            if (num > -1 && num < 1)\n                return \"{0}{1} Kurus\";\n            return \"{0} YTL {1} Kurus\";\n        }\n    }\n\n    public override string Negative\n    {\n        get { return \"-\"; }\n    }\n\n    public override bool UseValueIfZero\n    {\n        get { return false; }\n    }\n\n    public override string N0\n    {\n        get { return \"sifir\"; }\n    }\n\n    public override string N1\n    {\n        get { return \"bir\"; }\n    }\n\n    public override string N2\n    {\n        get { return \"iki\"; }\n    }\n\n    public override string N3\n    {\n        get { return \"\u00fc\u00e7\"; }\n    }\n\n    public override string N4\n    {\n        get { return \"d\u00f6rt\"; }\n    }\n\n    public override string N5\n    {\n        get { return \"bes\"; }\n    }\n\n    public override string N6\n    {\n        get { return \"alti\"; }\n    }\n\n    public override string N7\n    {\n        get { return \"yedi\"; }\n    }\n\n    public override string N8\n    {\n        get { return \"sekiz\"; }\n    }\n\n    public override string N9\n    {\n        get { return \"dokuz\"; }\n    }\n\n    public override string N10\n    {\n        get { return \"on\"; }\n    }\n\n    public override string N11\n    {\n        get { return \"onbir\"; }\n    }\n\n    public override string N12\n    {\n        get { return \"oniki\"; }\n    }\n\n    public override string N13\n    {\n        get { return \"on\u00fc\u00e7\"; }\n    }\n\n    public override string N14\n    {\n        get { return \"ond\u00f6rt\"; }\n    }\n\n    public override string N15\n    {\n        get { return \"onbes\"; }\n    }\n\n    public override string N16\n    {\n        get { return \"onalti\"; }\n    }\n\n    public override string N17\n    {\n        get { return \"onyedi\"; }\n    }\n\n    public override string N18\n    {\n        get { return \"onsekiz\"; }\n    }\n\n    public override string N19\n    {\n        get { return \"ondokuz\"; }\n    }\n\n    public override string N20\n    {\n        get { return \"yirmi\"; }\n    }\n\n    public override string N30\n    {\n        get { return \"otuz\"; }\n    }\n\n    public override string N40\n    {\n        get { return \"kirk\"; }\n    }\n\n    public override string N50\n    {\n        get { return \"elli\"; }\n    }\n\n    public override string N60\n    {\n        get { return \"altmis\"; }\n    }\n\n    public override string N70\n    {\n        get { return \"yetmis\"; }\n    }\n\n    public override string N80\n    {\n        get { return \"seksen\"; }\n    }\n\n    public override string N90\n    {\n        get { return \"doksan\"; }\n    }\n\n    public override string N100\n    {\n        get { return \"y\u00fcz\"; }\n    }\n\n    public override string NHundred\n    {\n        get { return \"y\u00fcz\"; }\n    }\n\n    public override string N1000\n    {\n        get { return \"bin\"; }\n    }\n\n    public override string NThousand\n    {\n        get { return \"bin\"; }\n    }\n\n    public override string NMillion\n    {\n        get { return \"milyon\"; }\n    }\n\n    public override string NBillion\n    {\n        get { return \"milyar\"; }\n    }\n\n    public override string NTrillion\n    {\n        get { return \"trilyon\"; }\n    }\n\n    public override string NQuadrillion\n    {\n        get { return \"trilyar\"; }\n    }\n}\n\nAnd it is used this way:\nMessageBox.show((string)(new TRYNumberTextValueSource(12345)).Value);\n\n"}, "995": {"topic": "How do I do dependency parsing in NLTK?", "user_name": "Pierre", "text": "\nGoing through the NLTK book, it's not clear how to generate a dependency tree from a given sentence.\nThe relevant section of the book: sub-chapter on dependency grammar gives an example figure but it doesn't show how to parse a sentence to come up with those relationships - or maybe I'm missing something fundamental in NLP?\nEDIT:\nI want something similar to what the stanford parser does:\nGiven a sentence \"I shot an elephant in my sleep\", it should return something like:\nnsubj(shot-2, I-1)\ndet(elephant-4, an-3)\ndobj(shot-2, elephant-4)\nprep(shot-2, in-5)\nposs(sleep-7, my-6)\npobj(in-5, sleep-7)\n\n"}, "996": {"topic": "How do I do dependency parsing in NLTK?", "user_name": "MrDMrD", "text": "\nWe can use Stanford Parser from NLTK.\nRequirements\nYou need to download two things from their website: \n\nThe Stanford CoreNLP parser.\nLanguage model for your desired language (e.g. english language model)\n\nWarning!\nMake sure that your language model version matches your Stanford CoreNLP parser version!\nThe current CoreNLP version as of May 22, 2018 is 3.9.1. \nAfter downloading the two files, extract the zip file anywhere you like.\nPython Code\nNext, load the model and use it through NLTK\nfrom nltk.parse.stanford import StanfordDependencyParser\n\npath_to_jar = 'path_to/stanford-parser-full-2014-08-27/stanford-parser.jar'\npath_to_models_jar = 'path_to/stanford-parser-full-2014-08-27/stanford-parser-3.4.1-models.jar'\n\ndependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n\nresult = dependency_parser.raw_parse('I shot an elephant in my sleep')\ndep = result.next()\n\nlist(dep.triples())\n\nOutput\nThe output of the last line is:\n[((u'shot', u'VBD'), u'nsubj', (u'I', u'PRP')),\n ((u'shot', u'VBD'), u'dobj', (u'elephant', u'NN')),\n ((u'elephant', u'NN'), u'det', (u'an', u'DT')),\n ((u'shot', u'VBD'), u'prep', (u'in', u'IN')),\n ((u'in', u'IN'), u'pobj', (u'sleep', u'NN')),\n ((u'sleep', u'NN'), u'poss', (u'my', u'PRP$'))]\n\nI think this is what you want.\n"}, "997": {"topic": "How do I do dependency parsing in NLTK?", "user_name": "dsapalo", "text": "\nI think you could use a corpus-based dependency parser instead of the grammar-based one NLTK provides.\nDoing corpus-based dependency parsing on a even a small amount of text in Python is not ideal performance-wise. So in NLTK they do provide a wrapper to MaltParser, a corpus based dependency parser.\nYou might find this other question about RDF representation of sentences relevant.\n"}, "998": {"topic": "How do I do dependency parsing in NLTK?", "user_name": "ywatywat", "text": "\nIf you need better performance, then spacy (https://spacy.io/) is the best choice. Usage is very simple:\nimport spacy\n\nnlp = spacy.load('en')\nsents = nlp(u'A woman is walking through the door.')\n\nYou'll get a dependency tree as output, and you can dig out very easily every information you need. You can also define your own custom pipelines. See more on their website.\nhttps://spacy.io/docs/usage/\n"}, "999": {"topic": "How do I do dependency parsing in NLTK?", "user_name": "CommunityBot", "text": "\nIf you want to be serious about dependance parsing don't use the NLTK, all the algorithms are dated, and slow. Try something like this: https://spacy.io/\n"}, "1000": {"topic": "How do I do dependency parsing in NLTK?", "user_name": "NeodawnNeodawn", "text": "\nTo use Stanford Parser from NLTK\n1) Run CoreNLP Server at localhost\nDownload Stanford CoreNLP here (and also model file for your language).\nThe server can be started by running the following command (more details here)\n# Run the server using all jars in the current directory (e.g., the CoreNLP home directory)\njava -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n\nor by NLTK API (need to configure the CORENLP_HOME environment variable first)  \nos.environ[\"CORENLP_HOME\"] = \"dir\"\nclient = corenlp.CoreNLPClient()\n# do something\nclient.stop()\n\n2) Call the dependency parser from NLTK\n>>> from nltk.parse.corenlp import CoreNLPDependencyParser\n>>> dep_parser = CoreNLPDependencyParser(url='http://localhost:9000')\n>>> parse, = dep_parser.raw_parse(\n...     'The quick brown fox jumps over the lazy dog.'\n... )\n>>> print(parse.to_conll(4))  \nThe     DT      4       det\nquick   JJ      4       amod\nbrown   JJ      4       amod\nfox     NN      5       nsubj\njumps   VBZ     0       ROOT\nover    IN      9       case\nthe     DT      9       det\nlazy    JJ      9       amod\ndog     NN      5       nmod\n.       .       5       punct\n\nSee detail documentation here, also this question NLTK CoreNLPDependencyParser: Failed to establish connection.\n"}, "1001": {"topic": "How do I do dependency parsing in NLTK?", "user_name": "Ram G Athreya", "text": "\nFrom the Stanford Parser documentation: \"the dependencies can be obtained using our software [...] on phrase-structure trees using the EnglishGrammaticalStructure class available in the parser package.\" http://nlp.stanford.edu/software/stanford-dependencies.shtml\nThe dependencies manual also mentions: \"Or our conversion tool can convert the\noutput of other constituency parsers to the Stanford Dependencies representation.\" http://nlp.stanford.edu/software/dependencies_manual.pdf\nNeither functionality seem to be implemented in NLTK currently.\n"}, "1002": {"topic": "How do I do dependency parsing in NLTK?", "user_name": "Aleksandar JovanovicAleksandar Jovanovic", "text": "\nA little late to the party, but I wanted to add some example code with SpaCy that gets you your desired output:\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"I shot an elephant in my sleep\")\nfor token in doc:\n    print(\"{2}({3}-{6}, {0}-{5})\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_, token.i+1, token.head.i+1))\n\nAnd here's the output, very similar to your desired output:\nnsubj(shot-2, I-1)\nROOT(shot-2, shot-2)\ndet(elephant-4, an-3)\ndobj(shot-2, elephant-4)\nprep(shot-2, in-5)\nposs(sleep-7, my-6)\npobj(in-5, sleep-7)\n\nHope that helps!\n"}, "1003": {"topic": "Is there a natural language parser for date/times in javascript?", "user_name": "antony.trupeantony.trupe", "text": "\nIs there a natural language parser for date/times in javascript?\n"}, "1004": {"topic": "Is there a natural language parser for date/times in javascript?", "user_name": "mikemaccana", "text": "\nI made Chrono a small library for parsing dates in JavaScript. I added a date range parsing feature (such as '12 Nov - 13 Dec 2012') .\n"}, "1005": {"topic": "Is there a natural language parser for date/times in javascript?", "user_name": "Wanasit TanakitrungruangWanasit Tanakitrungruang", "text": "\nSugarJS supports some natural language parsing of dates and times.\nYou can jump to the live example here: http://sugarjs.com/dates\nFor example, it supports the following inputs:\n\nthe day after tomorrow\n2 weeks from monday\nMay 25th of next year\n\nYou can then covert the result into different date formats or use the API to further manipulate the date.\n"}, "1006": {"topic": "Is there a natural language parser for date/times in javascript?", "user_name": "Bradley DwyerBradley Dwyer", "text": "\nDoes Date.js satisfy your needs? Or are you looking for something else?\n"}, "1007": {"topic": "Is there a natural language parser for date/times in javascript?", "user_name": "NosrednaNosredna", "text": "\nChrono v2 is the only library I've found that also parses timezones.\nUnfortunately, the only documented way of using it is via NPM (npm i chrono-node) or as ES6 Module (import * as chrono from 'chrono-node'), but I found a way to use the library with the traditional script tag approach for those interested.\nNon-NPM / non-module usage:\n\nInclude this script: https://www.unpkg.com/chrono-node/dist/bundle.js\nUse the global variable chrono in your script (available methods here)\n\nE.g. chrono.parseDate('Tomorrow at 4 PM PST')\n\n\u26a0 Note: I have not tested this extensively, so don't expect it to work flawlessly\n\n"}, "1008": {"topic": "Is there a natural language parser for date/times in javascript?", "user_name": "", "text": "\nFor node, I've found chrono to work well\nChrono supports most date and time formats, such as :\n\nToday, Tomorrow, Yesterday, Last Friday, etc \n17 August 2013 - 19 August 2013 \nThis Friday from 13:00 - 16.00 \n5 days ago \n2 weeks from now \nSat Aug 17 2013 18:40:39 GMT+0900 (JST) \n2014-11-30T08:15:30-05:30\n\n"}, "1009": {"topic": "Is there a natural language parser for date/times in javascript?", "user_name": "PridPrid", "text": "\nSherlock is a great one.\nvar Sherlock = require('sherlockjs');\nvar sherlocked = Sherlock.parse('Homework 5 due next monday at 3pm');\n\n// Basic properties\nvar title = sherlocked.eventTitle;    // 'Homework 5 due'\nvar startDate = sherlocked.startDate; // Date object pointing to next monday at 3pm\nvar endDate = sherlocked.endDate;     // null in this case, since no duration was given\nvar isAllDay = sherlocked.isAllDay;   // false, since a time is included with the event\n\n// Example of an additional custom property added by Watson\nvar validated = sherlocked.validated; // true\n\n"}, "1010": {"topic": "Is there a natural language parser for date/times in javascript?", "user_name": "Zameer Ansari - xameeramirZameer Ansari - xameeramir", "text": "\nYou can use the jQuery datepicker translation, get the day and month number and select the day from datepicker days.\nYou can add values to this object, and you can download up to 60 languages I think. (The object below is not complete, I removed some code to simplify it).\n$.datepicker.regional['sv'] = { \n  monthNames:['Januari','Februari','Mars','April','Maj','Juni','Juli','Augusti','September','Oktober','November','December'],\n  monthNamesShort: ['Jan','Feb','Mar','Apr','Maj','Jun','Jul','Aug','Sep','Okt','Nov','Dec'],\n  dayNamesShort: ['S\u00f6n','M\u00e5n','Tis','Ons','Tor','Fre','L\u00f6r'],\n  dayNames: ['S\u00f6ndag','M\u00e5ndag','Tisdag','Onsdag','Torsdag','Fredag','L\u00f6rdag'],\n  dayNamesMin: ['S\u00f6','M\u00e5','Ti','On','To','Fr','L\u00f6']\n};\n\nNow get the day and month number\nvar dateObject = new Date();\nvar day = dateObject.getDay();\nvar month = dateObject.getMonth();\nvar monthText = $.datepicker.regional['sv']['monthNames'][month];\nvar dayText = $.datepicker.regional['sv']['dayNames'][day];\n\n"}, "1011": {"topic": "Efficiently count word frequencies in python", "user_name": "CommunityBot", "text": "\nI'd like to count frequencies of all words in a text file.\n>>> countInFile('test.txt')\n\nshould return {'aaa':1, 'bbb': 2, 'ccc':1} if the target text file is like:\n# test.txt\naaa bbb ccc\nbbb\n\nI've implemented it with pure python following some posts. However, I've found out pure-python ways are insufficient due to huge file size (> 1GB).\nI think borrowing sklearn's power is a candidate.\nIf you let CountVectorizer count frequencies for each line, I guess you will get word frequencies by summing up each column. But, it sounds a bit indirect way.\nWhat is the most efficient and straightforward way to count words in a file with python?\nUpdate\nMy (very slow) code is here:\nfrom collections import Counter\n\ndef get_term_frequency_in_file(source_file_path):\n    wordcount = {}\n    with open(source_file_path) as f:\n        for line in f:\n            line = line.lower().translate(None, string.punctuation)\n            this_wordcount = Counter(line.split())\n            wordcount = add_merge_two_dict(wordcount, this_wordcount)\n    return wordcount\n\ndef add_merge_two_dict(x, y):\n    return { k: x.get(k, 0) + y.get(k, 0) for k in set(x) | set(y) }\n\n"}, "1012": {"topic": "Efficiently count word frequencies in python", "user_name": "Light YagmiLight Yagmi", "text": "\nThe most succinct approach is to use the tools Python gives you.\nfrom future_builtins import map  # Only on Python 2\n\nfrom collections import Counter\nfrom itertools import chain\n\ndef countInFile(filename):\n    with open(filename) as f:\n        return Counter(chain.from_iterable(map(str.split, f)))\n\nThat's it. map(str.split, f) is making a generator that returns lists of words from each line. Wrapping in chain.from_iterable converts that to a single generator that produces a word at a time. Counter takes an input iterable and counts all unique values in it. At the end, you return a  dict-like object (a Counter) that stores all unique words and their counts, and during creation, you only store a line of data at a time and the total counts, not the whole file at once.\nIn theory, on Python 2.7 and 3.1, you might do slightly better looping over the chained results yourself and using a dict or collections.defaultdict(int) to count (because Counter is implemented in Python, which can make it slower in some cases), but letting Counter do the work is simpler and more self-documenting (I mean, the whole goal is counting, so use a Counter). Beyond that, on CPython (the reference interpreter) 3.2 and higher Counter has a C level accelerator for counting iterable inputs that will run faster than anything you could write in pure Python.\nUpdate: You seem to want punctuation stripped and case-insensitivity, so here's a variant of my earlier code that does that:\nfrom string import punctuation\n\ndef countInFile(filename):\n    with open(filename) as f:\n        linewords = (line.translate(None, punctuation).lower().split() for line in f)\n        return Counter(chain.from_iterable(linewords))\n\nYour code runs much more slowly because it's creating and destroying many small Counter and set objects, rather than .update-ing a single Counter once per line (which, while slightly slower than what I gave in the updated code block, would be at least algorithmically similar in scaling factor).\n"}, "1013": {"topic": "Efficiently count word frequencies in python", "user_name": "", "text": "\nA memory efficient and accurate way is to make use of \n\nCountVectorizer in scikit (for ngram extraction)\nNLTK for word_tokenize\nnumpy matrix sum to collect the counts\ncollections.Counter for collecting the counts and vocabulary\n\nAn example:\nimport urllib.request\nfrom collections import Counter\n\nimport numpy as np \n\nfrom nltk import word_tokenize\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Our sample textfile.\nurl = 'https://raw.githubusercontent.com/Simdiva/DSL-Task/master/data/DSLCC-v2.0/test/test.txt'\nresponse = urllib.request.urlopen(url)\ndata = response.read().decode('utf8')\n\n\n# Note that `ngram_range=(1, 1)` means we want to extract Unigrams, i.e. tokens.\nngram_vectorizer = CountVectorizer(analyzer='word', tokenizer=word_tokenize, ngram_range=(1, 1), min_df=1)\n# X matrix where the row represents sentences and column is our one-hot vector for each token in our vocabulary\nX = ngram_vectorizer.fit_transform(data.split('\\n'))\n\n# Vocabulary\nvocab = list(ngram_vectorizer.get_feature_names())\n\n# Column-wise sum of the X matrix.\n# It's some crazy numpy syntax that looks horribly unpythonic\n# For details, see http://stackoverflow.com/questions/3337301/numpy-matrix-to-array\n# and http://stackoverflow.com/questions/13567345/how-to-calculate-the-sum-of-all-columns-of-a-2d-numpy-array-efficiently\ncounts = X.sum(axis=0).A1\n\nfreq_distribution = Counter(dict(zip(vocab, counts)))\nprint (freq_distribution.most_common(10))\n\n[out]:\n[(',', 32000),\n ('.', 17783),\n ('de', 11225),\n ('a', 7197),\n ('que', 5710),\n ('la', 4732),\n ('je', 4304),\n ('se', 4013),\n ('\u043d\u0430', 3978),\n ('na', 3834)]\n\nEssentially, you can also do this:\nfrom collections import Counter\nimport numpy as np \nfrom nltk import word_tokenize\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef freq_dist(data):\n    \"\"\"\n    :param data: A string with sentences separated by '\\n'\n    :type data: str\n    \"\"\"\n    ngram_vectorizer = CountVectorizer(analyzer='word', tokenizer=word_tokenize, ngram_range=(1, 1), min_df=1)\n    X = ngram_vectorizer.fit_transform(data.split('\\n'))\n    vocab = list(ngram_vectorizer.get_feature_names())\n    counts = X.sum(axis=0).A1\n    return Counter(dict(zip(vocab, counts)))\n\nLet's timeit:\nimport time\n\nstart = time.time()\nword_distribution = freq_dist(data)\nprint (time.time() - start)\n\n[out]:\n5.257147789001465\n\nNote that CountVectorizer can also take a file instead of a string and there's no need to read the whole file into memory. In code:\nimport io\nfrom collections import Counter\n\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ninfile = '/path/to/input.txt'\n\nngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1), min_df=1)\n\nwith io.open(infile, 'r', encoding='utf8') as fin:\n    X = ngram_vectorizer.fit_transform(fin)\n    vocab = ngram_vectorizer.get_feature_names()\n    counts = X.sum(axis=0).A1\n    freq_distribution = Counter(dict(zip(vocab, counts)))\n    print (freq_distribution.most_common(10))\n\n"}, "1014": {"topic": "Efficiently count word frequencies in python", "user_name": "ShadowRangerShadowRanger", "text": "\nHere's some benchmark. It'll look strange but the crudest code wins.\n[code]:\nfrom collections import Counter, defaultdict\nimport io, time\n\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ninfile = '/path/to/file'\n\ndef extract_dictionary_sklearn(file_path):\n    with io.open(file_path, 'r', encoding='utf8') as fin:\n        ngram_vectorizer = CountVectorizer(analyzer='word')\n        X = ngram_vectorizer.fit_transform(fin)\n        vocab = ngram_vectorizer.get_feature_names()\n        counts = X.sum(axis=0).A1\n    return Counter(dict(zip(vocab, counts)))\n\ndef extract_dictionary_native(file_path):\n    dictionary = Counter()\n    with io.open(file_path, 'r', encoding='utf8') as fin:\n        for line in fin:\n            dictionary.update(line.split())\n    return dictionary\n\ndef extract_dictionary_paddle(file_path):\n    dictionary = defaultdict(int)\n    with io.open(file_path, 'r', encoding='utf8') as fin:\n        for line in fin:\n            for words in line.split():\n                dictionary[word] +=1\n    return dictionary\n\nstart = time.time()\nextract_dictionary_sklearn(infile)\nprint time.time() - start\n\nstart = time.time()\nextract_dictionary_native(infile)\nprint time.time() - start\n\nstart = time.time()\nextract_dictionary_paddle(infile)\nprint time.time() - start\n\n[out]:\n38.306814909\n24.8241138458\n12.1182529926\n\nData size (154MB) used in the benchmark above:\n$ wc -c /path/to/file\n161680851\n\n$ wc -l /path/to/file\n2176141\n\nSome things to note:\n\nWith the sklearn version, there's an overhead of vectorizer creation + numpy manipulation and conversion into a Counter object\nThen native Counter update version, it seems like Counter.update() is an expensive operation\n\n"}, "1015": {"topic": "Efficiently count word frequencies in python", "user_name": "", "text": "\nThis should suffice.\ndef countinfile(filename):\n    d = {}\n    with open(filename, \"r\") as fin:\n        for line in fin:\n            words = line.strip().split()\n            for word in words:\n                try:\n                    d[word] += 1\n                except KeyError:\n                    d[word] = 1\n    return d\n\n"}, "1016": {"topic": "Efficiently count word frequencies in python", "user_name": "alvasalvas", "text": "\nInstead of decoding the whole bytes read from the url, I process the binary data. Because bytes.translate expects its second argument to be a byte string, I utf-8 encode punctuation. After removing punctuations, I utf-8 decode the byte string.  \nThe function freq_dist expects an iterable. That's why I've passed data.splitlines().  \nfrom urllib2 import urlopen\nfrom collections import Counter\nfrom string import punctuation\nfrom time import time\nimport sys\nfrom pprint import pprint\n\nurl = 'https://raw.githubusercontent.com/Simdiva/DSL-Task/master/data/DSLCC-v2.0/test/test.txt'\n\ndata = urlopen(url).read()\n\ndef freq_dist(data):\n    \"\"\"\n    :param data: file-like object opened in binary mode or\n                 sequence of byte strings separated by '\\n'\n    :type data: an iterable sequence\n    \"\"\"\n    #For readability   \n    #return Counter(word for line in data\n    #    for word in line.translate(\n    #    None,bytes(punctuation.encode('utf-8'))).decode('utf-8').split())\n\n    punc = punctuation.encode('utf-8')\n    words = (word for line in data for word in line.translate(None, punc).decode('utf-8').split())\n    return Counter(words)\n\n\nstart = time()\nword_dist = freq_dist(data.splitlines())\nprint('elapsed: {}'.format(time() - start))\npprint(word_dist.most_common(10))\n\nOutput;\nelapsed: 0.806480884552\n\n[(u'de', 11106),\n (u'a', 6742),\n (u'que', 5701),\n (u'la', 4319),\n (u'je', 4260),\n (u'se', 3938),\n (u'\\u043d\\u0430', 3929),\n (u'na', 3623),\n (u'da', 3534),\n (u'i', 3487)]\n\nIt seems dict is more efficient than Counter object.  \ndef freq_dist(data):\n    \"\"\"\n    :param data: A string with sentences separated by '\\n'\n    :type data: str\n    \"\"\"\n    d = {}\n    punc = punctuation.encode('utf-8')\n    words = (word for line in data for word in line.translate(None, punc).decode('utf-8').split())\n    for word in words:\n        d[word] = d.get(word, 0) + 1\n    return d\n\nstart = time()\nword_dist = freq_dist(data.splitlines())\nprint('elapsed: {}'.format(time() - start))\npprint(sorted(word_dist.items(), key=lambda x: (x[1], x[0]), reverse=True)[:10])\n\nOutput;\nelapsed: 0.642680168152\n\n[(u'de', 11106),\n (u'a', 6742),\n (u'que', 5701),\n (u'la', 4319),\n (u'je', 4260),\n (u'se', 3938),\n (u'\\u043d\\u0430', 3929),\n (u'na', 3623),\n (u'da', 3534),\n (u'i', 3487)]\n\nTo be more memory efficient when opening huge file, you have to pass just the opened url. But the timing will include file download time too.\ndata = urlopen(url)\nword_dist = freq_dist(data)\n\n"}, "1017": {"topic": "Efficiently count word frequencies in python", "user_name": "nat gillinnat gillin", "text": "\nSkip CountVectorizer and scikit-learn.\nThe file may be too large to load into memory but I doubt the python dictionary gets too large. The easiest option for you may be to split the large file into 10-20 smaller files and extend your code to loop over the smaller files.\n"}, "1018": {"topic": "Efficiently count word frequencies in python", "user_name": "GoodiesGoodies", "text": "\nyou can try with sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\n    vectorizer = CountVectorizer()\n\n    data=['i am student','the student suffers a lot']\n    transformed_data =vectorizer.fit_transform(data)\n    vocab= {a: b for a, b in zip(vectorizer.get_feature_names(), np.ravel(transformed_data.sum(axis=0)))}\n    print (vocab)\n\n"}, "1019": {"topic": "Efficiently count word frequencies in python", "user_name": "", "text": "\nCombining every ones else's views and some of my own :)\nHere is what I have for you\nfrom collections import Counter\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\n\ntext='''Note that if you use RegexpTokenizer option, you lose \nnatural language features special to word_tokenize \nlike splitting apart contractions. You can naively \nsplit on the regex \\w+ without any need for the NLTK.\n'''\n\n# tokenize\nraw = ' '.join(word_tokenize(text.lower()))\n\ntokenizer = RegexpTokenizer(r'[A-Za-z]{2,}')\nwords = tokenizer.tokenize(raw)\n\n# remove stopwords\nstop_words = set(stopwords.words('english'))\nwords = [word for word in words if word not in stop_words]\n\n# count word frequency, sort and return just 20\ncounter = Counter()\ncounter.update(words)\nmost_common = counter.most_common(20)\nmost_common\n\nOutput\n(All ones)\n\n[('note', 1),\n ('use', 1),\n ('regexptokenizer', 1),\n ('option', 1),\n ('lose', 1),\n ('natural', 1),\n ('language', 1),\n ('features', 1),\n ('special', 1),\n ('word', 1),\n ('tokenize', 1),\n ('like', 1),\n ('splitting', 1),\n ('apart', 1),\n ('contractions', 1),\n ('naively', 1),\n ('split', 1),\n ('regex', 1),\n ('without', 1),\n ('need', 1)]\n\nOne can do better than this in terms of efficiency but if you are not worried about it too much, this code is the best.\n"}, "1020": {"topic": "Can an algorithm detect sarcasm [closed]", "user_name": "CommunityBot", "text": "\n\n\n\n\n\n\nClosed. This question is off-topic. It is not currently accepting answers.                    \n\n\n\n\n\n\n\n\n\n\nWant to improve this question? Update the question so it's on-topic for Stack Overflow.\n\n\nClosed 10 years ago.\n\n\n\n\n\n\n\r\n                        Improve this question\r\n                    \n\n\n\nI was asked to write an algorithm to detect sarcasm but I came across a flaw (or what seems like one) in the logic.\nFor example if a person says\n\nA: I love Justin Beiber. Do you like him to?\nB: Yeah. Sure. I absolutely love him.\n\nNow this may be considered sarcasm or not and the only way to know seems to be to know if B is serious or not.\n(I wasn't supposed to be in depth. We were given a bunch of phrases and just were told that if these were in the sentence then it was sarcastic but I got interested?)\nIs there any way to work around this? Or are computers absolutely stuck when it comes to sarcasm?\n(I suppose it depends on the tone of the speaker but my input is text)\n"}, "1021": {"topic": "Can an algorithm detect sarcasm [closed]", "user_name": "cjdscjds", "text": "\nLooks like there are studies that attempted just that, but they have yet to come up with a well working algorithm.\nFrom Gonz\u00e1lez-Ib\u00e1\u00f1ez, R. et al. \"Identifying sarcasm in Twitter: a closer look\" \n\nSarcasm and irony are well-studied phenomena in linguistics,\n  psychology and cognitive science[...]. But in the text mining\n  literature, automatic detection of sarcasm is considered a difficult\n  problem [...] and\n  has been addressed in only a few studies. [...] The work most closely related to ours is that of Davidov et al.\n  (2010), whose objective was to identify sarcastic and non-sarcastic\n  utterances in Twitter and in Amazon product reviews. In this paper, we\n  consider the somewhat harder problem of distinguishing sarcastic tweets from non- sarcastic tweets\n\nThey conclude:\n\nPerhaps unsurprisingly, neither the human judges nor the machine\n  learning techniques perform very well. [...] Our results suggest that lexical features alone are not sufficient for identifying sarcasm and that pragmatic and contextual features merit further study\n\nHere is another recent, relevant paper:\nReyes, A. \"From humor recognition to irony detection: The \ufb01gurative language of social media\" \n"}, "1022": {"topic": "Can an algorithm detect sarcasm [closed]", "user_name": "", "text": "\n\n...sentences are written by users. Its a simulated conversation between 2 people.\n\nDetecting the sarcasm is close to impossible with a single phrase, but with context it might be a little more doable. Let's assume that you can parse the sentence and interpret its literal meaning (not a trivial task, but that problem has been solved at least somewhat).\nYou now have context from:\n\nAll the phrases in the conversation.\nThe response of the other speaker.\n\nCross-Referenced Phrases\nTo leverage #1, you might cross reference all phrases with each other. Are any of them directly contradictory?\nExample:\nSpeaker 1: I LOVE Justin Bieber. Do you?\nSpeaker 2: Totally! I love him.\nSpeaker 1: What's your favorite thing about him?\nSpeaker 2: His awesome music!\nSpeaker 1: Really? What's your favorite song?\nSpeaker 2: Come on, you know I hate his music.  \nWe know have two contradictory phrases, \"I love him!\" and \"I hate his music\". There's at least a chance sarcasm has occurred.\nQuestion or Response from Other Speaker(s)\nApproach #2 could be more effective (or useless...perhaps the sarcasm is known but unspoken between the two parties).\nExample:\nSpeaker 1: Justin Bieber is in town. I am SO going to see him.\nSpeaker 2: Ha.\nAnother example:\nSpeaker 1: I LOVE Justin Bieber. Do you?\nSpeaker 2: I have a giant poster of him above my bed.\nSpeaker 1: Yeah right.\nGetting even more elaborate, you could apply a heuristic to determine how sharply the conversation deviated after a particular phrase.\nSpeaker 1: I am so totally into Justin Bieber!!! Are you?\nSpeaker 1 has made an emphatic statement\nSpeaker 2: Yeah, sure.\nSarcastic. We don't know that, but the other person in the conversation does.\nWhat direction does Speaker 1 take now? do they change the subject? Depending on how sharply the conversation turns, it may indicate how they reacted to the perceived response.\nAll that said, most of this would require sophisticated processing and I would expect a very low accuracy rate at best. But it's a fascinating question.\n"}, "1023": {"topic": "Can an algorithm detect sarcasm [closed]", "user_name": "Enno ShiojiEnno Shioji", "text": "\nSarcasm is really about the tone in which it is said, text doesn't hold vocal tone, also how feasible the statement is to being true can also determine if its sarcasm... \nIf the inputs are typed by users theres two ways you could do it.\nOne is based on what they write using internet lingo.\nFor example:\nUser might type:\n\"Yeah. Sure. I absolutely love him.  /sarcasm\"\nYou could do look ups for such keywords like /sarcasm [/sarcasm] etc\n\nAlternatively you could use statistical odds:\n\"Yeah, and I'm the president of USA\".... statistical odds of it being factually correct are so low it could be flagged as sarcasm.\n"}, "1024": {"topic": "Can an algorithm detect sarcasm [closed]", "user_name": "", "text": "\nSo what is exactly sarcasm in a point of view of culture or language? If you want to resolve this complex problem you must clarify that. The problem is very complex because asks of using AI in therms of some human language. You can look at A.L.I.C.E. for some inspiration. \n"}, "1025": {"topic": "What are the available tools to summarize or simplify text? [closed]", "user_name": "user2589273", "text": "\n\n\n\n\n\n\nClosed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.                    \n\n\n\n\n\n\n\n\n\n\n We don\u2019t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\n\n\nClosed 5 years ago.\n\n\n\n\n\n\n\r\n                        Improve this question\r\n                    \n\n\n\nIs there any library, preferably in python but at least open source, that can summarize and or simplify natural-language text?\n"}, "1026": {"topic": "What are the available tools to summarize or simplify text? [closed]", "user_name": "captainandcokecaptainandcoke", "text": "\nMaybe you can try sumy. It's a quite small library that I wrote in Python. There are implemented Luhn's and Edmundson's approaches, LSA method, SumBasic, KL-Sum, LexRank and TextRank algorithms. It's Apache2 licensed and supports Czech, Slovak, English, French, Japanese, Chinese, Portuguese, Spanish and German languages.\nFeel free to open an issue or send a pull request if there is something you are missing.\n"}, "1027": {"topic": "What are the available tools to summarize or simplify text? [closed]", "user_name": "", "text": "\nI'm not sure if there is currently any libraries that do this, as text summarization, or at least understandable text summarization isn't something that will be easily accomplished by a simple plug & play library.\nHere are a few links that I managed to find regarding projects / resources that are related to text summarization to get you started:\n\nThe Lemur Project\nPython Natural Language Toolkit\nO'Reilly's Book on Natural Language Processing in Python\nGoogle Resource on Natural Language Processing\nTutorial : How to create a keyword summary of text in Python\n\nHope that helps :)\n"}, "1028": {"topic": "What are the available tools to summarize or simplify text? [closed]", "user_name": "Mi\u0161oMi\u0161o", "text": "\nI needed also the same thing but I couldn't find anything in Python that helped me have a Comprehensive Result.\nSo I found this Web Service really useful, and they have a free API which gives a JSON result, and I wanted to share it with you.\nCheck it out here: http://smmry.com\n"}, "1029": {"topic": "What are the available tools to summarize or simplify text? [closed]", "user_name": "Nick Bull", "text": "\nTry Open Text Summarizer which is released under the GPL open source license.  It works reasonably well but there has been no development work on it since 2007.  \nThe original code is written in C (both a library and a command line utility) but there are wrappers to it in a number of languages:\n\nPerl\nRuby\nPython\nC#\n\n"}, "1030": {"topic": "What are the available tools to summarize or simplify text? [closed]", "user_name": "Rion WilliamsRion Williams", "text": "\nNot python but MEAD will do text summarization (it's in Perl).  Usually what comes out is comprehensible, if not always particularly fluent sounding.  Also check out summarization.com for a lot of good information on the text summarization task.\n"}, "1031": {"topic": "What are the available tools to summarize or simplify text? [closed]", "user_name": "ant0niskant0nisk", "text": "\nTake a look at this article which does a detailed study of these methods and packages:\n\nLex_rank (sumy)\nLSA (sumy)\nLuhn (sumy)\nPyTeaser\nGensim TextRank\nPyTextRank\nGoogle TextSum\n\nThe ending of the article does a 'summary'. \nThe author of sumy @miso.belica has given a description in an answer above.\nVarious other ML techniques have risen, such as Facebook/NAMAS and Google/TextSum but still need extensive training in Gigaword Dataset and about 7000 GPU hours. The dataset itself is quite costly.\nIn conclusion I would say sumy is the best option in the market right now if you don't have access to high-end machines. Thanks a lot @miso.belica for this wonderful package.\n"}, "1032": {"topic": "What are the available tools to summarize or simplify text? [closed]", "user_name": "", "text": "\nA while back, I wrote a summarization library for python using NLTK, using an algorithm from the Classifier4J library. It's pretty simple but it may suit the needs of anyone that needs summarization: https://github.com/thavelick/summarize\n"}, "1033": {"topic": "Computing precision and recall in Named Entity Recognition", "user_name": "user2314737", "text": "\nNow I am about to report the results from Named Entity Recognition. One thing that I find a bit confusing is that my understanding of precision and recall was that one simply sums up true positives, true negatives, false positives and false negatives over all classes.\nBut this seems implausible now that I think of it as each misclassification would give simultaneously rise to one false positive and one false negative (e.g. a token that should have been labelled as \"A\" but was labelled as \"B\" is a false negative for \"A\" and false positive for \"B\"). Thus the number of the false positives and the false negatives over all classes would be the same which means that precision is (always!) equal to recall. This simply can't be true so there is an error in my reasoning and I wonder where it is. It is certainly something quite obvious and straight-forward but it escapes me right now.\n"}, "1034": {"topic": "Computing precision and recall in Named Entity Recognition", "user_name": "NickNick", "text": "\nThe way precision and recall is typically computed (this is what I use in my papers) is to measure entities against each other. Supposing the ground truth has the following (without any differentiaton as to what type of entities they are)\n[Microsoft Corp.] CEO [Steve Ballmer] announced the release of [Windows 7] today\nThis has 3 entities.\nSupposing your actual extraction has the following\n[Microsoft Corp.] [CEO] [Steve] Ballmer announced the release of Windows 7 [today]\nYou have an exact match for Microsoft Corp, false positives for CEO and today, a false negative for Windows 7 and a substring match for Steve\nWe compute precision and recall by first defining matching criteria. For example, do they have to be an exact match? Is it a match if they overlap at all? Do entity types matter? Typically we want to provide precision and recall for several of these criteria.\nExact match: True Positives = 1 (Microsoft Corp., the only exact match), False Positives =3 (CEO, today, and Steve, which isn't an exact match), False Negatives = 2 (Steve Ballmer and Windows 7)\nPrecision = True Positives / (True Positives + False Positives) = 1/(1+3) = 0.25\nRecall = True Positives / (True Positives + False Negatives) = 1/(1+2) = 0.33\n\nAny Overlap OK: True Positives = 2 (Microsoft Corp., and Steve which overlaps Steve Ballmer), False Positives =2 (CEO, and today), False Negatives = 1 (Windows 7)\nPrecision = True Positives / (True Positives + False Positives) = 2/(2+2) = 0.55\nRecall = True Positives / (True Positives + False Negatives) = 2/(2+1) = 0.66\n\nThe reader is then left to infer that the \"real performance\" (the precision and recall that an unbiased human checker would give when allowed to use human judgement to decide which overlap discrepancies are significant, and which are not) is somewhere between the two.\nIt's also often useful to report the F1 measure, which is the harmonic mean of precision and recall, and which gives some idea of \"performance\" when you have to trade off precision against recall.\n"}, "1035": {"topic": "Computing precision and recall in Named Entity Recognition", "user_name": "", "text": "\nIn the CoNLL-2003 NER task, the evaluation was based on correctly marked entities, not tokens, as described in the paper 'Introduction to the CoNLL-2003 Shared Task:\nLanguage-Independent Named Entity Recognition'. An entity is correctly marked if the system identifies an entity of the correct type with the correct start and end point in the document. I prefer this approach in evaluation because it's closer to a measure of performance on the actual task; a user of the NER system cares about entities, not individual tokens. \nHowever, the problem you described still exists. If you mark an entity of type ORG with type LOC you incur a false positive for LOC and a false negative for ORG. There is an interesting discussion on the problem in this blog post.\n"}, "1036": {"topic": "Computing precision and recall in Named Entity Recognition", "user_name": "Ken BloomKen Bloom", "text": "\nAs mentioned before, there are different ways of measuring NER performance. It is possible to evaluate separately how precisely entities are detected in terms of position in the text, and in terms of their class (person, location, organization, etc.). Or to combine both aspects in a single measure. \nYou'll find a nice review in the following thesis: D. Nadeau, Semi-Supervised Named Entity Recognition: Learning to Recognize 100 Entity Types with Little Supervision (2007). Have a look at section 2.6. Evaluation of NER.\n"}, "1037": {"topic": "Computing precision and recall in Named Entity Recognition", "user_name": "", "text": "\nThere is no simple right answer to this question. There are a variety of different ways to count errors. The MUC competitions used one, other people have used others.\nHowever, to help you with your immediate confusion:\nYou have a set of tags, no? Something like NONE, PERSON, ANIMAL, VEGETABLE?\nIf a token should be person, and you tag it NONE, then that's a false positive for NONE and a false negative for PERSON. If a token should be NONE and you tag it PERSON, it's the other way around.\nSo you get a score for each entity type.\nYou can also aggregate those scores.\n"}, "1038": {"topic": "Computing precision and recall in Named Entity Recognition", "user_name": "StompchickenStompchicken", "text": "\nJust to be clear, these are the definitions:\nPrecision = TP/(TP+FP) = What portion of what you found was ground truth?\nRecall = TP/(TP+FN) = What portion of the ground truth did you recover?\nThe won't necessarily always be equal, since the number of false negatives will not necessarily equal the number of false positives.\nIf I understand your problem right, you're assigning each token to one of more than two possible labels. In order for precision and recall to make sense, you need to have a binary classifier. So you could use precision and recall if you phrased the classifier as whether a token is in Group \"A\" or not, and then repeat for each group. In this case a missed classification would count twice as a false negative for one group and a false positive for another.\nIf you're doing a classification like this where it isn't binary (assigning each token to a group) it might be useful instead to look at pairs of tokens. Phrase your problem as \"Are tokens X and Y in the same classification group?\". This allows you to compute precision and recall over all pairs of nodes. This isn't as appropriate if your classification groups are labeled or have associated meanings. For example if your classification groups are \"Fruits\" and \"Vegetables\", and you classify both \"Apples\" and \"Oranges\" as \"Vegetables\" then this algorithm would score it as a true positive even though the wrong group was assigned. But if your groups are unlabled, for example \"A\" and \"B\", then if apples and oranges were both classified as \"A\", afterward you could say that \"A\" corresponds to \"Fruits\".  \n"}, "1039": {"topic": "Computing precision and recall in Named Entity Recognition", "user_name": "nhahtdh", "text": "\nIf you are training an spacy ner model then their scorer.py API which gives you precision, recall and recall of your ner.\nThe Code and output would be in this format:-\n17\nFor those one having the same question in the following link:\nspaCy/scorer.py\n'''python\nimport spacy\n\nfrom spacy.gold import GoldParse\n\nfrom spacy.scorer import Scorer\n\n\ndef evaluate(ner_model, examples):\n\nscorer = Scorer()\nfor input_, annot in examples:\n    doc_gold_text = ner_model.make_doc(input_)\n    gold = GoldParse(doc_gold_text, entities=annot)\n    pred_value = ner_model(input_)\n    scorer.score(pred_value, gold)\nreturn scorer.scores\n\nexample run\nexamples = [\n    ('Who is Shaka Khan?',\n     [(7, 17, 'PERSON')]),\n    ('I like London and Berlin.',\n     [(7, 13, 'LOC'), (18, 24, 'LOC')])\n]\n\nner_model = spacy.load(ner_model_path) # for spaCy's pretrained use 'en_core_web_sm'\nresults = evaluate(ner_model, examples)\n'''\nOutput will be in format like:-\n{'uas': 0.0, 'las': 0.0, **'ents_p'**: 43.75, **'ents_r'**: 35.59322033898305, **'ents_f'**: 39.252336448598136, 'tags_acc': 0.0, 'token_acc': 100.0}**strong text**\n\n"}, "1040": {"topic": "How to interpret scikit's learn confusion matrix and classification report?", "user_name": "john doejohn doe", "text": "\nI have a sentiment analysis task, for this Im using this corpus the opinions have 5 classes (very neg, neg, neu, pos, very pos), from 1 to 5. So I do the classification as follows:\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\ntfidf_vect= TfidfVectorizer(use_idf=True, smooth_idf=True,\n                            sublinear_tf=False, ngram_range=(2,2))\nfrom sklearn.cross_validation import train_test_split, cross_val_score\n\nimport pandas as pd\n\ndf = pd.read_csv('/corpus.csv',\n                     header=0, sep=',', names=['id', 'content', 'label'])\n\nX = tfidf_vect.fit_transform(df['content'].values)\ny = df['label'].values\n\n\nfrom sklearn import cross_validation\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X,\n                                                    y, test_size=0.33)\n\n\nfrom sklearn.svm import SVC\nsvm_1 = SVC(kernel='linear')\nsvm_1.fit(X, y)\nsvm_1_prediction = svm_1.predict(X_test)\n\nThen with the metrics I obtained the following confusion matrix and classification report, as follows:\nprint '\\nClasification report:\\n', classification_report(y_test, svm_1_prediction)\nprint '\\nConfussion matrix:\\n',confusion_matrix(y_test, svm_1_prediction)\n\nThen, this is the result:\nClasification report:\n             precision    recall  f1-score   support\n\n          1       1.00      0.76      0.86        71\n          2       1.00      0.84      0.91        43\n          3       1.00      0.74      0.85        89\n          4       0.98      0.95      0.96       288\n          5       0.87      1.00      0.93       367\n\navg / total       0.94      0.93      0.93       858\n\n\nConfussion matrix:\n[[ 54   0   0   0  17]\n [  0  36   0   1   6]\n [  0   0  66   5  18]\n [  0   0   0 273  15]\n [  0   0   0   0 367]]\n\nHow can I interpret the above confusion matrix and classification report. I tried reading the documentation and this question. But still can interpretate what happened here particularly with this data?. Wny this matrix is somehow \"diagonal\"?. By the other hand what means the recall, precision, f1score and support for this data?. What can I say about this data?. Thanks in advance guys\n"}, "1041": {"topic": "How to interpret scikit's learn confusion matrix and classification report?", "user_name": "AdityaAditya", "text": "\nClassification report must be straightforward - a report of P/R/F-Measure for each element in your test data. In Multiclass problems, it is not a good idea to read Precision/Recall and F-Measure over the whole data any imbalance would make you feel you've reached better results. That's where such reports help.\nComing to confusion matrix, it is much detailed representation of what's going on with your labels. So there were 71 points in the first class (label 0). Out of these, your model was successful in identifying 54 of those correctly in label 0, but 17 were marked as label 4. Similarly look at second row. There were 43 points in class 1, but 36 of them were marked correctly. Your classifier predicted 1 in class 3 and 6 in class 4.\n\nNow you can see the pattern this follows. An ideal classifiers with 100% accuracy would produce a pure diagonal matrix which would have all the points predicted in their correct class. \nComing to Recall/Precision. They are some of the mostly used measures in evaluating how good your system works. Now you had 71 points in first class (call it 0 class). Out of them your classifier was able to get 54 elements correctly. That's your recall. 54/71 = 0.76. Now look only at first column in the table. There is one cell with entry 54, rest all are zeros. This means your classifier marked 54 points in class 0, and all 54 of them were actually in class 0. This is precision. 54/54 = 1. Look at column marked 4. In this column, there are elements scattered in all the five rows. 367 of them were marked correctly. Rest all are incorrect. So that reduces your precision.\nF Measure is harmonic mean of Precision and Recall. \nBe sure you read details about these. https://en.wikipedia.org/wiki/Precision_and_recall\n"}, "1042": {"topic": "How to interpret scikit's learn confusion matrix and classification report?", "user_name": "Christopher ShymanskyChristopher Shymansky", "text": "\nHere's the documentation for scikit-learn's sklearn.metrics.precision_recall_fscore_support method: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support\nIt seems to indicate that the support is the number of occurrences of each particular class in the true responses (responses in your test set). You can calculate it by summing the rows of the confusion matrix. \n"}, "1043": {"topic": "How to interpret scikit's learn confusion matrix and classification report?", "user_name": "raman kumarraman kumar", "text": "\nConfusion Matrix tells us about the distribution of our predicted values across all the actual outcomes.Accuracy_scores, Recall(sensitivity), Precision, Specificity and other similar metrics are subsets of Confusion Matrix.\nF1 scores are the harmonic means of precision and recall.\nSupport columns in Classification_report tell us about the actual counts of each class in test data.\nWell, rest is explained above beautifully.\nThank you.\n"}, "1044": {"topic": "What is the difference between Luong attention and Bahdanau attention?", "user_name": "Peter Mortensen", "text": "\nThese two attentions are used in seq2seq modules. The two different attentions are introduced as multiplicative and additive attentions in this TensorFlow documentation. What is the difference?\n"}, "1045": {"topic": "What is the difference between Luong attention and Bahdanau attention?", "user_name": "Shamane SiriwardhanaShamane Siriwardhana", "text": "\nI went through this Effective Approaches to Attention-based Neural Machine Translation. In the section 3.1 They have mentioned the difference between two attentions as follows,\n\nLuong attention used top hidden layer states in both of encoder and decoder. But Bahdanau attention take concatenation of forward and backward source hidden state (Top Hidden Layer).\n\nIn Luong attention they get the decoder hidden state at time t. Then calculate attention scores and from that get the context vector which will be concatenated with hidden state of the decoder and then predict.\nBut in the Bahdanau at time t we consider about t-1 hidden state of the decoder. Then we calculate alignment , context vectors as above. But then we concatenate this context with hidden state of the decoder at t-1. So before the softmax this concatenated vector goes inside a GRU.\n\nLuong has diffferent types of alignments. Bahdanau has only concat score alignment model.\n\n\n\n"}, "1046": {"topic": "What is the difference between Luong attention and Bahdanau attention?", "user_name": "CommunityBot", "text": "\nThey are very well explained in a PyTorch seq2seq tutorial.\nThe main difference is how to score similarities between the current decoder input and encoder outputs.\n"}, "1047": {"topic": "What is the difference between Luong attention and Bahdanau attention?", "user_name": "Shamane SiriwardhanaShamane Siriwardhana", "text": "\nI just wanted to add a picture for a better understanding to the @shamane-siriwardhana\n\nthe main difference is in the output of the decoder network\n"}, "1048": {"topic": "What is the difference between Luong attention and Bahdanau attention?", "user_name": "Peter Mortensen", "text": "\nThere are actually many differences besides the scoring and the local/global attention. A brief summary of the differences:\n\nBahdanau et al use an extra function to derive hs_{t-1} from hs_t. I didn't see a good reason anywhere on why they do this but a paper by Pascanu et al throws a clue..maybe they are looking to make the RNN deeper. Luong of course uses the hs_t directly\nBahdanau recommend uni-directional encoder and bi-directional decoder. Luong has both as uni-directional. Luong also recommends taking just the top layer outputs; in general, their model is simpler\nThe more famous one - There is no dot product of hs_{t-1} (the decoder output) with encoder states in Bahdanau's. Instead they use separate weights for both and do an addition instead of a multiplication. This perplexed me for a long while as multiplication is more intuitive, until I read somewhere that addition is less resource intensive...so there are tradeoffs\nin Bahdanau, we have a choice to use more than one unit to determine w and  u - the weights that are applied individually on the decoder hidden state at t-1 and the encoder hidden states. Having done that, we need to massage the tensor shape back & hence, there is a need for a multiplication with another weight v. Determining v is a simple linear transformation and needs just 1 unit\nLuong gives us local attention in addition to global attention. Local attention is a combination of soft and hard attention\nLuong gives us many other ways to calculate the attention weights..most involving a dot product..hence the name multiplcative. I think there were 4 such equations. We can pick and choose the one we want\nThere are some minor changes like Luong concatenates the context and the decoder hidden state and uses one weight instead of 2 separate ones\nLast and the most important one is that Luong feeds the attentional vector to the next time-step as they believe that past attention weight history is important and helps predict better values\n\nThe good news is that most are superficial changes. Attention as a concept is so powerful that any basic implementation suffices. There are 2 things that seem to matter though - the passing of attentional vectors to the next time step and the concept of local attention(esp if resources are constrained). The rest dont influence the output in a big way.\nFor more specific details, please refer https://towardsdatascience.com/create-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e\n"}, "1049": {"topic": "What is the difference between Luong attention and Bahdanau attention?", "user_name": "J-minJ-min", "text": "\nLuong-style attention:    scores = tf.matmul(query, key, transpose_b=True)\nBahdanau-style attention: scores = tf.reduce_sum(tf.tanh(query + value), axis=-1)\n"}, "1050": {"topic": "What\u2019s a good Python profanity filter library? [closed]", "user_name": "CommunityBot", "text": "\n\n\n\n\n\n\nClosed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.                    \n\n\n\n\n\n\n\n\n\n\n We don\u2019t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\n\n\nClosed 7 years ago.\n\n\n\n\n\n\n\r\n                        Improve this question\r\n                    \n\n\n\nLike https://stackoverflow.com/questions/1521646/best-profanity-filter, but for Python \u2014 and I\u2019m looking for libraries I can run and control myself locally, as opposed to web services.\n(And whilst it\u2019s always great to hear your fundamental objections of principle to profanity filtering, I\u2019m not specifically looking for them here. I know profanity filtering can\u2019t pick up every hurtful thing being said. I know swearing, in the grand scheme of things, isn\u2019t a particularly big issue. I know you need some human input to deal with issues of content. I\u2019d just like to find a good library, and see what use I can make of it.)\n"}, "1051": {"topic": "What\u2019s a good Python profanity filter library? [closed]", "user_name": "Paul D. WaitePaul D. Waite", "text": "\nI didn't found any Python profanity library, so I made one myself.\nParameters\n\nfilterlist\nA list of regular expressions that match a forbidden word. Please do not use \\b, it will be inserted depending on inside_words.\nExample:\n ['bad', 'un\\w+']\nignore_case\nDefault: True\nSelf-explanatory.\nreplacements\nDefault: \"$@%-?!\"\nA string with characters from which the replacements strings will be randomly generated.\nExamples: \"%&$?!\" or \"-\" etc.\ncomplete\nDefault: True\nControls if the entire string will be replaced or if the first and last chars will be kept.\ninside_words\nDefault: False\nControls if words are searched inside other words too. Disabling this \nModule source\n\n(examples at the end)\n\"\"\"\nModule that provides a class that filters profanities\n\n\"\"\"\n\n__author__ = \"leoluk\"\n__version__ = '0.0.1'\n\nimport random\nimport re\n\nclass ProfanitiesFilter(object):\n    def __init__(self, filterlist, ignore_case=True, replacements=\"$@%-?!\", \n                 complete=True, inside_words=False):\n        \"\"\"\n        Inits the profanity filter.\n\n        filterlist -- a list of regular expressions that\n        matches words that are forbidden\n        ignore_case -- ignore capitalization\n        replacements -- string with characters to replace the forbidden word\n        complete -- completely remove the word or keep the first and last char?\n        inside_words -- search inside other words?\n\n        \"\"\"\n\n        self.badwords = filterlist\n        self.ignore_case = ignore_case\n        self.replacements = replacements\n        self.complete = complete\n        self.inside_words = inside_words\n\n    def _make_clean_word(self, length):\n        \"\"\"\n        Generates a random replacement string of a given length\n        using the chars in self.replacements.\n\n        \"\"\"\n        return ''.join([random.choice(self.replacements) for i in\n                  range(length)])\n\n    def __replacer(self, match):\n        value = match.group()\n        if self.complete:\n            return self._make_clean_word(len(value))\n        else:\n            return value[0]+self._make_clean_word(len(value)-2)+value[-1]\n\n    def clean(self, text):\n        \"\"\"Cleans a string from profanity.\"\"\"\n\n        regexp_insidewords = {\n            True: r'(%s)',\n            False: r'\\b(%s)\\b',\n            }\n\n        regexp = (regexp_insidewords[self.inside_words] % \n                  '|'.join(self.badwords))\n\n        r = re.compile(regexp, re.IGNORECASE if self.ignore_case else 0)\n\n        return r.sub(self.__replacer, text)\n\n\nif __name__ == '__main__':\n\n    f = ProfanitiesFilter(['bad', 'un\\w+'], replacements=\"-\")    \n    example = \"I am doing bad ungood badlike things.\"\n\n    print f.clean(example)\n    # Returns \"I am doing --- ------ badlike things.\"\n\n    f.inside_words = True    \n    print f.clean(example)\n    # Returns \"I am doing --- ------ ---like things.\"\n\n    f.complete = False    \n    print f.clean(example)\n    # Returns \"I am doing b-d u----d b-dlike things.\"\n\n"}, "1052": {"topic": "What\u2019s a good Python profanity filter library? [closed]", "user_name": "leolukleoluk", "text": "\narrBad = [\n'2g1c',\n'2 girls 1 cup',\n'acrotomophilia',\n'anal',\n'anilingus',\n'anus',\n'arsehole',\n'ass',\n'asshole',\n'assmunch',\n'auto erotic',\n'autoerotic',\n'babeland',\n'baby batter',\n'ball gag',\n'ball gravy',\n'ball kicking',\n'ball licking',\n'ball sack',\n'ball sucking',\n'bangbros',\n'bareback',\n'barely legal',\n'barenaked',\n'bastardo',\n'bastinado',\n'bbw',\n'bdsm',\n'beaver cleaver',\n'beaver lips',\n'bestiality',\n'bi curious',\n'big black',\n'big breasts',\n'big knockers',\n'big tits',\n'bimbos',\n'birdlock',\n'bitch',\n'black cock',\n'blonde action',\n'blonde on blonde action',\n'blow j',\n'blow your l',\n'blue waffle',\n'blumpkin',\n'bollocks',\n'bondage',\n'boner',\n'boob',\n'boobs',\n'booty call',\n'brown showers',\n'brunette action',\n'bukkake',\n'bulldyke',\n'bullet vibe',\n'bung hole',\n'bunghole',\n'busty',\n'butt',\n'buttcheeks',\n'butthole',\n'camel toe',\n'camgirl',\n'camslut',\n'camwhore',\n'carpet muncher',\n'carpetmuncher',\n'chocolate rosebuds',\n'circlejerk',\n'cleveland steamer',\n'clit',\n'clitoris',\n'clover clamps',\n'clusterfuck',\n'cock',\n'cocks',\n'coprolagnia',\n'coprophilia',\n'cornhole',\n'cum',\n'cumming',\n'cunnilingus',\n'cunt',\n'darkie',\n'date rape',\n'daterape',\n'deep throat',\n'deepthroat',\n'dick',\n'dildo',\n'dirty pillows',\n'dirty sanchez',\n'dog style',\n'doggie style',\n'doggiestyle',\n'doggy style',\n'doggystyle',\n'dolcett',\n'domination',\n'dominatrix',\n'dommes',\n'donkey punch',\n'double dong',\n'double penetration',\n'dp action',\n'eat my ass',\n'ecchi',\n'ejaculation',\n'erotic',\n'erotism',\n'escort',\n'ethical slut',\n'eunuch',\n'faggot',\n'fecal',\n'felch',\n'fellatio',\n'feltch',\n'female squirting',\n'femdom',\n'figging',\n'fingering',\n'fisting',\n'foot fetish',\n'footjob',\n'frotting',\n'fuck',\n'fucking',\n'fuck buttons',\n'fudge packer',\n'fudgepacker',\n'futanari',\n'g-spot',\n'gang bang',\n'gay sex',\n'genitals',\n'giant cock',\n'girl on',\n'girl on top',\n'girls gone wild',\n'goatcx',\n'goatse',\n'gokkun',\n'golden shower',\n'goo girl',\n'goodpoop',\n'goregasm',\n'grope',\n'group sex',\n'guro',\n'hand job',\n'handjob',\n'hard core',\n'hardcore',\n'hentai',\n'homoerotic',\n'honkey',\n'hooker',\n'hot chick',\n'how to kill',\n'how to murder',\n'huge fat',\n'humping',\n'incest',\n'intercourse',\n'jack off',\n'jail bait',\n'jailbait',\n'jerk off',\n'jigaboo',\n'jiggaboo',\n'jiggerboo',\n'jizz',\n'juggs',\n'kike',\n'kinbaku',\n'kinkster',\n'kinky',\n'knobbing',\n'leather restraint',\n'leather straight jacket',\n'lemon party',\n'lolita',\n'lovemaking',\n'make me come',\n'male squirting',\n'masturbate',\n'menage a trois',\n'milf',\n'missionary position',\n'motherfucker',\n'mound of venus',\n'mr hands',\n'muff diver',\n'muffdiving',\n'nambla',\n'nawashi',\n'negro',\n'neonazi',\n'nig nog',\n'nigga',\n'nigger',\n'nimphomania',\n'nipple',\n'nipples',\n'nsfw images',\n'nude',\n'nudity',\n'nympho',\n'nymphomania',\n'octopussy',\n'omorashi',\n'one cup two girls',\n'one guy one jar',\n'orgasm',\n'orgy',\n'paedophile',\n'panties',\n'panty',\n'pedobear',\n'pedophile',\n'pegging',\n'penis',\n'phone sex',\n'piece of shit',\n'piss pig',\n'pissing',\n'pisspig',\n'playboy',\n'pleasure chest',\n'pole smoker',\n'ponyplay',\n'poof',\n'poop chute',\n'poopchute',\n'porn',\n'porno',\n'pornography',\n'prince albert piercing',\n'pthc',\n'pubes',\n'pussy',\n'queaf',\n'raghead',\n'raging boner',\n'rape',\n'raping',\n'rapist',\n'rectum',\n'reverse cowgirl',\n'rimjob',\n'rimming',\n'rosy palm',\n'rosy palm and her 5 sisters',\n'rusty trombone',\n's&m',\n'sadism',\n'scat',\n'schlong',\n'scissoring',\n'semen',\n'sex',\n'sexo',\n'sexy',\n'shaved beaver',\n'shaved pussy',\n'shemale',\n'shibari',\n'shit',\n'shota',\n'shrimping',\n'slanteye',\n'slut',\n'smut',\n'snatch',\n'snowballing',\n'sodomize',\n'sodomy',\n'spic',\n'spooge',\n'spread legs',\n'strap on',\n'strapon',\n'strappado',\n'strip club',\n'style doggy',\n'suck',\n'sucks',\n'suicide girls',\n'sultry women',\n'swastika',\n'swinger',\n'tainted love',\n'taste my',\n'tea bagging',\n'threesome',\n'throating',\n'tied up',\n'tight white',\n'tit',\n'tits',\n'titties',\n'titty',\n'tongue in a',\n'topless',\n'tosser',\n'towelhead',\n'tranny',\n'tribadism',\n'tub girl',\n'tubgirl',\n'tushy',\n'twat',\n'twink',\n'twinkie',\n'two girls one cup',\n'undressing',\n'upskirt',\n'urethra play',\n'urophilia',\n'vagina',\n'venus mound',\n'vibrator',\n'violet blue',\n'violet wand',\n'vorarephilia',\n'voyeur',\n'vulva',\n'wank',\n'wet dream',\n'wetback',\n'white power',\n'women rapping',\n'wrapping men',\n'wrinkled starfish',\n'xx',\n'xxx',\n'yaoi',\n'yellow showers',\n'yiffy',\n'zoophilia']\n\ndef profanityFilter(text):\nbrokenStr1 = text.split()\nbadWordMask = '!@#$%!@#$%^~!@%^~@#$%!@#$%^~!'\nnew = ''\nfor word in brokenStr1:\n    if word in arrBad:\n        print word + ' <--Bad word!'\n        text = text.replace(word,badWordMask[:len(word)])\n        #print new\n\nreturn text\n\nprint profanityFilter(\"this thing sucks sucks sucks fucking stuff\")\n\nYou can add or remove from the bad words list,arrBad, as you please.\n"}, "1053": {"topic": "What\u2019s a good Python profanity filter library? [closed]", "user_name": "user2592414user2592414", "text": "\nWebPurify is a Profanity Filter Library for Python\n"}, "1054": {"topic": "What\u2019s a good Python profanity filter library? [closed]", "user_name": "Matt", "text": "\nYou could probably combine http://spambayes.sourceforge.net/ and http://www.cs.cmu.edu/~biglou/resources/bad-words.txt.\n"}, "1055": {"topic": "What\u2019s a good Python profanity filter library? [closed]", "user_name": "nu everestnu everest", "text": "\nProfanity? What the f***'s that? ;-)\nIt will still take a couple of years before a computer will really be able to recognize swearing and cursing and it is my sincere hope that people will have understood by then that profanity is human and not \"dangerous.\"\nInstead of a dumb filter, have a smart human moderator who can balance the tone of discussion as appropriate. A moderator who can detect abuse like:\n\"If you were my husband, I'd poison your tea.\" - \"If you were my wife, I'd drink it.\"\n(that was from Winston Churchill, btw.)\n"}, "1056": {"topic": "What\u2019s a good Python profanity filter library? [closed]", "user_name": "AnandAnand", "text": "\nIt's possible for users to work around this, of course, but it should do a fairly thorough job of removing profanity:\nimport re\ndef remove_profanity(s):\n    def repl(word):\n        m = re.match(r\"(\\w+)(.*)\", word)\n        if not m:\n            return word\n        word = \"Bork\" if m.group(1)[0].isupper() else \"bork\"\n        word += m.group(2)\n        return word\n    return \" \".join([repl(w) for w in s.split(\" \")])\n\nprint remove_profanity(\"You just come along with me and have a good time. The Galaxy's a fun place. You'll need to have this fish in your ear.\")\n\n"}, "1057": {"topic": "Server-side Voice Recognition [closed]", "user_name": "Nikolay Shmyrev", "text": "\n\n\n\n\n\n\nClosed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.                    \n\n\n\n\n\n\n\n\n\n\n We don\u2019t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\n\n\nClosed 7 years ago.\n\n\n\n\n\n\n\r\n                        Improve this question\r\n                    \n\n\n\nAnyone know of any good server side voice recognition engines that are already hosted? I.e. I want to be able to call a simple web API posting some sound data and get text back. Doesn't have to be free - but hopefully free to experiment with.\n"}, "1058": {"topic": "Server-side Voice Recognition [closed]", "user_name": "alooaloo", "text": "\nThere are several IVR services which host an entire VOIP session (telephone call) as a complete application, rather than offer individual service transactions \"\u00e0la carte\".  If you were to make your program look like a VOIP call, you might be able to get it done with some of these services. \nVoxeo published a list of free (and low cost) IVR hosting providers aimed towards developers for limited use.  Not surprisingly, all will require registration. \n\nVoiceGenie Developer Workshop (absorbed into Genesys) \nLoquendo C@f\u00e9 status unknown \nNuance Caf\u00e9 (Bevocal) now Nuance On-Demand \nPlum Voice Hosting now Plum DEV\nVOICE Testcenter of the VOICE Community\n\nAnother possibility would be to make a direct inquiries with Vlingo, Twilio,  or Tropo as they might sell you exactly what you need. \nUPDATE: July 25, 2012\nAT&T has announced availability of a Speech API on .  You send it audio \u2013 it returns text in XML or JSON data formats.  See also developer site. \nUPDATE: August 27, 2012\nAnother possibility is the Dragon Mobile SDK from Nuance, which is aimed at individual developers looking for an API enabling consumer applications with speech and/or text-to-speech functionality.\nUPDATE: September 21, 2012\nThere seem to be several new providers offering exactly what you are looking for: speech samples in, text out.  The following are listed on Programmable Web:\n\niSpeech\nSpeechAPI\nOneTok\nAISpeech API\nNexiWave\n\nAlso note that Loquendo is now part of Nuance. \nUPDATE: June 27, 2013\nAT&T's Speech API has a few targeted SDKs (Android, iOS, PhoneGap, Titanium, Windows) - some of which are hosted on GitHub. There's even source for a Unity 3D demo. \nUPDATE: January 23, 2014\nOneTok has reformulated it's offerings as an SDK for iOS and Android. \nApparently the Voice Genie product has been thoroughly digested by Genesys such that little trace of it can be found. Given Genesys' positioning towards large enterprises, is difficult to know if they have any small-volume or commodity offerings.\nPlumvoice seems to have expanded their offerings.\nAs with many before it, Vlingo is now part of Nuance. \n(I've tried to update any broken links in original answer.) \nUPDATE: October 31, 2015\nKeeping this answer up-to-date is a Sisyphean task. \nVoxeo's list of free (and low cost) IVR hosting providers now re-derects to AT&T Speech API, which, in full disclosure, I now have material involvement with therein, and as such, disqualifies me from providing linking to pretty much anything without impugning my credibility. \nThat said, there are many players in the speech/NLP market. Do diligence. \nUPDATE: April 8, 2016\nSo now Google is totally upsetting the apple cart. \n"}, "1059": {"topic": "Does an algorithm exist to help detect the \"primary topic\" of an English sentence?", "user_name": "Jim Bolla", "text": "\nI'm trying to find out if there is a known algorithm that can detect the \"key concept\" of a sentence.\nThe use case is as follows:\n\nUser enters a sentence as a query (Does chicken taste like turkey?)\nOur system identifies the concepts of the sentence (chicken, turkey)\nAnd it runs a search of our corpus content\n\nThe area that we're lacking in is identifying what the core \"topic\" of the sentence is really about.  The sentence \"Does chicken taste like turkey\" has a primary topic of \"chicken\", because the user is asking about the taste of chicken.  While \"turkey\" is a helper topic of less importance.\nSo... I'm trying to find out if there is an algorithm that will help me identify the primary topic of a sentence... Let me know if you are aware of any!!! \n"}, "1060": {"topic": "Does an algorithm exist to help detect the \"primary topic\" of an English sentence?", "user_name": "rockitrockit", "text": "\nI actually did a research project on this and won two competitions and am competing in nationals.\nThere are two steps to the method:\n\nParse the sentence with a Context-Free Grammar\nIn the resulting parse trees, find all nouns which are only subordinate to Noun-Phrase-like constituents\n\nFor example, \"I ate pie\" has 2 nouns: \"I\" and \"pie\". Looking at the parse tree, \"pie\" is inside of a Verb Phrase, so it cannot be a subject. \"I\", however, is only inside of NP-like constituents. being the only subject candidate, it is the subject. Find an early copy of this program on http://www.candlemind.com. Note that the vocabulary is limited to basic singular words, and there are no verb conjugations, so it has \"man\" but not \"men\", has \"eat\" but not \"ate.\" Also, the CFG I used was hand-made an limited. I will be updating this program shortly.\nAnyway, there are limitations to this program. My mentor pointed out in its currents state, it cannot recognize sentences with subjects that are \"real\" NPs (what grammar actually calls NPs). For example, \"that the moon is flat is not a debate any longer.\" The subject is actually \"that the moon is flat.\" However, the program would recognize \"moon\" as the subject. I will be fixing this shortly.\nAnyway, this is good enough for most sentences...\nMy research paper can be found there too. Go to page 11 of it to read the methods.\nHope this helps.\n"}, "1061": {"topic": "Does an algorithm exist to help detect the \"primary topic\" of an English sentence?", "user_name": "        user212218\r", "text": "\nMost of your basic NLP parsing techniques will be able to extract the basic aspects of the sentence - i.e., that chicken and turkey a NPs and they are linked by and adjective 'like', etc.  Getting these to a 'topic' or 'concept' is more difficult\nTechnique such as Latent Semantic Analysis and its many derivatives transform this information into a vector (some have methods of retaining in some part the hierarchy/relations between parts of speech) and then compares them to existing, usually pre-classified by concept, vectors.  See http://en.wikipedia.org/wiki/Latent_semantic_analysis to get started.\nEdit Here's an example LSA app you can play around with to see if you might want to pursue it further .  http://lsi.research.telcordia.com/lsi/demos.html\n"}, "1062": {"topic": "Does an algorithm exist to help detect the \"primary topic\" of an English sentence?", "user_name": "MichaelMichael", "text": "\nFor many longer sentences its difficult to say what exactly is a topic and also there may be more than one. \nOne way to get approximate ans is \n1.) First tag the sentence using openNLP, stanford Parser or any one.\n2.) Then remove all the stop words from the sentence.\n3.) Pick up Nouns( proper, singular and plural).\nOther way is \n1.) chuck the sentence into phrases by any parser.\n2.) Pick up all the noun phrases.\n3.) Remove the Noun phrases that doesn't have the Nouns as a child.\n4.) Keep only adjectives and Nouns, remove all words from remaining Noun Phrases. \nThis might give approx. guessing.\n"}, "1063": {"topic": "Does an algorithm exist to help detect the \"primary topic\" of an English sentence?", "user_name": "", "text": "\n\"Key concept\" is not a well-defined term in linguistics, but this may be a starting point: parse the sentence, find the subject in the parse tree or dependency structure that you get. (This doesn't always work; for example, the subject of \"Is it raining?\" is \"it\", while the key concept is likely \"rain\". Also, what's the key concept in \"Are spaghetti and lasagna the same thing?\")\nThis kind of problem (NLP + search) is more properly dealt with by methods such as LSA, but that's quite an advanced topic.\n"}, "1064": {"topic": "Does an algorithm exist to help detect the \"primary topic\" of an English sentence?", "user_name": "dfbdfb", "text": "\nOn the most basic level, a question in English is usually in the form of <verb> <subject> ... ? or <pronoun> <verb> <subject> ... ?. This is by no means a good algorithm, especially considering that the subject could span several words, but depending on how sophisticated a solution you need, it might be a useful starting point.\nIf you need precision, ignore this answer.\n"}, "1065": {"topic": "Does an algorithm exist to help detect the \"primary topic\" of an English sentence?", "user_name": "NaveenNaveen", "text": "\nIf you're willing to shell out money, http://www.connexor.com/ is supposed to be able to do this type of semantic analysis for a wide variety of languages, including English.  I have never directly used their product, and so can't comment on how well it works.\n"}, "1066": {"topic": "Does an algorithm exist to help detect the \"primary topic\" of an English sentence?", "user_name": "Fred FooFred Foo", "text": "\nThere's an article about Parsing Noun Phrases in the MIT Computational Linguistics journal of this month: http://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00076\n"}, "1067": {"topic": "Does an algorithm exist to help detect the \"primary topic\" of an English sentence?", "user_name": "biziclopbiziclop", "text": "\nCompound or complex sentences may have more than one key concept of a sentence.\nYou can use stanfordNLP or MaltParser which can give the dependency structure of a sentence. It also gives the parts of speech tagging including subject, verb , object etc.\nI think most of the times the object will be the key concept of the sentence.\n"}, "1068": {"topic": "Does an algorithm exist to help detect the \"primary topic\" of an English sentence?", "user_name": "btillybtilly", "text": "\nYou should look at Google's Cloud Natural Language API. It's their NLP service.\nhttps://cloud.google.com/natural-language/\n"}, "1069": {"topic": "Does an algorithm exist to help detect the \"primary topic\" of an English sentence?", "user_name": "ZeoSZeoS", "text": "\nSimple solution is to tag your sentence with part-of-speach tagger (e.g. from NLTK library for Python) then find matches with some predefined part-of-speach patterns in which it's clear where is main subject of the sentence\n"}, "1070": {"topic": "Does an algorithm exist to help detect the \"primary topic\" of an English sentence?", "user_name": "NaveenNaveen", "text": "\nOne option is to look into something like this as a first step:\nhttp://www.abisource.com/projects/link-grammar/\nBut how you derive the topic from these links is another problem in itself. But as Abiword is trying to detect grammatical problems, you might be able to use it to determine the topic. \n"}, "1071": {"topic": "Does an algorithm exist to help detect the \"primary topic\" of an English sentence?", "user_name": "TomTom", "text": "\nBy \"primary topic\" you're referring to what is termed the subject of the sentence.\nThe subject can be identified by understanding a sentence through natural language processing.\nThe answer to this question is the same as that for How to determine subject, object and other words? - this is a currently unsolved problem.\n"}, "1072": {"topic": "Java library for keywords extraction from input text [closed]", "user_name": "sp00m", "text": "\n\n\n\n\n\n\nClosed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.                    \n\n\n\n\n\n\n\n\n\n\n We don\u2019t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\n\n\nClosed 2 years ago.\n\n\n\n\n\n\n\r\n                        Improve this question\r\n                    \n\n\n\nI'm looking for a Java library to extract keywords from a block of text.\nThe process should be as follows:\nstop word cleaning -> stemming -> searching for keywords based on English linguistics statistical information - meaning if a word appears more times in the text than in the English language in terms of probability than it's a keyword candidate.\nIs there a library that performs this task?\n"}, "1073": {"topic": "Java library for keywords extraction from input text [closed]", "user_name": "ShayShay", "text": "\nHere is a possible solution using Apache Lucene. I didn't use the last version but the 3.6.2 one, since this is the one I know the best. Besides the /lucene-core-x.x.x.jar, don't forget to add the /contrib/analyzers/common/lucene-analyzers-x.x.x.jar from the downloaded archive to your project: it contains the language-specific analyzers (especially the English one in your case).\nNote that this will only find the frequencies of the input text words based on their respective stem. Comparing these frequencies with the English language statistics shall be done afterwards (this answer may help by the way).\n\nThe data model\nOne keyword for one stem. Different words may have the same stem, hence the terms set. The keyword frequency is incremented every time a new term is found (even if it has been already found - a set automatically removes duplicates).\npublic class Keyword implements Comparable<Keyword> {\n\n  private final String stem;\n  private final Set<String> terms = new HashSet<String>();\n  private int frequency = 0;\n\n  public Keyword(String stem) {\n    this.stem = stem;\n  }\n\n  public void add(String term) {\n    terms.add(term);\n    frequency++;\n  }\n\n  @Override\n  public int compareTo(Keyword o) {\n    // descending order\n    return Integer.valueOf(o.frequency).compareTo(frequency);\n  }\n\n  @Override\n  public boolean equals(Object obj) {\n    if (this == obj) {\n      return true;\n    } else if (!(obj instanceof Keyword)) {\n      return false;\n    } else {\n      return stem.equals(((Keyword) obj).stem);\n    }\n  }\n\n  @Override\n  public int hashCode() {\n    return Arrays.hashCode(new Object[] { stem });\n  }\n\n  public String getStem() {\n    return stem;\n  }\n\n  public Set<String> getTerms() {\n    return terms;\n  }\n\n  public int getFrequency() {\n    return frequency;\n  }\n\n}\n\n\nUtilities\nTo stem a word:\npublic static String stem(String term) throws IOException {\n\n  TokenStream tokenStream = null;\n  try {\n\n    // tokenize\n    tokenStream = new ClassicTokenizer(Version.LUCENE_36, new StringReader(term));\n    // stem\n    tokenStream = new PorterStemFilter(tokenStream);\n\n    // add each token in a set, so that duplicates are removed\n    Set<String> stems = new HashSet<String>();\n    CharTermAttribute token = tokenStream.getAttribute(CharTermAttribute.class);\n    tokenStream.reset();\n    while (tokenStream.incrementToken()) {\n      stems.add(token.toString());\n    }\n\n    // if no stem or 2+ stems have been found, return null\n    if (stems.size() != 1) {\n      return null;\n    }\n    String stem = stems.iterator().next();\n    // if the stem has non-alphanumerical chars, return null\n    if (!stem.matches(\"[a-zA-Z0-9-]+\")) {\n      return null;\n    }\n\n    return stem;\n\n  } finally {\n    if (tokenStream != null) {\n      tokenStream.close();\n    }\n  }\n\n}\n\nTo search into a collection (will be used by the list of potential keywords):\npublic static <T> T find(Collection<T> collection, T example) {\n  for (T element : collection) {\n    if (element.equals(example)) {\n      return element;\n    }\n  }\n  collection.add(example);\n  return example;\n}\n\n\nCore\nHere is the main input method:\npublic static List<Keyword> guessFromString(String input) throws IOException {\n\n  TokenStream tokenStream = null;\n  try {\n\n    // hack to keep dashed words (e.g. \"non-specific\" rather than \"non\" and \"specific\")\n    input = input.replaceAll(\"-+\", \"-0\");\n    // replace any punctuation char but apostrophes and dashes by a space\n    input = input.replaceAll(\"[\\\\p{Punct}&&[^'-]]+\", \" \");\n    // replace most common english contractions\n    input = input.replaceAll(\"(?:'(?:[tdsm]|[vr]e|ll))+\\\\b\", \"\");\n\n    // tokenize input\n    tokenStream = new ClassicTokenizer(Version.LUCENE_36, new StringReader(input));\n    // to lowercase\n    tokenStream = new LowerCaseFilter(Version.LUCENE_36, tokenStream);\n    // remove dots from acronyms (and \"'s\" but already done manually above)\n    tokenStream = new ClassicFilter(tokenStream);\n    // convert any char to ASCII\n    tokenStream = new ASCIIFoldingFilter(tokenStream);\n    // remove english stop words\n    tokenStream = new StopFilter(Version.LUCENE_36, tokenStream, EnglishAnalyzer.getDefaultStopSet());\n\n    List<Keyword> keywords = new LinkedList<Keyword>();\n    CharTermAttribute token = tokenStream.getAttribute(CharTermAttribute.class);\n    tokenStream.reset();\n    while (tokenStream.incrementToken()) {\n      String term = token.toString();\n      // stem each term\n      String stem = stem(term);\n      if (stem != null) {\n        // create the keyword or get the existing one if any\n        Keyword keyword = find(keywords, new Keyword(stem.replaceAll(\"-0\", \"-\")));\n        // add its corresponding initial token\n        keyword.add(term.replaceAll(\"-0\", \"-\"));\n      }\n    }\n\n    // reverse sort by frequency\n    Collections.sort(keywords);\n\n    return keywords;\n\n  } finally {\n    if (tokenStream != null) {\n      tokenStream.close();\n    }\n  }\n\n}\n\n\nExample\nUsing the guessFromString method on the Java wikipedia article introduction part, here are the first 10 most frequent keywords (i.e. stems) that were found:\njava         x12    [java]\ncompil       x5     [compiled, compiler, compilers]\nsun          x5     [sun]\ndevelop      x4     [developed, developers]\nlanguag      x3     [languages, language]\nimplement    x3     [implementation, implementations]\napplic       x3     [application, applications]\nrun          x3     [run]\norigin       x3     [originally, original]\ngnu          x3     [gnu]\n\nIterate over the output list to know which were the original found words for each stem by getting the terms sets (displayed between brackets [...] in the above example).\n\nWhat's next\nCompare the stem frequency / frequencies sum ratios with the English language statistics ones, and keep me in the loop if your managed it: I could be quite interested too :)\n"}, "1074": {"topic": "Java library for keywords extraction from input text [closed]", "user_name": "CommunityBot", "text": "\nAn updated and ready-to-use version of the code proposed above.\nThis code is compatible with Apache Lucene 5.x\u20266.x.\nCardKeyword class:\nimport java.util.HashSet;\nimport java.util.Set;\n\n/**\n * Keyword card with stem form, terms dictionary and frequency rank\n */\nclass CardKeyword implements Comparable<CardKeyword> {\n\n    /**\n     * Stem form of the keyword\n     */\n    private final String stem;\n\n    /**\n     * Terms dictionary\n     */\n    private final Set<String> terms = new HashSet<>();\n\n    /**\n     * Frequency rank\n     */\n    private int frequency;\n\n    /**\n     * Build keyword card with stem form\n     *\n     * @param stem\n     */\n    public CardKeyword(String stem) {\n        this.stem = stem;\n    }\n\n    /**\n     * Add term to the dictionary and update its frequency rank\n     *\n     * @param term\n     */\n    public void add(String term) {\n        this.terms.add(term);\n        this.frequency++;\n    }\n\n    /**\n     * Compare two keywords by frequency rank\n     *\n     * @param keyword\n     * @return int, which contains comparison results\n     */\n    @Override\n    public int compareTo(CardKeyword keyword) {\n        return Integer.valueOf(keyword.frequency).compareTo(this.frequency);\n    }\n\n    /**\n     * Get stem's hashcode\n     *\n     * @return int, which contains stem's hashcode\n     */\n    @Override\n    public int hashCode() {\n        return this.getStem().hashCode();\n    }\n\n    /**\n     * Check if two stems are equal\n     *\n     * @param o\n     * @return boolean, true if two stems are equal\n     */\n    @Override\n    public boolean equals(Object o) {\n\n        if (this == o) return true;\n\n        if (!(o instanceof CardKeyword)) return false;\n\n        CardKeyword that = (CardKeyword) o;\n\n        return this.getStem().equals(that.getStem());\n    }\n\n    /**\n     * Get stem form of keyword\n     *\n     * @return String, which contains getStemForm form\n     */\n    public String getStem() {\n        return this.stem;\n    }\n\n    /**\n     * Get terms dictionary of the stem\n     *\n     * @return Set<String>, which contains set of terms of the getStemForm\n     */\n    public Set<String> getTerms() {\n        return this.terms;\n    }\n\n    /**\n     * Get stem frequency rank\n     *\n     * @return int, which contains getStemForm frequency\n     */\n    public int getFrequency() {\n        return this.frequency;\n    }\n}\n\nKeywordsExtractor class:\nimport org.apache.lucene.analysis.TokenStream;\nimport org.apache.lucene.analysis.core.LowerCaseFilter;\nimport org.apache.lucene.analysis.core.StopFilter;\nimport org.apache.lucene.analysis.en.EnglishAnalyzer;\nimport org.apache.lucene.analysis.en.PorterStemFilter;\nimport org.apache.lucene.analysis.miscellaneous.ASCIIFoldingFilter;\nimport org.apache.lucene.analysis.standard.ClassicFilter;\nimport org.apache.lucene.analysis.standard.StandardTokenizer;\nimport org.apache.lucene.analysis.tokenattributes.CharTermAttribute;\n\nimport java.io.IOException;\nimport java.io.StringReader;\nimport java.util.*;\n\n/**\n * Keywords extractor functionality handler\n */\nclass KeywordsExtractor {\n\n    /**\n     * Get list of keywords with stem form, frequency rank, and terms dictionary\n     *\n     * @param fullText\n     * @return List<CardKeyword>, which contains keywords cards\n     * @throws IOException\n     */\n    static List<CardKeyword> getKeywordsList(String fullText) throws IOException {\n\n        TokenStream tokenStream = null;\n\n        try {\n            // treat the dashed words, don't let separate them during the processing\n            fullText = fullText.replaceAll(\"-+\", \"-0\");\n\n            // replace any punctuation char but apostrophes and dashes with a space\n            fullText = fullText.replaceAll(\"[\\\\p{Punct}&&[^'-]]+\", \" \");\n\n            // replace most common English contractions\n            fullText = fullText.replaceAll(\"(?:'(?:[tdsm]|[vr]e|ll))+\\\\b\", \"\");\n\n            StandardTokenizer stdToken = new StandardTokenizer();\n            stdToken.setReader(new StringReader(fullText));\n\n            tokenStream = new StopFilter(new ASCIIFoldingFilter(new ClassicFilter(new LowerCaseFilter(stdToken))), EnglishAnalyzer.getDefaultStopSet());\n            tokenStream.reset();\n\n            List<CardKeyword> cardKeywords = new LinkedList<>();\n\n            CharTermAttribute token = tokenStream.getAttribute(CharTermAttribute.class);\n\n            while (tokenStream.incrementToken()) {\n\n                String term = token.toString();\n                String stem = getStemForm(term);\n\n                if (stem != null) {\n                    CardKeyword cardKeyword = find(cardKeywords, new CardKeyword(stem.replaceAll(\"-0\", \"-\")));\n                    // treat the dashed words back, let look them pretty\n                    cardKeyword.add(term.replaceAll(\"-0\", \"-\"));\n                }\n            }\n\n            // reverse sort by frequency\n            Collections.sort(cardKeywords);\n\n            return cardKeywords;\n        } finally {\n            if (tokenStream != null) {\n                try {\n                    tokenStream.close();\n                } catch (IOException e) {\n                    e.printStackTrace();\n                }\n            }\n        }\n    }\n\n    /**\n     * Get stem form of the term\n     *\n     * @param term\n     * @return String, which contains the stemmed form of the term\n     * @throws IOException\n     */\n    private static String getStemForm(String term) throws IOException {\n\n        TokenStream tokenStream = null;\n\n        try {\n            StandardTokenizer stdToken = new StandardTokenizer();\n            stdToken.setReader(new StringReader(term));\n\n            tokenStream = new PorterStemFilter(stdToken);\n            tokenStream.reset();\n\n            // eliminate duplicate tokens by adding them to a set\n            Set<String> stems = new HashSet<>();\n\n            CharTermAttribute token = tokenStream.getAttribute(CharTermAttribute.class);\n\n            while (tokenStream.incrementToken()) {\n                stems.add(token.toString());\n            }\n\n            // if stem form was not found or more than 2 stems have been found, return null\n            if (stems.size() != 1) {\n                return null;\n            }\n\n            String stem = stems.iterator().next();\n\n            // if the stem form has non-alphanumerical chars, return null\n            if (!stem.matches(\"[a-zA-Z0-9-]+\")) {\n                return null;\n            }\n\n            return stem;\n        } finally {\n            if (tokenStream != null) {\n                try {\n                    tokenStream.close();\n                } catch (IOException e) {\n                    e.printStackTrace();\n                }\n            }\n        }\n    }\n\n    /**\n     * Find sample in collection\n     *\n     * @param collection\n     * @param sample\n     * @param <T>\n     * @return <T> T, which contains the found object within collection if exists, otherwise the initially searched object\n     */\n    private static <T> T find(Collection<T> collection, T sample) {\n\n        for (T element : collection) {\n            if (element.equals(sample)) {\n                return element;\n            }\n        }\n\n        collection.add(sample);\n\n        return sample;\n    }\n}\n\nThe call of function:\nString text = \"\u2026\";\nList<CardKeyword> keywordsList = KeywordsExtractor.getKeywordsList(text);\n\n"}, "1075": {"topic": "Java library for keywords extraction from input text [closed]", "user_name": "sp00msp00m", "text": "\nA relatively simple approach based on the RAKE algorithm and opennlp models wrapped by the rapidrake-java library.\nimport java.io.IOException;\nimport java.io.InputStream;\n\nimport org.apache.commons.io.IOUtils;\n\nimport io.github.crew102.rapidrake.RakeAlgorithm;\nimport io.github.crew102.rapidrake.model.RakeParams;\nimport io.github.crew102.rapidrake.model.Result;\n\npublic class KeywordExtractor {\n\n    private static String delims = \"[-,.?():;\\\"!/]\";\n    private static String posUrl = \"model-bin/en-pos-maxent.bin\";\n    private static String sentUrl = \"model-bin/en-sent.bin\";\n\n    public static void main(String[] args) throws IOException {\n        InputStream stopWordsStream = KeywordExtractor.class.getResourceAsStream(\"/stopword-list.txt\");\n        String[] stopWords = IOUtils.readLines(stopWordsStream, \"UTF-8\").toArray(new String[0]);\n        String[] stopPOS = {\"VBD\"};\n        RakeParams params = new RakeParams(stopWords, stopPOS, 0, true, delims);\n        RakeAlgorithm rakeAlg = new RakeAlgorithm(params, posUrl, sentUrl);\n        Result aRes = rakeAlg.rake(\"I'm looking for a Java library to extract keywords from a block of text.\");\n        System.out.println(aRes);\n        // OUTPUT:\n        // [looking (1), java library (4), extract keywords (4), block (1), text (1)]\n    }\n}\n\nAs you can see from the sample output you get a map of keywords with their relative weights.\nAs explained at https://github.com/crew102/rapidrake-java you need to download the files en-pos-maxent.bin and en-sent.bin from the opennlp download page, and put them into the model-bin folder in your project root (must be a sibling of your src folder if using the maven project structure). The stopwords file should go under src/main/resources/stopword-list.txt (assuming maven structure), can be downloaded for example from https://github.com/terrier-org/terrier-desktop/blob/master/share/stopword-list.txt.\n\n"}, "1076": {"topic": "Word frequency algorithm for natural language processing", "user_name": "Amit G", "text": "\nWithout getting a degree in information retrieval, I'd like to know if there exists any algorithms for counting the frequency that words occur in a given body of text.  The goal is to get a \"general feel\" of what people are saying over a set of textual comments.  Along the lines of Wordle.\nWhat I'd like:\n\nignore articles, pronouns, etc ('a', 'an', 'the', 'him', 'them' etc)\npreserve proper nouns\nignore hyphenation, except for soft kind\n\nReaching for the stars, these would be peachy:\n\nhandling stemming & plurals (e.g. like, likes, liked, liking match the same result)\ngrouping of adjectives (adverbs, etc) with their subjects (\"great service\" as opposed to \"great\", \"service\")\n\nI've attempted some basic stuff using Wordnet but I'm just tweaking things blindly and hoping it works for my specific data.  Something more generic would be great.\n"}, "1077": {"topic": "Word frequency algorithm for natural language processing", "user_name": "Mark McDonaldMark McDonald", "text": "\nYou'll need not one, but several nice algorithms, along the lines of the following.\n\nignoring pronouns is done via a stoplist.\npreserving proper nouns? You mean, detecting named entities, like Hoover Dam and saying \"it's one word\" or compound nouns, like programming language? I'll give you a hint: that's tough one, but there exist libraries for both. Look for NER (Named entitiy recognition) and lexical chunking. OpenNLP is a Java-Toolkit that does both.\nignoring hyphenation? You mean, like at line breaks? Use regular expressions and verify the resulting word via dictionary lookup.\nhandling plurals/stemming: you can look into the Snowball stemmer. It does the trick nicely.\n\"grouping\" adjectives with their nouns is generally a task of shallow parsing. But if you are looking specifically for qualitative adjectives (good, bad, shitty, amazing...) you may be interested in sentiment analysis. LingPipe does this, and a lot more.\n\nI'm sorry, I know you said you wanted to KISS, but unfortunately, your demands aren't that easy to meet. Nevertheless, there exist tools for all of this, and you should be able to just tie them together and not have to perform any task yourself, if you don't want to. If you want to perform a task yourself, I suggest you look at stemming, it's the easiest of all.\nIf you go with Java, combine Lucene with the OpenNLP toolkit. You will get very good results, as Lucene already has a stemmer built in and a lot of tutorial. The OpenNLP toolkit on the other hand is poorly documented, but you won't need too much out of it. You might also be interested in NLTK, written in Python.\nI would say you drop your last requirement, as it involves shallow parsing and will definetly not impove your results.\nAh, btw. the exact term of that document-term-frequency-thing you were looking for is called tf-idf. It's pretty much the best way to look for document frequency for terms. In order to do it properly, you won't get around using multidimenional vector matrices.\n... Yes, I know. After taking a seminar on IR, my respect for Google was even greater. After doing some stuff in IR, my respect for them fell just as quick, though.\n"}, "1078": {"topic": "Word frequency algorithm for natural language processing", "user_name": "", "text": "\nWelcome to the world of NLP ^_^\nAll you need is a little basic knowledge and some tools.\nThere are already tools that will tell you if a word in a sentence is a noun, adjective or verb. They are called part-of-speech taggers. Typically, they take plaintext English as input, and output the word, its base form, and the part-of-speech. Here is the output of a popular UNIX part-of-speech tagger on the first sentence of your post:\n$ echo \"Without getting a degree in information retrieval, I'd like to know if there exists any algorithms for counting the frequency that words occur in a given body of text.\" | tree-tagger-english \n# Word  POS     surface form\nWithout IN  without\ngetting VVG get\na   DT  a\ndegree  NN  degree\nin  IN  in\ninformation NN  information\nretrieval   NN  retrieval\n,   ,   ,\nI   PP  I\n'd  MD  will\nlike    VV  like\nto  TO  to\nknow    VV  know\nif  IN  if\nthere   EX  there\nexists  VVZ exist\nany DT  any\nalgorithms  NNS algorithm\nfor IN  for\ncounting    VVG count\nthe DT  the\nfrequency   NN  frequency\nthat    IN/that that\nwords   NNS word\noccur   VVP occur\nin  IN  in\na   DT  a\ngiven   VVN give\nbody    NN  body\nof  IN  of\ntext    NN  text\n.   SENT    .\n\nAs you can see, it identified \"algorithms\" as being the plural form (NNS) of \"algorithm\" and \"exists\" as being a conjugation (VBZ) of \"exist.\" It also identified \"a\" and \"the\" as \"determiners (DT)\" -- another word for article. As you can see, the POS tagger also tokenized the punctuation.\nTo do everything but the last point on your list, you just need to run the text through a POS tagger, filter out the categories that don't interest you (determiners, pronouns, etc.) and count the frequencies of the base forms of the words.\nHere are some popular POS taggers:\nTreeTagger (binary only: Linux, Solaris, OS-X)\nGENIA Tagger (C++: compile your self)\nStanford POS Tagger (Java)  \nTo do the last thing on your list, you need more than just word-level information. An easy way to start is by counting sequences of words rather than just words themselves. These are called n-grams. A good place to start is UNIX for Poets. If you are willing to invest in a book on NLP, I would recommend Foundations of Statistical Natural Language Processing.\n"}, "1079": {"topic": "Word frequency algorithm for natural language processing", "user_name": "Aleksandar DimitrovAleksandar Dimitrov", "text": "\nHere is an example of how you might do that in Python, the concepts are similar in any language.\n>>> import urllib2, string\n>>> devilsdict = urllib2.urlopen('http://www.gutenberg.org/files/972/972.txt').read()\n>>> workinglist = devilsdict.split()\n>>> cleanlist = [item.strip(string.punctuation) for item in workinglist]\n>>> results = {}\n>>> skip = {'a':'', 'the':'', 'an':''}\n>>> for item in cleanlist:\n      if item not in skip:\n        try:\n          results[item] += 1\n        except KeyError:\n          results[item] = 1\n\n>>> results\n{'': 17, 'writings': 3, 'foul': 1, 'Sugar': 1, 'four': 8, 'Does': 1, \"friend's\": 1, 'hanging': 4, 'Until': 1, 'marching': 2 ...\n\nThe first line just gets libraries that help with parts of the problem, as in the second line, where urllib2 downloads a copy of Ambrose Bierce's \"Devil's Dictionary\"  The next lines make a list of all the words in the text, without punctuation.  Then you create a hash table, which in this case is like a list of unique words associated with a number.  The for loop goes over each word in the Bierce book, if there is already a record of that word in the table, each new occurrence adds one to the value associated with that word in the table; if the word hasn't appeared yet, it gets added to the table, with a value of 1 (meaning one occurrence.)  For the cases you are talking about, you would want to pay much more attention to detail, for example using capitalization to help identify proper nouns only in the middle of sentences, etc., this is very rough but expresses the concept.\nTo get into the stemming and pluralization stuff, experiment, then look into 3rd party work, I have enjoyed parts of the NLTK, which is an academic open source project, also in python.\n"}, "1080": {"topic": "Word frequency algorithm for natural language processing", "user_name": "underspecifiedunderspecified", "text": "\nI wrote a full program to do just this a while back.  I can upload a demo later when I get home.  \nHere is a the code (asp.net/c#): http://naspinski.net/post/Findingcounting-Keywords-out-of-a-Text-Document.aspx\n"}, "1081": {"topic": "Word frequency algorithm for natural language processing", "user_name": "unmountedunmounted", "text": "\nThe first part of your question doesn't sound so bad. All you basically need to do is read each word from the file (or stream w/e) and place it into a prefix tree and each time you happen upon a word that already exists you increment the value associated with it. Of course you would have an ignore list of everything you'd like left out of your calculations as well.\nIf you use a prefix tree you ensure that to find any word is going to O(N) where N is the maximum length of a word in your data set. The advantage of a prefix tree in this situation is that if you want to look for plurals and stemming you can check in O(M+1) if that's even possible for the word, where M is the length of the word without stem or plurality (is that a word? hehe). Once you've built your prefix tree I would reanalyze it for the stems and such and condense it down so that the root word is what holds the results.\nUpon searching you could have some simple rules in place to have the match return positive in case of the root or stem or what have you.\nThe second part seems extremely challenging. My naive inclination would be to hold separate results for adjective-subject groupings. Use the same principles as above but just keep it separate.\nAnother option for the semantic analysis could be modeling each sentence as a tree of subject, verb, etc relationships (Sentence has a subject and verb, subject has a noun and adjective, etc). Once you've broken all of your text up in this way it seems like it might be fairly easy to run through and get a quick count of the different appropriate pairings that occurred.\nJust some ramblings, I'm sure there are better ideas, but I love thinking about this stuff.\n"}, "1082": {"topic": "Word frequency algorithm for natural language processing", "user_name": "naspinskinaspinski", "text": "\nThe algorithm you just described it. A program that does it out of the box with a big button saying \"Do it\"... I don't know.\nBut let me be constructive. I recommend you this book Programming Collective Intelligence. Chapters 3 and 4 contain very pragmatic examples (really, no complex theories, just examples).\n"}, "1083": {"topic": "Word frequency algorithm for natural language processing", "user_name": "Justin BozonierJustin Bozonier", "text": "\nU can use the worldnet dictionary to the get the basic information of the question keyword like its past of speech, extract synonym, u can also can do the same for your document to create the index for it.\nthen you can easily match the keyword with index file and rank the document. then summerize it. \n"}, "1084": {"topic": "Word frequency algorithm for natural language processing", "user_name": "grafficgraffic", "text": "\nEverything what you have listed is handled well by spacy. \n\nIgnore some words - use stop words\nExtract subject - use part of speech tagging to identify it (works out of the box). After a sentence is parsed, find \"ROOT\" - the main verb of the sentence. By navigating the parse tree you will find a noun that relates to this verb. It will be the subject. \nIgnore hyphenation - their tokenizer handles hyphens in most cases. It can be easily extended to handle more special cases. \n\nIf the list of topics is pre-determined and not huge, you may even go further: build a classification model that will predict the topic. \nLet's say you have 10 subjects. You collect sample sentences or texts. You load them into another product: prodigy. Using it's great interface you quickly assign subjects to the samples. And finally, using the categorized samples you train the spacy model to predict the subject of the texts or sentences. \n"}, "1085": {"topic": "Extracting all Nouns from a text file using nltk", "user_name": "Rakesh Adhikesavan", "text": "\nIs there a more efficient way of doing this?\nMy code reads a text file and extracts all Nouns.\nimport nltk\n\nFile = open(fileName) #open file\nlines = File.read() #read all lines\nsentences = nltk.sent_tokenize(lines) #tokenize sentences\nnouns = [] #empty to array to hold all nouns\n\nfor sentence in sentences:\n     for word,pos in nltk.pos_tag(nltk.word_tokenize(str(sentence))):\n         if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n             nouns.append(word)\n\nHow do I reduce the time complexity of this code? Is there a way to avoid using the nested for loops? \nThanks in advance!\n"}, "1086": {"topic": "Extracting all Nouns from a text file using nltk", "user_name": "Rakesh AdhikesavanRakesh Adhikesavan", "text": "\nIf you are open to options other than NLTK, check out TextBlob. It extracts all nouns and noun phrases easily:\n>>> from textblob import TextBlob\n>>> txt = \"\"\"Natural language processing (NLP) is a field of computer science, artificial intelligence, and computational linguistics concerned with the inter\nactions between computers and human (natural) languages.\"\"\"\n>>> blob = TextBlob(txt)\n>>> print(blob.noun_phrases)\n[u'natural language processing', 'nlp', u'computer science', u'artificial intelligence', u'computational linguistics']\n\n"}, "1087": {"topic": "Extracting all Nouns from a text file using nltk", "user_name": "", "text": "\nimport nltk\n\nlines = 'lines is some string of words'\n# function to test if something is a noun\nis_noun = lambda pos: pos[:2] == 'NN'\n# do the nlp stuff\ntokenized = nltk.word_tokenize(lines)\nnouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n\nprint nouns\n>>> ['lines', 'string', 'words']\n\nUseful tip: it is often the case that list comprehensions are a faster method of building a list than adding elements to a list with the .insert() or append() method, within a 'for' loop.\n"}, "1088": {"topic": "Extracting all Nouns from a text file using nltk", "user_name": "Aziz AltoAziz Alto", "text": "\nYou can achieve good results using nltk, Textblob, SpaCy or any of the many other libraries out there. These libraries will all do the job but with different degrees of efficiency.\nimport nltk\nfrom textblob import TextBlob\nimport spacy\nnlp = spacy.load('en')\nnlp1 = spacy.load('en_core_web_lg')\n\ntxt = \"\"\"Natural language processing (NLP) is a field of computer science, artificial intelligence, and computational linguistics concerned with the interactions between computers and human (natural) languages.\"\"\"\n\nOn my windows 10 2 cores, 4 processors, 8GB ram i5 hp  laptop, in jupyter notebook, I ran some comparisons and here are the results.\nFor TextBlob:\n%%time\nprint([w for (w, pos) in TextBlob(txt).pos_tags if pos[0] == 'N'])\n\nAnd the output is\n>>> ['language', 'processing', 'NLP', 'field', 'computer', 'science', 'intelligence', 'linguistics', 'inter', 'actions', 'computers', 'languages']\n    Wall time: 8.01 ms #average over 20 iterations\n\nFor nltk:\n%%time\nprint([word for (word, pos) in nltk.pos_tag(nltk.word_tokenize(txt)) if pos[0] == 'N'])\n\nAnd the output is\n>>> ['language', 'processing', 'NLP', 'field', 'computer', 'science', 'intelligence', 'linguistics', 'inter', 'actions', 'computers', 'languages']\n    Wall time: 7.09 ms #average over 20 iterations\n\nFor spacy:\n%%time\nprint([ent.text for ent in nlp(txt) if ent.pos_ == 'NOUN'])\n\nAnd the output is\n>>> ['language', 'processing', 'field', 'computer', 'science', 'intelligence', 'linguistics', 'inter', 'actions', 'computers', 'languages']\n    Wall time: 30.19 ms #average over 20 iterations\n\nIt seems nltk and TextBlob are reasonably faster and this is to be expected since store nothing else about the input text, txt. Spacy is way slower. One more thing. SpaCy missed the noun NLP while nltk and TextBlob got it. I would shot for nltk or TextBlob unless there is something else I wish to extract from the input txt.\n Check out a quick start into spacy here.\n Check out some basics about TextBlob here. Check out nltk HowTos here\n"}, "1089": {"topic": "Extracting all Nouns from a text file using nltk", "user_name": "", "text": "\nimport nltk\nlines = 'lines is some string of words'\ntokenized = nltk.word_tokenize(lines)\nnouns = [word for (word, pos) in nltk.pos_tag(tokenized) if(pos[:2] == 'NN')]\nprint (nouns)\n\nJust simplied abit more.\n"}, "1090": {"topic": "Extracting all Nouns from a text file using nltk", "user_name": "BoaBoa", "text": "\nI'm not an NLP expert, but I think you're pretty close already, and there likely isn't a way to get better than quadratic time complexity in these outer loops here. \nRecent versions of NLTK have a built in function that does what you're doing by hand, nltk.tag.pos_tag_sents, and it returns a list of lists of tagged words too.\n"}, "1091": {"topic": "Extracting all Nouns from a text file using nltk", "user_name": "Samuel NdeSamuel Nde", "text": "\nYour code has no redundancy: You read the file once and visit each sentence, and each tagged word, exactly once. No matter how you write your code (e.g., using comprehensions), you will only be hiding the nested loops, not skipping any processing.\nThe only potential for improvement is in its space complexity: Instead of reading the whole file at once, you could read it in increments. But since you need to process a whole sentence at a time, it's not as simple as reading and processing one line at a time; so I wouldn't bother unless your files are whole gigabytes long; for short files it's not going to make any difference.\nIn short, your loops are fine. There are a thing or two in your code that you could clean up (e.g. the if clause that matches the POS tags), but it's not going to change anything efficiency-wise.\n"}, "1092": {"topic": "How can I use NLP to parse recipe ingredients?", "user_name": "Fabian Steeg", "text": "\nI need to parse recipe ingredients into amount, measurement, item, and description as applicable to the line, such as 1 cup flour, the peel of 2 lemons and 1 cup packed brown sugar etc. What would be the best way of doing this? I am interested in using python for the project so I am assuming using the nltk is the best bet but I am open to other languages.\n"}, "1093": {"topic": "How can I use NLP to parse recipe ingredients?", "user_name": "GregGreg", "text": "\nI actually do this for my website, which is now part of an open source project for others to use.\nI wrote a blog post on my techniques, enjoy!\nhttp://blog.kitchenpc.com/2011/07/06/chef-watson/\n"}, "1094": {"topic": "How can I use NLP to parse recipe ingredients?", "user_name": "", "text": "\nThe New York Times faced this problem when they were parsing their recipe archive. They used an NLP technique called linear-chain condition random field (CRF). This blog post provides a good overview:\n\n\"Extracting Structured Data From Recipes Using Conditional Random Fields\"\n\nThey open-sourced their code, but quickly abandoned it. I maintain the most up-to-date version of it and I wrote a bit about how I modernized it.\nIf you're looking for a ready-made solution, several companies offer ingredient parsing as a service:\n\nZestful (full disclosure: I'm the author)\nSpoonacular\nEdamam\n\n"}, "1095": {"topic": "How can I use NLP to parse recipe ingredients?", "user_name": "Mike ChristensenMike Christensen", "text": "\nI guess this is a few years out, but I was thinking of doing something similar myself and came across this, so thought I might have a stab at it in case it is useful to anyone else in f\nEven though you say you want to parse free test, most recipes have a pretty standard format for their recipe lists: each ingredient is on a separate line, exact sentence structure is rarely all that important.  The range of vocab is relatively small as well.\nOne way might be to check each line for words which might be nouns and words/symbols which express quantities.  I think WordNet may help with seeing if a word is likely to be a noun or not, but I've not used it before myself.  Alternatively, you could use http://en.wikibooks.org/wiki/Cookbook:Ingredients as a word list, though again, I wouldn't know exactly how comprehensive it is.\nThe other part is to recognise quantities.  These come in a few different forms, but few enough that you could probably create a list of keywords.  In particular, make sure you have good error reporting.  If the program can't fully parse a line, get it to report back to you what that line is, along with what it has/hasn't recognised so you can adjust your keyword lists accordingly.\nAaanyway, I'm not guaranteeing any of this will work (and it's almost certain not to be 100% reliable) but that's how I'd start to approach the problem\n"}, "1096": {"topic": "How can I use NLP to parse recipe ingredients?", "user_name": "", "text": "\nThis is an incomplete answer, but you're looking at writing up a free-text parser, which as you know, is non-trivial :)\nSome ways to cheat, using knowledge specific to cooking:\n\nConstruct lists of words for the \"adjectives\" and \"verbs\", and filter against them\n\n\nmeasurement units form a closed set, using words and abbreviations like {L., c, cup, t, dash}\ninstructions -- cut, dice, cook, peel.  Things that come after this are almost certain to be ingredients\n\nRemember that you're mostly looking for nouns, and you can take a labeled list of non-nouns (from WordNet, for example) and filter against them.  \n\nIf you're more ambitious, you can look in the NLTK Book at the chapter on parsers.  \nGood luck!  This sounds like a mostly doable project!  \n"}, "1097": {"topic": "How can I use NLP to parse recipe ingredients?", "user_name": "mtlynchmtlynch", "text": "\nCan you be more specific what your input is? If you just have input like this:\n1 cup flour\n2 lemon peels\n1 cup packed brown sugar\n\nIt won't be too hard to parse it without using any NLP at all. \n"}, "1098": {"topic": "How to perform Lemmatization in R?", "user_name": "CommunityBot", "text": "\nThis question is a possible duplicate of Lemmatizer in R or python (am, are, is -> be?), but I'm adding it again since the previous one was closed saying it was too broad and the only answer it has is not efficient (as it accesses an external website for this, which is too slow as I have very large corpus to find the lemmas for). So a part of this question will be similar to the above mentioned question.\nAccording to Wikipedia, lemmatization is defined as:\n\nLemmatisation (or lemmatization) in linguistics, is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.\n\nA simple Google search for lemmatization in R will only point to the package wordnet of R. When I tried this package expecting that a character vector c(\"run\", \"ran\", \"running\") input to the lemmatization function would result in c(\"run\", \"run\", \"run\"), I saw that this package only provides functionality similar to grepl function through various filter names and a dictionary.\nAn example code from wordnet package, which gives maximum of 5 words starting with \"car\", as the filter name explains itself:\nfilter <- getTermFilter(\"StartsWithFilter\", \"car\", TRUE)\nterms <- getIndexTerms(\"NOUN\", 5, filter)\nsapply(terms, getLemma)\n\nThe above is NOT the lemmatization that I'm looking for. What I'm looking for is, using R I want to find true roots of the words: (For e.g. from c(\"run\", \"ran\", \"running\") to c(\"run\", \"run\", \"run\")).\n"}, "1099": {"topic": "How to perform Lemmatization in R?", "user_name": "StrikeRStrikeR", "text": "\nHello you can try package koRpus which allow to use Treetagger :\ntagged.results <- treetag(c(\"run\", \"ran\", \"running\"), treetagger=\"manual\", format=\"obj\",\n                      TT.tknz=FALSE , lang=\"en\",\n                      TT.options=list(path=\"./TreeTagger\", preset=\"en\"))\ntagged.results@TT.res\n\n##     token tag lemma lttr wclass                               desc stop stem\n## 1     run  NN   run    3   noun             Noun, singular or mass   NA   NA\n## 2     ran VVD   run    3   verb                   Verb, past tense   NA   NA\n## 3 running VVG   run    7   verb Verb, gerund or present participle   NA   NA\n\nSee the lemma column for the result you're asking for.\n"}, "1100": {"topic": "How to perform Lemmatization in R?", "user_name": "VictorpVictorp", "text": "\nAs a previous post mentioned, the function lemmatize_words() from the R package textstem can perform this and give you what I understand as your desired results:\nlibrary(textstem)\nvector <- c(\"run\", \"ran\", \"running\")\nlemmatize_words(vector)\n\n## [1] \"run\" \"run\" \"run\"\n\n"}, "1101": {"topic": "How to perform Lemmatization in R?", "user_name": "", "text": "\n@Andy and @Arunkumar are correct when they say textstem library can be used to perform stemming and/or lemmatization. However, lemmatize_words() will only work on a vector of words. But in a corpus, we do not have vector of words; we have strings, with each string being a document's content. Hence, to perform lemmatization on a corpus, you can use function lemmatize_strings() as an argument to tm_map() of tm package. \n> corpus[[1]]\n[1] \" earnest roughshod document serves workable primer regions recent history make \nterrific th-grade learning tool samuel beckett applied iranian voting process bard \nblack comedy willie loved another trumpet blast may new mexican cinema -bornin \"\n> corpus <- tm_map(corpus, lemmatize_strings)\n> corpus[[1]]\n[1] \"earnest roughshod document serve workable primer region recent history make \nterrific th - grade learn tool samuel beckett apply iranian vote process bard black \ncomedy willie love another trumpet blast may new mexican cinema - bornin\"\n\nDo not forget to run the following line of code after you have done lemmatization:\n> corpus <- tm_map(corpus, PlainTextDocument)\n\nThis is because in order to create a document-term matrix, you need to have 'PlainTextDocument' type object, which gets changed after you use lemmatize_strings() (to be more specific, the corpus object does not contain content and meta-data of each document anymore - it is now just a structure containing documents' content; this is not the type of object that DocumentTermMatrix() takes as an argument).\nHope this helps!\n"}, "1102": {"topic": "How to perform Lemmatization in R?", "user_name": "AndyAndy", "text": "\nMaybe stemming is enough for you? Typical natural language processing tasks make do with stemmed texts. You can find several packages from CRAN Task View of NLP: http://cran.r-project.org/web/views/NaturalLanguageProcessing.html\nIf you really do require something more complex, then there's specialized solutsions based on mapping sentences to neural nets. As far as I know, these require massive amount of training data. There is lots of open software created and made available by Stanford NLP Group.\nIf you really want to dig into the topic, then you can dig through the event archives linked at the same Stanford NLP Group publications section. There's some books on the topic as well.\n"}, "1103": {"topic": "How to perform Lemmatization in R?", "user_name": "", "text": "\nI think the answers are a bit outdated here. You should be using R package udpipe now - available at https://CRAN.R-project.org/package=udpipe - see https://github.com/bnosac/udpipe or docs at https://bnosac.github.io/udpipe/en\nNotice the difference between the word meeting (NOUN) and the word meet (VERB) in the following example when doing lemmatisation and when doing stemming, and the annoying screwing up of the word 'someone' to 'someon' when doing stemming.\nlibrary(udpipe)\nx <- c(doc_a = \"In our last meeting, someone said that we are meeting again tomorrow\",\n       doc_b = \"It's better to be good at being the best\")\nanno <- udpipe(x, \"english\")\nanno[, c(\"doc_id\", \"sentence_id\", \"token\", \"lemma\", \"upos\")]\n#>    doc_id sentence_id    token    lemma  upos\n#> 1   doc_a           1       In       in   ADP\n#> 2   doc_a           1      our       we  PRON\n#> 3   doc_a           1     last     last   ADJ\n#> 4   doc_a           1  meeting  meeting  NOUN\n#> 5   doc_a           1        ,        , PUNCT\n#> 6   doc_a           1  someone  someone  PRON\n#> 7   doc_a           1     said      say  VERB\n#> 8   doc_a           1     that     that SCONJ\n#> 9   doc_a           1       we       we  PRON\n#> 10  doc_a           1      are       be   AUX\n#> 11  doc_a           1  meeting     meet  VERB\n#> 12  doc_a           1    again    again   ADV\n#> 13  doc_a           1 tomorrow tomorrow  NOUN\n#> 14  doc_b           1       It       it  PRON\n#> 15  doc_b           1       's       be   AUX\n#> 16  doc_b           1   better   better   ADJ\n#> 17  doc_b           1       to       to  PART\n#> 18  doc_b           1       be       be   AUX\n#> 19  doc_b           1     good     good   ADJ\n#> 20  doc_b           1       at       at SCONJ\n#> 21  doc_b           1    being       be   AUX\n#> 22  doc_b           1      the      the   DET\n#> 23  doc_b           1     best     best   ADJ\nlemmatisation <- paste.data.frame(anno, term = \"lemma\", \n                                  group = c(\"doc_id\", \"sentence_id\"))\nlemmatisation\n#>   doc_id sentence_id\n#> 1  doc_a           1\n#> 2  doc_b           1\n#>                                                             lemma\n#> 1 in we last meeting , someone say that we be meet again tomorrow\n#> 2                          it be better to be good at be the best\n\nlibrary(SnowballC)\ntokens   <- strsplit(x, split = \"[[:space:][:punct:]]+\")\nstemming <- lapply(tokens, FUN = function(x) wordStem(x, language = \"en\"))\nstemming\n#> $doc_a\n#>  [1] \"In\"       \"our\"      \"last\"     \"meet\"     \"someon\"   \"said\"    \n#>  [7] \"that\"     \"we\"       \"are\"      \"meet\"     \"again\"    \"tomorrow\"\n#> \n#> $doc_b\n#>  [1] \"It\"     \"s\"      \"better\" \"to\"     \"be\"     \"good\"   \"at\"     \"be\"    \n#>  [9] \"the\"    \"best\"\n\n"}, "1104": {"topic": "How to perform Lemmatization in R?", "user_name": "Harshit LambaHarshit Lamba", "text": "\nLemmatization can be done in R easily with textStem package.\nSteps are:\n1) Install textstem\n2) Load the package by \n       library(textstem)\n3) stem_word=lemmatize_words(word, dictionary = lexicon::hash_lemmas)\n     where stem_word is the result of lemmatization and word is the input word.   \n"}, "1105": {"topic": "What meaning does the length of a Word2vec vector have?", "user_name": "Mark AmeryMark Amery", "text": "\nI am using Word2vec through gensim with Google's pretrained vectors trained on Google News. I have noticed that the word vectors I can access by doing direct index lookups on the Word2Vec object are not unit vectors:\n>>> import numpy\n>>> from gensim.models import Word2Vec\n>>> w2v = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n>>> king_vector = w2v['king']\n>>> numpy.linalg.norm(king_vector)\n2.9022589\n\nHowever, in the most_similar method, these non-unit vectors are not used; instead, normalised versions are used from the undocumented .syn0norm property, which contains only unit vectors:\n>>> w2v.init_sims()\n>>> unit_king_vector = w2v.syn0norm[w2v.vocab['king'].index]\n>>> numpy.linalg.norm(unit_king_vector)\n0.99999994\n\nThe larger vector is just a scaled up version of the unit vector:\n>>> king_vector - numpy.linalg.norm(king_vector) * unit_king_vector\narray([  0.00000000e+00,  -1.86264515e-09,   0.00000000e+00,\n         0.00000000e+00,  -1.86264515e-09,   0.00000000e+00,\n        -7.45058060e-09,   0.00000000e+00,   3.72529030e-09,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n        ... (some lines omitted) ...\n        -1.86264515e-09,  -3.72529030e-09,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00], dtype=float32)\n\nGiven that word similarity comparisons in Word2Vec are done by cosine similarity, it's not obvious to me what the lengths of the non-normalised vectors mean - although I assume they mean something, since gensim exposes them to me rather than only exposing the unit vectors in .syn0norm.\nHow are the lengths of these non-normalised Word2vec vectors generated, and what is their meaning? For what calculations does it make sense to use the normalised vectors, and when should I use the non-normalised ones?\n"}, "1106": {"topic": "What meaning does the length of a Word2vec vector have?", "user_name": "Mark Amery", "text": "\nI think the answer you are looking for is described in the 2015 paper Measuring Word Significance\nusing\nDistributed Representations of Words by Adriaan Schakel and Benjamin Wilson. The key points:\n\nWhen a word appears\n  in different contexts, its vector gets moved in\n  different directions during updates. The final vector\n  then represents some sort of weighted average\n  over the various contexts. Averaging over vectors\n  that point in different directions typically results in\n  a vector that gets shorter with increasing number\n  of different contexts in which the word appears.\n  For words to be used in many different contexts,\n  they must carry little meaning. Prime examples of\n  such insignificant words are high-frequency stop\n  words, which are indeed represented by short vectors\n  despite their high term frequencies ...\n\n\n\nFor given term frequency,\n  the vector length is seen to take values only in a\n  narrow interval. That interval initially shifts upwards\n  with increasing frequency. Around a frequency\n  of about 30, that trend reverses and the interval\n  shifts downwards.\n...\nBoth forces determining the length of a word\n  vector are seen at work here. Small-frequency\n  words tend to be used consistently, so that the\n  more frequently such words appear, the longer\n  their vectors. This tendency is reflected by the upwards\n  trend in Fig. 3 at low frequencies. High-frequency\n  words, on the other hand, tend to be\n  used in many different contexts, the more so, the\n  more frequently they occur. The averaging over\n  an increasing number of different contexts shortens\n  the vectors representing such words. This tendency\n  is clearly reflected by the downwards trend\n  in Fig. 3 at high frequencies, culminating in punctuation\n  marks and stop words with short vectors at\n  the very end.\n...\n\nFigure 3: Word vector length v versus term frequency\n  tf of all words in the hep-th vocabulary.\n  Note the logarithmic scale used on the frequency\n  axis. The dark symbols denote bin means with the\n  kth bin containing the frequencies in the interval\n  [2k\u22121, 2k \u2212 1] with k = 1, 2, 3, . . .. These means\n  are included as a guide to the eye. The horizontal\n  line indicates the length v = 1.37 of the mean\n  vector\n\n\n\n4 Discussion\nMost applications of distributed representations of\n  words obtained through word2vec so far centered\n  around semantics. A host of experiments have\n  demonstrated the extent to which the direction of\n  word vectors captures semantics. In this brief report,\n  it was pointed out that not only the direction,\n  but also the length of word vectors carries important\n  information. Specifically, it was shown that\n  word vector length furnishes, in combination with\n  term frequency, a useful measure of word significance. \n\n"}, "1107": {"topic": "Stack Overflow Related questions algorithm [closed]", "user_name": "STLDev", "text": "\n\n\n\n\n\n\nClosed. This question needs to be more focused. It is not currently accepting answers.                    \n\n\n\n\n\n\n\n\n\n\nWant to improve this question? Update the question so it focuses on one problem only by editing this post.\n\n\nClosed 2 years ago.\n\n\n\n\n\n\n\r\n                        Improve this question\r\n                    \n\n\n\nThe related questions that appear after entering the title, and those that are in the right side bar when viewing a question seem to suggest very apt questions.\nStack Overflow only does a SQL search for it and uses no special algorithms, said Spolsky in a talk.\nWhat algorithms exist to give good answers in such a case.\nHow do U do database search in such a case? Make the title searchable and search on the keywords or search on tags and those questions with many votes on top?\n"}, "1108": {"topic": "Stack Overflow Related questions algorithm [closed]", "user_name": "lprsdlprsd", "text": "\nIf you listen to the Stack Overflow podcast 32 (unfortunately the transcript doesn't have much in) you can hear Jeff Atwood say a little about how he does it.\nIt seems like the algorithm is something like:\n\nTake the question\nRemove the most common words in English (from a list he got from google)\nsubmit a full text search to the SQL server 2008 full text search engine\n\nMore details about the full text search can be found here: http://msdn.microsoft.com/en-us/library/ms142571.aspx\nThis may be out of date by now - they were talking about moving to a better/faster full text search such as Lucene, and I vaguely remember Jeff saying in the podcast that this had been done.\n"}, "1109": {"topic": "Stack Overflow Related questions algorithm [closed]", "user_name": "CommunityBot", "text": "\nThe related questions sidebar will be building on the tags for each question (probably by ranking them based on tag overlap, so 5 tags in common > 4 tags in common etc). \nThe rest will be building on heuristics and algorithms suitable for natural language processing. These aren't normally very good in general purpose language, but most of them are VERY good once the vocabulary is reduced down to a single technical area such as programming.\n"}, "1110": {"topic": "Stack Overflow Related questions algorithm [closed]", "user_name": "Nick FortescueNick Fortescue", "text": "\nHave a look at Porter stemming for a stemming algorithm if you are looking to get into \"related\" algorithms.\n\nA stemmer for English, for example,\n  should identify the string \"cats\" (and\n  possibly \"catlike\", \"catty\" etc.) as\n  based on the root \"cat\", and\n  \"stemmer\", \"stemming\", \"stemmed\" as\n  based on \"stem\". A stemming algorithm\n  reduces the words \"fishing\", \"fished\",\n  \"fish\", and \"fisher\" to the root word,\n  \"fish\".\n\nOnce you have processed a document and done stemming on it, you can index the stemmed words by count and then compare against other documents. This is the most basic approach to tackling this problem.\nAlso take care to ignore stop words like \"the\", \"an\", \"of\" etc.\n"}, "1111": {"topic": "Stack Overflow Related questions algorithm [closed]", "user_name": "workmad3workmad3", "text": "\nThis post will help you Is there an algorithm that tells the semantic similarity of two phrases\n"}, "1112": {"topic": "Stack Overflow Related questions algorithm [closed]", "user_name": "", "text": "\nI do not know how SO implements it, but my hunch is that they use a variant of approximate string matching.\n"}, "1113": {"topic": "Stack Overflow Related questions algorithm [closed]", "user_name": "aleembaleemb", "text": "\nSuch problems are solved by making a \"bag of words\" of the stemmed words. That is basically a word count vector. Those words are preprocessed (stemmed) and weighted with their probability to occur in a sentence (\"the\" has a higher probability than \"probability\" and should thus be weighted less). You then can perceive this bag of words either as a vector in euclidean space or as a sample of a probability density.\nYou can apply algorithms as nearest neighbour search or semantic hashing. The latter seems to be SOTA (see http://www.cs.toronto.edu/~rsalakhu/papers/semantic_final.pdf).\n"}, "1114": {"topic": "Python - RegEx for splitting text into sentences (sentence-tokenizing) [duplicate]", "user_name": "smci", "text": "\n\n\n\n\n\n\nThis question already has answers here:                    \n\n\n\nHow can I split a text into sentences?\n\r\n                                (20 answers)\r\n                            \n\nClosed 4 years ago.\n\n\n\nI want to make a list of sentences from a string and then print them out. I don't want to use NLTK to do this.  So it needs to split on a period at the end of the sentence and not at decimals or abbreviations or title of a name or if the sentence has a .com   This is attempt at regex that doesn't work.\nimport re\n\ntext = \"\"\"\\\nMr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't.\n\"\"\"\nsentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)\n\nfor stuff in sentences:\n        print(stuff)    \n\nExample output of what it should look like\nMr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. \nDid he mind?\nAdam Jones Jr. thinks he didn't.\nIn any case, this isn't true...\nWell, with a probability of .9 it isn't.\n\n"}, "1115": {"topic": "Python - RegEx for splitting text into sentences (sentence-tokenizing) [duplicate]", "user_name": "user3590149user3590149", "text": "\n(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\n\nTry this. split your string this.You can also check demo.\nhttp://regex101.com/r/nG1gU7/27\n"}, "1116": {"topic": "Python - RegEx for splitting text into sentences (sentence-tokenizing) [duplicate]", "user_name": "", "text": "\nOk so sentence-tokenizers are something I looked at in a little detail, using regexes, nltk, CoreNLP, spaCy. You end up writing your own and it depends on the application. This stuff is tricky and valuable and people don't just give their tokenizer code away. (Ultimately, tokenization is not a deterministic procedure, it's probabilistic, and also depends very heavily on your corpus or domain, e.g. legal/financial documents vs social-media posts vs Yelp reviews vs biomedical papers...)\nIn general you can't rely on one single Great White infallible regex, you have to write a function which uses several regexes (both positive and negative); also a dictionary of abbreviations, and some basic language parsing which knows that e.g. 'I', 'USA', 'FCC', 'TARP' are capitalized in English.\nTo illustrate how easily this can get seriously complicated, let's try to write you that functional spec for a deterministic tokenizer just to decide whether single or multiple period ('.'/'...') indicates end-of-sentence, or something else:\nfunction isEndOfSentence(leftContext, rightContext)\n\nReturn False for decimals inside numbers or currency e.g. 1.23 , $1.23, \"That's just my $.02\" Consider also section references like 1.2.A.3.a, European date formats like 09.07.2014, IP addresses like 192.168.1.1, MAC addresses...\nReturn False (and don't tokenize into individual letters) for known abbreviations e.g. \"U.S. stocks are falling\" ; this requires a dictionary of known abbreviations. Anything outside that dictionary you will get wrong, unless you add code to detect unknown abbreviations like A.B.C. and add them to a list.\nEllipses '...' at ends of sentences are terminal, but in the middle of sentences are not. This is not as easy as you might think: you need to look at the left context and the right context, specifically is the RHS capitalized and again consider capitalized words like 'I' and   abbreviations. Here's an example proving ambiguity which : She asked me to stay... I left an hour later. (Was that one sentence or two? Impossible to determine)\nYou may also want to write a few patterns to detect and reject miscellaneous non-sentence-ending uses of punctuation: emoticons :-), ASCII art, spaced ellipses . . . and other stuff esp. Twitter. (Making that adaptive is even harder). How do we tell if @midnight is a Twitter user, the show on Comedy Central, text shorthand, or simply unwanted/junk/typo punctuation? Seriously non-trivial.\nAfter you handle all those negative cases, you could arbitrarily say that any isolated period followed by whitespace is likely to be an end of sentence. (Ultimately, if you really want to buy extra accuracy, you end up writing your own probabilistic sentence-tokenizer which uses weights, and training it on a specific corpus(e.g. legal texts, broadcast media, StackOverflow, Twitter, forums comments etc.)) Then you have to manually review exemplars and training errors. See Manning and Jurafsky book or Coursera course [a].\nUltimately you get as much correctness as you are prepared to pay for.\nAll of the above is clearly specific to the English-language/ abbreviations, US number/time/date formats. If you want to make it country- and language-independent, that's a bigger proposition, you'll need corpora, native-speaking people to label and QA it all, etc.\nAll of the above is still only ASCII, which is practically speaking only 96 characters. Allow the input to be Unicode, and things get harder still (and the training-set necessarily must be either much bigger or much sparser)\n\nIn the simple (deterministic) case, function isEndOfSentence(leftContext, rightContext) would return boolean, but in the more general sense, it's probabilistic: it returns a float 0.0-1.0 (confidence level that that particular '.' is a sentence end).\nReferences: [a] Coursera video: \"Basic Text Processing 2-5 - Sentence Segmentation - Stanford NLP - Professor Dan Jurafsky & Chris Manning\" [UPDATE: an unofficial version used to be on YouTube, was taken down]\n"}, "1117": {"topic": "Python - RegEx for splitting text into sentences (sentence-tokenizing) [duplicate]", "user_name": "vksvks", "text": "\nTry to split the input according to the spaces rather than a dot or ?, if you do like this then the dot or ? won't be printed in the final result.\n>>> import re\n>>> s = \"\"\"Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't.\"\"\"\n>>> m = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', s)\n>>> for i in m:\n...     print i\n... \nMr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it.\nDid he mind?\nAdam Jones Jr. thinks he didn't.\nIn any case, this isn't true...\nWell, with a probability of .9 it isn't.\n\n"}, "1118": {"topic": "Python - RegEx for splitting text into sentences (sentence-tokenizing) [duplicate]", "user_name": "", "text": "\nsent = re.split('(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)(\\s|[A-Z].*)',text)\nfor s in sent:\n    print s\n\nHere the regex used is : (?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)(\\s|[A-Z].*)\nFirst block: (?<!\\w\\.\\w.) : this pattern searches in a negative feedback loop (?<!) for all words (\\w) followed by fullstop (\\.) , followed by other words (\\.)\nSecond block: (?<![A-Z][a-z]\\.): this pattern searches in a negative feedback loop for anything starting with uppercase alphabets ([A-Z]), followed by lower case alphabets ([a-z]) till a dot (\\.) is found.\nThird block: (?<=\\.|\\?): this pattern searches in a feedback loop of dot (\\.) OR question mark (\\?)\nFourth block: (\\s|[A-Z].*): this pattern searches after the dot OR question mark from the third block. It searches for blank space (\\s) OR any sequence of characters starting with a upper case alphabet ([A-Z].*).\nThis block is important to split if the input is as \n\nHello world.Hi I am here today.\n\ni.e. if there is space or no space after the dot.\n"}, "1119": {"topic": "Python - RegEx for splitting text into sentences (sentence-tokenizing) [duplicate]", "user_name": "smcismci", "text": "\nNaive approach for proper english sentences not starting with non-alphas and not containing quoted parts of speech:\nimport re\ntext = \"\"\"\\\nMr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't.\n\"\"\"\nEndPunctuation = re.compile(r'([\\.\\?\\!]\\s+)')\nNonEndings = re.compile(r'(?:Mrs?|Jr|i\\.e)\\.\\s*$')\nparts = EndPunctuation.split(text)\nsentence = []\nfor part in parts:\n  if len(part) and len(sentence) and EndPunctuation.match(sentence[-1]) and not NonEndings.search(''.join(sentence)):\n    print(''.join(sentence))\n    sentence = []\n  if len(part):\n    sentence.append(part)\nif len(sentence):\n  print(''.join(sentence))\n\nFalse positive splitting may be reduced by extending NonEndings a bit. Other cases will require additional code. Handling typos in a sensible way will prove difficult with this approach.\nYou will never reach perfection with this approach. But depending on the task it might just work \"enough\"...\n"}, "1120": {"topic": "Python - RegEx for splitting text into sentences (sentence-tokenizing) [duplicate]", "user_name": "Avinash RajAvinash Raj", "text": "\nI'm not great at regular expressions, but a simpler version, \"brute force\" actually, of above is \nsentence = re.compile(\"([\\'\\\"][A-Z]|([A-Z][a-z]*\\. )|[A-Z])(([a-z]*\\.[a-z]*\\.)|([A-Za-z0-9]*\\.[A-Za-z0-9])|([A-Z][a-z]*\\. [A-Za-z]*)|[^\\.?]|[A-Za-z])*[\\.?]\")\n\nwhich means \nstart acceptable units are '[A-Z] or \"[A-Z]\nplease note, most regular expressions are greedy so the order is very important when we do |(or). That's, why I have written i.e. regular expression first, then is come forms like Inc. \n"}, "1121": {"topic": "Python - RegEx for splitting text into sentences (sentence-tokenizing) [duplicate]", "user_name": "Mehul GuptaMehul Gupta", "text": "\nTry this:\n(?<!\\b(?:[A-Z][a-z]|\\d|[i.e]))\\.(?!\\b(?:com|\\d+)\\b)\n\n"}, "1122": {"topic": "Python - RegEx for splitting text into sentences (sentence-tokenizing) [duplicate]", "user_name": "OktokoloOktokolo", "text": "\nI wrote this taking into consideration smci's comments above.  It is a middle-of-the-road approach that doesn't require external libraries and doesn't use regex.  It allows you to provide a list of abbreviations and accounts for sentences ended by terminators in wrappers, such as a period and quote: [.\", ?', .)].\nabbreviations = {'dr.': 'doctor', 'mr.': 'mister', 'bro.': 'brother', 'bro': 'brother', 'mrs.': 'mistress', 'ms.': 'miss', 'jr.': 'junior', 'sr.': 'senior', 'i.e.': 'for example', 'e.g.': 'for example', 'vs.': 'versus'}\nterminators = ['.', '!', '?']\nwrappers = ['\"', \"'\", ')', ']', '}']\n\n\ndef find_sentences(paragraph):\n   end = True\n   sentences = []\n   while end > -1:\n       end = find_sentence_end(paragraph)\n       if end > -1:\n           sentences.append(paragraph[end:].strip())\n           paragraph = paragraph[:end]\n   sentences.append(paragraph)\n   sentences.reverse()\n   return sentences\n\n\ndef find_sentence_end(paragraph):\n    [possible_endings, contraction_locations] = [[], []]\n    contractions = abbreviations.keys()\n    sentence_terminators = terminators + [terminator + wrapper for wrapper in wrappers for terminator in terminators]\n    for sentence_terminator in sentence_terminators:\n        t_indices = list(find_all(paragraph, sentence_terminator))\n        possible_endings.extend(([] if not len(t_indices) else [[i, len(sentence_terminator)] for i in t_indices]))\n    for contraction in contractions:\n        c_indices = list(find_all(paragraph, contraction))\n        contraction_locations.extend(([] if not len(c_indices) else [i + len(contraction) for i in c_indices]))\n    possible_endings = [pe for pe in possible_endings if pe[0] + pe[1] not in contraction_locations]\n    if len(paragraph) in [pe[0] + pe[1] for pe in possible_endings]:\n        max_end_start = max([pe[0] for pe in possible_endings])\n        possible_endings = [pe for pe in possible_endings if pe[0] != max_end_start]\n    possible_endings = [pe[0] + pe[1] for pe in possible_endings if sum(pe) > len(paragraph) or (sum(pe) < len(paragraph) and paragraph[sum(pe)] == ' ')]\n    end = (-1 if not len(possible_endings) else max(possible_endings))\n    return end\n\n\ndef find_all(a_str, sub):\n    start = 0\n    while True:\n        start = a_str.find(sub, start)\n        if start == -1:\n            return\n        yield start\n        start += len(sub)\n\nI used Karl's find_all function from this entry: Find all occurrences of a substring in Python\n"}, "1123": {"topic": "Python - RegEx for splitting text into sentences (sentence-tokenizing) [duplicate]", "user_name": "Priyank PathakPriyank Pathak", "text": "\nMy example is based on the example of Ali, adapted to Brazilian Portuguese. Thanks Ali.\nABREVIACOES = ['sra?s?', 'exm[ao]s?', 'ns?', 'nos?', 'doc', 'ac', 'publ', 'ex', 'lv', 'vlr?', 'vls?',\n               'exmo(a)', 'ilmo(a)', 'av', 'of', 'min', 'livr?', 'co?ls?', 'univ', 'resp', 'cli', 'lb',\n               'dra?s?', '[a-z]+r\\(as?\\)', 'ed', 'pa?g', 'cod', 'prof', 'op', 'plan', 'edf?', 'func', 'ch',\n               'arts?', 'artigs?', 'artg', 'pars?', 'rel', 'tel', 'res', '[a-z]', 'vls?', 'gab', 'bel',\n               'ilm[oa]', 'parc', 'proc', 'adv', 'vols?', 'cels?', 'pp', 'ex[ao]', 'eg', 'pl', 'ref',\n               '[0-9]+', 'reg', 'f[il\u00ed]s?', 'inc', 'par', 'alin', 'fts', 'publ?', 'ex', 'v. em', 'v.rev']\n\nABREVIACOES_RGX = re.compile(r'(?:{})\\.\\s*$'.format('|\\s'.join(ABREVIACOES)), re.IGNORECASE)\n\n        def sentencas(texto, min_len=5):\n            # baseado em https://stackoverflow.com/questions/25735644/python-regex-for-splitting-text-into-sentences-sentence-tokenizing\n            texto = re.sub(r'\\s\\s+', ' ', texto)\n            EndPunctuation = re.compile(r'([\\.\\?\\!]\\s+)')\n            # print(NonEndings)\n            parts = EndPunctuation.split(texto)\n            sentencas = []\n            sentence = []\n            for part in parts:\n                txt_sent = ''.join(sentence)\n                q_len = len(txt_sent)\n                if len(part) and len(sentence) and q_len >= min_len and \\\n                        EndPunctuation.match(sentence[-1]) and \\\n                        not ABREVIACOES_RGX.search(txt_sent):\n                    sentencas.append(txt_sent)\n                    sentence = []\n\n                if len(part):\n                    sentence.append(part)\n            if sentence:\n                sentencas.append(''.join(sentence))\n            return sentencas\n\nFull code in: https://github.com/luizanisio/comparador_elastic\n"}, "1124": {"topic": "Python - RegEx for splitting text into sentences (sentence-tokenizing) [duplicate]", "user_name": "walid toumiwalid toumi", "text": "\nIf you want to break up sentences at 3 periods (not sure if this is what you want) you can use this regular expresion:\n\nimport re\n\ntext = \"\"\"\\\nMr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't.\n\"\"\"\nsentences = re.split(r'\\.{3}', text)\n\nfor stuff in sentences:\n     print(stuff)    \n\n\n"}, "1125": {"topic": "How do I test whether an nltk resource is already installed on the machine running my code?", "user_name": "alvas", "text": "\nI just started my first NLTK project and am confused about the proper setup. I need several resources like the Punkt Tokenizer and the maxent pos tagger. I myself downloaded them using the GUI nltk.download(). For my collaborators I of course want that this things get downloaded automatically. I haven't found any idiomatic code for that in the docu. \nAm I supposed to just put nltk.data.load('tokenizers/punkt/english.pickle') and their like into the code? Is this going to download the resources every time the script is run? Am I to provide feedback to the  user (i.e. my co-developers) of what is being downloaded and why this is taking so long? There MUST be gear out there that does the job, right? :)\n//Edit To explify my question: \nHow do I test whether an nltk resource (like the Punkt Tokenizer) is already installed on the machine running my code, and install it if it is not? \n"}, "1126": {"topic": "How do I test whether an nltk resource is already installed on the machine running my code?", "user_name": "ZakumZakum", "text": "\nYou can use the nltk.data.find() function, see https://github.com/nltk/nltk/blob/develop/nltk/data.py:\n>>> import nltk\n>>> nltk.data.find('tokenizers/punkt.zip')\nZipFilePathPointer(u'/home/alvas/nltk_data/tokenizers/punkt.zip', u'')\n\nWhen the resource is not available you'll find the error:\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/dist-packages/nltk-3.0a3-py2.7.egg/nltk/data.py\", line 615, in find\n    raise LookupError(resource_not_found)\nLookupError: \n**********************************************************************\n  Resource u'punkt.zip' not found.  Please use the NLTK Downloader\n  to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/alvas/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n\nMost probably, you would like to do something like this to ensure that your collaborators have the package:\n>>> try:\n...     nltk.data.find('tokenizers/punkt')\n... except LookupError:\n...     nltk.download('punkt')\n... \n[nltk_data] Downloading package punkt to /home/alvas/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nTrue\n\n"}, "1127": {"topic": "How do I test whether an nltk resource is already installed on the machine running my code?", "user_name": "ChrisG", "text": "\nAfter Somnath comment, I am posting an example of the try-except workaround. Here we search for the comtrans module that is not in the nltk data by default.\nfrom nltk.corpus import comtrans\nfrom nltk import download\n\ntry:\n    words = comtrans.words('alignment-en-fr.txt')\nexcept LookupError:\n    print('resource not found. Downloading now...')\n    download('comtrans')\n    words = comtrans.words('alignment-en-fr.txt')\n\n"}, "1128": {"topic": "Clause Extraction using Stanford parser", "user_name": "mbatchkarov", "text": "\nI have a complex sentence and I  need to separate it into main and dependent clause.\nFor example for the sentence\nABC cites the fact that chemical additives are banned in many countries and feels they may be banned in this state too.\nThe split required\n1)ABC cites the fact   \n2)chemical additives are banned in many countries   \n3)ABC feels they may be banned in this state too.    \n\nI think I could use the Stanford Parser tree or dependencies, but I am not  sure how to proceed from here.  \nThe tree \n\n(ROOT\n  (S\n    (NP (NNP ABC))\n    (VP (VBZ cites)\n      (NP (DT the) (NN fact))\n      (SBAR (IN that)\n        (S\n          (NP (NN chemical) (NNS additives))\n          (VP\n            (VP (VBP are)\n              (VP (VBN banned)\n                (PP (IN in)\n                  (NP (JJ many) (NNS countries)))))\n            (CC and)\n            (VP (VBZ feels)\n              (SBAR\n                (S\n                  (NP (PRP they))\n                  (VP (MD may)\n                    (VP (VB be)\n                      (VP (VBN banned)\n                        (PP (IN in)\n                          (NP (DT this) (NN state)))\n                        (ADVP (RB too))))))))))))\n    (. .)))\n\nand the collapsed dependency parse  \n\nnsubj(cites-2, ABC-1)  \nroot(ROOT-0, cites-2)  \ndet(fact-4, the-3)   \ndobj(cites-2, fact-4)  \nmark(banned-9, that-5)  \nnn(additives-7, chemical-6)  \nnsubjpass(banned-9, additives-7)   \nnsubj(feels-14, additives-7)   \nauxpass(banned-9, are-8)   \nccomp(cites-2, banned-9)   \namod(countries-12, many-11)  \nprep_in(banned-9, countries-12)   \nccomp(cites-2, feels-14)    \nconj_and(banned-9, feels-14)    \nnsubjpass(banned-18, they-15)   \naux(banned-18, may-16)    \nauxpass(banned-18, be-17)    \nccomp(feels-14, banned-18)   \ndet(state-21, this-20)    \nprep_in(banned-18, state-21)    \nadvmod(banned-18, too-22)   \n\n"}, "1129": {"topic": "Clause Extraction using Stanford parser", "user_name": "AMisraAMisra", "text": "\nIt is probably better if you primarily use the constituenty-based parse tree, and not the dependencies. The dependencies will be helpful, but only after the main work is done! I am going to explain this towards the end of my answer.\nThis is because constituency-parse is based on phrase structure grammar, which is the most relevant if you are seeking to extract clauses from a sentence. It can be done using dependencies as well, but in that case, you will essentially be reconstructing the phrase structure -- starting from the root and looking at dependent nodes (e.g. ABC and facts are the nominal subject and direct object of the verb cites, and so on ... ).\nIt is helpful to visualize the parse tree, however. In your example, the clauses are indicated by the SBAR tag, which is a clause introduced by a (possibly empty) subordinating conjunction. All you need to do is the following:\n\nIdentify the non-root clausal nodes in the parse tree\nRemove (but retain separately) the subtrees rooted at these clausal nodes from the main tree.\nIn the main tree (after removal of subtrees in step 2), remove any hanging prepositions, subordinating conjunctions and adverbs.\n\nIn step 3, what I mean by \"hanging\" is that any prepositions, etc. whose dependency has been removed in step 2. E.g., from \"ABC cites the fact that\", you need to remove the preposition/subordinating-conjunction \"that\" because its dependent node \"banned\" was removed in step 2. You will thus have three independent clauses:\n\nchemical additives are banned in many countries (SBAR removal in step 2)\nthey may be banned in this state too (SBAR removal in step 2)\nABC cites the fact (step 3)\n\nThe only issue here is the connection ABC--feels. For this, note that both \"banned\" and \"feels\" are complements of the verb \"cites\", and hence, have the same subject, which is \"ABC\"! And you're done. When this is done, you will get a fourth clause, \"ABC feels\", which is something you may or may not want to include in your final result.\nFor a list of all clausal tags (and, in fact, all Penn Treebank tags), see this list: http://www.surdeanu.info/mihai/teaching/ista555-fall13/readings/PennTreebankConstituents.html\nFor an online parse-tree visualization, you may want to use the online Berkeley parser demo. It helps a lot in forming a better intuition. Here's the image generated for your example sentence:\n\nCaveats\n\nEven the best parsers will not always parse sentences correctly, so keep that in mind.\nAdditionally, many complex sentences involve right node raising, which is almost never parsed correctly by most parsers.\nYou may need to modify the algorithm a little if a clause is in passive voice.\n\nApart from these three pitfalls, the above algorithm should work quite accurately.\n"}, "1130": {"topic": "What's the disadvantage of LDA for short texts?", "user_name": "Rob\u2666", "text": "\nI am trying to understand why Latent Dirichlet Allocation(LDA) performs poorly in short text environments like Twitter. I've read the paper 'A biterm topic model for short text', however, I still do not understand \"the sparsity of word co-occurrences\". \nFrom my point of view, the generation part of LDA is reasonable for any kind of texts, but what causes bad results in short texts is the sampling procedure. I am guessing LDA samples a topic for a word based on two parts: (1) topics of other words in the same doc (2) topic assignments of other occurrences of this word. Since the (1) part of a short text cannot reflect the true distribution of it, that causes a poor topic assignment for each word.\nIf you have found this question, please feel free to post your idea and help me understand this.\n"}, "1131": {"topic": "What's the disadvantage of LDA for short texts?", "user_name": "Shuguang ZhuShuguang Zhu", "text": "\nProbabilistic models such as LDA exploit statistical inference to discover latent patterns of data. In short, they infer model parameters from observations. For instance, there is a black box containing many balls with different colors. You draw some balls out from the box and then infer the distributions of colors of the balls. That is a typical process of statistical inference. The accuracy of statistical inference depends on the number of your observations.\nNow consider the problem of LDA over short texts. LDA models a document as a mixture of topics, and then each word is drawn from one of its topic. You can imagine a black box contains tons of words generated from such a model. Now you have seen a short document with only a few of words. The observations is obvious too few to infer the parameters. It is the data sparsity problem we mentioned.\nActually, besides the the lack of observations, the problem also comes from the over-complexity of the model. Usually, a more flexible model requires more observations to infer. The Biterm Topic Model tries to making topic inference easier by reducing the model complexity. First, it models the whole corpus as a mixture of topics. Since inferring the topic mixture over the corpus is easier than inferring the topic mixture over a short document. Second, it supposes each biterm is draw from a topic. Inferring the topic of a biterm is also easier than inferring the topic of a single word in LDA, since more context is added.\nI hope the explanation make sense for you. Thanks for mentioning our paper.\n"}, "1132": {"topic": "What's the disadvantage of LDA for short texts?", "user_name": "Rob\u2666", "text": "\nDoing a bit of digging, Hong and Davison (2010) showed up as a great example of these not working well on classifying tweets. Unfortunately, they don't really give much insight into why it doesn't work. \nI suspect there's two reasons LDA doesn't work well for short documents.\nFirst of all, when working on smaller documents, the extra topic layer doesn't add anything to the classification, and what doesn't help probably hurts. If you have really short documents, like tweets, it's really hard to break documents into topics. There isn't much room for anything but one topic in a tweet, after all. Since the topic layer can't contribute much to the classification, it makes room for error to arise in the system. \nSecond, linguistically, Twitter users prefer to strip off \"unnecessary fluff\" when tweeting. When working with full documents, there are features --words, word collocations, etc.--that are probably specific, common, and often repeated within a genre. When tweeting, though, these common elements get dropped first because what's interesting, new, and more perplex is what remains when the fluff is removed. \nFor example, let's look at my own tweets because I believe in shameless self-promotion:\nProgressbar.py is a fun little package, though I don't get \na chance to use it too often. it even does ETAs for you \nhttps://pypi.python.org/pypi/progressbar \u2026\n\nFrom a capitalist perspective, the social sciences exist so \nidiot engineers don't waste money on building **** no one needs.\n\nAbstract enough to be reusable, specific enough to be useful.\n\nThe first is about Python. If you're parsing the URLs, you'll get that--and the .py would give it to you too. However, in a more expressive medium, I'd probably have put the word \"Python\" in somewhere. The second is programming related as well, but a bit more on the business end. Not once does it even mention anything specific to programming, though. The last one too is programming related, but ties more into the art of programming, expressing a sort of double-bind programmers face while coding. It is as difficult as the second, feature-wise. \nIn both of those last two examples, had I not been writing a microblog post, these would have immediately been followed up with examples that would have been very useful to a classifier, or themselves included more data. Twitter doesn't have room for that kind of stuff, though, and the content that would typify the genre a tweet belongs to is stripped out.\nSo, in the end, we have two problems. The length is a problem for LDA, because the topics add an extra, unnecessary degree of freedom, and the tweets are a problem for any classifier, because features typically useful in classification get selectively removed by the authors.\n"}, "1133": {"topic": "How do I determine if a random string sounds like English?", "user_name": "Dori", "text": "\nI have an algorithm that generates strings based on a list of input words. How do I separate only the strings that sounds like English words? ie. discard RDLO while keeping LORD.\nEDIT: To clarify, they do not need to be actual words in the dictionary. They just need to sound like English. For example KEAL would be accepted.\n"}, "1134": {"topic": "How do I determine if a random string sounds like English?", "user_name": "Ozgur OzcitakOzgur Ozcitak", "text": "\nYou can build a markov-chain of a huge english text.\nAfterwards you can feed words into the markov chain and check how high the probability is that the word is english.\nSee here: http://en.wikipedia.org/wiki/Markov_chain \nAt the bottom of the page you can see the markov text generator. What you want is exactly the reverse of it. \nIn a nutshell: The markov-chain stores for each character the probabilities of which next character will follow. You can extend this idea to two or three characters if you have enough memory.\n"}, "1135": {"topic": "How do I determine if a random string sounds like English?", "user_name": "", "text": "\nThe easy way with Bayesian filters (Python example from http://sebsauvage.net/python/snyppets/#bayesian)\nfrom reverend.thomas import Bayes\nguesser = Bayes()\nguesser.train('french','La souris est rentr\u00e9e dans son trou.')\nguesser.train('english','my tailor is rich.')\nguesser.train('french','Je ne sais pas si je viendrai demain.')\nguesser.train('english','I do not plan to update my website soon.')\n\n>>> print guesser.guess('Jumping out of cliffs it not a good idea.')\n[('english', 0.99990000000000001), ('french', 9.9999999999988987e-005)]\n\n>>> print guesser.guess('Demain il fera tr\u00e8s probablement chaud.')\n[('french', 0.99990000000000001), ('english', 9.9999999999988987e-005)]\n\n"}, "1136": {"topic": "How do I determine if a random string sounds like English?", "user_name": "Nils PipenbrinckNils Pipenbrinck", "text": "\nYou could approach this by tokenizing a candidate string into bigrams\u2014pairs of adjascent letters\u2014and checking each bigram against a table of English bigram frequencies.  \n\nSimple: if any bigram is sufficiently low on the frequency table (or outright absent), reject the string as implausible.  (String contains a \"QZ\" bigram?  Reject!)\nLess simple: calculate the overall plausibility of the whole string in terms of, say, a product of the frequencies of each bigram divided by the mean frequency of a valid English string of that length.  This would allow you to both (a) accept a string with an odd low-frequency bigram among otherwise high-frequency bigrams, and (b) reject a string with several individual low-but-not-quite-below-the-threshold bigrams.  \n\nEither of those would require some tuning of the threshold(s), the second technique more so than the first.\nDoing the same thing with trigrams would likely be more robust, though it'll also likely lead to a somewhat more strict set of \"valid\" strings.  Whether that's a win or not depends on your application.\nBigram and trigram tables based on existing research corpora may be available for free or purchase (I didn't find any freely available but only did a cursory google so far), but you can calculate a bigram or trigram table from yourself from any good-sized corpus of English text.  Just crank through each word as a token and tally up each bigram\u2014you might handle this as a hash with a given bigram as the key and an incremented integer counter as the value.\nEnglish morphology and English phonetics are (famously!) less than isometric, so this technique might well generate strings that \"look\" English but present troublesome prounciations.  This is another argument for trigrams rather than bigrams\u2014the weirdness produced by analysis of sounds that use several letters in sequence to produce a given phoneme will be reduced if the n-gram spans the whole sound.  (Think \"plough\" or \"tsunami\", for example.)\n"}, "1137": {"topic": "How do I determine if a random string sounds like English?", "user_name": "e-satise-satis", "text": "\nIt's quite easy to generate English sounding words using a Markov chain. Going backwards is more of a challenge, however. What's the acceptable margin of error for the results? You could always have a list of common letter pairs, triples, etc, and grade them based on that.\n"}, "1138": {"topic": "How do I determine if a random string sounds like English?", "user_name": "Josh MillardJosh Millard", "text": "\nYou should research \"pronounceable\" password generators, since they're trying to accomplish the same task. \nA Perl solution would be Crypt::PassGen, which you can train with a dictionary (so you could train it to various languages if you need to). It walks through the dictionary and collects statistics on 1, 2, and 3-letter sequences, then builds new \"words\" based on relative frequencies.\n"}, "1139": {"topic": "How do I determine if a random string sounds like English?", "user_name": "", "text": "\nI'd be tempted to run the soundex algorithm over a dictionary of English words and cache the results, then soundex your candidate string and match against the cache. \nDepending on performance requirements, you could work out a distance algorithm for soundex codes and accept strings within a certain tolerance.\nSoundex is very easy to implement - see Wikipedia for a description of the algorithm.\nAn example implementation of what you want to do would be:\ndef soundex(name, len=4):\n    digits = '01230120022455012623010202'\n    sndx = ''\n    fc = ''\n\n    for c in name.upper():\n        if c.isalpha():\n            if not fc: fc = c\n            d = digits[ord(c)-ord('A')]\n            if not sndx or (d != sndx[-1]):\n                sndx += d\n\n    sndx = fc + sndx[1:]\n    sndx = sndx.replace('0','')\n    return (sndx + (len * '0'))[:len]\n\nreal_words = load_english_dictionary()\nsoundex_cache = [ soundex(word) for word in real_words ]\n\nif soundex(candidate) in soundex_cache:\n    print \"keep\"\nelse:\n    print \"discard\"\n\nObviously you'll need to provide an implementation of read_english_dictionary.\nEDIT: Your example of \"KEAL\" will be fine, since it has the same soundex code (K400) as \"KEEL\". You may need to log rejected words and manually verify them if you want to get an idea of failure rate.\n"}, "1140": {"topic": "How do I determine if a random string sounds like English?", "user_name": "William KellerWilliam Keller", "text": "\nMetaphone and Double Metaphone are similar to SOUNDEX, except they may be tuned more toward your goal than SOUNDEX. They're designed to \"hash\" words based on their phonetic \"sound\", and are good at doing this for the English language (but not so much other languages and proper names).\nOne thing to keep in mind with all three algorithms is that they're extremely sensitive to the first letter of your word. For example, if you're trying to figure out if KEAL is English-sounding, you won't find a match to REAL because the initial letters are different.\n"}, "1141": {"topic": "How do I determine if a random string sounds like English?", "user_name": "Andrew BarnettAndrew Barnett", "text": "\nDo they have to be real English words, or just strings that look like they could be English words?\nIf they just need to look like possible English words you could do some statistical analysis on some real English texts and work out which combinations of letters occur frequently.  Once you've done that you can throw out strings that are too improbable, although some of them may be real words.\nOr you could just use a dictionary and reject words that aren't in it (with some allowances for plurals and other variations).\n"}, "1142": {"topic": "How do I determine if a random string sounds like English?", "user_name": "", "text": "\nYou could compare them to a dictionary (freely available on the internet), but that may be costly in terms of CPU usage. Other than that, I don't know of any other programmatic way to do it. \n"}, "1143": {"topic": "How do I determine if a random string sounds like English?", "user_name": "RussRuss", "text": "\nThat sounds like quite an involved task! Off the top of my head, a consonant phoneme needs a vowel either before or after it. Determining what a phoneme is will be quite hard though! You'll probably need to manually write out a list of them. For example, \"TR\" is ok but not \"TD\", etc.\n"}, "1144": {"topic": "How do I determine if a random string sounds like English?", "user_name": "Andrew BarnettAndrew Barnett", "text": "\nI would probably evaluate each word using a SOUNDEX algorithm against a database of english words. If you're doing this on a SQL-server it should be pretty easy to setup a database containing a list of most english words (using a freely available dictionary), and MSSQL server has SOUNDEX implemented as an available search-algorithm. \nObviously you can implement this yourself if you want, in any language - but it might be quite a task. \nThis way you'd get an evaluation of how much each word sounds like an existing english word, if any, and you could setup some limits for how low you'd want to accept results. You'd probably want to consider how to combine results for multiple words, and you would probably tweak the acceptance-limits based on testing. \n"}, "1145": {"topic": "How do I determine if a random string sounds like English?", "user_name": "Kevin ORourkeKevin ORourke", "text": "\nI'd suggest looking at the phi test and index of coincidence.  http://www.threaded.com/cryptography2.htm\n"}, "1146": {"topic": "How do I determine if a random string sounds like English?", "user_name": "Ryan BiggRyan Bigg", "text": "\nI'd suggest a few simple rules and standard pairs and triplets would be good.\nFor example, english sounding words tend to follow the pattern of vowel-consonant-vowel, apart from some dipthongs and standard consonant pairs (e.g. th, ie and ei, oo, tr). With a system like that you should strip out almost all words that don't sound like they could be english. You'd find on closer inspection that you will probably strip out a lot of words that do sound like english as well, but you can then start adding rules that allow for a wider range of words and 'train' your algorithm manually.\nYou won't remove all false negatives (e.g. I don't think you could manage to come up with a rule to include 'rythm' without explicitly coding in that rythm is a word) but it will provide a method of filtering.\nI'm also assuming that you want strings that could be english words (they sound reasonable when pronounced) rather than strings that are definitely words with an english meaning.\n"}, "1147": {"topic": "What is \"unk\" in the pretrained GloVe vector files (e.g. glove.6B.50d.txt)?", "user_name": "kmario23", "text": "\nI found \"unk\" token in the glove vector file glove.6B.50d.txt downloaded from https://nlp.stanford.edu/projects/glove/. Its value is as follows:\nunk -0.79149 0.86617 0.11998 0.00092287 0.2776 -0.49185 0.50195 0.00060792 -0.25845 0.17865 0.2535 0.76572 0.50664 0.4025 -0.0021388 -0.28397 -0.50324 0.30449 0.51779 0.01509 -0.35031 -1.1278 0.33253 -0.3525 0.041326 1.0863 0.03391 0.33564 0.49745 -0.070131 -1.2192 -0.48512 -0.038512 -0.13554 -0.1638 0.52321 -0.31318 -0.1655 0.11909 -0.15115 -0.15621 -0.62655 -0.62336 -0.4215 0.41873 -0.92472 1.1049 -0.29996 -0.0063003 0.3954\n\nIs it a token to be used for unknown words or is it some kind of abbreviation?\n"}, "1148": {"topic": "What is \"unk\" in the pretrained GloVe vector files (e.g. glove.6B.50d.txt)?", "user_name": "Abhay GuptaAbhay Gupta", "text": "\nThe unk token in the pretrained GloVe files is not an unknown token!\nSee this google groups thread where Jeffrey Pennington (GloVe author) writes:\n\nThe pre-trained vectors do not have an unknown token, and currently the code just ignores out-of-vocabulary words when producing the co-occurrence counts.\n\nIt's an embedding learned like any other on occurrences of \"unk\" in the corpus (which appears to happen occasionally!)\nInstead, Pennington suggests (in the same post):\n\n...I've found that just taking an average of all or a subset of the word vectors produces a good unknown vector.\n\nYou can do that with the following code (should work with any pretrained GloVe file):\nimport numpy as np\n\nGLOVE_FILE = 'glove.6B.50d.txt'\n\n# Get number of vectors and hidden dim\nwith open(GLOVE_FILE, 'r') as f:\n    for i, line in enumerate(f):\n        pass\nn_vec = i + 1\nhidden_dim = len(line.split(' ')) - 1\n\nvecs = np.zeros((n_vec, hidden_dim), dtype=np.float32)\n\nwith open(GLOVE_FILE, 'r') as f:\n    for i, line in enumerate(f):\n        vecs[i] = np.array([float(n) for n in line.split(' ')[1:]], dtype=np.float32)\n\naverage_vec = np.mean(vecs, axis=0)\nprint(average_vec)\n\nFor glove.6B.50d.txt this gives:\n[-0.12920076 -0.28866628 -0.01224866 -0.05676644 -0.20210965 -0.08389011\n  0.33359843  0.16045167  0.03867431  0.17833012  0.04696583 -0.00285802\n  0.29099807  0.04613704 -0.20923874 -0.06613114 -0.06822549  0.07665912\n  0.3134014   0.17848536 -0.1225775  -0.09916984 -0.07495987  0.06413227\n  0.14441176  0.60894334  0.17463093  0.05335403 -0.01273871  0.03474107\n -0.8123879  -0.04688699  0.20193407  0.2031118  -0.03935686  0.06967544\n -0.01553638 -0.03405238 -0.06528071  0.12250231  0.13991883 -0.17446303\n -0.08011883  0.0849521  -0.01041659 -0.13705009  0.20127155  0.10069408\n  0.00653003  0.01685157]\n\nAnd because it is fairly compute intensive to do this with the larger glove files, I went ahead and computed the vector for glove.840B.300d.txt for you:\n0.22418134 -0.28881392 0.13854356 0.00365387 -0.12870757 0.10243822 0.061626635 0.07318011 -0.061350107 -1.3477012 0.42037755 -0.063593924 -0.09683349 0.18086134 0.23704372 0.014126852 0.170096 -1.1491593 0.31497982 0.06622181 0.024687296 0.076693475 0.13851812 0.021302193 -0.06640582 -0.010336159 0.13523154 -0.042144544 -0.11938788 0.006948221 0.13333307 -0.18276379 0.052385733 0.008943111 -0.23957317 0.08500333 -0.006894406 0.0015864656 0.063391194 0.19177166 -0.13113557 -0.11295479 -0.14276934 0.03413971 -0.034278486 -0.051366422 0.18891625 -0.16673574 -0.057783455 0.036823478 0.08078679 0.022949161 0.033298038 0.011784158 0.05643189 -0.042776518 0.011959623 0.011552498 -0.0007971594 0.11300405 -0.031369694 -0.0061559738 -0.009043574 -0.415336 -0.18870236 0.13708843 0.005911723 -0.113035575 -0.030096142 -0.23908928 -0.05354085 -0.044904727 -0.20228513 0.0065645403 -0.09578946 -0.07391877 -0.06487607 0.111740574 -0.048649278 -0.16565254 -0.052037314 -0.078968436 0.13684988 0.0757494 -0.006275573 0.28693774 0.52017444 -0.0877165 -0.33010918 -0.1359622 0.114895485 -0.09744406 0.06269521 0.12118575 -0.08026362 0.35256687 -0.060017522 -0.04889904 -0.06828978 0.088740796 0.003964443 -0.0766291 0.1263925 0.07809314 -0.023164088 -0.5680669 -0.037892066 -0.1350967 -0.11351585 -0.111434504 -0.0905027 0.25174105 -0.14841858 0.034635577 -0.07334565 0.06320108 -0.038343467 -0.05413284 0.042197507 -0.090380974 -0.070528865 -0.009174437 0.009069661 0.1405178 0.02958134 -0.036431845 -0.08625681 0.042951006 0.08230793 0.0903314 -0.12279937 -0.013899368 0.048119213 0.08678239 -0.14450377 -0.04424887 0.018319942 0.015026873 -0.100526 0.06021201 0.74059093 -0.0016333034 -0.24960588 -0.023739101 0.016396184 0.11928964 0.13950661 -0.031624354 -0.01645025 0.14079992 -0.0002824564 -0.08052984 -0.0021310581 -0.025350995 0.086938225 0.14308536 0.17146006 -0.13943303 0.048792403 0.09274929 -0.053167373 0.031103406 0.012354865 0.21057427 0.32618305 0.18015954 -0.15881181 0.15322933 -0.22558987 -0.04200665 0.0084689725 0.038156632 0.15188617 0.13274793 0.113756925 -0.095273495 -0.049490947 -0.10265804 -0.27064866 -0.034567792 -0.018810693 -0.0010360252 0.10340131 0.13883452 0.21131058 -0.01981019 0.1833468 -0.10751636 -0.03128868 0.02518242 0.23232952 0.042052146 0.11731903 -0.15506615 0.0063580726 -0.15429358 0.1511722 0.12745973 0.2576985 -0.25486213 -0.0709463 0.17983761 0.054027 -0.09884228 -0.24595179 -0.093028545 -0.028203879 0.094398156 0.09233813 0.029291354 0.13110267 0.15682974 -0.016919162 0.23927948 -0.1343307 -0.22422817 0.14634751 -0.064993896 0.4703685 -0.027190214 0.06224946 -0.091360025 0.21490277 -0.19562101 -0.10032754 -0.09056772 -0.06203493 -0.18876675 -0.10963594 -0.27734384 0.12616494 -0.02217992 -0.16058226 -0.080475815 0.026953284 0.110732645 0.014894041 0.09416802 0.14299914 -0.1594008 -0.066080004 -0.007995227 -0.11668856 -0.13081996 -0.09237365 0.14741232 0.09180138 0.081735 0.3211204 -0.0036552632 -0.047030564 -0.02311798 0.048961394 0.08669574 -0.06766279 -0.50028914 -0.048515294 0.14144728 -0.032994404 -0.11954345 -0.14929578 -0.2388355 -0.019883996 -0.15917352 -0.052084364 0.2801028 -0.0029121689 -0.054581646 -0.47385484 0.17112483 -0.12066923 -0.042173345 0.1395337 0.26115036 0.012869649 0.009291686 -0.0026459037 -0.075331464 0.017840583 -0.26869613 -0.21820338 -0.17084768 -0.1022808 -0.055290595 0.13513643 0.12362477 -0.10980586 0.13980341 -0.20233242 0.08813751 0.3849736 -0.10653763 -0.06199595 0.028849555 0.03230154 0.023856193 0.069950655 0.19310954 -0.077677034 -0.144811\n\n"}, "1149": {"topic": "What is \"unk\" in the pretrained GloVe vector files (e.g. glove.6B.50d.txt)?", "user_name": "", "text": "\nSince I can't comment, writing another answer.\nIf anyone's having trouble using the above vector given by @jayelm because copy pasting won't work. I am writing 2 lines of code that will give you the vector ready to be used in python. \nvec_string = '0.22418134 -0.28881392 0.13854356 0.00365387 -0.12870757 0.10243822 0.061626635 0.07318011 -0.061350107 -1.3477012 0.42037755 -0.063593924 -0.09683349 0.18086134 0.23704372 0.014126852 0.170096 -1.1491593 0.31497982 0.06622181 0.024687296 0.076693475 0.13851812 0.021302193 -0.06640582 -0.010336159 0.13523154 -0.042144544 -0.11938788 0.006948221 0.13333307 -0.18276379 0.052385733 0.008943111 -0.23957317 0.08500333 -0.006894406 0.0015864656 0.063391194 0.19177166 -0.13113557 -0.11295479 -0.14276934 0.03413971 -0.034278486 -0.051366422 0.18891625 -0.16673574 -0.057783455 0.036823478 0.08078679 0.022949161 0.033298038 0.011784158 0.05643189 -0.042776518 0.011959623 0.011552498 -0.0007971594 0.11300405 -0.031369694 -0.0061559738 -0.009043574 -0.415336 -0.18870236 0.13708843 0.005911723 -0.113035575 -0.030096142 -0.23908928 -0.05354085 -0.044904727 -0.20228513 0.0065645403 -0.09578946 -0.07391877 -0.06487607 0.111740574 -0.048649278 -0.16565254 -0.052037314 -0.078968436 0.13684988 0.0757494 -0.006275573 0.28693774 0.52017444 -0.0877165 -0.33010918 -0.1359622 0.114895485 -0.09744406 0.06269521 0.12118575 -0.08026362 0.35256687 -0.060017522 -0.04889904 -0.06828978 0.088740796 0.003964443 -0.0766291 0.1263925 0.07809314 -0.023164088 -0.5680669 -0.037892066 -0.1350967 -0.11351585 -0.111434504 -0.0905027 0.25174105 -0.14841858 0.034635577 -0.07334565 0.06320108 -0.038343467 -0.05413284 0.042197507 -0.090380974 -0.070528865 -0.009174437 0.009069661 0.1405178 0.02958134 -0.036431845 -0.08625681 0.042951006 0.08230793 0.0903314 -0.12279937 -0.013899368 0.048119213 0.08678239 -0.14450377 -0.04424887 0.018319942 0.015026873 -0.100526 0.06021201 0.74059093 -0.0016333034 -0.24960588 -0.023739101 0.016396184 0.11928964 0.13950661 -0.031624354 -0.01645025 0.14079992 -0.0002824564 -0.08052984 -0.0021310581 -0.025350995 0.086938225 0.14308536 0.17146006 -0.13943303 0.048792403 0.09274929 -0.053167373 0.031103406 0.012354865 0.21057427 0.32618305 0.18015954 -0.15881181 0.15322933 -0.22558987 -0.04200665 0.0084689725 0.038156632 0.15188617 0.13274793 0.113756925 -0.095273495 -0.049490947 -0.10265804 -0.27064866 -0.034567792 -0.018810693 -0.0010360252 0.10340131 0.13883452 0.21131058 -0.01981019 0.1833468 -0.10751636 -0.03128868 0.02518242 0.23232952 0.042052146 0.11731903 -0.15506615 0.0063580726 -0.15429358 0.1511722 0.12745973 0.2576985 -0.25486213 -0.0709463 0.17983761 0.054027 -0.09884228 -0.24595179 -0.093028545 -0.028203879 0.094398156 0.09233813 0.029291354 0.13110267 0.15682974 -0.016919162 0.23927948 -0.1343307 -0.22422817 0.14634751 -0.064993896 0.4703685 -0.027190214 0.06224946 -0.091360025 0.21490277 -0.19562101 -0.10032754 -0.09056772 -0.06203493 -0.18876675 -0.10963594 -0.27734384 0.12616494 -0.02217992 -0.16058226 -0.080475815 0.026953284 0.110732645 0.014894041 0.09416802 0.14299914 -0.1594008 -0.066080004 -0.007995227 -0.11668856 -0.13081996 -0.09237365 0.14741232 0.09180138 0.081735 0.3211204 -0.0036552632 -0.047030564 -0.02311798 0.048961394 0.08669574 -0.06766279 -0.50028914 -0.048515294 0.14144728 -0.032994404 -0.11954345 -0.14929578 -0.2388355 -0.019883996 -0.15917352 -0.052084364 0.2801028 -0.0029121689 -0.054581646 -0.47385484 0.17112483 -0.12066923 -0.042173345 0.1395337 0.26115036 0.012869649 0.009291686 -0.0026459037 -0.075331464 0.017840583 -0.26869613 -0.21820338 -0.17084768 -0.1022808 -0.055290595 0.13513643 0.12362477 -0.10980586 0.13980341 -0.20233242 0.08813751 0.3849736 -0.10653763 -0.06199595 0.028849555 0.03230154 0.023856193 0.069950655 0.19310954 -0.077677034 -0.144811'\n\nimport numpy as np\naverage_glove_vector = np.array(vec_string.split(\" \"))\nprint(average_glove_vector)\n\n"}, "1150": {"topic": "How to proceed with NLP task for recognizing intent and slots", "user_name": "tumbleweed", "text": "\nI wanted to write a program for asking questions about weather. What are the algorithms and techniques I should start looking at.\nex: Will it be sunny this weekend in Chicago.\nI wanted to know the intent = weather query, date = this weekend, location = chicago.\nUser can express the same query in many forms. \nI would like to solve some constrained form and looking for ideas on how to get started. The solution needs to be just good enough.\n"}, "1151": {"topic": "How to proceed with NLP task for recognizing intent and slots", "user_name": "searavsearav", "text": "\nSince your input is in the natural language form, best way to start looking into it, first by parsing the sentence structure. and running the sentence through NER (Named Entity Recognizer).\nParsing the sentence lets you come up with rules such as, certain types of dependencies always give you the intent. Running the NER will let you identify places and dates. If it's not simple to come up with rules to classify the intent, you can as well use a classifier to do the same using feature vector formulated from the input sentence. In fact some of the parser out put can go into formulating the feature vector. \nFor both there exists software's from Stanford NLP Group \nMay be you can look into:\n\nStanford parser\nStanford NER Tagger\n\nOnce you parse the sentence, you have intent and other information require to answer the question.\nEx: I took your sentence \"Will it be sunny this weekend in Chicago.\" and ran it through Online Stanford NER Tagger. Which gave me the following: \nWill it be sunny this <DATE>weekend</DATE> in <LOCATION>Chicago</LOCATION>\n\nNow you have identified date and location.\nI hope this helps. I know the answer is quite generic, and may be helpful in just getting started. \n"}, "1152": {"topic": "How to proceed with NLP task for recognizing intent and slots", "user_name": "darshandarshan", "text": "\nI think this api is exactly what you are looking for.  It's easy and awesome to use.\nhttps://wit.ai/\n"}, "1153": {"topic": "How to proceed with NLP task for recognizing intent and slots", "user_name": "Weezy.FWeezy.F", "text": "\nAdditionally, https://www.luis.ai/ is a good implementation of an NLP framework. They have an API as well as a nuget SDK. We've been using them for awhile now. They were cheaper than the other options we looked at. i.e. wit.ai. \nSo re your example - \nex: Will it be sunny this weekend in Chicago -> would map to a LUIS intent called WeatherQuery.\ndate -> would map to a pre-built LUIS dateTime entity\nlocation -> chicago -> would map to a pre-built LUIS entity -> geography or address I think.\n"}, "1154": {"topic": "What are co-occurence matrixes and how are they used in NLP?", "user_name": "Evgenia Karunus", "text": "\nThe pypi docs for a google ngram downloader say that \"sometimes you need an aggregate data over the dataset. For example to build a co-occurrence matrix.\"\nThe wikipedia for co-occurence matrix has to do with image processing and googling the term seems to bring up some sort of SEO trick.\nSo what are co-occurrence matrixes (in computational linguistics/NLP)? How are they used in NLP?\n"}, "1155": {"topic": "What are co-occurence matrixes and how are they used in NLP?", "user_name": "bernie2436bernie2436", "text": "\nWhat is a co-occurrence matrix ?\nGenerally speaking, a co-occurrence matrix will have specific entities in rows (ER) and columns (EC). The purpose of this matrix is to present the number of times each ER appears in the same context as each EC.\nAs a consequence, in order to use a co-occurrence matrix, you have to define your entites and the context in which they co-occur.\nIn NLP, the most classic approach is to define each entity (ie, lines and columns) as a word present in a text, and the context as a sentence.\nConsider the following text :\n\nRoses are red. Sky is blue.\n\nWith the classic approach described before, we'll have the following matrix :\n      |  Roses | are | red | Sky | is | blue\nRoses |    1   |  1  |  1  |  0  |  0 |   0\nare   |    1   |  1  |  1  |  0  |  0 |   0\nred   |    1   |  1  |  1  |  0  |  0 |   0\nSky   |    0   |  0  |  0  |  1  |  1 |   1\nis    |    0   |  0  |  0  |  1  |  1 |   1\nBlue  |    0   |  0  |  0  |  1  |  1 |   1\n\nHere, each cell indicates wether the two items co-occur or not. You may replace it with the number of times it appears, or with a more sophisticated approach. You may also change the entities themselves, by putting nouns in columns and adjective in lines instead of every word.\nWhat are they used for in NLP ?\nThe most evident use of these matrix is their ability to provide links between notions. Let's suppose you're working on products reviews. Let's also suppose for simplicity that each review is only composed of short sentences. You'll have something like that :\n\nProductX is amazing.\nI hate productY.\n\nRepresenting these reviews as one co-occurrence matrix will enable you associate products with appreciations.\n"}, "1156": {"topic": "What are co-occurence matrixes and how are they used in NLP?", "user_name": "CommunityBot", "text": "\nThe co-occurrence matrix indicates how many times the row word (e.g. 'digital') is surrounded (in a sentence, or in the \u00b14 word window - depends on the application) by the column word (e.g. 'pie').\nThe entry '5' in the following table, for example, means that we had 5 sentences in our text where 'digital' was surrounded by 'pie'.\n\nThese sentences could have been:\n\nI love a digital pie.\nWhat's digital is often a pie.\nMay I have some digital pie?\nDigital world necessitates pie-eating.\nThere's something digital about this pie.\n\n\nNote that the co-occurrence matrix is always symmetric - the entry with the row word 'pie' and the column word 'digital' will be 5 as well (as these words co-occur in the very same sentences!).\n"}, "1157": {"topic": "Natural English language words", "user_name": "jimmym715", "text": "\nI need the most exhaustive English word list I can find for several types of language processing operations, but I could not find anything on the internet that has good enough quality.\nThere are 1,000,000 words in the English language including foreign and/or technical words. \nCan you please suggest such a source (or close to 500k words) that can be downloaded from the internet that is maybe a bit categorized? What input do you use for your language processing applications?\n"}, "1158": {"topic": "Natural English language words", "user_name": "GermstormGermstorm", "text": "\nKevin's wordlists is the best I know just for lists of words.\nWordNet is better if you want to know about things being nouns, verbs etc, synonyms, etc.\n"}, "1159": {"topic": "Natural English language words", "user_name": "Nick FortescueNick Fortescue", "text": "\n`The \"million word\" hoax rolls along', I see ;-) \nHow to make your word lists longer: given a noun, add any of the following to it: non-, pseudo-, semi-, -arific, -geek, ...; mutatis mutandis for verbs etc. \n"}, "1160": {"topic": "Natural English language words", "user_name": "unhammerunhammer", "text": "\nI did research for Purdue on controlled / natural english and language domain knowledge processing. \nI would take a look at the attempto project: http://attempto.ifi.uzh.ch/site/description/ which is a project to help build a controlled natural english.\nYou can download their entire word lexicon at: http://attempto.ifi.uzh.ch/site/downloads/files/clex-6.0-080806.zip it has ~ 100,000 natural English words. \nYou can also supply your own lexicon for domain specific words, this is what we did in our research. They offer webservices to parse and format natural english text. \n"}, "1161": {"topic": "Natural English language words", "user_name": "mmattaxmmattax", "text": "\nWho told you there was 1 million words?  According to Wikipedia, the Oxford English Dictionary only has 600,000.  And the OED tries to include all technical and slang terms that are used.\n"}, "1162": {"topic": "Natural English language words", "user_name": "KibbeeKibbee", "text": "\nTry directly Wikipedia's extracts : http://dbpedia.org\n"}, "1163": {"topic": "Natural English language words", "user_name": "CyrilCyril", "text": "\nThere aren't too many base words(171k according to this- oxford. Which is what I remember being told in my CS program in college. \nBut if include all forms of the words- then it rises considerably. \nThat said, why not make one yourself? Get a Wikipedia dump and parse it and create a set of all tokens you encounter. \nExpect misspellings though- like all things crowd-sources there will be errors. \n"}, "1164": {"topic": "Natural English language words", "user_name": "jimmym715", "text": "\nI need the most exhaustive English word list I can find for several types of language processing operations, but I could not find anything on the internet that has good enough quality.\nThere are 1,000,000 words in the English language including foreign and/or technical words. \nCan you please suggest such a source (or close to 500k words) that can be downloaded from the internet that is maybe a bit categorized? What input do you use for your language processing applications?\n"}, "1165": {"topic": "Natural English language words", "user_name": "GermstormGermstorm", "text": "\nKevin's wordlists is the best I know just for lists of words.\nWordNet is better if you want to know about things being nouns, verbs etc, synonyms, etc.\n"}, "1166": {"topic": "Natural English language words", "user_name": "Nick FortescueNick Fortescue", "text": "\n`The \"million word\" hoax rolls along', I see ;-) \nHow to make your word lists longer: given a noun, add any of the following to it: non-, pseudo-, semi-, -arific, -geek, ...; mutatis mutandis for verbs etc. \n"}, "1167": {"topic": "Natural English language words", "user_name": "unhammerunhammer", "text": "\nI did research for Purdue on controlled / natural english and language domain knowledge processing. \nI would take a look at the attempto project: http://attempto.ifi.uzh.ch/site/description/ which is a project to help build a controlled natural english.\nYou can download their entire word lexicon at: http://attempto.ifi.uzh.ch/site/downloads/files/clex-6.0-080806.zip it has ~ 100,000 natural English words. \nYou can also supply your own lexicon for domain specific words, this is what we did in our research. They offer webservices to parse and format natural english text. \n"}, "1168": {"topic": "Natural English language words", "user_name": "mmattaxmmattax", "text": "\nWho told you there was 1 million words?  According to Wikipedia, the Oxford English Dictionary only has 600,000.  And the OED tries to include all technical and slang terms that are used.\n"}, "1169": {"topic": "Natural English language words", "user_name": "KibbeeKibbee", "text": "\nTry directly Wikipedia's extracts : http://dbpedia.org\n"}, "1170": {"topic": "Natural English language words", "user_name": "CyrilCyril", "text": "\nThere aren't too many base words(171k according to this- oxford. Which is what I remember being told in my CS program in college. \nBut if include all forms of the words- then it rises considerably. \nThat said, why not make one yourself? Get a Wikipedia dump and parse it and create a set of all tokens you encounter. \nExpect misspellings though- like all things crowd-sources there will be errors. \n"}, "1171": {"topic": "What are all possible pos tags of NLTK?", "user_name": "OrangeTuxOrangeTux", "text": "\nHow do I find a list with all possible pos tags used by the Natural Language Toolkit (nltk)?\n"}, "1172": {"topic": "What are all possible pos tags of NLTK?", "user_name": "", "text": "\nTo save some folks some time, here is a list I extracted from a small corpus.  I do not know if it is complete, but it should have most (if not all) of the help definitions from upenn_tagset...\nCC: conjunction, coordinating\n& 'n and both but either et for less minus neither nor or plus so\ntherefore times v. versus vs. whether yet\n\nCD: numeral, cardinal\nmid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\nseven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\nfifteen 271,124 dozen quintillion DM2,000 ...\n\nDT: determiner\nall an another any both del each either every half la many much nary\nneither no some such that the them these this those\n\nEX: existential there\nthere\n\nIN: preposition or conjunction, subordinating\nastride among upon whether out inside pro despite on by throughout\nbelow within for towards near behind atop around if like until below\nnext into if beside ...\n\nJJ: adjective or numeral, ordinal\nthird ill-mannered pre-war regrettable oiled calamitous first separable\nectoplasmic battery-powered participatory fourth still-to-be-named\nmultilingual multi-disciplinary ...\n\nJJR: adjective, comparative\nbleaker braver breezier briefer brighter brisker broader bumper busier\ncalmer cheaper choosier cleaner clearer closer colder commoner costlier\ncozier creamier crunchier cuter ...\n\nJJS: adjective, superlative\ncalmest cheapest choicest classiest cleanest clearest closest commonest\ncorniest costliest crassest creepiest crudest cutest darkest deadliest\ndearest deepest densest dinkiest ...\n\nLS: list item marker\nA A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\nSP-44007 Second Third Three Two * a b c d first five four one six three\ntwo\n\nMD: modal auxiliary\ncan cannot could couldn't dare may might must need ought shall should\nshouldn't will would\n\nNN: noun, common, singular or mass\ncommon-carrier cabbage knuckle-duster Casino afghan shed thermostat\ninvestment slide humour falloff slick wind hyena override subhumanity\nmachinist ...\n\nNNP: noun, proper, singular\nMotown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\nOceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\nShannon A.K.C. Meltex Liverpool ...\n\nNNS: noun, common, plural\nundergraduates scotches bric-a-brac products bodyguards facets coasts\ndivestitures storehouses designs clubs fragrances averages\nsubjectivists apprehensions muses factory-jobs ...\n\nPDT: pre-determiner\nall both half many quite such sure this\n\nPOS: genitive marker\n' 's\n\nPRP: pronoun, personal\nhers herself him himself hisself it itself me myself one oneself ours\nourselves ownself self she thee theirs them themselves they thou thy us\n\nPRP$: pronoun, possessive\nher his mine my our ours their thy your\n\nRB: adverb\noccasionally unabatingly maddeningly adventurously professedly\nstirringly prominently technologically magisterially predominately\nswiftly fiscally pitilessly ...\n\nRBR: adverb, comparative\nfurther gloomier grander graver greater grimmer harder harsher\nhealthier heavier higher however larger later leaner lengthier less-\nperfectly lesser lonelier longer louder lower more ...\n\nRBS: adverb, superlative\nbest biggest bluntest earliest farthest first furthest hardest\nheartiest highest largest least less most nearest second tightest worst\n\nRP: particle\naboard about across along apart around aside at away back before behind\nby crop down ever fast for forth from go high i.e. in into just later\nlow more off on open out over per pie raising start teeth that through\nunder unto up up-pp upon whole with you\n\nTO: \"to\" as preposition or infinitive marker\nto\n\nUH: interjection\nGoodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\nhuh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\nman baby diddle hush sonuvabitch ...\n\nVB: verb, base form\nask assemble assess assign assume atone attention avoid bake balkanize\nbank begin behold believe bend benefit bevel beware bless boil bomb\nboost brace break bring broil brush build ...\n\nVBD: verb, past tense\ndipped pleaded swiped regummed soaked tidied convened halted registered\ncushioned exacted snubbed strode aimed adopted belied figgered\nspeculated wore appreciated contemplated ...\n\nVBG: verb, present participle or gerund\ntelegraphing stirring focusing angering judging stalling lactating\nhankerin' alleging veering capping approaching traveling besieging\nencrypting interrupting erasing wincing ...\n\nVBN: verb, past participle\nmultihulled dilapidated aerosolized chaired languished panelized used\nexperimented flourished imitated reunifed factored condensed sheared\nunsettled primed dubbed desired ...\n\nVBP: verb, present tense, not 3rd person singular\npredominate wrap resort sue twist spill cure lengthen brush terminate\nappear tend stray glisten obtain comprise detest tease attract\nemphasize mold postpone sever return wag ...\n\nVBZ: verb, present tense, 3rd person singular\nbases reconstructs marks mixes displeases seals carps weaves snatches\nslumps stretches authorizes smolders pictures emerges stockpiles\nseduces fizzes uses bolsters slaps speaks pleads ...\n\nWDT: WH-determiner\nthat what whatever which whichever\n\nWP: WH-pronoun\nthat what whatever whatsoever which who whom whosoever\n\nWRB: Wh-adverb\nhow however whence whenever where whereby whereever wherein whereof why\n\n"}, "1173": {"topic": "What are all possible pos tags of NLTK?", "user_name": "binarymaxbinarymax", "text": "\nThe book has a note how to find help on tag sets, e.g.:\nnltk.help.upenn_tagset()\n\nOthers are probably similar. (Note: Maybe you first have to download tagsets from the download helper's Models section for this)\n"}, "1174": {"topic": "What are all possible pos tags of NLTK?", "user_name": "phipsgablerphipsgabler", "text": "\nThe tag set depends on the corpus that was used to train the tagger. \nThe default tagger of nltk.pos_tag() uses the Penn Treebank Tag Set. \nIn NLTK 2, you could check which tagger is the default tagger as follows: \nimport nltk\nnltk.tag._POS_TAGGER\n>>> 'taggers/maxent_treebank_pos_tagger/english.pickle'\n\nThat means that it's a Maximum Entropy tagger trained on the Treebank corpus. \nnltk.tag._POS_TAGGER does not exist anymore in NLTK 3 but the documentation states that the off-the-shelf tagger still uses the Penn Treebank tagset. \n"}, "1175": {"topic": "What are all possible pos tags of NLTK?", "user_name": "", "text": "\nThe below can be useful to access a dict keyed by abbreviations:\n>>> from nltk.data import load\n>>> tagdict = load('help/tagsets/upenn_tagset.pickle')\n>>> tagdict['NN'][0]\n'noun, common, singular or mass'\n>>> tagdict.keys()\n['PRP$', 'VBG', 'VBD', '``', 'VBN', ',', \"''\", 'VBP', 'WDT', ...\n\n"}, "1176": {"topic": "What are all possible pos tags of NLTK?", "user_name": "SuzanaSuzana", "text": "\nThe reference is available at the official site\nCopy and pasting from there:\n\nCC | Coordinating conjunction |\nCD | Cardinal number |\nDT | Determiner |\nEX | Existential there |\nFW | Foreign word |\nIN | Preposition or subordinating conjunction |\nJJ | Adjective |\nJJR | Adjective, comparative |\nJJS | Adjective, superlative |\nLS | List item marker |\nMD | Modal |\nNN | Noun, singular or mass |\nNNS | Noun, plural |\nNNP | Proper noun, singular |\nNNPS | Proper noun, plural |\nPDT | Predeterminer |\nPOS | Possessive ending |\nPRP | Personal pronoun |\nPRP$ | Possessive pronoun |\nRB | Adverb |\nRBR | Adverb, comparative |\nRBS | Adverb, superlative |\nRP | Particle |\nSYM | Symbol |\nTO | to |\nUH | Interjection |\nVB | Verb, base form |\nVBD | Verb, past tense |\nVBG | Verb, gerund or present participle |\nVBN | Verb, past participle |\nVBP | Verb, non-3rd person singular present |\nVBZ | Verb, 3rd person singular present |\nWDT | Wh-determiner |\nWP | Wh-pronoun |\nWP$ | Possessive wh-pronoun |\nWRB | Wh-adverb |\n\n"}, "1177": {"topic": "What are all possible pos tags of NLTK?", "user_name": "Doug ShoreDoug Shore", "text": "\n['LS', 'TO', 'VBN', \"''\", 'WP', 'UH', 'VBG', 'JJ', 'VBZ', '--', 'VBP', 'NN', 'DT', 'PRP', ':', 'WP$', 'NNPS', 'PRP$', 'WDT', '(', ')', '.', ',', '``', '$', 'RB', 'RBR', 'RBS', 'VBD', 'IN', 'FW', 'RP', 'JJR', 'JJS', 'PDT', 'MD', 'VB', 'WRB', 'NNP', 'EX', 'NNS', 'SYM', 'CC', 'CD', 'POS']\n\nBased on Doug Shore's method but make it more copy-paste friendly\n"}, "1178": {"topic": "What are all possible pos tags of NLTK?", "user_name": "mdubezmdubez", "text": "\nYou can download the list here: ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz. It includes confusing parts of speech, capitalization, and other conventions. Also, wikipedia has an interesting section similar to this. Section: Part-of-speech tags used. \n"}, "1179": {"topic": "What are all possible pos tags of NLTK?", "user_name": "jcchuks", "text": "\nJust run this verbatim. \nimport nltk\nnltk.download('tagsets')\nnltk.help.upenn_tagset()\n\nnltk.tag._POS_TAGGER won't work. It will give AttributeError: module 'nltk.tag' has no attribute '_POS_TAGGER'. It's not available in NLTK 3 anymore. \n"}, "1180": {"topic": "What are all possible pos tags of NLTK?", "user_name": "little_thumblittle_thumb", "text": "\nRun the below code in python to get information about all pos tag.\nimport nltk\nnltk.help.upenn_tagset()\n\n"}, "1181": {"topic": "SSL error downloading NLTK data", "user_name": "CommunityBot", "text": "\nI am trying to download NLTK 3.0 for use with Python 3.6 on Mac OS X 10.7.5, but am getting an SSL error:\nimport nltk\nnltk.download()\n\n\nI downloaded NLTK with a pip3 command: sudo pip3 install -U nltk.\nChanging the index in the NLTK downloader allows the downloader to show all of NLTK's files, but when one tries to download all, one gets another SSL error (see bottom of photo):\n\nI am relatively new to computer science and am not at all savvy with respect to SSL.\nMy question is how to simply resolve this issue?\n\nHere is a similar question by a user who is having the same problem:\nUnable to download nltk data\nI decided to post a new question with screenshots, since my edit to that other question was rejected.\nSimilar questions which I did not find helpful:\nNLTK download SSL: Certificate verify failed\ndownloading error using nltk.download()\n"}, "1182": {"topic": "SSL error downloading NLTK data", "user_name": "DyingIsFunDyingIsFun", "text": "\nYou don't need to disable SSL checking if you run the following terminal command:\n/Applications/Python 3.6/Install Certificates.command\n\nIn the place of 3.6, put your version of Python if it's an earlier one. Then you should be able to open your Python interpreter (using the command python3) and successfully run nltk.download() there.\nThis is an issue wherein urllib uses an embedded version of OpenSSL that not in the system certificate store. Here's an answer with more information on what's going on.\n"}, "1183": {"topic": "SSL error downloading NLTK data", "user_name": "", "text": "\nPlease see answer by @doctorBroctor. It is more correct and safer to use. Leaving answer below as it might be useful for something else. \nhttps://stackoverflow.com/a/42890688/1167890\n\nThis will work by disabling SSL checking. \nimport nltk\nimport ssl\n\ntry:\n    _create_unverified_https_context = ssl._create_unverified_context\nexcept AttributeError:\n    pass\nelse:\n    ssl._create_default_https_context = _create_unverified_https_context\n\nnltk.download()\n\n"}, "1184": {"topic": "SSL error downloading NLTK data", "user_name": "doctorBroctordoctorBroctor", "text": "\nIn Finder, search for Python 3.6.\nIt will appear under Application folder.\nExpand the Python 3.6 folder.\nThen install certificates using \"Install Certificates.command\".\n\n"}, "1185": {"topic": "SSL error downloading NLTK data", "user_name": "", "text": "\nTo install in codestar only way is manually download modules and save them into nltk_data folder, create a lambda variable environment NLTK_DATA with valie ./nltk_data.\n"}, "1186": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "lizarisk", "text": "\nI'm just starting to use NLTK and I don't quite understand how to get a list of words from text. If I use nltk.word_tokenize(), I get a list of words and punctuation. I need only the words instead. How can I get rid of punctuation? Also word_tokenize doesn't work with multiple sentences: dots are added to the last word.\n"}, "1187": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "lizarisklizarisk", "text": "\nTake a look at the other tokenizing options that nltk provides here. For example, you can define a tokenizer that picks out sequences of alphanumeric characters as tokens and drops everything else:\nfrom nltk.tokenize import RegexpTokenizer\n\ntokenizer = RegexpTokenizer(r'\\w+')\ntokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')\n\nOutput:\n['Eighty', 'seven', 'miles', 'to', 'go', 'yet', 'Onward']\n\n"}, "1188": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "kgraney", "text": "\nYou do not really need NLTK to remove punctuation. You can remove it with simple python. For strings:\nimport string\ns = '... some string with punctuation ...'\ns = s.translate(None, string.punctuation)\n\nOr for unicode:\nimport string\ntranslate_table = dict((ord(char), None) for char in string.punctuation)   \ns.translate(translate_table)\n\nand then use this string in your tokenizer.\nP.S. string module have some other sets of elements that can be removed (like digits).\n"}, "1189": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "rmaloufrmalouf", "text": "\nBelow code will remove all punctuation marks as well as non alphabetic characters. Copied from their book.\nhttp://www.nltk.org/book/ch01.html \nimport nltk\n\ns = \"I can't do this now, because I'm so tired.  Please give me some time. @ sd  4 232\"\n\nwords = nltk.word_tokenize(s)\n\nwords=[word.lower() for word in words if word.isalpha()]\n\nprint(words)\n\noutput\n['i', 'ca', 'do', 'this', 'now', 'because', 'i', 'so', 'tired', 'please', 'give', 'me', 'some', 'time', 'sd']\n\n"}, "1190": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "Eli", "text": "\nAs noticed in comments start with sent_tokenize(), because word_tokenize() works only on a single sentence. You can filter out punctuation with filter(). And if you have an unicode strings make sure that is a unicode object (not a 'str' encoded with some encoding like 'utf-8'). \nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\ntext = '''It is a blue, small, and extraordinary ball. Like no other'''\ntokens = [word for sent in sent_tokenize(text) for word in word_tokenize(sent)]\nprint filter(lambda word: word not in ',-', tokens)\n\n"}, "1191": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "Salvador DaliSalvador Dali", "text": "\nI just used the following code, which removed all the punctuation:\ntokens = nltk.wordpunct_tokenize(raw)\n\ntype(tokens)\n\ntext = nltk.Text(tokens)\n\ntype(text)  \n\nwords = [w.lower() for w in text if w.isalpha()]\n\n"}, "1192": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "Madura PradeepMadura Pradeep", "text": "\nSincerely asking, what is a word? If your assumption is that a word consists of alphabetic characters only, you are wrong since words such as can't will be destroyed into pieces (such as can and t) if you remove punctuation before tokenisation, which is very likely to affect your program negatively.\nHence the solution is to tokenise and then remove punctuation tokens.\nimport string\n\nfrom nltk.tokenize import word_tokenize\n\ntokens = word_tokenize(\"I'm a southern salesman.\")\n# ['I', \"'m\", 'a', 'southern', 'salesman', '.']\n\ntokens = list(filter(lambda token: token not in string.punctuation, tokens))\n# ['I', \"'m\", 'a', 'southern', 'salesman']\n\n...and then if you wish, you can replace certain tokens such as 'm with am.\n"}, "1193": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "paloohpalooh", "text": "\nI think you need some sort of regular expression matching (the following code is in Python 3):\nimport string\nimport re\nimport nltk\n\ns = \"I can't do this now, because I'm so tired.  Please give me some time.\"\nl = nltk.word_tokenize(s)\nll = [x for x in l if not re.fullmatch('[' + string.punctuation + ']+', x)]\nprint(l)\nprint(ll)\n\nOutput:\n['I', 'ca', \"n't\", 'do', 'this', 'now', ',', 'because', 'I', \"'m\", 'so', 'tired', '.', 'Please', 'give', 'me', 'some', 'time', '.']\n['I', 'ca', \"n't\", 'do', 'this', 'now', 'because', 'I', \"'m\", 'so', 'tired', 'Please', 'give', 'me', 'some', 'time']\n\nShould work well in most cases since it removes punctuation while preserving tokens like \"n't\", which can't be obtained from regex tokenizers such as wordpunct_tokenize.\n"}, "1194": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "", "text": "\nYou can do it in one line without nltk (python 3.x).\nimport string\nstring_text= string_text.translate(str.maketrans('','',string.punctuation))\n\n"}, "1195": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "vishvish", "text": "\nI use this code to remove punctuation:\nimport nltk\ndef getTerms(sentences):\n    tokens = nltk.word_tokenize(sentences)\n    words = [w.lower() for w in tokens if w.isalnum()]\n    print tokens\n    print words\n\ngetTerms(\"hh, hh3h. wo shi 2 4 A . fdffdf. A&&B \")\n\nAnd If you want to check whether a token is a valid English word or not, you may need PyEnchant\nTutorial:\n import enchant\n d = enchant.Dict(\"en_US\")\n d.check(\"Hello\")\n d.check(\"Helo\")\n d.suggest(\"Helo\")\n\n"}, "1196": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "Bora M. AlperBora M. Alper", "text": "\nJust adding to the solution by @rmalouf, this will not include any numbers because \\w+ is equivalent to [a-zA-Z0-9_]\nfrom nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'[a-zA-Z]')\ntokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')\n\n"}, "1197": {"topic": "How to get rid of punctuation using NLTK tokenizer?", "user_name": "Quan GanQuan Gan", "text": "\nRemove punctuaion(It will remove . as well as part of punctuation handling using below code)\n        tbl = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n        text_string = text_string.translate(tbl) #text_string don't have punctuation\n        w = word_tokenize(text_string)  #now tokenize the string \n\nSample Input/Output:\ndirect flat in oberoi esquire. 3 bhk 2195 saleable 1330 carpet. rate of 14500 final plus 1% floor rise. tax approx 9% only. flat cost with parking 3.89 cr plus taxes plus possession charger. middle floor. north door. arey and oberoi woods facing. 53% paymemt due. 1% transfer charge with buyer. total cost around 4.20 cr approx plus possession charges. rahul soni\n\n['direct', 'flat', 'oberoi', 'esquire', '3', 'bhk', '2195', 'saleable', '1330', 'carpet', 'rate', '14500', 'final', 'plus', '1', 'floor', 'rise', 'tax', 'approx', '9', 'flat', 'cost', 'parking', '389', 'cr', 'plus', 'taxes', 'plus', 'possession', 'charger', 'middle', 'floor', 'north', 'door', 'arey', 'oberoi', 'woods', 'facing', '53', 'paymemt', 'due', '1', 'transfer', 'charge', 'buyer', 'total', 'cost', 'around', '420', 'cr', 'approx', 'plus', 'possession', 'charges', 'rahul', 'soni']\n\n"}, "1198": {"topic": "Creating a new corpus with NLTK", "user_name": "halfer", "text": "\nI reckoned that often the answer to my title is to go and read the documentations, but I ran through the NLTK book but it doesn't give the answer. I'm kind of new to Python.\nI have a bunch of .txt files and I want to be able to use the corpus functions that NLTK provides for the corpus nltk_data. \nI've tried PlaintextCorpusReader but I couldn't get further than:\n>>>import nltk\n>>>from nltk.corpus import PlaintextCorpusReader\n>>>corpus_root = './'\n>>>newcorpus = PlaintextCorpusReader(corpus_root, '.*')\n>>>newcorpus.words()\n\nHow do I segment the newcorpus sentences using punkt? I tried using the punkt functions but the punkt functions couldn't read PlaintextCorpusReader class?\nCan you also lead me to how I can write the segmented data into text files?\n"}, "1199": {"topic": "Creating a new corpus with NLTK", "user_name": "alvasalvas", "text": "\nAfter some years of figuring out how it works, here's the updated tutorial of \nHow to create an NLTK corpus with a directory of textfiles?\nThe main idea is to make use of the nltk.corpus.reader package. In the case that you have a directory of textfiles in English, it's best to use the PlaintextCorpusReader. \nIf you have a directory that looks like this:\nnewcorpus/\n         file1.txt\n         file2.txt\n         ...\n\nSimply use these lines of code and you can get a corpus:\nimport os\nfrom nltk.corpus.reader.plaintext import PlaintextCorpusReader\n\ncorpusdir = 'newcorpus/' # Directory of corpus.\n\nnewcorpus = PlaintextCorpusReader(corpusdir, '.*')\n\nNOTE: that the PlaintextCorpusReader will use the default nltk.tokenize.sent_tokenize() and nltk.tokenize.word_tokenize() to split your texts into sentences and words and these functions are build for English, it may NOT work for all languages.\nHere's the full code with creation of test textfiles and how to create a corpus with NLTK and how to access the corpus at different levels:\nimport os\nfrom nltk.corpus.reader.plaintext import PlaintextCorpusReader\n\n# Let's create a corpus with 2 texts in different textfile.\ntxt1 = \"\"\"This is a foo bar sentence.\\nAnd this is the first txtfile in the corpus.\"\"\"\ntxt2 = \"\"\"Are you a foo bar? Yes I am. Possibly, everyone is.\\n\"\"\"\ncorpus = [txt1,txt2]\n\n# Make new dir for the corpus.\ncorpusdir = 'newcorpus/'\nif not os.path.isdir(corpusdir):\n    os.mkdir(corpusdir)\n\n# Output the files into the directory.\nfilename = 0\nfor text in corpus:\n    filename+=1\n    with open(corpusdir+str(filename)+'.txt','w') as fout:\n        print>>fout, text\n\n# Check that our corpus do exist and the files are correct.\nassert os.path.isdir(corpusdir)\nfor infile, text in zip(sorted(os.listdir(corpusdir)),corpus):\n    assert open(corpusdir+infile,'r').read().strip() == text.strip()\n\n\n# Create a new corpus by specifying the parameters\n# (1) directory of the new corpus\n# (2) the fileids of the corpus\n# NOTE: in this case the fileids are simply the filenames.\nnewcorpus = PlaintextCorpusReader('newcorpus/', '.*')\n\n# Access each file in the corpus.\nfor infile in sorted(newcorpus.fileids()):\n    print infile # The fileids of each file.\n    with newcorpus.open(infile) as fin: # Opens the file.\n        print fin.read().strip() # Prints the content of the file\nprint\n\n# Access the plaintext; outputs pure string/basestring.\nprint newcorpus.raw().strip()\nprint \n\n# Access paragraphs in the corpus. (list of list of list of strings)\n# NOTE: NLTK automatically calls nltk.tokenize.sent_tokenize and \n#       nltk.tokenize.word_tokenize.\n#\n# Each element in the outermost list is a paragraph, and\n# Each paragraph contains sentence(s), and\n# Each sentence contains token(s)\nprint newcorpus.paras()\nprint\n\n# To access pargraphs of a specific fileid.\nprint newcorpus.paras(newcorpus.fileids()[0])\n\n# Access sentences in the corpus. (list of list of strings)\n# NOTE: That the texts are flattened into sentences that contains tokens.\nprint newcorpus.sents()\nprint\n\n# To access sentences of a specific fileid.\nprint newcorpus.sents(newcorpus.fileids()[0])\n\n# Access just tokens/words in the corpus. (list of strings)\nprint newcorpus.words()\n\n# To access tokens of a specific fileid.\nprint newcorpus.words(newcorpus.fileids()[0])\n\nFinally, to read a directory of texts and create an NLTK corpus in another languages, you must first ensure that you have a python-callable word tokenization and sentence tokenization modules that takes string/basestring input and produces such output:\n>>> from nltk.tokenize import sent_tokenize, word_tokenize\n>>> txt1 = \"\"\"This is a foo bar sentence.\\nAnd this is the first txtfile in the corpus.\"\"\"\n>>> sent_tokenize(txt1)\n['This is a foo bar sentence.', 'And this is the first txtfile in the corpus.']\n>>> word_tokenize(sent_tokenize(txt1)[0])\n['This', 'is', 'a', 'foo', 'bar', 'sentence', '.']\n\n"}, "1200": {"topic": "Creating a new corpus with NLTK", "user_name": "", "text": "\nI think the PlaintextCorpusReader already segments the input with a punkt tokenizer, at least if your input language is english.\nPlainTextCorpusReader's constructor\ndef __init__(self, root, fileids,\n             word_tokenizer=WordPunctTokenizer(),\n             sent_tokenizer=nltk.data.LazyLoader(\n                 'tokenizers/punkt/english.pickle'),\n             para_block_reader=read_blankline_block,\n             encoding='utf8'):\n\nYou can pass the reader a word and sentence tokenizer, but for the latter the default already is nltk.data.LazyLoader('tokenizers/punkt/english.pickle').\nFor a single string, a tokenizer would be used as follows (explained here, see section 5 for punkt tokenizer).\n>>> import nltk.data\n>>> text = \"\"\"\n... Punkt knows that the periods in Mr. Smith and Johann S. Bach\n... do not mark sentence boundaries.  And sometimes sentences\n... can start with non-capitalized words.  i is a good variable\n... name.\n... \"\"\"\n>>> tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n>>> tokenizer.tokenize(text.strip())\n\n"}, "1201": {"topic": "Creating a new corpus with NLTK", "user_name": "alvasalvas", "text": "\n >>> import nltk\n >>> from nltk.corpus import PlaintextCorpusReader\n >>> corpus_root = './'\n >>> newcorpus = PlaintextCorpusReader(corpus_root, '.*')\n \"\"\"\n if the ./ dir contains the file my_corpus.txt, then you \n can view say all the words it by doing this \n \"\"\"\n >>> newcorpus.words('my_corpus.txt')\n\n"}, "1202": {"topic": "Creating a new corpus with NLTK", "user_name": "Skippy le Grand Gourou", "text": "\nfrom nltk.corpus.reader.plaintext import PlaintextCorpusReader\n\n\nfilecontent1 = \"This is a cow\"\nfilecontent2 = \"This is a Dog\"\n\ncorpusdir = 'nltk_data/'\nwith open(corpusdir + 'content1.txt', 'w') as text_file:\n    text_file.write(filecontent1)\nwith open(corpusdir + 'content2.txt', 'w') as text_file:\n    text_file.write(filecontent2)\n\ntext_corpus = PlaintextCorpusReader(corpusdir, [\"content1.txt\", \"content2.txt\"])\n\nno_of_words_corpus1 = len(text_corpus.words(\"content1.txt\"))\nprint(no_of_words_corpus1)\nno_of_unique_words_corpus1 = len(set(text_corpus.words(\"content1.txt\")))\n\nno_of_words_corpus2 = len(text_corpus.words(\"content2.txt\"))\nno_of_unique_words_corpus2 = len(set(text_corpus.words(\"content2.txt\")))\n\nenter code here\n\n"}, "1203": {"topic": "How to remove stop words using nltk or python", "user_name": "Karl Knechtel", "text": "\nI have a dataset from which I would like to remove stop words.\nI used NLTK to get a list of stop words:\nfrom nltk.corpus import stopwords\n\nstopwords.words('english')\n\nExactly how do I compare the data to the list of stop words, and thus remove the stop words from the data?\n"}, "1204": {"topic": "How to remove stop words using nltk or python", "user_name": "AlexAlex", "text": "\nfrom nltk.corpus import stopwords\n# ...\nfiltered_words = [word for word in word_list if word not in stopwords.words('english')]\n\n"}, "1205": {"topic": "How to remove stop words using nltk or python", "user_name": "Stefan Falk", "text": "\nYou could also do a set diff, for example:\nlist(set(nltk.regexp_tokenize(sentence, pattern, gaps=True)) - set(nltk.corpus.stopwords.words('english')))\n\n"}, "1206": {"topic": "How to remove stop words using nltk or python", "user_name": "Daren ThomasDaren Thomas", "text": "\nTo exclude all type of stop-words including nltk stop-words, you could do something like this:    \nfrom stop_words import get_stop_words\nfrom nltk.corpus import stopwords\n\nstop_words = list(get_stop_words('en'))         #About 900 stopwords\nnltk_words = list(stopwords.words('english')) #About 150 stopwords\nstop_words.extend(nltk_words)\n\noutput = [w for w in word_list if not w in stop_words]\n\n"}, "1207": {"topic": "How to remove stop words using nltk or python", "user_name": "David LemphersDavid Lemphers", "text": "\nI suppose you have a list of words (word_list) from which you want to remove stopwords. You could do something like this:\nfiltered_word_list = word_list[:] #make a copy of the word_list\nfor word in word_list: # iterate over word_list\n  if word in stopwords.words('english'): \n    filtered_word_list.remove(word) # remove word from filtered_word_list if it is a stopword\n\n"}, "1208": {"topic": "How to remove stop words using nltk or python", "user_name": "Tannaz Khaleghi", "text": "\nThere's a very simple light-weight python package stop-words just for this sake.\nFist install the package using:\npip install stop-words\nThen you can remove your words in one line using list comprehension:\nfrom stop_words import get_stop_words\n\nfiltered_words = [word for word in dataset if word not in get_stop_words('english')]\n\n\nThis package is very light-weight to download (unlike nltk), works for both Python 2 and Python 3 ,and it has stop words for many other languages like:\n    Arabic\n    Bulgarian\n    Catalan\n    Czech\n    Danish\n    Dutch\n    English\n    Finnish\n    French\n    German\n    Hungarian\n    Indonesian\n    Italian\n    Norwegian\n    Polish\n    Portuguese\n    Romanian\n    Russian\n    Spanish\n    Swedish\n    Turkish\n    Ukrainian\n\n"}, "1209": {"topic": "How to remove stop words using nltk or python", "user_name": "sumitjainjrsumitjainjr", "text": "\nHere is my take on this, in case you want to immediately get the answer into a string (instead of a list of filtered words):\nSTOPWORDS = set(stopwords.words('english'))\ntext =  ' '.join([word for word in text.split() if word not in STOPWORDS]) # delete stopwords from text\n\n"}, "1210": {"topic": "How to remove stop words using nltk or python", "user_name": "das_weezuldas_weezul", "text": "\nUse textcleaner library to remove stopwords from your data.\nFollow this link:https://yugantm.github.io/textcleaner/documentation.html#remove_stpwrds\nFollow these steps to do so with this library.\npip install textcleaner\n\nAfter installing:\nimport textcleaner as tc\ndata = tc.document(<file_name>) \n#you can also pass list of sentences to the document class constructor.\ndata.remove_stpwrds() #inplace is set to False by default\n\nUse above code to remove the stop-words.\n"}, "1211": {"topic": "How to remove stop words using nltk or python", "user_name": "user_3pijuser_3pij", "text": "\nAlthough the question is a bit old, here is a new library, which is worth mentioning, that can do extra tasks.\nIn some cases, you don't want only to remove stop words. Rather, you would want to find the stopwords in the text data and store it in a list so that you can find the noise in the data and make it more interactive.\nThe library is called 'textfeatures'. You can use it as follows:\n! pip install textfeatures\nimport textfeatures as tf\nimport pandas as pd\n\nFor example, suppose you have the following set of strings:\ntexts = [\n    \"blue car and blue window\",\n    \"black crow in the window\",\n    \"i see my reflection in the window\"]\n\ndf = pd.DataFrame(texts) # Convert to a dataframe\ndf.columns = ['text'] # give a name to the column\ndf\n\nNow, call the stopwords() function and pass the parameters you want:\ntf.stopwords(df,\"text\",\"stopwords\") # extract stop words\ndf[[\"text\",\"stopwords\"]].head() # give names to columns\n\nThe result is going to be:\n    text                                 stopwords\n0   blue car and blue window             [and]\n1   black crow in the window             [in, the]\n2   i see my reflection in the window    [i, my, in, the]\n\nAs you can see, the last column has the stop words included in that docoument (record).\n"}, "1212": {"topic": "How to remove stop words using nltk or python", "user_name": "justadevjustadev", "text": "\nyou can use this function, you should notice that you need to lower all the words\nfrom nltk.corpus import stopwords\n\ndef remove_stopwords(word_list):\n        processed_word_list = []\n        for word in word_list:\n            word = word.lower() # in case they arenet all lower cased\n            if word not in stopwords.words(\"english\"):\n                processed_word_list.append(word)\n        return processed_word_list\n\n"}, "1213": {"topic": "How to remove stop words using nltk or python", "user_name": "", "text": "\nusing filter:\nfrom nltk.corpus import stopwords\n# ...  \nfiltered_words = list(filter(lambda word: word not in stopwords.words('english'), word_list))\n\n"}, "1214": {"topic": "How to remove stop words using nltk or python", "user_name": "Yugant HadiyalYugant Hadiyal", "text": "\nfrom nltk.corpus import stopwords \n\nfrom nltk.tokenize import word_tokenize \n\nexample_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n\n  \nstop_words = set(stopwords.words('english')) \n  \nword_tokens = word_tokenize(example_sent) \n  \nfiltered_sentence = [w for w in word_tokens if not w in stop_words] \n  \nfiltered_sentence = [] \n  \nfor w in word_tokens: \n    if w not in stop_words: \n        filtered_sentence.append(w) \n  \nprint(word_tokens) \nprint(filtered_sentence) \n\n"}, "1215": {"topic": "How to remove stop words using nltk or python", "user_name": "TaieTaie", "text": "\nI will show you some example\nFirst I extract the text data from the data frame (twitter_df) to process further as following\n     from nltk.tokenize import word_tokenize\n     tweetText = twitter_df['text']\n\nThen to tokenize I use the following method\n     from nltk.tokenize import word_tokenize\n     tweetText = tweetText.apply(word_tokenize)\n\nThen, to remove stop words,\n     from nltk.corpus import stopwords\n     nltk.download('stopwords')\n\n     stop_words = set(stopwords.words('english'))\n     tweetText = tweetText.apply(lambda x:[word for word in x if word not in stop_words])\n     tweetText.head()\n\nI Think this will help you\n"}, "1216": {"topic": "How to remove stop words using nltk or python", "user_name": "Mohammed_AshourMohammed_Ashour", "text": "\nIn case your data are stored as a Pandas DataFrame, you can use remove_stopwords from textero that use the NLTK stopwords list by default.\nimport pandas as pd\nimport texthero as hero\ndf['text_without_stopwords'] = hero.remove_stopwords(df['text'])\n\n"}, "1217": {"topic": "NLTK Lookup Error", "user_name": "erip", "text": "\nWhile running a Python script using NLTK I got this:\nTraceback (most recent call last):\n  File \"cpicklesave.py\", line 56, in <module>\n    pos = nltk.pos_tag(words)\n  File \"/usr/lib/python2.7/site-packages/nltk/tag/__init__.py\", line 110, in pos_tag\n    tagger = PerceptronTagger()\n  File \"/usr/lib/python2.7/site-packages/nltk/tag/perceptron.py\", line 140, in __init__\n    AP_MODEL_LOC = str(find('taggers/averaged_perceptron_tagger/'+PICKLE))\n  File \"/usr/lib/python2.7/site-packages/nltk/data.py\", line 641, in find\n    raise LookupError(resource_not_found)\nLookupError:\n**********************************************************************\n  Resource u'taggers/averaged_perceptron_tagger/averaged_perceptro\n  n_tagger.pickle' not found.  Please use the NLTK Downloader to\n  obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n\nCan anyone explain the problem?\n"}, "1218": {"topic": "NLTK Lookup Error", "user_name": "Shiv ShankarShiv Shankar", "text": "\nUse \n>>> nltk.download()\n\nto install the missing module (the Perceptron Tagger).\n(check also the answers to Failed loading english.pickle with nltk.data.load)\n"}, "1219": {"topic": "NLTK Lookup Error", "user_name": "CommunityBot", "text": "\nFirst answer said the missing module is 'the Perceptron Tagger', actually its name in nltk.download is 'averaged_perceptron_tagger'\nYou can use this to fix the error\nnltk.download('averaged_perceptron_tagger')\n"}, "1220": {"topic": "NLTK Lookup Error", "user_name": "user2314737user2314737", "text": "\nTL;DR\nimport nltk\nnltk.download('averaged_perceptron_tagger')\n\nOr to download all packages + data + docs:\nimport nltk\nnltk.download('all')\n\nSee How do I download NLTK data?\n"}, "1221": {"topic": "NLTK Lookup Error", "user_name": "", "text": "\nInstall all nltk resources in one line:\npython3 -c \"import nltk; nltk.download('all')\"\n\nthe data will be saved at ~/nltk_data\n\nInstall only specific resource:\nSubstitute \"all\" for \"averaged_perceptron_tagger\" to install only this module.\npython3 -c \"import nltk; nltk.download('averaged_perceptron_tagger')\"\n\n"}, "1222": {"topic": "NLTK Lookup Error", "user_name": "PosuerPosuer", "text": "\nProblem:\nLookup error when extracting count vectorizer from scikit learn. Below is code snippet.\nfrom sklearn.feature_extraction.text import CountVectorizer\nbow_transformer = CountVectorizer(analyzer=text_process).fit(X)\n\nSolution:\nTry to run the below code and then try to install the stopwords from corpora natural language processing toolkit!!\nimport nltk\nnltk.download()\n\n"}, "1223": {"topic": "NLTK Lookup Error", "user_name": "CommunityBot", "text": "\nYou can download NLTK missing module just by \nimport nltk\nnltk.download()\n\nThis will shows the NLTK download screen. \nIf it shows SSL Certificate verify failed error. Then it should works by disabling SSL check with below code! \nimport nltk\nimport ssl\n\ntry:\n    _create_unverified_https_context = ssl._create_unverified_context\nexcept AttributeError:\n    pass\nelse:\n    ssl._create_default_https_context = _create_unverified_https_context\n\nnltk.download()\n\n"}, "1224": {"topic": "NLTK Lookup Error", "user_name": "alvasalvas", "text": "\nSorry! if I missed other editor but this is working fine in Google Colab\nimport nltk\nnltk.download('all')\n\n"}, "1225": {"topic": "NLTK Lookup Error", "user_name": "", "text": "\nSometimes even by writing\nnltk.download('module_name'), it does not get downloaded. At those times, you can open python in interactive mode and then download by using nltk.download('module_name').\n"}, "1226": {"topic": "NLTK Lookup Error", "user_name": "Lucas AzevedoLucas Azevedo", "text": "\nYou just need to download that module for nltk.\nThe better way is to open a python command line and type\nimport nltk\nnltk.download('all')\n\nThat's all.\n"}, "1227": {"topic": "NLTK Lookup Error", "user_name": "Rayudu YarlagaddaRayudu Yarlagadda", "text": "\nif you have already executed python -m textblob.download_corpora if not first run import nltk nltk.downlod('all') or nltk.downlod('all-corpora')\nand if issue remains there then it might be because some packages are not being unzipp.\nin my case have to unzip wordnet as my error was\nResource wordnet not found. Please use the NLTK Downloader to obtain the resource:\nsolutioncd /home/app/nltk_data/corpora then unzip wordnet.zip\nhttps://github.com/nltk/nltk/issues/3028\n"}, "1228": {"topic": "NLTK Lookup Error", "user_name": "ishwardgretishwardgret", "text": "\nIf you have not downloaded ntlk then firstly download ntlk and then use this nltk.download('punkt') it will give you the result.\n"}, "1229": {"topic": "NLTK Lookup Error", "user_name": "akDakD", "text": "\nimport nltk\n\n\nnltk.download('vader_lexicon')\n\nUse this this might work\n"}, "1230": {"topic": "How do I download NLTK data?", "user_name": "Q-ximi", "text": "\nUpdated answer:NLTK works for 2.7 well. I had 3.2. I uninstalled 3.2 and installed 2.7. Now it works!!\nI have installed NLTK and tried to download NLTK Data. What I did was to follow the instrution on this site: http://www.nltk.org/data.html\nI downloaded NLTK, installed it, and then tried to run the following code:\n>>> import nltk\n>>> nltk.download()\n\nIt gave me the error message like below:\nTraceback (most recent call last):\n  File \"<pyshell#6>\", line 1, in <module>\n    nltk.download()\nAttributeError: 'module' object has no attribute 'download'\n Directory of C:\\Python32\\Lib\\site-packages\n\nTried both nltk.download() and nltk.downloader(), both gave me error messages.\nThen I used help(nltk) to pull out the package, it shows the following info:\nNAME\n    nltk\n\nPACKAGE CONTENTS\n    align\n    app (package)\n    book\n    ccg (package)\n    chat (package)\n    chunk (package)\n    classify (package)\n    cluster (package)\n    collocations\n    corpus (package)\n    data\n    decorators\n    downloader\n    draw (package)\n    examples (package)\n    featstruct\n    grammar\n    help\n    inference (package)\n    internals\n    lazyimport\n    metrics (package)\n    misc (package)\n    model (package)\n    parse (package)\n    probability\n    sem (package)\n    sourcedstring\n    stem (package)\n    tag (package)\n    test (package)\n    text\n    tokenize (package)\n    toolbox\n    tree\n    treetransforms\n    util\n    yamltags\n\nFILE\n    c:\\python32\\lib\\site-packages\\nltk\n\nI do see Downloader there, not sure why it does not work. Python 3.2.2, system Windows vista.\n"}, "1231": {"topic": "How do I download NLTK data?", "user_name": "Q-ximiQ-ximi", "text": "\nTL;DR\nTo download a particular dataset/models, use the nltk.download() function, e.g. if you are looking to download the punkt sentence tokenizer, use:\n$ python3\n>>> import nltk\n>>> nltk.download('punkt')\n\nIf you're unsure of which data/model you need, you can start out with the basic list of data + models with:\n>>> import nltk\n>>> nltk.download('popular')\n\nIt will download a list of \"popular\" resources, these includes:\n<collection id=\"popular\" name=\"Popular packages\">\n      <item ref=\"cmudict\" />\n      <item ref=\"gazetteers\" />\n      <item ref=\"genesis\" />\n      <item ref=\"gutenberg\" />\n      <item ref=\"inaugural\" />\n      <item ref=\"movie_reviews\" />\n      <item ref=\"names\" />\n      <item ref=\"shakespeare\" />\n      <item ref=\"stopwords\" />\n      <item ref=\"treebank\" />\n      <item ref=\"twitter_samples\" />\n      <item ref=\"omw\" />\n      <item ref=\"wordnet\" />\n      <item ref=\"wordnet_ic\" />\n      <item ref=\"words\" />\n      <item ref=\"maxent_ne_chunker\" />\n      <item ref=\"punkt\" />\n      <item ref=\"snowball_data\" />\n      <item ref=\"averaged_perceptron_tagger\" />\n    </collection>\n\n\nEDITED\nIn case anyone is avoiding errors from downloading larger datasets from nltk, from https://stackoverflow.com/a/38135306/610569\n$ rm /Users/<your_username>/nltk_data/corpora/panlex_lite.zip\n$ rm -r /Users/<your_username>/nltk_data/corpora/panlex_lite\n$ python\n\n>>> import nltk\n>>> dler = nltk.downloader.Downloader()\n>>> dler._update_index()\n>>> dler._status_cache['panlex_lite'] = 'installed' # Trick the index to treat panlex_lite as it's already installed.\n>>> dler.download('popular')\n\nUpdated\nFrom v3.2.5, NLTK has a more informative error message when nltk_data resource is not found, e.g.:\n>>> from nltk import word_tokenize\n>>> word_tokenize('x')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/l/alvas/git/nltk/nltk/tokenize/__init__.py\", line 128, in word_tokenize\n    sentences = [text] if preserve_line else sent_tokenize(text, language)\n  File \"/Users//alvas/git/nltk/nltk/tokenize/__init__.py\", line 94, in sent_tokenize\n    tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))\n  File \"/Users/alvas/git/nltk/nltk/data.py\", line 820, in load\n    opened_resource = _open(resource_url)\n  File \"/Users/alvas/git/nltk/nltk/data.py\", line 938, in _open\n    return find(path_, path + ['']).open()\n  File \"/Users/alvas/git/nltk/nltk/data.py\", line 659, in find\n    raise LookupError(resource_not_found)\nLookupError: \n**********************************************************************\n  Resource punkt not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  >>> import nltk\n  >>> nltk.download('punkt')\n\n  Searched in:\n    - '/Users/alvas/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n\nRelated\n\nTo find nltk_data directory (auto-magically), see https://stackoverflow.com/a/36383314/610569 \nTo download nltk_data to a different path, see https://stackoverflow.com/a/48634212/610569\nTo config nltk_data path (i.e. set a different path for NLTK to find nltk_data), see https://stackoverflow.com/a/22987374/610569\n\n"}, "1232": {"topic": "How do I download NLTK data?", "user_name": "", "text": "\nTry \nnltk.download('all')\nthis will download all the data and no need to download individually.\n"}, "1233": {"topic": "How do I download NLTK data?", "user_name": "alvasalvas", "text": "\nInstall Pip: run in terminal : sudo easy_install pip\nInstall Numpy (optional): run : sudo pip install -U numpy\nInstall NLTK: run : sudo pip install -U nltk\nTest installation: run:  python \nthen type : import nltk\nTo download the corpus \nrun : python -m nltk.downloader all\n"}, "1234": {"topic": "How do I download NLTK data?", "user_name": "Noordeen", "text": "\nDo not name your file nltk.py I used the same code and name it nltk, and got the same error as you have, I changed the file name and it went well.\n"}, "1235": {"topic": "How do I download NLTK data?", "user_name": "B KB K", "text": "\nThis worked for me:\nnltk.set_proxy('http://user:password@proxy.example.com:8080')\nnltk.download()\n\n"}, "1236": {"topic": "How do I download NLTK data?", "user_name": "", "text": "\nPlease Try\nimport nltk\n\nnltk.download()\n\nAfter running this you get something like this\nNLTK Downloader\n---------------------------------------------------------------------------\n   d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\n\nThen, Press d\nDo As Follows:\nDownloader> d all\n\nYou will get following message on completion, and Prompt then Press q\nDone downloading collection all\n"}, "1237": {"topic": "How do I download NLTK data?", "user_name": "NoordeenNoordeen", "text": "\nyou can't have a saved python file called nltk.py because the interpreter is reading from that and not from the actual file. \nChange the name of your file that the python shell is reading from and try what you were doing originally: \nimport nltk and then nltk.download()\n"}, "1238": {"topic": "How do I download NLTK data?", "user_name": "GerardGerard", "text": "\nIt's very simple....\n\nOpen pyScripter or any editor\nCreate a python file eg: install.py\nwrite the below code in it.\n\nimport nltk\nnltk.download()\n\n\nA pop-up window will apper and click on download .\n\n\n"}, "1239": {"topic": "How do I download NLTK data?", "user_name": "Morteza MashayekhiMorteza Mashayekhi", "text": "\nI had the similar issue. Probably check if you are using proxy. \nIf yes, set up the proxy before doing download:\nnltk.set_proxy('http://proxy.example.com:3128', ('USERNAME', 'PASSWORD'))\n\n"}, "1240": {"topic": "How do I download NLTK data?", "user_name": "Ankit Jayaswal", "text": "\nIf you are running a really old version of nltk, then there is indeed no download module available (reference)\nTry this:\nimport nltk\nprint(nltk.__version__)\n\nAs per the reference, anything after 0.9.5 should be fine\n"}, "1241": {"topic": "How do I download NLTK data?", "user_name": "balabala", "text": "\nyou should add python to your PATH during installation of python...after installation.. open cmd prompt type  command-pip   install nltk\nthen go to IDLE and open a new file..save it as file.py..then open file.py\ntype the following:\nimport nltk\nnltk.download()\n\n"}, "1242": {"topic": "How do I download NLTK data?", "user_name": "mrsrinivas", "text": "\nTry download the zip files from http://www.nltk.org/nltk_data/ and then unzip, save in your Python folder, such as  C:\\ProgramData\\Anaconda3\\nltk_data\n"}, "1243": {"topic": "How do I download NLTK data?", "user_name": "user3682157user3682157", "text": "\nif you have already saved a file name nltk.py and again rename as my_nltk_script.py. check whether you have still the file nltk.py existing. If yes, then delete them and run the file my_nltk.scripts.py it should work!\n"}, "1244": {"topic": "How do I download NLTK data?", "user_name": "layog", "text": "\njust do like\nimport nltk\nnltk.download()\n\nthen you will be show a popup asking what to download , select 'all'. it will take some time because of its size, but eventually we will get it.\nand if you are using Google Colab, you can use\nnltk.download(download_dir='/content/nltkdata')\n\nafter running that you will be asked to select from a list\nNLTK Downloader\n----------------------------------------------------------------- \n----------\nd) Download   l) List    u) Update   c) Config   h) Help   q) \nQuit\n----------------------------------------------------------------- \n----------\nDownloader> d\n\nhere you have to enter d as you want to download.\nafter that you will be asked to enter the identifier that you want to download . You can see the list of available indentifier with l command or if you want all of them just enter 'all' in the input box.\nthen you will see something like -\nDownloading collection 'all'\n       | \n       | Downloading package abc to /content/nltkdata...\n       |   Unzipping corpora/abc.zip.\n       | Downloading package alpino to /content/nltkdata...\n       |   Unzipping corpora/alpino.zip.\n       | Downloading package biocreative_ppi to /content/nltkdata...\n       |   Unzipping corpora/biocreative_ppi.zip.\n       | Downloading package brown to /content/nltkdata...\n       |   Unzipping corpora/brown.zip.\n       | Downloading package brown_tei to /content/nltkdata...\n       |   Unzipping corpora/brown_tei.zip.\n       | Downloading package cess_cat to /content/nltkdata...\n       |   Unzipping corpora/cess_cat.zip.\n.\n.\n. \n |   Unzipping models/wmt15_eval.zip.\n       | Downloading package mwa_ppdb to /content/nltkdata...\n       |   Unzipping misc/mwa_ppdb.zip.\n       | \n     Done downloading collection all\n\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\nDownloader> q\nTrue\n\nat last you can enter q to quit.\n"}, "1245": {"topic": "How do I download NLTK data?", "user_name": "Arun DasArun Das", "text": "\nYou may try:\n>> $ import nltk\n>> $ nltk.download_shell()\n>> $ d\n>> $ *name of the package*\n\nhappy nlp'ing.\n"}, "1246": {"topic": "Practical examples of NLTK use [closed]", "user_name": "pelumi", "text": "\n\n\n\n\n\n\n\n\n\r\nAs it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references,  or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question  can be improved and possibly reopened, visit the help center for guidance.                    \n\n\nClosed 10 years ago.\n\n\n\nI'm playing around with the Natural Language Toolkit (NLTK).\nIts documentation (Book and HOWTO) are quite bulky and the examples are sometimes slightly advanced. \nAre there any good but basic examples of uses/applications of NLTK? I'm thinking of things like the NTLK articles on the Stream Hacker blog. \n"}, "1247": {"topic": "Practical examples of NLTK use [closed]", "user_name": "MatMat", "text": "\nHere's my own practical example for the benefit of anyone else looking this question up (excuse the sample text, it was the first thing I found on Wikipedia):\nimport nltk\nimport pprint\n\ntokenizer = None\ntagger = None\n\ndef init_nltk():\n    global tokenizer\n    global tagger\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+|[^\\w\\s]+')\n    tagger = nltk.UnigramTagger(nltk.corpus.brown.tagged_sents())\n\ndef tag(text):\n    global tokenizer\n    global tagger\n    if not tokenizer:\n        init_nltk()\n    tokenized = tokenizer.tokenize(text)\n    tagged = tagger.tag(tokenized)\n    tagged.sort(lambda x,y:cmp(x[1],y[1]))\n    return tagged\n\ndef main():\n    text = \"\"\"Mr Blobby is a fictional character who featured on Noel\n    Edmonds' Saturday night entertainment show Noel's House Party,\n    which was often a ratings winner in the 1990s. Mr Blobby also\n    appeared on the Jamie Rose show of 1997. He was designed as an\n    outrageously over the top parody of a one-dimensional, mute novelty\n    character, which ironically made him distinctive, absurd and popular.\n    He was a large pink humanoid, covered with yellow spots, sporting a\n    permanent toothy grin and jiggling eyes. He communicated by saying\n    the word \"blobby\" in an electronically-altered voice, expressing\n    his moods through tone of voice and repetition.\n\n    There was a Mrs. Blobby, seen briefly in the video, and sold as a\n    doll.\n\n    However Mr Blobby actually started out as part of the 'Gotcha'\n    feature during the show's second series (originally called 'Gotcha\n    Oscars' until the threat of legal action from the Academy of Motion\n    Picture Arts and Sciences[citation needed]), in which celebrities\n    were caught out in a Candid Camera style prank. Celebrities such as\n    dancer Wayne Sleep and rugby union player Will Carling would be\n    enticed to take part in a fictitious children's programme based around\n    their profession. Mr Blobby would clumsily take part in the activity,\n    knocking over the set, causing mayhem and saying \"blobby blobby\n    blobby\", until finally when the prank was revealed, the Blobby\n    costume would be opened - revealing Noel inside. This was all the more\n    surprising for the \"victim\" as during rehearsals Blobby would be\n    played by an actor wearing only the arms and legs of the costume and\n    speaking in a normal manner.[citation needed]\"\"\"\n    tagged = tag(text)    \n    l = list(set(tagged))\n    l.sort(lambda x,y:cmp(x[1],y[1]))\n    pprint.pprint(l)\n\nif __name__ == '__main__':\n    main()\n\nOutput:\n[('rugby', None),\n ('Oscars', None),\n ('1990s', None),\n ('\",', None),\n ('Candid', None),\n ('\"', None),\n ('blobby', None),\n ('Edmonds', None),\n ('Mr', None),\n ('outrageously', None),\n ('.[', None),\n ('toothy', None),\n ('Celebrities', None),\n ('Gotcha', None),\n (']),', None),\n ('Jamie', None),\n ('humanoid', None),\n ('Blobby', None),\n ('Carling', None),\n ('enticed', None),\n ('programme', None),\n ('1997', None),\n ('s', None),\n (\"'\", \"'\"),\n ('[', '('),\n ('(', '('),\n (']', ')'),\n (',', ','),\n ('.', '.'),\n ('all', 'ABN'),\n ('the', 'AT'),\n ('an', 'AT'),\n ('a', 'AT'),\n ('be', 'BE'),\n ('were', 'BED'),\n ('was', 'BEDZ'),\n ('is', 'BEZ'),\n ('and', 'CC'),\n ('one', 'CD'),\n ('until', 'CS'),\n ('as', 'CS'),\n ('This', 'DT'),\n ('There', 'EX'),\n ('of', 'IN'),\n ('inside', 'IN'),\n ('from', 'IN'),\n ('around', 'IN'),\n ('with', 'IN'),\n ('through', 'IN'),\n ('-', 'IN'),\n ('on', 'IN'),\n ('in', 'IN'),\n ('by', 'IN'),\n ('during', 'IN'),\n ('over', 'IN'),\n ('for', 'IN'),\n ('distinctive', 'JJ'),\n ('permanent', 'JJ'),\n ('mute', 'JJ'),\n ('popular', 'JJ'),\n ('such', 'JJ'),\n ('fictional', 'JJ'),\n ('yellow', 'JJ'),\n ('pink', 'JJ'),\n ('fictitious', 'JJ'),\n ('normal', 'JJ'),\n ('dimensional', 'JJ'),\n ('legal', 'JJ'),\n ('large', 'JJ'),\n ('surprising', 'JJ'),\n ('absurd', 'JJ'),\n ('Will', 'MD'),\n ('would', 'MD'),\n ('style', 'NN'),\n ('threat', 'NN'),\n ('novelty', 'NN'),\n ('union', 'NN'),\n ('prank', 'NN'),\n ('winner', 'NN'),\n ('parody', 'NN'),\n ('player', 'NN'),\n ('actor', 'NN'),\n ('character', 'NN'),\n ('victim', 'NN'),\n ('costume', 'NN'),\n ('action', 'NN'),\n ('activity', 'NN'),\n ('dancer', 'NN'),\n ('grin', 'NN'),\n ('doll', 'NN'),\n ('top', 'NN'),\n ('mayhem', 'NN'),\n ('citation', 'NN'),\n ('part', 'NN'),\n ('repetition', 'NN'),\n ('manner', 'NN'),\n ('tone', 'NN'),\n ('Picture', 'NN'),\n ('entertainment', 'NN'),\n ('night', 'NN'),\n ('series', 'NN'),\n ('voice', 'NN'),\n ('Mrs', 'NN'),\n ('video', 'NN'),\n ('Motion', 'NN'),\n ('profession', 'NN'),\n ('feature', 'NN'),\n ('word', 'NN'),\n ('Academy', 'NN-TL'),\n ('Camera', 'NN-TL'),\n ('Party', 'NN-TL'),\n ('House', 'NN-TL'),\n ('eyes', 'NNS'),\n ('spots', 'NNS'),\n ('rehearsals', 'NNS'),\n ('ratings', 'NNS'),\n ('arms', 'NNS'),\n ('celebrities', 'NNS'),\n ('children', 'NNS'),\n ('moods', 'NNS'),\n ('legs', 'NNS'),\n ('Sciences', 'NNS-TL'),\n ('Arts', 'NNS-TL'),\n ('Wayne', 'NP'),\n ('Rose', 'NP'),\n ('Noel', 'NP'),\n ('Saturday', 'NR'),\n ('second', 'OD'),\n ('his', 'PP$'),\n ('their', 'PP$'),\n ('him', 'PPO'),\n ('He', 'PPS'),\n ('more', 'QL'),\n ('However', 'RB'),\n ('actually', 'RB'),\n ('also', 'RB'),\n ('clumsily', 'RB'),\n ('originally', 'RB'),\n ('only', 'RB'),\n ('often', 'RB'),\n ('ironically', 'RB'),\n ('briefly', 'RB'),\n ('finally', 'RB'),\n ('electronically', 'RB-HL'),\n ('out', 'RP'),\n ('to', 'TO'),\n ('show', 'VB'),\n ('Sleep', 'VB'),\n ('take', 'VB'),\n ('opened', 'VBD'),\n ('played', 'VBD'),\n ('caught', 'VBD'),\n ('appeared', 'VBD'),\n ('revealed', 'VBD'),\n ('started', 'VBD'),\n ('saying', 'VBG'),\n ('causing', 'VBG'),\n ('expressing', 'VBG'),\n ('knocking', 'VBG'),\n ('wearing', 'VBG'),\n ('speaking', 'VBG'),\n ('sporting', 'VBG'),\n ('revealing', 'VBG'),\n ('jiggling', 'VBG'),\n ('sold', 'VBN'),\n ('called', 'VBN'),\n ('made', 'VBN'),\n ('altered', 'VBN'),\n ('based', 'VBN'),\n ('designed', 'VBN'),\n ('covered', 'VBN'),\n ('communicated', 'VBN'),\n ('needed', 'VBN'),\n ('seen', 'VBN'),\n ('set', 'VBN'),\n ('featured', 'VBN'),\n ('which', 'WDT'),\n ('who', 'WPS'),\n ('when', 'WRB')]\n\n"}, "1248": {"topic": "Practical examples of NLTK use [closed]", "user_name": "MatMat", "text": "\nNLP in general is very useful so you might want to broaden your search to general application of text analytics. I used NLTK to aid MOSS 2010 by generating file taxonomy by extracting concept maps. It worked really well. It doesn't take long before files start to cluster in useful ways.\nOften times to understand text analytics you have to think in tangents to the ways you are used to thinking. For example, text analytics is extremely useful to discovery. Most people, though, don't even know what the difference is between search and discovery. If you read up on those subjects you will likely \"discover\" ways in which you might want to put NLTK to work.\nAlso, consider your world view of text files without NLTK. You have a bunch of random length strings separated by whitespace and punctuation. Some of the punctuation changes how it is used such as the period (which is also a decimal point and a postfix marker for an abbreviation.) With NLTK you get words and more to the point you get parts of speech. Now you have a handle on the content. Use NLTK to discover the concepts and actions in the document. Use NLTK to get at the \"meaning\" of the document. Meaning in this case refers to the essencial relationships in the document.\nIt is a good thing to be curious about NLTK. Text Analytics is set to breakout in a big way in the next few years. Those who understand it will be better suited to take advantage of the new opportunities better.\n"}, "1249": {"topic": "Practical examples of NLTK use [closed]", "user_name": "Pete ManciniPete Mancini", "text": "\nI'm the author of streamhacker.com (and thanks for the mention, I get a fair amount of click traffic from this particular question). What specifically are you trying to do? NLTK has a lot of tools for doing various things, but is somewhat lacking clear information on what to use the tools for, and how best to use them. It's also oriented towards academic problems, and so it can be heavy going to translate the pedagogical examples to practical solutions.\n"}, "1250": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "agiledatabase", "text": "\nI get the following error when trying to install Punkt for nltk:\nnltk.download('punkt')    \n [nltk_data] Error loading Punkt: <urlopen error [SSL:\n [nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed\n [nltk_data]     (_ssl.c:590)>\nFalse\n\n"}, "1251": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "user3429986user3429986", "text": "\nTLDR: Here is a better solution: https://github.com/gunthercox/ChatterBot/issues/930#issuecomment-322111087\nNote that when you run nltk.download(), a window will pop up and let you select which packages to download (Download is not automatically started right away).\nTo complement the accepted answer, the following is a complete list of directories that will be searched on Mac (not limited to the one mentioned in the accepted answer):\n\n    - '/Users/YOUR_USERNAME/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/YOUR_USERNAME/YOUR_VIRTUAL_ENV_DIRECTORY/nltk_data'\n    - '/Users/YOUR_USERNAME/YOUR_VIRTUAL_ENV_DIRECTORY/share/nltk_data'\n    - '/Users/YOUR_USERNAME/YOUR_VIRTUAL_ENV_DIRECTORY/lib/nltk_data'\n\nIn case the link above dies, here is the solution pasted in its entirety:\nimport nltk\nimport ssl\n\ntry:\n    _create_unverified_https_context = ssl._create_unverified_context\nexcept AttributeError:\n    pass\nelse:\n    ssl._create_default_https_context = _create_unverified_https_context\n\nnltk.download()\n\nRun the above code in your favourite Python IDE or via the command line.\n"}, "1252": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "pookie", "text": "\nThis works by disabling SSL check!\nimport nltk\nimport ssl\n\ntry:\n    _create_unverified_https_context = ssl._create_unverified_context\nexcept AttributeError:\n    pass\nelse:\n    ssl._create_default_https_context = _create_unverified_https_context\n\nnltk.download()\n\n"}, "1253": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "fstangfstang", "text": "\nRun the Python interpreter and type the commands:\nimport nltk\nnltk.download()\n\nfrom here: http://www.nltk.org/data.html\nif you get an SSL/Certificate error, run the following command\nbash /Applications/Python 3.6/Install Certificates.command\nfrom here: ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:749)\n"}, "1254": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "ishwardgretishwardgret", "text": "\nSearch 'Install Certificates.command' in the finder and open it.\nThen do the following steps in the terminal:\npython3\nimport nltk\nnltk.download()\n\n"}, "1255": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "Selvaram G", "text": "\nThe downloader script is broken. As a temporal workaround can manually download the punkt tokenizer from here and then place the unzipped folder in the corresponding location. The default folders for each OS are:\n\nWindows: C:\\nltk_data\\tokenizers\nOSX: /usr/local/share/nltk_data/tokenizers\nUnix: /usr/share/nltk_data/tokenizers\n\n"}, "1256": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "AdiAdi", "text": "\nThis is how I solved it for MAC OS.\nInitially after installing nltk, I was getting the SSL error.\nSolution:\nGoto\ncd /Applications/Python\\ 3.8\n\nRun the command\n./Install\\ Certificates.command\n\nNow if you try again, it should work!\nThanks a lot to this article!\n"}, "1257": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "xxx", "text": "\nYou just need to Install the certificate doing this simple step \nIn the python application folder double-click on the file 'Certificates.command'\nthis will make a prompt window show in your screen and basically will automatically install the certificate for you, close this window and try again.\n"}, "1258": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "yashyash", "text": "\nMy solution is:\n\nDownload punkt.zip from here and unzip\nCreate nltk_data/tokenizers folders under home folder\nPut punkt folder under tokenizers folder\n\n"}, "1259": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "elyaseelyase", "text": "\nThere is a very simple way to fix all of this as written in the formal bug report for anyone else coming across this problem recently (e.g. 2019) and using MacOS. From the bug report at https://bugs.python.org/issue28150:\n\n...there is a simple double-clickable or command-line-runnable script (\"/Applications/Python 3.6/Install Certificates.command\") that does two things: 1. uses pip to install certifi and 2. creates a symlink in the OpenSSL directory to certifi's installed bundle location. \n\nSimply running the \"Install Certificates.command\" script worked for me on MacOS (10.15 beta as of this writing) and I was off and running.\n"}, "1260": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "Sreekiran A RSreekiran A R", "text": "\nMy solution after nothing worked. I navigated, via the GUI to the Python 3.7 folder, opened the 'Certificates.command' file in terminal and the SSL issue was immediately resolved. \n"}, "1261": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "thiago89thiago89", "text": "\nA bit late to the party but I just entered Certificates.command into Spotlight which found it and ran it. All fixed in seconds.\nI'm running mac Catalina and using python 3.7 installed by Homebrew\n"}, "1262": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "gocengocen", "text": "\nIt means that you are not using HTTPS to work consistently with other run time dependencies for Python etc.\nIf you are using Linux (Ubuntu)\n~$ sudo apt-get install ca-certificates\n\nShould solve the issue.\nIf you are using this in a script with a docker file, you have to make sure you have install the the ca-certificates modules in your docker file.\n"}, "1263": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "Michael HawkinsMichael Hawkins", "text": "\nFor mac users,\njust copy paste the following in the terminal:\n/Applications/Python\\ 3.10/Install\\ Certificates.command ; exit;\n\n"}, "1264": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "Cormac O'KeeffeCormac O'Keeffe", "text": "\nFirst go to the path  /Applications/Python 3.6/ and run \nInstall Certificates.command\nYou will admin rights for the same.\nIf you are unable to download it, then as other answer suggest you can download directly and place it. You need to place them in the following directory structure.\n> nltk_data\n          > corpora\n                   > brown\n                   > conll2000\n                   > movie_reviews\n                   > wordnet\n          > taggers\n                   > averaged_perceptron_tagger\n          > tokenizers\n                      > punkt\n\n"}, "1265": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "ChezChez", "text": "\nUpdating the python certificates worked for me.\nAt the top of your script, keep:\nimport nltk\nnltk.download('punkt')\n\nIn a separate terminal run (Mac):\nbash /Applications/Python <version>/Install Certificates.command\n\n"}, "1266": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "Sibeesh Venu", "text": "\nFor me, the solution was much simpler: I was still connected to my corporate network/VPN which blocks certain types of downloads. Switching the network made the SSL error disappear.\n"}, "1267": {"topic": "How to config nltk data directory from code?", "user_name": "alvas", "text": "\nHow to config nltk data directory from code?\n"}, "1268": {"topic": "How to config nltk data directory from code?", "user_name": "Juanjo ContiJuanjo Conti", "text": "\nJust change items of nltk.data.path, it's a simple list.\n"}, "1269": {"topic": "How to config nltk data directory from code?", "user_name": "Tim McNamaraTim McNamara", "text": "\nFrom the code, http://www.nltk.org/_modules/nltk/data.html: \n\n``nltk:path``: Specifies the file stored in the NLTK data\n package at *path*.  NLTK will search for these files in the\n directories specified by ``nltk.data.path``.\n\n\nThen within the code:\n######################################################################\n# Search Path\n######################################################################\n\npath = []\n\"\"\"A list of directories where the NLTK data package might reside.\n   These directories will be checked in order when looking for a\n   resource in the data package.  Note that this allows users to\n   substitute in their own versions of resources, if they have them\n   (e.g., in their home directory under ~/nltk_data).\"\"\"\n\n# User-specified locations:\npath += [d for d in os.environ.get('NLTK_DATA', str('')).split(os.pathsep) if d]\nif os.path.expanduser('~/') != '~/':\n    path.append(os.path.expanduser(str('~/nltk_data')))\n\nif sys.platform.startswith('win'):\n    # Common locations on Windows:\n    path += [\n        str(r'C:\\nltk_data'), str(r'D:\\nltk_data'), str(r'E:\\nltk_data'),\n        os.path.join(sys.prefix, str('nltk_data')),\n        os.path.join(sys.prefix, str('lib'), str('nltk_data')),\n        os.path.join(os.environ.get(str('APPDATA'), str('C:\\\\')), str('nltk_data'))\n    ]\nelse:\n    # Common locations on UNIX & OS X:\n    path += [\n        str('/usr/share/nltk_data'),\n        str('/usr/local/share/nltk_data'),\n        str('/usr/lib/nltk_data'),\n        str('/usr/local/lib/nltk_data')\n    ]\n\nTo modify the path, simply append to the list of possible paths:\nimport nltk\nnltk.data.path.append(\"/home/yourusername/whateverpath/\")\n\nOr in windows:\nimport nltk\nnltk.data.path.append(\"C:\\somewhere\\farfar\\away\\path\")\n\n"}, "1270": {"topic": "How to config nltk data directory from code?", "user_name": "alvasalvas", "text": "\nI use append, example\nnltk.data.path.append('/libs/nltk_data/')\n\n"}, "1271": {"topic": "How to config nltk data directory from code?", "user_name": "Tushar Gupta - curioustushar", "text": "\nInstead of adding nltk.data.path.append('your/path/to/nltk_data') to every script, NLTK accepts NLTK_DATA environment variable. (code link)\nOpen ~/.bashrc (or ~/.profile) with text editor (e.g. nano, vim, gedit), and add following line:  \nexport NLTK_DATA=\"your/path/to/nltk_data\"\n\nExecute source to load environmental variable  \nsource ~/.bashrc\n\n\nTest\nOpen python and execute following lines\nimport nltk\nnltk.data.path\n\nYour can see your nltk data path already in there.\nReference: @alvations's answer on\nnltk/nltk #1997 \n"}, "1272": {"topic": "How to config nltk data directory from code?", "user_name": "bahlumbahlum", "text": "\nFor those using uwsgi: \nI was having trouble because I wanted a uwsgi app (running as a different user than myself) to have access to nltk data that I had previously downloaded. What worked for me was adding the following line to myapp_uwsgi.ini:\nenv = NLTK_DATA=/home/myuser/nltk_data/\n\nThis sets the environment variable NLTK_DATA, as suggested by @schemacs.\nYou may need to restart your uwsgi process after making this change. \n"}, "1273": {"topic": "How to config nltk data directory from code?", "user_name": "", "text": "\nUsing fnjn's advice above on printing out the path:\nprint(nltk.data.path)\n\nI saw the path strings in this type of format on windows:\nC:\\\\Users\\\\my_user_name\\\\AppData\\\\Roaming\\\\SPB_Data\n\nSo I switched my path from the python type forward slash '/', to a double backslash '\\\\' when I used path.append:\nnltk.data.path.append(\"C:\\\\workspace\\\\my_project\\\\data\\\\nltk_books\")\n\nThe exception went away.\n"}, "1274": {"topic": "How to config nltk data directory from code?", "user_name": "fnjnfnjn", "text": "\nAnother solution is to get ahead of it. \ntry \n    import nltk \n    nltk.download()  \nWhen the window box pops up asking if you want to download the corpus , you can specify there which directory it is to be downloaded to. \n"}, "1275": {"topic": "Corpora/stopwords not found when import nltk library", "user_name": "Frits VerstratenFrits Verstraten", "text": "\nI trying to import the nltk package in python 2.7\n  import nltk\n  stopwords = nltk.corpus.stopwords.words('english')\n  print(stopwords[:10])\n\nRunning this gives me the following error:\nLookupError: \n**********************************************************************\nResource 'corpora/stopwords' not found.  Please use the NLTK\nDownloader to obtain the resource:  >>> nltk.download()\n\nSo therefore I open my python termin and did the following:\nimport nltk  \nnltk.download()\n\nWhich gives me:\nshowing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n\nHowever this does not seem to stop. And running it again still gives me the same error. Any thoughts where this goes wrong?\n"}, "1276": {"topic": "Corpora/stopwords not found when import nltk library", "user_name": "", "text": "\nYou are currently trying to download every item in nltk data, so this can take long. You can try downloading only the stopwords that you need:\nimport nltk\nnltk.download('stopwords')\n\nOr from command line (thanks to Rafael Valero's answer):\npython -m nltk.downloader stopwords\n\n\nReference:\n\nInstalling NLTK Data - Command line installation\n\n"}, "1277": {"topic": "Corpora/stopwords not found when import nltk library", "user_name": "Kurt BourbakiKurt Bourbaki", "text": "\nThe some as mentioned here by  Kurt Bourbaki but in the command line:\npython -m nltk.downloader stopwords\n\n"}, "1278": {"topic": "Corpora/stopwords not found when import nltk library", "user_name": "", "text": "\nYou can do this in separately in console.\nIt will give you a result.\nimport nltk\nnltk.download('stopwords')\n\nI used jupyter console  when I faced this problem.\n"}, "1279": {"topic": "Corpora/stopwords not found when import nltk library", "user_name": "Rafael ValeroRafael Valero", "text": "\nif you get an SSL/Certificate error, run the following command.\nThis works by disabling SSL check!\nimport nltk\nimport ssl\n\ntry:\n    _create_unverified_https_context = ssl._create_unverified_context\nexcept AttributeError:\n    pass\nelse:\n    ssl._create_default_https_context = _create_unverified_https_context\n\nnltk.download()\n\n"}, "1280": {"topic": "Corpora/stopwords not found when import nltk library", "user_name": "L_J", "text": "\nIf your PC uses proxy for connectivity, then try this:\nimport nltk\n\nnltk.set_proxy('http://proxy.example.com:3128', ('USERNAME', 'PASSWORD'))\nnltk.download('stopwords')\n\n"}, "1281": {"topic": "Corpora/stopwords not found when import nltk library", "user_name": "UmeshUmesh", "text": "\nUse GPU runtime, it will not give you any error.\nThe same code will work which you are using\nimport nltk\nstopwords = nltk.corpus.stopwords.words('english')\nprint(stopwords[:10])\n\n"}, "1282": {"topic": "Corpora/stopwords not found when import nltk library", "user_name": "Reshma2kReshma2k", "text": "\nI know the comment is quite late, but if it helps:\nAlthough the nltk.download('stopwords') will do the job, there might be times when it won't work due to proxy issues if your organization has blocked it.\nI found this github link pretty handy, from where I can just pick up the list of words and integrate it manually in my project just as a workaround.\n"}, "1283": {"topic": "Corpora/stopwords not found when import nltk library", "user_name": "koPytok", "text": "\nshowing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n\nIf you are running this command in a jupyter notebook, it opens another window titled 'NLTK Downloader'. Once you go in that window, you can select the topics you want to download and then click on download button to start downloading.\nUntil you close the NLTK Downloader window, the cell in the Jupyter keeps on running.\n"}, "1284": {"topic": "Corpora/stopwords not found when import nltk library", "user_name": "R KumarR Kumar", "text": "\ncheck what error you are getting --\npython3 -m nltk.downloader stopwords\n\nError :\nRuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n\n\nwarn(RuntimeWarning(msg))\n[nltk_data] Error loading stopwords: <urlopen error [SSL:\n[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n[nltk_data]     unable to get local issuer certificate (_ssl.c:1123)>\n\nUse the solution provided my @reshma2k\n"}, "1285": {"topic": "Corpora/stopwords not found when import nltk library", "user_name": "deevasdeevas", "text": "\nInstalled the ntlk and imported the stopwords\n!pip3 install nltk\nimport nltk\nnltk.download('stopwords')\n\n"}, "1286": {"topic": "English grammar for parsing in NLTK", "user_name": "Fred Foo", "text": "\nIs there a ready-to-use English grammar that I can just load it and use in NLTK? I've searched around examples of parsing with NLTK, but it seems like that I have to manually specify grammar before parsing a sentence. \nThanks a lot!\n"}, "1287": {"topic": "English grammar for parsing in NLTK", "user_name": "roborenroboren", "text": "\nYou can take a look at pyStatParser, a simple python statistical parser that returns NLTK parse Trees. It comes with public treebanks and it generates the grammar model only the first time you instantiate a Parser object (in about 8 seconds). It uses a CKY algorithm and it parses average length sentences (like the one below) in under a second.\n>>> from stat_parser import Parser\n>>> parser = Parser()\n>>> print parser.parse(\"How can the net amount of entropy of the universe be massively decreased?\")\n(SBARQ\n  (WHADVP (WRB how))\n  (SQ\n    (MD can)\n    (NP\n      (NP (DT the) (JJ net) (NN amount))\n      (PP\n        (IN of)\n        (NP\n          (NP (NNS entropy))\n          (PP (IN of) (NP (DT the) (NN universe))))))\n    (VP (VB be) (ADJP (RB massively) (VBN decreased))))\n  (. ?))\n\n"}, "1288": {"topic": "English grammar for parsing in NLTK", "user_name": "emilmontemilmont", "text": "\nMy library, spaCy, provides a high performance dependency parser.\nInstallation:\npip install spacy\npython -m spacy.en.download all\n\nUsage:\nfrom spacy.en import English\nnlp = English()\ndoc = nlp(u'A whole document.\\nNo preprocessing require.   Robust to arbitrary formating.')\nfor sent in doc:\n    for token in sent:\n        if token.is_alpha:\n            print token.orth_, token.tag_, token.head.lemma_\n\nChoi et al. (2015) found spaCy to be the fastest dependency parser available. It processes over 13,000 sentences a second, on a single thread. On the standard WSJ evaluation it scores 92.7%, over 1% more accurate than any of CoreNLP's models.\n"}, "1289": {"topic": "English grammar for parsing in NLTK", "user_name": "syllogism_syllogism_", "text": "\nThere are a few grammars in the nltk_data distribution. In your Python interpreter, issue nltk.download().\n"}, "1290": {"topic": "English grammar for parsing in NLTK", "user_name": "Fred FooFred Foo", "text": "\nThere is a Library called Pattern. It is quite fast and easy to use.\n>>> from pattern.en import parse\n>>>  \n>>> s = 'The mobile web is more important than mobile apps.'\n>>> s = parse(s, relations=True, lemmata=True)\n>>> print s\n\n'The/DT/B-NP/O/NP-SBJ-1/the mobile/JJ/I-NP/O/NP-SBJ-1/mobile' ... \n\n"}, "1291": {"topic": "English grammar for parsing in NLTK", "user_name": "user3798928user3798928", "text": "\nUse the MaltParser, there you have a pretrained english-grammar, and also some other pretrained languages.\nAnd the Maltparser is a dependency parser and not some simple bottom-up, or top-down Parser.\nJust download the MaltParser from http://www.maltparser.org/index.html and use the NLTK like this:\nimport nltk\nparser = nltk.parse.malt.MaltParser()\n\n"}, "1292": {"topic": "English grammar for parsing in NLTK", "user_name": "blackmambablackmamba", "text": "\nI've tried NLTK, PyStatParser, Pattern. IMHO Pattern is best English parser introduced in above article. Because it supports pip install and There is a fancy document on the website (http://www.clips.ua.ac.be/pages/pattern-en). I couldn't find reasonable document for NLTK (And it gave me inaccurate result for me by its default. And I couldn't find how to tune it). pyStatParser is much slower than described above in my Environment. (About one minute for initialization and It took couple of seconds to parse long sentences. Maybe I didn't use it correctly).  \n"}, "1293": {"topic": "English grammar for parsing in NLTK", "user_name": "Piyo HogePiyo Hoge", "text": "\nDid you try POS tagging in NLTK?\ntext = word_tokenize(\"And now for something completely different\")\nnltk.pos_tag(text)\n\nThe answer is something like this\n[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'),('completely', 'RB'), ('different', 'JJ')]\n\nGot this example from here NLTK_chapter03\n"}, "1294": {"topic": "English grammar for parsing in NLTK", "user_name": "maverik_akagamimaverik_akagami", "text": "\nI'm found out that nltk working good with parser grammar developed by Stanford.\nSyntax Parsing with Stanford CoreNLP and NLTK\nIt is very easy to start to use Stanford CoreNLP and NLTK. All you need is small preparation, after that you can parse sentences with following code:\nfrom nltk.parse.corenlp import CoreNLPParser\nparser = CoreNLPParser()\nparse = next(parser.raw_parse(\"I put the book in the box on the table.\"))\n\nPreparation:\n\nDownload Java Stanford model\nRun CoreNLPServer\n\nYou can use following code to run CoreNLPServer:\nimport os\nfrom nltk.parse.corenlp import CoreNLPServer\n# The server needs to know the location of the following files:\n#   - stanford-corenlp-X.X.X.jar\n#   - stanford-corenlp-X.X.X-models.jar\nSTANFORD = os.path.join(\"models\", \"stanford-corenlp-full-2018-02-27\")\n# Create the server\nserver = CoreNLPServer(\n   os.path.join(STANFORD, \"stanford-corenlp-3.9.1.jar\"),\n   os.path.join(STANFORD, \"stanford-corenlp-3.9.1-models.jar\"),    \n)\n# Start the server in the background\nserver.start()\n\n\nDo not forget stop server with executing server.stop() \n\n"}, "1295": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "Abu Shoeb", "text": "\nIs it possible to use Stanford Parser in NLTK? (I am not talking about Stanford POS.)\n"}, "1296": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "ThanaDarayThanaDaray", "text": "\nNote that this answer applies to NLTK v 3.0, and not to more recent versions.\nSure, try the following in Python:\nimport os\nfrom nltk.parse import stanford\nos.environ['STANFORD_PARSER'] = '/path/to/standford/jars'\nos.environ['STANFORD_MODELS'] = '/path/to/standford/jars'\n\nparser = stanford.StanfordParser(model_path=\"/location/of/the/englishPCFG.ser.gz\")\nsentences = parser.raw_parse_sents((\"Hello, My name is Melroy.\", \"What is your name?\"))\nprint sentences\n\n# GUI\nfor line in sentences:\n    for sentence in line:\n        sentence.draw()\n\nOutput:\n\n[Tree('ROOT', [Tree('S', [Tree('INTJ', [Tree('UH', ['Hello'])]),\n  Tree(',', [',']), Tree('NP', [Tree('PRP$', ['My']), Tree('NN',\n  ['name'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('ADJP', [Tree('JJ',\n  ['Melroy'])])]), Tree('.', ['.'])])]), Tree('ROOT', [Tree('SBARQ',\n  [Tree('WHNP', [Tree('WP', ['What'])]), Tree('SQ', [Tree('VBZ',\n  ['is']), Tree('NP', [Tree('PRP$', ['your']), Tree('NN', ['name'])])]),\n  Tree('.', ['?'])])])]\n\nNote 1:\nIn this example both the parser & model jars are in the same folder.\nNote 2:\n\nFile name of stanford parser is: stanford-parser.jar \nFile name of stanford models is: stanford-parser-x.x.x-models.jar\n\nNote 3:\nThe englishPCFG.ser.gz file can be found inside the models.jar file (/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz). Please use come archive manager to 'unzip' the models.jar file.\nNote 4:\nBe sure you are using Java JRE (Runtime Environment) 1.8 also known as Oracle JDK 8. Otherwise you will get: Unsupported major.minor version 52.0.\nInstallation\n\nDownload NLTK v3 from: https://github.com/nltk/nltk. And install NLTK:\nsudo python setup.py install\nYou can use the NLTK downloader to get Stanford Parser, using Python:\nimport nltk\nnltk.download()\n\nTry my example! (don't forget the change the jar paths and change the model path to the ser.gz location)\n\nOR:\n\nDownload and install NLTK v3, same as above.\nDownload the latest version from (current version filename is stanford-parser-full-2015-01-29.zip):\nhttp://nlp.stanford.edu/software/lex-parser.shtml#Download\nExtract the standford-parser-full-20xx-xx-xx.zip. \nCreate a new folder ('jars' in my example). Place the extracted files into this jar folder:  stanford-parser-3.x.x-models.jar and stanford-parser.jar.\nAs shown above you can use the environment variables (STANFORD_PARSER & STANFORD_MODELS) to point to this 'jars' folder. I'm using Linux, so if you use Windows please use something like: C://folder//jars.\nOpen the stanford-parser-3.x.x-models.jar using an Archive manager (7zip).\nBrowse inside the jar file; edu/stanford/nlp/models/lexparser. Again, extract the file called 'englishPCFG.ser.gz'. Remember the location where you extract this ser.gz file.\nWhen creating a StanfordParser instance, you can provide the model path as parameter. This is the complete path to the model, in our case /location/of/englishPCFG.ser.gz.\nTry my example! (don't forget the change the jar paths and change the model path to the ser.gz location)\n\n"}, "1297": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "Servy", "text": "\nDeprecated Answer\nThe answer below is deprecated, please use the solution on https://stackoverflow.com/a/51981566/610569 for NLTK v3.3 and above.\n\nEDITED\nNote: The following answer will only work on:\n\nNLTK version >=3.2.4\nStanford Tools compiled since 2015-04-20\nPython 2.7, 3.4 and 3.5 (Python 3.6 is not yet officially supported)\n\nAs both tools changes rather quickly and the API might look very different 3-6 months later. Please treat the following answer as temporal and not an eternal fix.\nAlways refer to https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software for the latest instruction on how to interface Stanford NLP tools using NLTK!! \n\nTL;DR\ncd $HOME\n\n# Update / Install NLTK\npip install -U nltk\n\n# Download the Stanford NLP tools\nwget http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip\nwget http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip\nwget http://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip\n# Extract the zip file.\nunzip stanford-ner-2015-04-20.zip \nunzip stanford-parser-full-2015-04-20.zip \nunzip stanford-postagger-full-2015-04-20.zip\n\n\nexport STANFORDTOOLSDIR=$HOME\n\nexport CLASSPATH=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/stanford-postagger.jar:$STANFORDTOOLSDIR/stanford-ner-2015-04-20/stanford-ner.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-04-20/stanford-parser.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar\n\nexport STANFORD_MODELS=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/models:$STANFORDTOOLSDIR/stanford-ner-2015-04-20/classifiers\n\nThen:\n>>> from nltk.tag.stanford import StanfordPOSTagger\n>>> st = StanfordPOSTagger('english-bidirectional-distsim.tagger')\n>>> st.tag('What is the airspeed of an unladen swallow ?'.split())\n[(u'What', u'WP'), (u'is', u'VBZ'), (u'the', u'DT'), (u'airspeed', u'NN'), (u'of', u'IN'), (u'an', u'DT'), (u'unladen', u'JJ'), (u'swallow', u'VB'), (u'?', u'.')]\n\n>>> from nltk.tag import StanfordNERTagger\n>>> st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') \n>>> st.tag('Rami Eid is studying at Stony Brook University in NY'.split())\n[(u'Rami', u'PERSON'), (u'Eid', u'PERSON'), (u'is', u'O'), (u'studying', u'O'), (u'at', u'O'), (u'Stony', u'ORGANIZATION'), (u'Brook', u'ORGANIZATION'), (u'University', u'ORGANIZATION'), (u'in', u'O'), (u'NY', u'O')]\n\n\n>>> from nltk.parse.stanford import StanfordParser\n>>> parser=StanfordParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n>>> list(parser.raw_parse(\"the quick brown fox jumps over the lazy dog\"))\n[Tree('ROOT', [Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['quick']), Tree('JJ', ['brown']), Tree('NN', ['fox'])]), Tree('NP', [Tree('NP', [Tree('NNS', ['jumps'])]), Tree('PP', [Tree('IN', ['over']), Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['lazy']), Tree('NN', ['dog'])])])])])])]\n\n>>> from nltk.parse.stanford import StanfordDependencyParser\n>>> dep_parser=StanfordDependencyParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n>>> print [parse.tree() for parse in dep_parser.raw_parse(\"The quick brown fox jumps over the lazy dog.\")]\n[Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over', 'the', 'lazy'])])]\n\n\nIn Long:\n\nFirstly, one must note that the Stanford NLP tools are written in Java and NLTK is written in Python. The way NLTK is interfacing the tool is through the call the Java tool through the command line interface. \nSecondly, the NLTK API to the Stanford NLP tools have changed quite a lot since the version 3.1. So it is advisable to update your NLTK package to v3.1.\nThirdly, the NLTK API to Stanford NLP Tools wraps around the individual NLP tools, e.g. Stanford POS tagger, Stanford NER Tagger, Stanford Parser. \nFor the POS and NER tagger, it DOES NOT wrap around the Stanford Core NLP package. \nFor the Stanford Parser, it's a special case where it wraps around both the Stanford Parser and the Stanford Core NLP (personally, I have not used the latter using NLTK, i would rather follow @dimazest's demonstration on http://www.eecs.qmul.ac.uk/~dm303/stanford-dependency-parser-nltk-and-anaconda.html )\nNote that as of NLTK v3.1, the STANFORD_JAR and STANFORD_PARSER variables is deprecated and NO LONGER used\n\nIn Longer:\n\nSTEP 1\nAssuming that you have installed Java appropriately on your OS.\nNow, install/update your NLTK version (see http://www.nltk.org/install.html):\n\nUsing pip: sudo pip install -U nltk\nDebian distro (using apt-get): sudo apt-get install python-nltk\n\nFor Windows (Use the 32-bit binary installation):\n\nInstall Python 3.4: http://www.python.org/downloads/ (avoid the 64-bit versions)\nInstall Numpy (optional): http://sourceforge.net/projects/numpy/files/NumPy/ (the version that specifies pythnon3.4)\nInstall NLTK: http://pypi.python.org/pypi/nltk\nTest installation: Start>Python34, then type import nltk\n\n(Why not 64 bit? See https://github.com/nltk/nltk/issues/1079)\n\nThen out of paranoia, recheck your nltk version inside python:\nfrom __future__ import print_function\nimport nltk\nprint(nltk.__version__)\n\nOr on the command line:\npython3 -c \"import nltk; print(nltk.__version__)\"\n\nMake sure that you see 3.1 as the output.\nFor even more paranoia, check that all your favorite Stanford NLP tools API are available:\nfrom nltk.parse.stanford import StanfordParser\nfrom nltk.parse.stanford import StanfordDependencyParser\nfrom nltk.parse.stanford import StanfordNeuralDependencyParser\nfrom nltk.tag.stanford import StanfordPOSTagger, StanfordNERTagger\nfrom nltk.tokenize.stanford import StanfordTokenizer\n\n(Note: The imports above will ONLY ensure that you are using a correct NLTK version that contains these APIs. Not seeing errors in the import doesn't mean that you have successfully configured the NLTK API to use the Stanford Tools)\n\nSTEP 2\nNow that you have checked that you have the correct version of NLTK that contains the necessary Stanford NLP tools interface. You need to download and extract all the necessary Stanford NLP tools.\nTL;DR, in Unix:\ncd $HOME\n\n# Download the Stanford NLP tools\nwget http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip\nwget http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip\nwget http://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip\n# Extract the zip file.\nunzip stanford-ner-2015-04-20.zip \nunzip stanford-parser-full-2015-04-20.zip \nunzip stanford-postagger-full-2015-04-20.zip\n\nIn Windows / Mac:\n\nDownload and unzip the parser from http://nlp.stanford.edu/software/lex-parser.shtml#Download\nDownload and unizp the FULL VERSION tagger from http://nlp.stanford.edu/software/tagger.shtml#Download\nDownload and unizp the NER tagger from http://nlp.stanford.edu/software/CRF-NER.shtml#Download\n\n\nSTEP 3\nSetup the environment variables such that NLTK can find the relevant file path automatically. You have to set the following variables:\n\nAdd the appropriate Stanford NLP .jar file to the  CLASSPATH environment variable.\n\ne.g. for the NER, it will be stanford-ner-2015-04-20/stanford-ner.jar\ne.g. for the POS, it will be stanford-postagger-full-2015-04-20/stanford-postagger.jar\ne.g. for the parser, it will be stanford-parser-full-2015-04-20/stanford-parser.jar and the parser model jar file, stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar\n\nAdd the appropriate model directory to the STANFORD_MODELS variable (i.e. the directory where you can find where the pre-trained models are saved)\n\ne.g. for the NER, it will be in stanford-ner-2015-04-20/classifiers/\ne.g. for the POS, it will be in stanford-postagger-full-2015-04-20/models/\ne.g. for the Parser, there won't be a model directory.\n\n\nIn the code, see that it searches for the STANFORD_MODELS directory before appending the model name. Also see that, the API also automatically tries to search the OS environments for the `CLASSPATH)\nNote that as of NLTK v3.1, the STANFORD_JAR variables is deprecated and NO LONGER used. Code snippets found in the following Stackoverflow questions might not work:\n\nStanford Dependency Parser Setup and NLTK\nnltk interface to stanford parser\ntrouble importing stanford pos tagger into nltk\nStanford Entity Recognizer (caseless) in Python Nltk\nHow to improve speed with Stanford NLP Tagger and NLTK\nHow can I get the stanford NLTK python module?\nStanford Parser and NLTK windows\nStanford Named Entity Recognizer (NER) functionality with NLTK\nStanford parser with NLTK produces empty output\nExtract list of Persons and Organizations using Stanford NER Tagger in NLTK\nError using Stanford POS Tagger in NLTK Python\n\nTL;DR for STEP 3 on Ubuntu\nexport STANFORDTOOLSDIR=/home/path/to/stanford/tools/\n\nexport CLASSPATH=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/stanford-postagger.jar:$STANFORDTOOLSDIR/stanford-ner-2015-04-20/stanford-ner.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-04-20/stanford-parser.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar\n\nexport STANFORD_MODELS=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/models:$STANFORDTOOLSDIR/stanford-ner-2015-04-20/classifiers\n\n(For Windows: See https://stackoverflow.com/a/17176423/610569 for instructions for setting environment variables)\nYou MUST set the variables as above before starting python, then:\n>>> from nltk.tag.stanford import StanfordPOSTagger\n>>> st = StanfordPOSTagger('english-bidirectional-distsim.tagger')\n>>> st.tag('What is the airspeed of an unladen swallow ?'.split())\n[(u'What', u'WP'), (u'is', u'VBZ'), (u'the', u'DT'), (u'airspeed', u'NN'), (u'of', u'IN'), (u'an', u'DT'), (u'unladen', u'JJ'), (u'swallow', u'VB'), (u'?', u'.')]\n\n>>> from nltk.tag import StanfordNERTagger\n>>> st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') \n>>> st.tag('Rami Eid is studying at Stony Brook University in NY'.split())\n[(u'Rami', u'PERSON'), (u'Eid', u'PERSON'), (u'is', u'O'), (u'studying', u'O'), (u'at', u'O'), (u'Stony', u'ORGANIZATION'), (u'Brook', u'ORGANIZATION'), (u'University', u'ORGANIZATION'), (u'in', u'O'), (u'NY', u'O')]\n\n\n>>> from nltk.parse.stanford import StanfordParser\n>>> parser=StanfordParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n>>> list(parser.raw_parse(\"the quick brown fox jumps over the lazy dog\"))\n[Tree('ROOT', [Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['quick']), Tree('JJ', ['brown']), Tree('NN', ['fox'])]), Tree('NP', [Tree('NP', [Tree('NNS', ['jumps'])]), Tree('PP', [Tree('IN', ['over']), Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['lazy']), Tree('NN', ['dog'])])])])])])]\n\n\nAlternatively, you could try add the environment variables inside python, as the previous answers have suggested but you can also directly tell the parser/tagger to initialize to the direct path where you kept the .jar file and your models. \nThere is NO need to set the environment variables if you use the following method BUT when the API changes its parameter names, you will need to change accordingly. That is why it is MORE advisable to set the environment variables than to modify your python code to suit the NLTK version.\nFor example (without setting any environment variables):\n# POS tagging:\n\nfrom nltk.tag import StanfordPOSTagger\n\nstanford_pos_dir = '/home/alvas/stanford-postagger-full-2015-04-20/'\neng_model_filename= stanford_pos_dir + 'models/english-left3words-distsim.tagger'\nmy_path_to_jar= stanford_pos_dir + 'stanford-postagger.jar'\n\nst = StanfordPOSTagger(model_filename=eng_model_filename, path_to_jar=my_path_to_jar) \nst.tag('What is the airspeed of an unladen swallow ?'.split())\n\n\n# NER Tagging:\nfrom nltk.tag import StanfordNERTagger\n\nstanford_ner_dir = '/home/alvas/stanford-ner/'\neng_model_filename= stanford_ner_dir + 'classifiers/english.all.3class.distsim.crf.ser.gz'\nmy_path_to_jar= stanford_ner_dir + 'stanford-ner.jar'\n\nst = StanfordNERTagger(model_filename=eng_model_filename, path_to_jar=my_path_to_jar) \nst.tag('Rami Eid is studying at Stony Brook University in NY'.split())\n\n# Parsing:\nfrom nltk.parse.stanford import StanfordParser\n\nstanford_parser_dir = '/home/alvas/stanford-parser/'\neng_model_path = stanford_parser_dir  + \"edu/stanford/nlp/models/lexparser/englishRNN.ser.gz\"\nmy_path_to_models_jar = stanford_parser_dir  + \"stanford-parser-3.5.2-models.jar\"\nmy_path_to_jar = stanford_parser_dir  + \"stanford-parser.jar\"\n\nparser=StanfordParser(model_path=eng_model_path, path_to_models_jar=my_path_to_models_jar, path_to_jar=my_path_to_jar)\n\n"}, "1298": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "Melroy van den BergMelroy van den Berg", "text": "\nAs of NLTK v3.3, users should avoid the Stanford NER or POS taggers from nltk.tag, and avoid Stanford tokenizer/segmenter from nltk.tokenize.\nInstead use the new nltk.parse.corenlp.CoreNLPParser API. \nPlease see https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK\n\n(Avoiding link only answer, I've pasted the docs from NLTK github wiki below)\nFirst, update your NLTK\npip3 install -U nltk # Make sure is >=3.3\n\nThen download the necessary CoreNLP packages:\ncd ~\nwget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-02-27.zip\nunzip stanford-corenlp-full-2018-02-27.zip\ncd stanford-corenlp-full-2018-02-27\n\n# Get the Chinese model \nwget http://nlp.stanford.edu/software/stanford-chinese-corenlp-2018-02-27-models.jar\nwget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-chinese.properties \n\n# Get the Arabic model\nwget http://nlp.stanford.edu/software/stanford-arabic-corenlp-2018-02-27-models.jar\nwget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-arabic.properties \n\n# Get the French model\nwget http://nlp.stanford.edu/software/stanford-french-corenlp-2018-02-27-models.jar\nwget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-french.properties \n\n# Get the German model\nwget http://nlp.stanford.edu/software/stanford-german-corenlp-2018-02-27-models.jar\nwget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-german.properties \n\n\n# Get the Spanish model\nwget http://nlp.stanford.edu/software/stanford-spanish-corenlp-2018-02-27-models.jar\nwget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-spanish.properties \n\nEnglish\nStill in the stanford-corenlp-full-2018-02-27 directory, start the server:\njava -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-preload tokenize,ssplit,pos,lemma,ner,parse,depparse \\\n-status_port 9000 -port 9000 -timeout 15000 & \n\nThen in Python:\n>>> from nltk.parse import CoreNLPParser\n\n# Lexical Parser\n>>> parser = CoreNLPParser(url='http://localhost:9000')\n\n# Parse tokenized text.\n>>> list(parser.parse('What is the airspeed of an unladen swallow ?'.split()))\n[Tree('ROOT', [Tree('SBARQ', [Tree('WHNP', [Tree('WP', ['What'])]), Tree('SQ', [Tree('VBZ', ['is']), Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('NN', ['airspeed'])]), Tree('PP', [Tree('IN', ['of']), Tree('NP', [Tree('DT', ['an']), Tree('JJ', ['unladen'])])]), Tree('S', [Tree('VP', [Tree('VB', ['swallow'])])])])]), Tree('.', ['?'])])])]\n\n# Parse raw string.\n>>> list(parser.raw_parse('What is the airspeed of an unladen swallow ?'))\n[Tree('ROOT', [Tree('SBARQ', [Tree('WHNP', [Tree('WP', ['What'])]), Tree('SQ', [Tree('VBZ', ['is']), Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('NN', ['airspeed'])]), Tree('PP', [Tree('IN', ['of']), Tree('NP', [Tree('DT', ['an']), Tree('JJ', ['unladen'])])]), Tree('S', [Tree('VP', [Tree('VB', ['swallow'])])])])]), Tree('.', ['?'])])])]\n\n# Neural Dependency Parser\n>>> from nltk.parse.corenlp import CoreNLPDependencyParser\n>>> dep_parser = CoreNLPDependencyParser(url='http://localhost:9000')\n>>> parses = dep_parser.parse('What is the airspeed of an unladen swallow ?'.split())\n>>> [[(governor, dep, dependent) for governor, dep, dependent in parse.triples()] for parse in parses]\n[[(('What', 'WP'), 'cop', ('is', 'VBZ')), (('What', 'WP'), 'nsubj', ('airspeed', 'NN')), (('airspeed', 'NN'), 'det', ('the', 'DT')), (('airspeed', 'NN'), 'nmod', ('swallow', 'VB')), (('swallow', 'VB'), 'case', ('of', 'IN')), (('swallow', 'VB'), 'det', ('an', 'DT')), (('swallow', 'VB'), 'amod', ('unladen', 'JJ')), (('What', 'WP'), 'punct', ('?', '.'))]]\n\n\n# Tokenizer\n>>> parser = CoreNLPParser(url='http://localhost:9000')\n>>> list(parser.tokenize('What is the airspeed of an unladen swallow?'))\n['What', 'is', 'the', 'airspeed', 'of', 'an', 'unladen', 'swallow', '?']\n\n# POS Tagger\n>>> pos_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='pos')\n>>> list(pos_tagger.tag('What is the airspeed of an unladen swallow ?'.split()))\n[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'), ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]\n\n# NER Tagger\n>>> ner_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\n>>> list(ner_tagger.tag(('Rami Eid is studying at Stony Brook University in NY'.split())))\n[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'STATE_OR_PROVINCE')]\n\nChinese\nStart the server a little differently, still from the `stanford-corenlp-full-2018-02-27 directory:\njava -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-serverProperties StanfordCoreNLP-chinese.properties \\\n-preload tokenize,ssplit,pos,lemma,ner,parse \\\n-status_port 9001  -port 9001 -timeout 15000\n\nIn Python:\n>>> parser = CoreNLPParser('http://localhost:9001')\n>>> list(parser.tokenize(u'\u6211\u5bb6\u6ca1\u6709\u7535\u8111\u3002'))\n['\u6211\u5bb6', '\u6ca1\u6709', '\u7535\u8111', '\u3002']\n\n>>> list(parser.parse(parser.tokenize(u'\u6211\u5bb6\u6ca1\u6709\u7535\u8111\u3002')))\n[Tree('ROOT', [Tree('IP', [Tree('IP', [Tree('NP', [Tree('NN', ['\u6211\u5bb6'])]), Tree('VP', [Tree('VE', ['\u6ca1\u6709']), Tree('NP', [Tree('NN', ['\u7535\u8111'])])])]), Tree('PU', ['\u3002'])])])]\n\nArabic\nStart the server:\njava -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-serverProperties StanfordCoreNLP-arabic.properties \\\n-preload tokenize,ssplit,pos,parse \\\n-status_port 9005  -port 9005 -timeout 15000\n\nIn Python:\n>>> from nltk.parse import CoreNLPParser\n>>> parser = CoreNLPParser('http://localhost:9005')\n>>> text = u'\u0627\u0646\u0627 \u062d\u0627\u0645\u0644'\n\n# Parser.\n>>> parser.raw_parse(text)\n<list_iterator object at 0x7f0d894c9940>\n>>> list(parser.raw_parse(text))\n[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('PRP', ['\u0627\u0646\u0627'])]), Tree('NP', [Tree('NN', ['\u062d\u0627\u0645\u0644'])])])])]\n>>> list(parser.parse(parser.tokenize(text)))\n[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('PRP', ['\u0627\u0646\u0627'])]), Tree('NP', [Tree('NN', ['\u062d\u0627\u0645\u0644'])])])])]\n\n# Tokenizer / Segmenter.\n>>> list(parser.tokenize(text))\n['\u0627\u0646\u0627', '\u062d\u0627\u0645\u0644']\n\n# POS tagg\n>>> pos_tagger = CoreNLPParser('http://localhost:9005', tagtype='pos')\n>>> list(pos_tagger.tag(parser.tokenize(text)))\n[('\u0627\u0646\u0627', 'PRP'), ('\u062d\u0627\u0645\u0644', 'NN')]\n\n\n# NER tag\n>>> ner_tagger = CoreNLPParser('http://localhost:9005', tagtype='ner')\n>>> list(ner_tagger.tag(parser.tokenize(text)))\n[('\u0627\u0646\u0627', 'O'), ('\u062d\u0627\u0645\u0644', 'O')]\n\nFrench\nStart the server:\njava -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-serverProperties StanfordCoreNLP-french.properties \\\n-preload tokenize,ssplit,pos,parse \\\n-status_port 9004  -port 9004 -timeout 15000\n\nIn Python:\n>>> parser = CoreNLPParser('http://localhost:9004')\n>>> list(parser.parse('Je suis enceinte'.split()))\n[Tree('ROOT', [Tree('SENT', [Tree('NP', [Tree('PRON', ['Je']), Tree('VERB', ['suis']), Tree('AP', [Tree('ADJ', ['enceinte'])])])])])]\n>>> pos_tagger = CoreNLPParser('http://localhost:9004', tagtype='pos')\n>>> pos_tagger.tag('Je suis enceinte'.split())\n[('Je', 'PRON'), ('suis', 'VERB'), ('enceinte', 'ADJ')]\n\nGerman\nStart the server:\njava -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-serverProperties StanfordCoreNLP-german.properties \\\n-preload tokenize,ssplit,pos,ner,parse \\\n-status_port 9002  -port 9002 -timeout 15000\n\nIn Python:\n>>> parser = CoreNLPParser('http://localhost:9002')\n>>> list(parser.raw_parse('Ich bin schwanger'))\n[Tree('ROOT', [Tree('NUR', [Tree('S', [Tree('PPER', ['Ich']), Tree('VAFIN', ['bin']), Tree('AP', [Tree('ADJD', ['schwanger'])])])])])]\n>>> list(parser.parse('Ich bin schwanger'.split()))\n[Tree('ROOT', [Tree('NUR', [Tree('S', [Tree('PPER', ['Ich']), Tree('VAFIN', ['bin']), Tree('AP', [Tree('ADJD', ['schwanger'])])])])])]\n\n\n>>> pos_tagger = CoreNLPParser('http://localhost:9002', tagtype='pos')\n>>> pos_tagger.tag('Ich bin schwanger'.split())\n[('Ich', 'PPER'), ('bin', 'VAFIN'), ('schwanger', 'ADJD')]\n\n>>> pos_tagger = CoreNLPParser('http://localhost:9002', tagtype='pos')\n>>> pos_tagger.tag('Ich bin schwanger'.split())\n[('Ich', 'PPER'), ('bin', 'VAFIN'), ('schwanger', 'ADJD')]\n\n>>> ner_tagger = CoreNLPParser('http://localhost:9002', tagtype='ner')\n>>> ner_tagger.tag('Donald Trump besuchte Angela Merkel in Berlin.'.split())\n[('Donald', 'PERSON'), ('Trump', 'PERSON'), ('besuchte', 'O'), ('Angela', 'PERSON'), ('Merkel', 'PERSON'), ('in', 'O'), ('Berlin', 'LOCATION'), ('.', 'O')]\n\nSpanish\nStart the server:\njava -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-serverProperties StanfordCoreNLP-spanish.properties \\\n-preload tokenize,ssplit,pos,ner,parse \\\n-status_port 9003  -port 9003 -timeout 15000\n\nIn Python:\n>>> pos_tagger = CoreNLPParser('http://localhost:9003', tagtype='pos')\n>>> pos_tagger.tag(u'Barack Obama sali\u00f3 con Michael Jackson .'.split())\n[('Barack', 'PROPN'), ('Obama', 'PROPN'), ('sali\u00f3', 'VERB'), ('con', 'ADP'), ('Michael', 'PROPN'), ('Jackson', 'PROPN'), ('.', 'PUNCT')]\n>>> ner_tagger = CoreNLPParser('http://localhost:9003', tagtype='ner')\n>>> ner_tagger.tag(u'Barack Obama sali\u00f3 con Michael Jackson .'.split())\n[('Barack', 'PERSON'), ('Obama', 'PERSON'), ('sali\u00f3', 'O'), ('con', 'O'), ('Michael', 'PERSON'), ('Jackson', 'PERSON'), ('.', 'O')]\n\n"}, "1299": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "", "text": "\nDeprecated Answer\nThe answer below is deprecated, please use the solution on https://stackoverflow.com/a/51981566/610569 for NLTK v3.3 and above.\n\nEdited\nAs of the current Stanford parser (2015-04-20), the default output for the lexparser.sh has changed so the script below will not work.\nBut this answer is kept for legacy sake, it will still work with http://nlp.stanford.edu/software/stanford-parser-2012-11-12.zip though.\n\nOriginal Answer\nI suggest you don't mess with Jython, JPype. Let python do python stuff and let java do java stuff, get the Stanford Parser output through the console.\nAfter you've installed the Stanford Parser in your home directory ~/, just use this python recipe to get the flat bracketed parse:\nimport os\nsentence = \"this is a foo bar i want to parse.\"\n\nos.popen(\"echo '\"+sentence+\"' > ~/stanfordtemp.txt\")\nparser_out = os.popen(\"~/stanford-parser-2012-11-12/lexparser.sh ~/stanfordtemp.txt\").readlines()\n\nbracketed_parse = \" \".join( [i.strip() for i in parser_out if i.strip()[0] == \"(\"] )\nprint bracketed_parse\n\n"}, "1300": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "alvasalvas", "text": "\nThere is python interface for stanford parser\nhttp://projects.csail.mit.edu/spatial/Stanford_Parser\n"}, "1301": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "", "text": "\nThe Stanford Core NLP software page has a list of python wrappers:\nhttp://nlp.stanford.edu/software/corenlp.shtml#Extensions\n"}, "1302": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "alvasalvas", "text": "\nIf I remember well, the Stanford parser is a java library, therefore you must have a Java interpreter running on your server/computer.\nI used it once a server, combined with a php script. The script used php's exec() function to make a command-line call to the parser like so:\n<?php\n\nexec( \"java -cp /pathTo/stanford-parser.jar -mx100m edu.stanford.nlp.process.DocumentPreprocessor /pathTo/fileToParse > /pathTo/resultFile 2>/dev/null\" );\n\n?>\n\nI don't remember all the details of this command, it basically opened the fileToParse, parsed it, and wrote the output in the resultFile. PHP would then open the result file for further use.\nThe end of the command directs the parser's verbose to NULL, to prevent unnecessary command line information from disturbing the script.\nI don't know much about Python, but there might be a way to make command line calls.\nIt might not be the exact route you were hoping for, but hopefully it'll give you some inspiration. Best of luck.\n"}, "1303": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "", "text": "\nNote that this answer applies to NLTK v 3.0, and not to more recent versions.\nHere is an adaptation of danger98's code that works with nltk3.0.0 on windoze, and presumably the other platforms as well, adjust directory names as appropriate for your setup:\nimport os\nfrom nltk.parse import stanford\nos.environ['STANFORD_PARSER'] = 'd:/stanford-parser'\nos.environ['STANFORD_MODELS'] = 'd:/stanford-parser'\nos.environ['JAVAHOME'] = 'c:/Program Files/java/jre7/bin'\n\nparser = stanford.StanfordParser(model_path=\"d:/stanford-grammars/englishPCFG.ser.gz\")\nsentences = parser.raw_parse_sents((\"Hello, My name is Melroy.\", \"What is your name?\"))\nprint sentences\n\nNote that the parsing command has changed (see the source code at www.nltk.org/_modules/nltk/parse/stanford.html), and that you need to define the JAVAHOME variable.  I tried to get it to read the grammar file in situ in the jar, but have so far failed to do that.\n"}, "1304": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "alvasalvas", "text": "\nYou can use the Stanford Parsers output to create a Tree in nltk (nltk.tree.Tree).\nAssuming the stanford parser gives you a file in which there is exactly one parse tree for every sentence.\nThen this example works, though it might not look very pythonic:\nf = open(sys.argv[1]+\".output\"+\".30\"+\".stp\", \"r\")\nparse_trees_text=[]\ntree = \"\"\nfor line in f:\n  if line.isspace():\n    parse_trees_text.append(tree)\ntree = \"\"\n  elif \"(. ...))\" in line:\n#print \"YES\"\ntree = tree+')'\nparse_trees_text.append(tree)\ntree = \"\"\n  else:\ntree = tree + line\n\nparse_trees=[]\nfor t in parse_trees_text:\n  tree = nltk.Tree(t)\n  tree.__delitem__(len(tree)-1) #delete \"(. .))\" from tree (you don't need that)\n  s = traverse(tree)\n  parse_trees.append(tree)\n\n"}, "1305": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "RohithRohith", "text": "\nNote that this answer applies to NLTK v 3.0, and not to more recent versions.\nSince nobody really mentioned and it's somehow troubled me a lot, here is an alternative way to use Stanford parser in python:\nstanford_parser_jar = '../lib/stanford-parser-full-2015-04-20/stanford-parser.jar'\nstanford_model_jar = '../lib/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar'    \nparser = StanfordParser(path_to_jar=stanford_parser_jar, \n                        path_to_models_jar=stanford_model_jar)\n\nin this way, you don't need to worry about the path thing anymore.\nFor those who cannot use it properly on Ubuntu or run the code in Eclipse.\n"}, "1306": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "silverasmsilverasm", "text": "\nI am on a windows machine and you can simply run the parser normally as you do from the command like but as in a different directory so you don't need to edit the lexparser.bat file. Just put in the full path. \ncmd = r'java -cp \\Documents\\stanford_nlp\\stanford-parser-full-2015-01-30 edu.stanford.nlp.parser.lexparser.LexicalizedParser -outputFormat \"typedDependencies\" \\Documents\\stanford_nlp\\stanford-parser-full-2015-01-30\\stanford-parser-3.5.1-models\\edu\\stanford\\nlp\\models\\lexparser\\englishFactored.ser.gz stanfordtemp.txt'\nparse_out = os.popen(cmd).readlines()\n\nThe tricky part for me was realizing how to run a java program from a different path. There must be a better way but this works.\n"}, "1307": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "bob dopebob dope", "text": "\nNote that this answer applies to NLTK v 3.0, and not to more recent versions.\nA slight update (or simply alternative) on danger89's comprehensive answer on using Stanford Parser in NLTK and Python\nWith stanford-parser-full-2015-04-20, JRE 1.8 and nltk 3.0.4 (python 2.7.6), it appears that you no longer need to extract the englishPCFG.ser.gz from stanford-parser-x.x.x-models.jar or setting up any os.environ\nfrom nltk.parse.stanford import StanfordParser\n\nenglish_parser = StanfordParser('path/stanford-parser.jar', 'path/stanford-parser-3.5.2-models.jar')\n\ns = \"The real voyage of discovery consists not in seeking new landscapes, but in having new eyes.\"\n\nsentences = english_parser.raw_parse_sents((s,))\nprint sentences #only print <listiterator object> for this version\n\n#draw the tree\nfor line in sentences:\n    for sentence in line:\n        sentence.draw()\n\n"}, "1308": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "Servy", "text": "\nNote that this answer applies to NLTK v 3.0, and not to more recent versions.\nHere is the windows version of alvas's answer\nsentences = ('. '.join(['this is sentence one without a period','this is another foo bar sentence '])+'.').encode('ascii',errors = 'ignore')\ncatpath =r\"YOUR CURRENT FILE PATH\"\n\nf = open('stanfordtemp.txt','w')\nf.write(sentences)\nf.close()\n\nparse_out = os.popen(catpath+r\"\\nlp_tools\\stanford-parser-2010-08-20\\lexparser.bat \"+catpath+r\"\\stanfordtemp.txt\").readlines()\n\nbracketed_parse = \" \".join( [i.strip() for i in parse_out if i.strip() if i.strip()[0] == \"(\"] )\nbracketed_parse = \"\\n(ROOT\".join(bracketed_parse.split(\" (ROOT\")).split('\\n')\naa = map(lambda x :ParentedTree.fromstring(x),bracketed_parse)\n\nNOTES:\n\nIn lexparser.bat  you need to change all the paths into absolute path to avoid java errors such as \"class not found\"\nI strongly recommend you to apply this method under windows since I Tried several answers on the page and   all the methods communicates python with Java fails.\nwish to hear from you if you succeed on windows and wish you can tell me how you overcome all these problems.\nsearch python wrapper for stanford coreNLP to get the python version\n\n\n"}, "1309": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "Avery AndrewsAvery Andrews", "text": "\nI took many hours and finally found a simple solution for Windows users. Basically its summarized version of an existing answer by alvas, but made easy to follow(hopefully) for those who are new to stanford NLP and are Window users.\n1) Download the module you want to use, such as NER, POS etc. In my case i wanted to use NER, so i downloaded the module from http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip\n2) Unzip the file.\n3) Set the environment variables(classpath and stanford_modules) from the unzipped folder.\nimport os\nos.environ['CLASSPATH'] = \"C:/Users/Downloads/stanford-ner-2015-04-20/stanford-ner.jar\"\nos.environ['STANFORD_MODELS'] = \"C:/Users/Downloads/stanford-ner-2015-04-20/classifiers/\"\n\n4) set the environment variables for JAVA, as in where you have JAVA installed. for me it was below\nos.environ['JAVAHOME'] = \"C:/Program Files/Java/jdk1.8.0_102/bin/java.exe\"\n\n5) import the module you want\nfrom nltk.tag import StanfordNERTagger\n\n6) call the pretrained model which is present in classifier folder in the unzipped folder. add \".gz\" in the end for file extension. for me the model i wanted to use was english.all.3class.distsim.crf.ser\nst = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n\n7) Now execute the parser!! and we are done!!\nst.tag('Rami Eid is studying at Stony Brook University in NY'.split())\n\n"}, "1310": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "Sad\u0131kSad\u0131k", "text": "\nDeprecated Answer\nThe answer below is deprecated, please use the solution on https://stackoverflow.com/a/51981566/610569 for NLTK v3.3 and above.\n\nEDITED\nNote: The following answer will only work on:\n\nNLTK version ==3.2.5\nStanford Tools compiled since 2016-10-31\nPython 2.7, 3.5 and 3.6\n\nAs both tools changes rather quickly and the API might look very different 3-6 months later. Please treat the following answer as temporal and not an eternal fix.\nAlways refer to https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software for the latest instruction on how to interface Stanford NLP tools using NLTK!!\nTL;DR\nThe follow code comes from https://github.com/nltk/nltk/pull/1735#issuecomment-306091826\nIn terminal:\nwget http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip\nunzip stanford-corenlp-full-2016-10-31.zip && cd stanford-corenlp-full-2016-10-31\n\njava -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-preload tokenize,ssplit,pos,lemma,parse,depparse \\\n-status_port 9000 -port 9000 -timeout 15000\n\nIn Python:\n>>> from nltk.tag.stanford import CoreNLPPOSTagger, CoreNLPNERTagger\n>>> from nltk.parse.corenlp import CoreNLPParser\n\n>>> stpos, stner = CoreNLPPOSTagger(), CoreNLPNERTagger()\n\n>>> stpos.tag('What is the airspeed of an unladen swallow ?'.split())\n[(u'What', u'WP'), (u'is', u'VBZ'), (u'the', u'DT'), (u'airspeed', u'NN'), (u'of', u'IN'), (u'an', u'DT'), (u'unladen', u'JJ'), (u'swallow', u'VB'), (u'?', u'.')]\n\n>>> stner.tag('Rami Eid is studying at Stony Brook University in NY'.split())\n[(u'Rami', u'PERSON'), (u'Eid', u'PERSON'), (u'is', u'O'), (u'studying', u'O'), (u'at', u'O'), (u'Stony', u'ORGANIZATION'), (u'Brook', u'ORGANIZATION'), (u'University', u'ORGANIZATION'), (u'in', u'O'), (u'NY', u'O')]\n\n\n>>> parser = CoreNLPParser(url='http://localhost:9000')\n\n>>> next(\n...     parser.raw_parse('The quick brown fox jumps over the lazy dog.')\n... ).pretty_print()  # doctest: +NORMALIZE_WHITESPACE\n                     ROOT\n                      |\n                      S\n       _______________|__________________________\n      |                         VP               |\n      |                _________|___             |\n      |               |             PP           |\n      |               |     ________|___         |\n      NP              |    |            NP       |\n  ____|__________     |    |     _______|____    |\n DT   JJ    JJ   NN  VBZ   IN   DT      JJ   NN  .\n |    |     |    |    |    |    |       |    |   |\nThe quick brown fox jumps over the     lazy dog  .\n\n>>> (parse_fox, ), (parse_wolf, ) = parser.raw_parse_sents(\n...     [\n...         'The quick brown fox jumps over the lazy dog.',\n...         'The quick grey wolf jumps over the lazy fox.',\n...     ]\n... )\n\n>>> parse_fox.pretty_print()  # doctest: +NORMALIZE_WHITESPACE\n                     ROOT\n                      |\n                      S\n       _______________|__________________________\n      |                         VP               |\n      |                _________|___             |\n      |               |             PP           |\n      |               |     ________|___         |\n      NP              |    |            NP       |\n  ____|__________     |    |     _______|____    |\n DT   JJ    JJ   NN  VBZ   IN   DT      JJ   NN  .\n |    |     |    |    |    |    |       |    |   |\nThe quick brown fox jumps over the     lazy dog  .\n\n>>> parse_wolf.pretty_print()  # doctest: +NORMALIZE_WHITESPACE\n                     ROOT\n                      |\n                      S\n       _______________|__________________________\n      |                         VP               |\n      |                _________|___             |\n      |               |             PP           |\n      |               |     ________|___         |\n      NP              |    |            NP       |\n  ____|_________      |    |     _______|____    |\n DT   JJ   JJ   NN   VBZ   IN   DT      JJ   NN  .\n |    |    |    |     |    |    |       |    |   |\nThe quick grey wolf jumps over the     lazy fox  .\n\n>>> (parse_dog, ), (parse_friends, ) = parser.parse_sents(\n...     [\n...         \"I 'm a dog\".split(),\n...         \"This is my friends ' cat ( the tabby )\".split(),\n...     ]\n... )\n\n>>> parse_dog.pretty_print()  # doctest: +NORMALIZE_WHITESPACE\n        ROOT\n         |\n         S\n  _______|____\n |            VP\n |    ________|___\n NP  |            NP\n |   |         ___|___\nPRP VBP       DT      NN\n |   |        |       |\n I   'm       a      dog\n\nPlease take a look at http://www.nltk.org/_modules/nltk/parse/corenlp.html  for more information on of the Stanford API. Take a look at the docstrings!\n"}, "1311": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "Servy", "text": "\nNote that this answer applies to NLTK v 3.0, and not to more recent versions.\nI cannot leave this as a comment because of reputation, but since I spent (wasted?) some time solving this I would rather share my problem/solution to get this parser to work in NLTK.\nIn the excellent answer from alvas, it is mentioned that:\n\ne.g. for the Parser, there won't be a model directory.\n\nThis led me wrongly to:\n\nnot be careful to the value I put to STANFORD_MODELS  (and only care about my CLASSPATH)\nleave ../path/tostanford-parser-full-2015-2012-09/models directory * virtually empty* (or with a jar file whose name did not match nltk regex)!\n\nIf the OP, like me, just wanted to use the parser, it may be confusing that when not downloading anything else (no POStagger, no NER,...) and following all these instructions, we still get an error.\nEventually, for any CLASSPATH given (following examples and explanations in answers from this thread) I would still get the error:\n\nNLTK was unable to find stanford-parser-(\\d+)(.(\\d+))+-models.jar!\n  Set the CLASSPATH environment variable. For more information, on\n  stanford-parser-(\\d+)(.(\\d+))+-models.jar,\n\nsee:\n    http://nlp.stanford.edu/software/lex-parser.shtml\nOR:\n\nNLTK was unable to find stanford-parser.jar! Set the CLASSPATH\n  environment variable. For more information, on stanford-parser.jar,\n  see: http://nlp.stanford.edu/software/lex-parser.shtml\n\nThough, importantly, I could correctly load and use the parser if I called the function with all arguments and path fully specified, as in:\nstanford_parser_jar = '../lib/stanford-parser-full-2015-04-20/stanford-parser.jar'\nstanford_model_jar = '../lib/stanford-parser-full-2015-04-20/stanfor-parser-3.5.2-models.jar'    \nparser = StanfordParser(path_to_jar=stanford_parser_jar, \n                    path_to_models_jar=stanford_model_jar)\n\nSolution for Parser alone:\nTherefore the error came from NLTK and how it is looking for jars using the supplied STANFORD_MODELS and CLASSPATH environment variables. To solve this, the *-models.jar, with the correct formatting (to match the regex in NLTK code, so no -corenlp-....jar) must be located in the folder designated by STANFORD_MODELS.\nNamely, I first created:\nmkdir stanford-parser-full-2015-12-09/models\n\nThen added in .bashrc:\nexport STANFORD_MODELS=/path/to/stanford-parser-full-2015-12-09/models\n\nAnd finally, by copying stanford-parser-3.6.0-models.jar (or corresponding version), into:\npath/to/stanford-parser-full-2015-12-09/models/\n\nI could get StanfordParser to load smoothly in python with the classic CLASSPATH that points to stanford-parser.jar. Actually, as such, you can call StanfordParser with no parameters, the default will just work.\n"}, "1312": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "Zhong ZhuZhong Zhu", "text": "\nI am using nltk version 3.2.4. And following code worked for me.\nfrom nltk.internals import find_jars_within_path\nfrom nltk.tag import StanfordPOSTagger\nfrom nltk import word_tokenize\n\n# Alternatively to setting the CLASSPATH add the jar and model via their \npath:\njar = '/home/ubuntu/stanford-postagger-full-2017-06-09/stanford-postagger.jar'\nmodel = '/home/ubuntu/stanford-postagger-full-2017-06-09/models/english-left3words-distsim.tagger'\n\npos_tagger = StanfordPOSTagger(model, jar)\n\n# Add other jars from Stanford directory\nstanford_dir = pos_tagger._stanford_jar.rpartition('/')[0]\nstanford_jars = find_jars_within_path(stanford_dir)\npos_tagger._stanford_jar = ':'.join(stanford_jars)\n\ntext = pos_tagger.tag(word_tokenize(\"Open app and play movie\"))\nprint(text)\n\nOutput:\n[('Open', 'VB'), ('app', 'NN'), ('and', 'CC'), ('play', 'VB'), ('movie', 'NN')]\n\n"}, "1313": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "Ted PetrouTed Petrou", "text": "\nA new development of the Stanford parser based on a neural model, trained using Tensorflow is very recently made available to be used as a python API. This model is supposed to be far more accurate than the Java-based moel. You can certainly integrate with an NLTK pipeline.\nLink to the parser. Ther repository contains pre-trained parser models for 53 languages.\n"}, "1314": {"topic": "Stopword removal with NLTK", "user_name": "alvas", "text": "\nI am trying to process a user entered text by removing stopwords using nltk toolkit, but with stopword-removal the words like 'and', 'or', 'not' gets removed. I want these words to be present after stopword removal process as they are operators which are required for later processing text as query. I don't know which are the words which can be operators in text query, and I also want to remove unnecessary words from my text.\n"}, "1315": {"topic": "Stopword removal with NLTK", "user_name": "Grahesh ParkarGrahesh Parkar", "text": "\nThere is an in-built stopword list in NLTK made up of 2,400 stopwords for 11 languages (Porter et al), see http://nltk.org/book/ch02.html\n>>> from nltk import word_tokenize\n>>> from nltk.corpus import stopwords\n>>> stop = set(stopwords.words('english'))\n>>> sentence = \"this is a foo bar sentence\"\n>>> print([i for i in sentence.lower().split() if i not in stop])\n['foo', 'bar', 'sentence']\n>>> [i for i in word_tokenize(sentence.lower()) if i not in stop] \n['foo', 'bar', 'sentence']\n\nI recommend looking at using tf-idf to remove stopwords, see Effects of Stemming on the term frequency?\n"}, "1316": {"topic": "Stopword removal with NLTK", "user_name": "Johan", "text": "\nI suggest you create your own list of operator words that you take out of the stopword list. Sets can be conveniently subtracted, so:\noperators = set(('and', 'or', 'not'))\nstop = set(stopwords...) - operators\n\nThen you can simply test if a word is in or not in the set without relying on whether your operators are part of the stopword list. You can then later switch to another stopword list or add an operator.\nif word.lower() not in stop:\n    # use word\n\n"}, "1317": {"topic": "Stopword removal with NLTK", "user_name": "alvasalvas", "text": "\n@alvas's answer does the job but it can be done way faster. Assuming that you have documents: a list of strings.\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import wordpunct_tokenize\n\nstop_words = set(stopwords.words('english'))\nstop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) # remove it if you need punctuation \n\nfor doc in documents:\n    list_of_words = [i.lower() for i in wordpunct_tokenize(doc) if i.lower() not in stop_words]\n\nNotice that due to the fact that here you are searching in a set (not in a list) the speed would be theoretically len(stop_words)/2 times faster, which is significant if you need to operate through many documents.\nFor 5000 documents of approximately 300 words each the difference is between 1.8 seconds for my example and 20 seconds for @alvas's.\nP.S. in most of the cases you need to divide the text into words to perform some other classification tasks for which tf-idf is used. So most probably it would be better to use stemmer as well:\nfrom nltk.stem.porter import PorterStemmer\nporter = PorterStemmer()\n\nand to use [porter.stem(i.lower()) for i in wordpunct_tokenize(doc) if i.lower() not in stop_words] inside of a loop.\n"}, "1318": {"topic": "Stopword removal with NLTK", "user_name": "", "text": "\n@alvas has a good answer. But again it depends on the nature of the task, for example in your application you want to consider all conjunction e.g. and, or, but, if, while and all determiner e.g. the, a, some, most, every, no as stop words considering all others parts of speech as legitimate, then you might want to look into this solution which use Part-of-Speech Tagset to discard words, Check table 5.1:\nimport nltk\n\nSTOP_TYPES = ['DET', 'CNJ']\n\ntext = \"some data here \"\ntokens = nltk.pos_tag(nltk.word_tokenize(text))\ngood_words = [w for w, wtype in tokens if wtype not in STOP_TYPES]\n\n"}, "1319": {"topic": "Stopword removal with NLTK", "user_name": "otusotus", "text": "\nYou can use string.punctuation with built-in NLTK stopwords list:\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\nwords = tokenize(text)\nwordsWOStopwords = removeStopWords(words)\n\ndef tokenize(text):\n        sents = sent_tokenize(text)\n        return [word_tokenize(sent) for sent in sents]\n\ndef removeStopWords(words):\n        customStopWords = set(stopwords.words('english')+list(punctuation))\n        return [word for word in words if word not in customStopWords]\n\nNLTK stopwords complete list \n"}, "1320": {"topic": "Stopword removal with NLTK", "user_name": "", "text": "\nSTOPWORDS REMOVAL FROM STRING\nHere I added Custom stopword list also\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords                    # Stop words\n\nstop_words = set(stopwords.words('english'))\nstop_words.update(list(set(['zero'    , 'one'     , 'two'      ,\n               'three'   , 'four'    , 'five'     ,\n               'six'     , 'seven'   , 'eight'    ,\n               'nine'    , 'ten'     ,\n               \n               'may'     , 'also'    , 'across'   ,\n               'among'   , 'beside'  , 'however'  ,\n               'yet'     , 'within'  ,\n               \n               'jan'     ,  'feb'    , 'mar'      ,\n               'apr'     ,  'may'    , 'jun'      ,\n               'jul'     ,  'aug'    , 'sep'      ,\n               'oct'     ,  'nov'    , 'dec'      ,\n               \n               'january' , 'february', 'march'    ,\n               'april'   , 'may'     , 'june'     ,\n               'july'    , 'august'  , 'september',\n               'october' , 'november', 'december' ,\n               \n               'summer'  , 'winter'  , 'fall'     ,\n               'spring'                          \n\n               \"a\"         , \"about\"     ,   \"above\"  , \"after\"   ,\n               \"again\"     , \"against\"   ,   \"ain\"    , \"aren't\"  ,\n               \"all\"       , \"am\"        ,   \"an\"     , \"and\"     ,\n               \"any\"       , \"are\"       ,   \"aren\"   ,  \"as\"     ,\n               \"at\"        ,\n               \n               \"be\"        , \"because\"   ,   \"been\"   , \"before\"  ,\n               \"being\"     , \"below\"     ,   \"between\", \"both\"    ,\n               \"but\"       , \"by\"        ,                  \n               \n               \"can\"       , \"couldn\"    , \"couldn't\" , \"could\"   ,\n               \n               \"d\"         , \"did\"       , \"didn\"     , \"didn't\"  ,\n               \"do\"        , \"does\"      , \"doesn\"    , \"doesn't\" ,\n               \"doing\"     , \"don\"       , \"don't\"    , \"down\"    ,\n               \"during\"    ,\n               \n               \"each\"      ,  \n               \n               \"few\"       , \"for\"      , \"from\"      , \"further\" ,\n               \n               \"had\"       , \"hadn\"     , \"hadn't\"    , \"has\"     ,\n               \"hasn\"      , \"hasn't\"   , \"have\"      , \"haven\"   ,\n               \"haven't\"   , \"having\"   , \"he\"        , \"her\"     ,\n               \"here\"      , \"hers\"     , \"herself\"   , \"him\"     ,\n               \"himself\"   , \"his\"      , \"how\"       ,\n               \"he'd\"      , \"he'll\"    , \"he's\"      , \"here's\"  ,\n               \"how's\"     ,\n               \n               \"i\"         , \"if\"       , \"in\"        , \"into\"    ,\n               \"is\"        , \"isn\"      , \"isn't\"     , \"it\"      ,\n               \"it's\"      , \"its\"      , \"itself\"    , \"i'd\"     ,\n               \"i'll\"      , \"i'm\"      , \"i've\"      ,\n               \n               \"just\"      ,\n               \n               \"ll\"        , \"let's\"    ,\n               \n               \"m\"         , \"ma\"       ,\"me\"         ,\n               \"mightn\"    , \"mightn't\" , \"more\"      , \"most\"    ,\n               \"mustn\"     , \"mustn't\"  , \"my\"        , \"myself\"  ,\n               \"needn\"     , \"needn't\"  , \"no\"        , \"nor\"     ,\n               \"not\"       , \"now\"      ,\n               \n               \"o\"         , \"of\"       , \"off\"       , \"on\"      ,\n               \"once\"      , \"only\"     , \"or\"        , \"other\"   ,\n               \"our\"       , \"ours\"     , \"ourselves\" , \"out\"     ,\n               \"over\"      , \"own\"      , \"ought\"     ,\n               \n               \"re\"        ,\n               \n               \"s\"         , \"same\"     , \"shan\"      , \"shan't\"   ,\n               \"she\"       , \"she's\"    , \"should\"    , \"should've\",\n               \"shouldn\"   , \"shouldn't\", \"so\"        , \"some\"     ,\n               \"such\"      , \"she'd\"    , \"she'll\"    ,\n               \n               \"t\"         , \"than\"     , \"that\"      , \"that'll\"  ,\n               \"the\"       , \"their\"    , \"theirs\"    , \"them\"     ,\n               \"themselves\", \"then\"     , \"there\"     , \"these\"    ,\n               \"they\"      , \"this\"     , \"those\"     , \"through\"  ,\n               \"to\"        , \"too\"      , \"that's\"    , \"there's\"  ,\n               \"they'd\"    , \"they'll\"  , \"they're\"   , \"they've\"  ,\n               \n               \"under\"     , \"until\"    , \"up\"        ,\n               \n               \"ve\"        , \"very\"     ,\n               \n               \"was\"       , \"wasn\"     , \"wasn't\"    , \"we\"       ,\n               \"were\"      , \"weren\"    , \"weren't\"   , \"what\"     ,\n               \"when\"      , \"where\"    , \"which\"     , \"while\"    ,\n               \"who\"       , \"whom\"     , \"why\"       , \"will\"     ,\n               \"with\"      , \"won\"      , \"won't\"     , \"wouldn\"   ,\n               \"wouldn't\"  , \"we'd\"     , \"we'll\"     , \"we're\"    ,\n               \"we've\"     , \"what's\"   , \"when's\"    , \"where's\"  ,\n               \"who's\"     , \"why's\"    , \"would\"     ,\n               \n               \"y\"         , \"you\"      , \"you'd\"     , \"you'll\"   ,\n               \"you're\"    , \"you've\"   , \"your\"      , \"yours\"    , \"yourself\",\n               \"yourselves\",\n               \n               'a',\"able\", \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\"          ,\n               \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afterwards\", \"ah\",      \"almost\"          ,\n               \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"among\", \"amongst\", \"anyone\"        ,  \n               \"announce\", \"another\", \"anybody\", \"anyhow\", \"anymore\",  \"anything\", \"anyway\", \"anyways\"        ,\n               \"anywhere\", \"apparently\", \"approximately\", \"arent\", \"arise\", \"around\", \"aside\", \"ask\"          ,\n               \"asking\", \"auth\", \"available\", \"away\", \"awfully\", \"a's\", \"ain't\", \"allow\", \"allows\", \"apart\"   ,\n               \"appear\", \"appreciate\", \"appropriate\", \"associated\"                                            ,\n               \n               \"b\", \"back\", \"became\", \"become\", \"becomes\", \"becoming\", \"beforehand\", \"begin\", \"beginning\"     ,\n               \"beginnings\", \"begins\", \"behind\", \"believe\", \"beside\", \"besides\", \"beyond\", \"biol\", \"brief\"    ,\n               \"briefly\"                                                                                      ,\n               \n               \"c\", \"ca\", \"came\", \"cannot\", \"can't\", \"cause\", \"causes\", \"certain\", \"certainly\", \"co\", \"com\"   ,\n               \"come\", \"comes\", \"contain\", \"containing\", \"contains\", \"couldnt\"                                ,\n               \n               'd',\"date\", \"different\", \"done\", \"downwards\", \"due\"                                                ,\n               \n               \"e\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\", \"either\", \"else\", \"elsewhere\", \"end\"      ,\n               \"ending\", \"enough\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\",\"except\"   ,\n               \"everyone\", \"everything\", \"everywhere\", \"ex\"                                                   ,  \n               \n               \"f\", \"far\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\", \"followed\", \"following\", \"follows\", \"four\"  ,\n               \"former\", \"formerly\", \"forth\", \"found\",  \"furthermore\"                                         ,\n               \n               \"g\", \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\",  \"go\", \"goes\", \"got\",\"gone\"   ,  \n               \"gotten\", \"giving\"                                                                             ,\n               \n               \"h\", \"happens\", \"hardly\", \"hed\", \"hence\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"however\"  ,\n               \"hereupon\", \"hes\", \"hi\", \"hid\", \"hither\", \"home\", \"howbeit\",  \"hundred\"                        ,\n               \n               \"id\", \"ie\", \"im\", \"immediately\", \"importance\", \"important\", \"inc\", \"indeed\", \"itd\", \"index\"    ,\n               'i',\"information\", \"instead\", \"invention\",   \"it'll\", \"inward\", \"immediate\"                        ,\n               \n               \"j\",\n               \n               \"k\", \"keep\", \"keeps\", \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\"                             ,\n               \n               \"l\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"ltd\",    \n               \"let\", \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"'ll\", \"look\", \"looking\", \"looks\"  ,  \n               \n               'm',\"made\", \"mainly\", \"make\", \"makes\", \"many\", \"maybe\", \"mean\", \"means\", \"meantime\", \"merely\", \"mg\",\n               \"might\", \"million\", \"miss\", \"ml\", \"moreover\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\", \"must\"     ,\n               \"meanwhile\", \"may\"                                                                             ,\n               \n               \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\" ,\n               \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"nobody\", \"non\"  ,\n               \"none\", \"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"nothing\", \"nowhere\", \"n2\", \"nc\"   ,\n               \"nd\", \"ne\", \"ng\", \"ni\", \"nj\", \"nl\", \"nn\", \"nr\", \"ns\", \"nt\", \"ny\"                               ,\n               \n               'o',\"obtain\", \"obtained\", \"obviously\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"omitted\", \"one\", \"ones\",\n               \"onto\", \"ord\", \"others\", \"otherwise\", \"outside\", \"overall\", \"owing\",  \"oa\", \"ob\", \"oc\", \"od\"   ,\n               \"of\", \"og\", \"oi\", \"oj\", \"ol\", \"om\", \"on\", \"oo\", \"oq\", \"or\", \"os\", \"ot\", \"ou\", \"ow\", \"ox\", \"oz\" ,\n               \n               \"p\", \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\" ,\n               \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\"       ,\n               \"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"put\"       ,\n               \"p1\", \"p2\", \"p3\", \"pc\", \"pd\", \"pe\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"pm\", \"pn\", \"po\", \"pq\" ,\n               \"pr\", \"ps\", \"pt\", \"pu\", \"py\"                                                                   ,\n               \n               \"q\", \"que\", \"quickly\", \"quite\", \"qv\",  \"qj\", \"qu\"                                              ,\n               \n               'r',\"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\" ,\n               \"related\", \"relatively\", \"research\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"run\" ,\n               \"right\",  \"r2\", \"ra\", \"rc\", \"rd\", \"rf\", \"rh\", \"ri\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\"   ,\n               \"rs\", \"rt\", \"ru\", \"rv\", \"ry\" \"r\", \"ran\", \"rather\", \"rd\"                                        ,                                                                  \n               \n               's',\"said\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\"    ,\n               \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\", \"several\", \"shall\", \"shed\"      ,\n               \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\"           ,\n               \"similar\", \"similarly\", \"since\", \"six\", \"slightly\", \"somebody\", \"somehow\", \"someone\", \"soon\"   ,\n               \"somewhat\", \"somewhere\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\", \"stop\" ,\n               \"strongly\", \"sub\", \"substantially\", \"successfully\", \"sufficiently\", \"suggest\", \"sup\", \"sure\"   ,\n               \"s2\", \"sa\", \"sc\", \"sd\", \"se\", \"sf\", \"si\", \"sj\", \"sl\", \"sm\", \"sn\", \"sp\", \"sq\", \"sr\", \"ss\", \"st\" ,\n               \"sy\", \"sz\",   \"sorry\", \"sometime\", \"somethan\", \"something\", \"sometimes\"                        ,\n               \n               't',\"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"thank\", \"thanx\", \"that've\", \"thence\", \"thereafter\",\n               \"thereby\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"thereto\", \"thereupon\"    ,\n               \"there've\", \"theyd\", \"theyre\", \"think\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\"      ,\n               \"throughout\", \"thru\", \"thus\", \"til\", \"tip\", \"together\", \"took\", \"toward\", \"towards\", \"tried\"   ,\n               \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"thats\",  \"thanks\",  \"th\",  \"thered\"  ,\n               \"theres\" \"t1\", \"t2\", \"t3\", \"tb\", \"tc\", \"td\", \"te\", \"tf\", \"th\", \"ti\", \"tj\", \"tl\", \"tm\", \"tn\"    ,\n               \"tp\", \"tq\", \"tr\", \"ts\", \"tt\", \"tv\", \"tx\"                                                       ,                                                                                        \n               \n               \"u\", \"un\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"unto\", \"upon\", \"ups\", \"us\", \"use\" ,\n               \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ue\", \"ui\", \"uj\", \"uk\" ,\n               \"um\", \"un\", \"uo\", \"ur\", \"ut\",\n               \n               \"v\", \"value\", \"various\", \"'ve\", \"via\", \"viz\", \"vol\", \"vols\", \"vs\", \"va\", \"vd\", \"vj\", \"vo\", \"vq\",\n               \"vt\", \"vu\"                                                                                     ,\n               \n               \"w\", \"want\", \"wants\", \"wasnt\", \"way\", \"wed\", \"welcome\", \"went\", \"werent\", \"whatever\", \"what'll\",\n               \"whats\", \"whence\", \"whenever\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"wherever\", \"whether\",  \n               \"whim\", \"whither\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whomever\", \"whos\", \"whose\", \"widely\" ,\n               \"whereupon\", \"willing\", \"wish\", \"within\", \"without\", \"wont\", \"words\", \"world\", \"wouldnt\", \"www\",\n               \"wi\", \"wa\", \"wo\",\n               \n               \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\",\n               \n               \"yes\", \"yet\", \"youd\", \"youre\", \"y2\", \"yj\", \"yl\", \"yr\", \"ys\", \"yt\",\n               \n               \"z\", \"zero\", \"zi\", \"zz\"\n               \n               \"best\", \"better\", \"c'mon\", \"c's\", \"cant\", \"changes\", \"clearly\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"corresponding\", \"course\", \"currently\", \"definitely\", \"described\", \"despite\", \"entirely\", \"exactly\", \"example\", \"going\", \"greetings\", \"hello\", \"help\", \"hopefully\", \"ignored\", \"inasmuch\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\", \"it'd\", \"keep\", \"keeps\", \"novel\", \"presumably\", \"reasonably\", \"second\", \"secondly\", \"sensible\", \"serious\", \"seriously\", \"sure\", \"t's\", \"third\", \"thorough\", \"thoroughly\", \"three\", \"well\", \"wonder\", \"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\",                   \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"co\", \"op\", \"research-articl\", \"pagecount\", \"cit\", \"ibid\", \"les\", \"le\", \"au\", \"que\", \"est\", \"pas\", \"vol\", \"el\", \"los\", \"pp\", \"u201d\", \"well-b\", \"http\", \"volumtype\", \"par\",\n               \"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\",\n               \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"ac\", \"ad\", \"ae\", \"af\", \"ag\", \"aj\", \"al\", \"an\", \"ao\", \"ap\", \"ar\", \"av\", \"aw\", \"ax\", \"ay\", \"az\",\n               \"b1\", \"b2\", \"b3\", \"ba\", \"bc\", \"bd\", \"be\", \"bi\", \"bj\", \"bk\", \"bl\", \"bn\", \"bp\", \"br\", \"bs\", \"bt\", \"bu\", \"bx\",\n               \"c1\", \"c2\", \"c3\", \"cc\", \"cd\", \"ce\", \"cf\", \"cg\", \"ch\", \"ci\", \"cj\", \"cl\", \"cm\", \"cn\", \"cp\", \"cq\", \"cr\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\",\n               \"d2\", \"da\", \"dc\", \"dd\", \"de\", \"df\", \"di\", \"dj\", \"dk\", \"dl\", \"do\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"dx\", \"dy\",\n               \"e2\", \"e3\", \"ea\", \"ec\", \"ed\", \"ee\", \"ef\", \"ei\", \"ej\", \"el\", \"em\", \"en\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"et\", \"eu\", \"ev\", \"ex\", \"ey\",\n               \"f2\", \"fa\", \"fc\", \"ff\", \"fi\", \"fj\", \"fl\", \"fn\", \"fo\", \"fr\", \"fs\", \"ft\", \"fu\", \"fy\",\n               \"ga\", \"ge\", \"gi\", \"gj\", \"gl\", \"go\", \"gr\", \"gs\", \"gy\",\n               \"h2\", \"h3\", \"hh\", \"hi\", \"hj\", \"ho\", \"hr\", \"hs\", \"hu\", \"hy\",\n               \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ic\", \"ie\", \"ig\", \"ih\", \"ii\", \"ij\", \"il\", \"in\", \"io\", \"ip\", \"iq\", \"ir\", \"iv\", \"ix\", \"iy\", \"iz\",\n               \"jj\", \"jr\", \"js\", \"jt\", \"ju\",\n               \"ke\", \"kg\", \"kj\", \"km\", \"ko\",\n               \"l2\", \"la\", \"lb\", \"lc\", \"lf\", \"lj\", \"ln\", \"lo\", \"lr\", \"ls\", \"lt\",\n               \"m2\", \"ml\", \"mn\", \"mo\", \"ms\", \"mt\", \"mu\",\n               \n               'i',  'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii','ix', 'x',\n               'xi', 'xii', 'xiii', 'xiv', 'xv', 'xvi', 'xvii', 'xviii', 'xix', 'xx',\n                'xxi', 'xxii', 'xxiii', 'xxiv', 'xxv', 'xxvi', 'xxvii', 'xxviii', 'xxix', 'xxx',\n                'xxxi', 'xxxii', 'xxxiii', 'xxxiv', 'xxxv', 'xxxvi', 'xxxvii', 'xxxviii', 'xxxix', 'xl',\n               'xli', 'xlii', 'xliii', 'xliv', 'xlv', 'xlvi', 'xlvii', 'xlviii', 'xlix', 'l',\n               'li', 'lii', 'liii', 'liv', 'lv', 'lvi', 'lvii', 'lviii', 'lix', 'lx',\n               'lxi', 'lxii', 'lxiii', 'lxiv', 'lxv', 'lxvi', 'lxvii', 'lxviii', 'lxix', 'lxx',\n                'lxxi', 'lxxii', 'lxxiii', 'lxxiv', 'lxxv', 'lxxvi', 'lxxvii', 'lxxviii', 'lxxix', 'lxxx',\n                'lxxxi', 'lxxxii', 'lxxxiii', 'lxxxiv', 'lxxxv', 'lxxxvi', 'lxxxvii', 'lxxxviii', 'lxxxix', 'xc',\n                'xci', 'xcii', 'xciii', 'xciv', 'xcv', 'xcvi', 'xcvii', 'xcviii', 'xcix', 'c',\n               \n                \"one\", \"first\", \"two\", \"second\", \"three\", \"third\",\n                \"four\", \"fourth\", \"five\", \"fifth\", \"six\",  \"sixth\", \"seven\",\n                \"seventh\", \"eight\", \"eighth\", \"nine\", \"ninth\", \"ten\",\n                \"tenth\", \"eleven\", \"eleventh\", \"twelve\", \"twelfth\", \"thirteen\",\n                \"thirteenth\", \"fourteen\", \"fourteenth\", \"fifteen\", \"fifteenth\",\n                \"sixteen\", \"sixteenth\",  \"seventeen\", \"seventeenth\", \"eighteen\",\n                \"eighteenth\", \"nineteen\", \"nineteenth\", \"twenty\", \"twentieth\",\n                \"one\", \"22nd\", \"second\", \"nd\", \"st\", \"rd\", \"th\",\n               \n                \"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10th\",\"11th\",\"12th\",\"13th\",\"14th\",\"15th\",\n                \"16th\",\"17th\",\"18th\",\"19th\",\"20th\",\"21st\",\"22nd\",\"23rd\",\"24th\",\"25th\",\"26th\",\"27th\",\n                \"28th\",\"29th\",\"30th\",\"31st\",\"32nd\",\"33rd\",\"34th\",\"35th\",\"36th\",\"37th\",\"38th\",\"39th\",\n                \"40th\",\"41st\",\"42nd\",\"43rd\",\"44th\",\"45th\",\"46th\",\"47th\",\"48th\",\"49th\",\"50th\",\"51st\",\n                \"52nd\",\"53rd\",\"54th\",\"55th\",\"56th\",\"57th\",\"58th\",\"59th\",\"60th\",\"61st\",\"62nd\",\"63rd\",\n                \"64th\",\"65th\",\"66th\",\"67th\",\"68th\",\"69th\",\"70th\",\"71st\",\"72nd\",\"73rd\",\"74th\",\"75th\",\n                \"76th\",\"77th\",\"78th\",\"79th\",\"80th\",\"81st\",\"82nd\",\"83rd\",\"84th\",\"85th\",\"86th\",\"87th\",\n                \"88th\",\"89th\",\"90th\", \"91st\", \"92nd\", \"93rd\", \"94th\", \"95th\", \"96th\",\"97th\", \"98th\",\n                \"99th\",\"100th\",\"thirty\",\"forty\",\"fifty\",\"thirty\",\"thirtieth\",\"forty\",\"fortieth\",\n                \"fifty\", \"fiftiethiftieth\",\"sixty\",\"sixtieth\",\"seventy\",\"seventieth\", \"eighty\",\n                \"eightieth\", \"ninety\", \"ninetieth\",\"one\", \"hundred\", \"100th\", \"hundredth\",\n                \"order\",\"state\",\"page\",\"file\",\n                \n                \"'d\",\"'ll\",  \"'m\",  \"'re\",  \"'s\",  \"'ve\",  'a',  \n                'about',  'above',  'across',  'after',  'afterwards',  'again',  'against',  'all',  \n                'almost',  'alone',  'along',  'already',  'also',  'although',  'always',  'am',  \n                'among',  'amongst',  'amount',  'an',  'and',  'another',  'any',  'anyhow',  'anyone',  \n                'anything',  'anyway',  'anywhere',  'are',  'around',  'as',  'at',  'back',  'be',\n                'became',  'because',  'become',  'becomes',  'becoming',  'been',  'before',  'beforehand',\n                'behind',  'being',  'below',  'beside',  'besides',  'between',  'beyond',  'both',\n                'bottom',  'but',  'by',  'ca',  'call',  'can',  'cannot',  'could',  'did',  'do',  'does',\n                'doing',  'done',  'down',  'due',  'during',  'each',  'eight',  'either',  'eleven',\n                'else',  'elsewhere',  'empty',  'enough',  'even',  'ever',  'every',  'everyone',\n                'everything',  'everywhere',  'except',  'few',  'fifteen',  'fifty',  'first',\n                'five',  'for',  'former',  'formerly',  'forty',  'four',  'from',  'front',  'full',\n                'further',  'get',  'give',  'go',  'had',  'has',  'have',  'he',  'hence',  'her',\n                'here',  'hereafter',  'hereby',  'herein',  'hereupon',  'hers',  'herself',  'him',  'himself',\n                'his',  'how',  'however',  'hundred',  'i',  'if',  'in',  'indeed',  'into',  'is',  'it',\n                'its',  'itself',  'just',  'keep',  'last',  'latter',  'latterly',  'least',  'less',  'made',\n                'make',  'many',  'may',  'me',  'meanwhile',  'might',  'mine',  'more',  'moreover',  'most',\n                'mostly',  'move',  'much',  'must',  'my',  'myself',  \"n't\",  'name',  'namely',  'neither',\n                'never',  'nevertheless',  'next',  'nine',  'no',  'nobody',  'none',  'noone',  'nor',  'not',\n                'nothing',  'now',  'nowhere',  'n\u2018t',  'n\u2019t',  'of',  'off',  'often',  'on',  'once',  'one',\n                'only',  'onto',  'or',  'other',  'others',  'otherwise',  'our',  'ours',  'ourselves',  'out',\n                'over',  'own',  'part',  'per',  'perhaps',  'please',  'put',  'quite',  'rather',  're',  'really',\n                'regarding',  'same',  'say',  'see',  'seem',  'seemed',  'seeming',  'seems',  'serious',  'several',\n                'she',  'should',  'show',  'side',  'since',  'six',  'sixty',  'so',  'some',  'somehow',  'someone',\n                'something',  'sometime',  'sometimes',  'somewhere',  'still',  'such',  'take',  'ten',  'than',\n                'that',  'the',  'their',  'them',  'themselves',  'then',  'thence',  'there',  'thereafter',\n                'thereby',  'therefore',  'therein',  'thereupon',  'these',  'they',  'third',  'this',  'those',\n                'though',  'three',  'through',  'throughout',  'thru',  'thus',  'to',  'together',  'too',  'top',\n                'toward',  'towards',  'twelve',  'twenty',  'two',  'under',  'unless',  'until',  'up',  'upon',  'us',\n                'used',  'using',  'various',  'very',  'via',  'was',  'we',  'well',  'were',  'what',  'whatever',  'when',\n                'whence',  'whenever',  'where',  'whereafter',  'whereas',  'whereby',  'wherein',  'whereupon',  'wherever',\n                'whether',  'which',  'while',  'whither',  'who',  'whoever',  'whole',  'whom',  'whose',  'why',  'will',\n                'with',  'within',  'without',  'would',  'yet',  'you',  'your',  'yours',  'yourself',  'yourselves',  '\u2018d',\n                '\u2018ll',  '\u2018m',  '\u2018re',  '\u2018s',  '\u2018ve',  '\u2019d',  '\u2019ll',  '\u2019m',  '\u2019re',  '\u2019s',  '\u2019ve'\n\n                       \n                       ])))\n\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\n\nstop_words = stopwords.words(\"english\")\n\nsentence = \"PDF.co is a website that contains different tools to read, write and process PDF documents\"\nwords = word_tokenize(sentence)\n\nsentence_wo_stopwords = [word for word in words if not word in stop_words]\n\nprint(\" \".join(sentence_wo_stopwords))\n\n"}, "1321": {"topic": "Python NLTK: SyntaxError: Non-ASCII character '\\xc3' in file (Sentiment Analysis -NLP)", "user_name": "bad_coder", "text": "\nI am playing around with NLTK to do an assignment on sentiment analysis. I am using Python 2.7. NLTK 3.0 and NumPy1.9.1 version. \nThis is the code :\n__author__ = 'karan'\nimport nltk\nimport re\nimport sys\n\n\n\ndef main():\n    print(\"Start\");\n    # getting the stop words\n    stopWords = open(\"english.txt\",\"r\");\n    stop_word = stopWords.read().split();\n    AllStopWrd = []\n    for wd in stop_word:\n        AllStopWrd.append(wd);\n    print(\"stop words-> \",AllStopWrd);\n\n    # sample and also cleaning it\n    tweet1= 'Love, my new toy\u00ed\u00a0\u00bd\u00ed\u00b8\u00ed\u00a0\u00bd\u00ed\u00b8#iPhone6. Its good https://twitter.com/Sandra_Ortega/status/513807261769424897/photo/1'\n    print(\"old tweet-> \",tweet1)\n    tweet1 = tweet1.lower()\n    tweet1 = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",tweet1).split())\n    print(tweet1);\n    tw = tweet1.split()\n    print(tw)\n\n\n    #tokenize\n    sentences = nltk.word_tokenize(tweet1)\n    print(\"tokenized ->\", sentences)\n\n\n    #remove stop words\n    Otweet =[]\n    for w in tw:\n        if w not in AllStopWrd:\n            Otweet.append(w);\n    print(\"sans stop word-> \",Otweet)\n\n\n    # get taggers for neg/pos/inc/dec/inv words\n    taggers ={}\n    negWords = open(\"neg.txt\",\"r\");\n    neg_word = negWords.read().split();\n    print(\"ned words-> \",neg_word)\n    posWords = open(\"pos.txt\",\"r\");\n    pos_word = posWords.read().split();\n    print(\"pos words-> \",pos_word)\n    incrWords = open(\"incr.txt\",\"r\");\n    inc_word = incrWords.read().split();\n    print(\"incr words-> \",inc_word)\n    decrWords = open(\"decr.txt\",\"r\");\n    dec_word = decrWords.read().split();\n    print(\"dec wrds-> \",dec_word)\n    invWords = open(\"inverse.txt\",\"r\");\n    inv_word = invWords.read().split();\n    print(\"inverse words-> \",inv_word)\n    for nw in neg_word:\n        taggers.update({nw:'negative'});\n    for pw in pos_word:\n        taggers.update({pw:'positive'});\n    for iw in inc_word:\n        taggers.update({iw:'inc'});\n    for dw in dec_word:\n        taggers.update({dw:'dec'});\n    for ivw in inv_word:\n        taggers.update({ivw:'inv'});\n    print(\"tagger-> \",taggers)\n    print(taggers.get('little'))\n\n    # get parts of speech\n    posTagger = [nltk.pos_tag(tw)]\n    print(\"posTagger-> \",posTagger)\n\nmain();\n\nThis is the error that I am getting when running my code:\nSyntaxError: Non-ASCII character '\\xc3' in file C:/Users/karan/PycharmProjects/mainProject/sentiment.py on line 19, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details\n\nHow do I fix this error?\nI also tried the code using Python 3.4.2 and with NLTK 3.0 and NumPy 1.9.1 but then I get the error:\nTraceback (most recent call last):\n  File \"C:/Users/karan/PycharmProjects/mainProject/sentiment.py\", line 80, in <module>\n    main();\n  File \"C:/Users/karan/PycharmProjects/mainProject/sentiment.py\", line 72, in main\n    posTagger = [nltk.pos_tag(tw)]\n  File \"C:\\Python34\\lib\\site-packages\\nltk\\tag\\__init__.py\", line 100, in pos_tag\n    tagger = load(_POS_TAGGER)\n  File \"C:\\Python34\\lib\\site-packages\\nltk\\data.py\", line 779, in load\n    resource_val = pickle.load(opened_resource)\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xcb in position 0: ordinal not in range(128)\n\n"}, "1322": {"topic": "Python NLTK: SyntaxError: Non-ASCII character '\\xc3' in file (Sentiment Analysis -NLP)", "user_name": "rkbom9rkbom9", "text": "\nAdd the following to the top of your file   # coding=utf-8\nIf you go to the link in the error you can seen the reason why:\nDefining the Encoding\nPython will default to ASCII as standard encoding if no other\n    encoding hints are given.\n    To define a source code encoding, a magic comment must\n    be placed into the source files either as first or second\n    line in the file, such as:\n          # coding=\n"}, "1323": {"topic": "NLTK vs Stanford NLP", "user_name": "Shayan Shafiq", "text": "\nI have recently started to use NLTK toolkit for creating few solutions using Python.\nI hear a lot of community activity regarding using Stanford NLP.\nCan anyone tell me the difference between NLTK and Stanford NLP? Are they two different libraries? I know that NLTK has an interface to Stanford NLP but can anyone throw some light on few basic differences or even more in detail.\nCan Stanford NLP be used using Python?\n"}, "1324": {"topic": "NLTK vs Stanford NLP", "user_name": "RDataRData", "text": "\n\nCan anyone tell me what is the difference between NLTK and Stanford NLP? Are they 2 different libraries ? I know that NLTK has an interface to Stanford NLP but can anyone throw some light on few basic differences or even more in detail.\n\n(I'm assuming you mean \"Stanford CoreNLP\".)\nThey are two different libraries.\n\nStanford CoreNLP is written in Java\nNLTK is a Python library\n\nThe main functional difference is that NLTK has multiple versions or interfaces to other versions of NLP tools, while Stanford CoreNLP only has their version. NLTK also supports installing third-party Java projects, and even includes instructions for installing some Stanford NLP packages on the wiki.\nBoth have good support for English, but if you are dealing with other languages:\n\nStanford CoreNLP comes with models for English, Chinese, French, German, Spanish, and Arabic.\nNLTK comes with corpora in additional languages like Portugese, Russian, and Polish. Individual tools may support even more languages (e.g. no Danish corpora, but has a DanishStemmer).\n\nThat said, which one is \"best\" will depend on your specific application and required performance (what features you are using, language, vocabulary, desired speed, etc.).\n\nCan Stanford NLP be used using Python?\n\nYes, there are a number of interfaces and packages for using Stanford CoreNLP in Python  (independent of NLTK).\n"}, "1325": {"topic": "NLTK vs Stanford NLP", "user_name": "user812786user812786", "text": "\nThe choice will depend upon your use case. NLTK is great for pre-processing and tokenizing text. It also includes a good POS tagger. Standford Core NLP for only tokenizing/POS tagging is a bit of overkill, because Standford NLP requires more resources.\n But one fundamental difference is, you can't parse syntactic dependencies out of the box with NLTK. You need to specify a Grammar for that which can be very tedious if the text domain is not restricted. Whereas Standford NLP provides a probabilistic parser for general text as a down-loadable model, which is quite accurate. It also has built in NER (Named Entity Recognition) and more. Also I will recomend to take a look at Spacy, which is written in python, easy to use and much faster than CoreNLP.  \n"}, "1326": {"topic": "NLTK vs Stanford NLP", "user_name": "", "text": "\nIt appears that you are new to NLP. \n\nI have recently started to use NLTK toolkit \n\nIf indeed you are new to NLP, then the best thing would be to start simple. So ideally you would start off with nltk. I am relatively new to natural language processing (a few months old). I can confirm that for beginners, nltk is better, since it has a great and free online book which helps the beginner learn quickly. \nOnce you are comfortable and actually have a problem to solve, look at Stanford Core NLP to see if it will be better at solving your problem. \nIf you want to stick to NLTK, you can also access the Stanford CoreNLP API in NLTK. \nNow for the similarities and differences:\n\nCan anyone tell me what is the difference between NLTK and Stanford\n  NLP ? \n  Are they 2 different libraries?\n\nBoth offer natural language processing. Some of the most useful parts of Stanford Core NLP include the part-of-speech tagger, the named entity recognizer, sentiment analysis, and pattern learning. \nThe named entity recognizer is better in the Stanford Core NLP. Stanford Core NLP is better at grammatical functions for instance picking up subject, object, predictae (that is partially why I switched from nltk to Stanford Core NLP). As @user812786 said, NLTK has multiple interfaces to other versions of NLP tools. NLTK is also better for learning NLP. If you need to use multiple corpora, use NLTK, as you can easily access a wide multitude of text corpora and lexical resources. Both have POS tagging and sentiment analysis. \n\nCan stanford NLP be used using Python ?\n\nYes absolutely. You can use StanfordNLP which is a Python natural language analysis package that is able to call the CoreNLP Java package. There are also multiple Python packages using the Stanford CoreNLP server\n"}, "1327": {"topic": "NLTK vs Stanford NLP", "user_name": "0x50500x5050", "text": "\nI would add to this answer that if you are looking to parse date/time events StanfordCoreNLP contains SuTime which is the best datetime parser available. The support for arbitrary texts like 'Next Monday afternoon' is not present in any other package. \n"}, "1328": {"topic": "NLTK vs Stanford NLP", "user_name": "Tanya GuptaTanya Gupta", "text": "\nNLTK can be used for the learning phase to and perform natural language process from scratch and basic level.\nStandford NLP gives you high-level flexibility to done task very fast and easiest way.\nIf you want fast and production use, can go for Standford NLP.\n"}, "1329": {"topic": "NLTK vs Stanford NLP", "user_name": "Pradeep BanavaraPradeep Banavara", "text": "\nIn 2020, Stanford released STANZA, Python library based on Stanford NLP.\nYou  can find it here https://stanfordnlp.github.io/stanza/\nIf you familiar with Spacy NLP, it quite similar :\n>>> import stanza\n>>> stanza.download('en') # download English model\n>>> nlp = stanza.Pipeline('en') # initialize English neural pipeline\n>>> doc = nlp(\"Barack Obama was born in Hawaii.\") # run annotation over a sentence\n\n"}, "1330": {"topic": "NLTK vs Stanford NLP", "user_name": "Rahul RawatRahul Rawat", "text": "\nThose 2 are different libraries. \nThey are written in different languages like Standford CoreNLP is written in Java and NLTK is written in python, you can check the documentation in the main website, in my point of view NLTK is much more useful to be used for tokenizing and Data PRE-PROCESSING.\n"}, "1331": {"topic": "Use of PunktSentenceTokenizer in NLTK", "user_name": "arqam", "text": "\nI am learning Natural Language Processing using NLTK.\nI came across the code using PunktSentenceTokenizer whose actual use I cannot understand in the given code. The code is given :\nimport nltk\nfrom nltk.corpus import state_union\nfrom nltk.tokenize import PunktSentenceTokenizer\n\ntrain_text = state_union.raw(\"2005-GWBush.txt\")\nsample_text = state_union.raw(\"2006-GWBush.txt\")\n\ncustom_sent_tokenizer = PunktSentenceTokenizer(train_text) #A\n\ntokenized = custom_sent_tokenizer.tokenize(sample_text)   #B\n\ndef process_content():\ntry:\n    for i in tokenized[:5]:\n        words = nltk.word_tokenize(i)\n        tagged = nltk.pos_tag(words)\n        print(tagged)\n\nexcept Exception as e:\n    print(str(e))\n\n\nprocess_content()\n\nSo, why do we use PunktSentenceTokenizer.  And what is going on in the line marked A and B. I mean there is a training text and the other a sample text, but what is the need for two data sets to get the Part of Speech tagging.\nLine marked as A and B is which I am not able to understand.\nPS : I did try to look in the NLTK book but could not understand what is the real use of PunktSentenceTokenizer\n"}, "1332": {"topic": "Use of PunktSentenceTokenizer in NLTK", "user_name": "arqamarqam", "text": "\nPunktSentenceTokenizer is the abstract class for the default sentence tokenizer, i.e. sent_tokenize(), provided in NLTK. It is an implmentation of Unsupervised Multilingual Sentence\nBoundary Detection (Kiss and Strunk (2005). See https://github.com/nltk/nltk/blob/develop/nltk/tokenize/init.py#L79\nGiven a paragraph with multiple sentence, e.g:\n>>> from nltk.corpus import state_union\n>>> train_text = state_union.raw(\"2005-GWBush.txt\").split('\\n')\n>>> train_text[11]\nu'Two weeks ago, I stood on the steps of this Capitol and renewed the commitment of our nation to the guiding ideal of liberty for all. This evening I will set forth policies to advance that ideal at home and around the world. '\n\nYou can use the sent_tokenize():\n>>> sent_tokenize(train_text[11])\n[u'Two weeks ago, I stood on the steps of this Capitol and renewed the commitment of our nation to the guiding ideal of liberty for all.', u'This evening I will set forth policies to advance that ideal at home and around the world. ']\n>>> for sent in sent_tokenize(train_text[11]):\n...     print sent\n...     print '--------'\n... \nTwo weeks ago, I stood on the steps of this Capitol and renewed the commitment of our nation to the guiding ideal of liberty for all.\n--------\nThis evening I will set forth policies to advance that ideal at home and around the world. \n--------\n\nThe sent_tokenize() uses a pre-trained model from nltk_data/tokenizers/punkt/english.pickle. You can also specify other languages, the list of available languages with pre-trained models in NLTK are:\nalvas@ubi:~/nltk_data/tokenizers/punkt$ ls\nczech.pickle     finnish.pickle  norwegian.pickle   slovene.pickle\ndanish.pickle    french.pickle   polish.pickle      spanish.pickle\ndutch.pickle     german.pickle   portuguese.pickle  swedish.pickle\nenglish.pickle   greek.pickle    PY3                turkish.pickle\nestonian.pickle  italian.pickle  README\n\nGiven a text in another language, do this:\n>>> german_text = u\"Die Orgellandschaft S\u00fcdniedersachsen umfasst das Gebiet der Landkreise Goslar, G\u00f6ttingen, Hameln-Pyrmont, Hildesheim, Holzminden, Northeim und Osterode am Harz sowie die Stadt Salzgitter. \u00dcber 70 historische Orgeln vom 17. bis 19. Jahrhundert sind in der s\u00fcdnieders\u00e4chsischen Orgellandschaft vollst\u00e4ndig oder in Teilen erhalten. \"\n\n>>> for sent in sent_tokenize(german_text, language='german'):\n...     print sent\n...     print '---------'\n... \nDie Orgellandschaft S\u00fcdniedersachsen umfasst das Gebiet der Landkreise Goslar, G\u00f6ttingen, Hameln-Pyrmont, Hildesheim, Holzminden, Northeim und Osterode am Harz sowie die Stadt Salzgitter.\n---------\n\u00dcber 70 historische Orgeln vom 17. bis 19. Jahrhundert sind in der s\u00fcdnieders\u00e4chsischen Orgellandschaft vollst\u00e4ndig oder in Teilen erhalten. \n---------\n\nTo train your own punkt model, see https://github.com/nltk/nltk/blob/develop/nltk/tokenize/punkt.py and training data format for nltk punkt\n"}, "1333": {"topic": "Use of PunktSentenceTokenizer in NLTK", "user_name": "CommunityBot", "text": "\nPunktSentenceTokenizer is an sentence boundary detection algorithm that must be trained to be used [1]. NLTK already includes a pre-trained version of the PunktSentenceTokenizer. \nSo if you use initialize the tokenizer without any arguments, it will default to the pre-trained version:\nIn [1]: import nltk\nIn [2]: tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\nIn [3]: txt = \"\"\" This is one sentence. This is another sentence.\"\"\"\nIn [4]: tokenizer.tokenize(txt)\nOut[4]: [' This is one sentence.', 'This is another sentence.']\n\nYou can also provide your own training data to train the tokenizer before using it. Punkt tokenizer uses an unsupervised algorithm, meaning you just train it with regular text.\ncustom_sent_tokenizer = PunktSentenceTokenizer(train_text)\nFor most of the cases, it is totally fine to use the pre-trained version. So you can simply initialize the tokenizer without providing any arguments.\nSo \"what all this has to do with POS tagging\"? The NLTK POS tagger works with tokenized sentences, so you need to break your text into sentences and word tokens before you can POS tag.\nNLTK's documentation.\n[1] Kiss and Strunk, \"\nUnsupervised Multilingual Sentence Boundary Detection\"  \n"}, "1334": {"topic": "Use of PunktSentenceTokenizer in NLTK", "user_name": "alvasalvas", "text": "\nYou can refer below link to get more insight on usage of PunktSentenceTokenizer.\nIt vividly explains why PunktSentenceTokenizer is used instead of sent-tokenize() with regard to your case.\nhttp://nlpforhackers.io/splitting-text-into-sentences/\n"}, "1335": {"topic": "Use of PunktSentenceTokenizer in NLTK", "user_name": "", "text": "\ndef process_content(corpus):\n\n    tokenized = PunktSentenceTokenizer().tokenize(corpus)\n\n    try:\n        for sent in tokenized:\n            words = nltk.word_tokenize(sent)\n            tagged = nltk.pos_tag(words)\n            print(tagged)\n    except Exception as e:\n        print(str(e))\n\nprocess_content(train_text)\n\nWithout even training it on other text data it works the same as it is pre-trained.\n"}, "1336": {"topic": "What is \"entropy and information gain\"?", "user_name": "rayryeng", "text": "\nI am reading this book (NLTK) and it is confusing.  Entropy is defined as:\n\nEntropy is the sum of the probability of each label\n  times the log probability of that same label\n\nHow can I apply entropy and maximum entropy in terms of text mining?  Can someone give me a easy, simple example (visual)?\n"}, "1337": {"topic": "What is \"entropy and information gain\"?", "user_name": "TIMEXTIMEX", "text": "\nI assume entropy was mentioned in the context of building decision trees.\nTo illustrate, imagine the task of learning to classify first-names into male/female groups. That is given a list of names each labeled with either m or f, we want to learn a model that fits the data and can be used to predict the gender of a new unseen first-name.\nname       gender\n-----------------        Now we want to predict \nAshley        f              the gender of \"Amro\" (my name)\nBrian         m\nCaroline      f\nDavid         m\n\nFirst step is deciding what features of the data are relevant to the target class we want to predict. Some example features include: first/last letter, length, number of vowels, does it end with a vowel, etc.. So after feature extraction, our data looks like:\n# name    ends-vowel  num-vowels   length   gender\n# ------------------------------------------------\nAshley        1         3           6        f\nBrian         0         2           5        m\nCaroline      1         4           8        f\nDavid         0         2           5        m\n\nThe goal is to build a decision tree. An example of a tree would be:\nlength<7\n|   num-vowels<3: male\n|   num-vowels>=3\n|   |   ends-vowel=1: female\n|   |   ends-vowel=0: male\nlength>=7\n|   length=5: male\n\nbasically each node represent a test performed on a single attribute, and we go left or right depending on the result of the test. We keep traversing the tree until we reach a leaf node which contains the class prediction (m or f)\nSo if we run the name Amro down this tree, we start by testing \"is the length<7?\" and the answer is yes, so we go down that branch. Following the branch, the next test \"is the number of vowels<3?\" again evaluates to true. This leads to a leaf node labeled m, and thus the prediction is male (which I happen to be, so the tree predicted the outcome correctly).\nThe decision tree is built in a top-down fashion, but the question is how do you choose which attribute to split at each node? The answer is find the feature that best splits the target class into the purest possible children nodes (ie: nodes that don't contain a mix of both male and female, rather pure nodes with only one class).\nThis measure of purity is called the information. It represents the expected amount of information that would be needed to specify whether a new instance (first-name) should be classified male or female, given the example that reached the node. We calculate it\nbased on the number of male and female classes at the node.\nEntropy on the other hand is a measure of impurity (the opposite). It is defined for a binary class with values a/b as:\nEntropy = - p(a)*log(p(a)) - p(b)*log(p(b))\n\nThis binary entropy function is depicted in the figure below (random variable can take one of two values). It reaches its maximum when the probability is p=1/2, meaning that p(X=a)=0.5 or similarlyp(X=b)=0.5 having a 50%/50% chance of being either a or b (uncertainty is at a maximum). The entropy function is at zero minimum when probability is p=1 or p=0 with complete certainty (p(X=a)=1 or p(X=a)=0 respectively, latter implies p(X=b)=1).\n\nOf course the definition of entropy can be generalized for a discrete random variable X with N outcomes (not just two):\n\n(the log in the formula is usually taken as the logarithm to the base 2)\n\nBack to our task of name classification, lets look at an example. Imagine at some point during the process of constructing the tree, we were considering the following split:\n     ends-vowel\n      [9m,5f]          <--- the [..,..] notation represents the class\n    /          \\            distribution of instances that reached a node\n   =1          =0\n -------     -------\n [3m,4f]     [6m,1f]\n\nAs you can see, before the split we had 9 males and 5 females, i.e. P(m)=9/14 and P(f)=5/14. According to the definition of entropy:\nEntropy_before = - (5/14)*log2(5/14) - (9/14)*log2(9/14) = 0.9403\n\nNext we compare it with the entropy computed after considering the split by looking at two child branches. In the left branch of ends-vowel=1, we have:\nEntropy_left = - (3/7)*log2(3/7) - (4/7)*log2(4/7) = 0.9852\n\nand the right branch of ends-vowel=0, we have:\nEntropy_right = - (6/7)*log2(6/7) - (1/7)*log2(1/7) = 0.5917\n\nWe combine the left/right entropies using the number of instances down each branch as weight factor (7 instances went left, and 7 instances went right), and get the final entropy after the split:\nEntropy_after = 7/14*Entropy_left + 7/14*Entropy_right = 0.7885\n\nNow by comparing the entropy before and after the split, we obtain a measure of information gain, or how much information we gained by doing the split using that particular feature:\nInformation_Gain = Entropy_before - Entropy_after = 0.1518\n\nYou can interpret the above calculation as following: by doing the split with the end-vowels feature, we were able to reduce uncertainty in the sub-tree prediction outcome by a small amount of 0.1518 (measured in bits as units of information).\nAt each node of the tree, this calculation is performed for every feature, and the feature with the largest information gain is chosen for the split in a greedy manner (thus favoring features that produce pure splits with low uncertainty/entropy). This process is applied recursively from the root-node down, and stops when a leaf node contains instances all having the same class (no need to split it further).\nNote that I skipped over some details which are beyond the scope of this post, including how to handle numeric features, missing values, overfitting and pruning trees, etc..\n"}, "1338": {"topic": "What is \"entropy and information gain\"?", "user_name": "tne", "text": "\nTo begin with, it would be best to understand the measure of information. \nHow do we measure the information?\nWhen something unlikely happens, we say it's a big news. Also, when we say something predictable, it's not really interesting. So to quantify this interesting-ness, the function should satisfy\n\nif the probability of the event is 1 (predictable), then the function gives 0\nif the probability of the event is close to 0, then the function should give high number\nif probability 0.5 events happens it give one bit of information.\n\nOne natural measure that satisfy the constraints is\nI(X) = -log_2(p)\n\nwhere p is the probability of the event X. And the unit is in bit, the same bit computer uses. 0 or 1.\nExample 1\nFair coin flip : \nHow much information can we get from one coin flip?\nAnswer : -log(p) = -log(1/2) = 1 (bit)\nExample 2\nIf a meteor strikes the Earth tomorrow, p=2^{-22} then we can get 22 bits of information.\nIf the Sun rises tomorrow, p ~ 1 then it is 0 bit of information.\nEntropy\nSo if we take expectation on the interesting-ness of an event Y, then it is the entropy.\ni.e. entropy is an expected value of the interesting-ness of an event.\nH(Y) = E[ I(Y)]\n\nMore formally, the entropy is the expected number of bits of an event.\nExample\nY = 1 : an event X occurs with probability p\nY = 0 : an event X does not occur with probability 1-p\nH(Y) = E[I(Y)] = p I(Y==1) + (1-p) I(Y==0) \n     = - p log p - (1-p) log (1-p)\n\nLog base 2 for all log.\n"}, "1339": {"topic": "What is \"entropy and information gain\"?", "user_name": "AmroAmro", "text": "\nI can't give you graphics, but maybe I can give a clear explanation.\nSuppose we have an information channel, such as a light that flashes once every day either red or green. How much information does it convey? The first guess might be one bit per day. But what if we add blue, so that the sender has three options? We would like to have a measure of information that can handle things other than powers of two, but still be additive (the way that multiplying the number of possible messages by two adds one bit). We could do this by taking log2(number of possible messages), but it turns out there's a more general way.\nSuppose we're back to red/green, but the red bulb has burned out (this is common knowledge) so that the lamp must always flash green. The channel is now useless, we know what the next flash will be so the flashes convey no information, no news. Now we repair the bulb but impose a rule that the red bulb may not flash twice in a row. When the lamp flashes red, we know what the next flash will be. If you try to send a bit stream by this channel, you'll find that you must encode it with more flashes than you have bits (50% more, in fact). And if you want to describe a sequence of flashes, you can do so with fewer bits. The same applies if each flash is independent (context-free), but green flashes are more common than red: the more skewed the probability the fewer bits you need to describe the sequence, and the less information it contains, all the way to the all-green, bulb-burnt-out limit.\nIt turns out there's a way to measure the amount of information in a signal, based on the the probabilities of the different symbols. If the probability of receiving symbol xi is pi, then consider the quantity\n\n-log pi\n\nThe smaller pi, the larger this value. If xi becomes twice as unlikely, this value increases by a fixed amount (log(2)). This should remind you of adding one bit to a message.\nIf we don't know what the symbol will be (but we know the probabilities) then we can calculate the average of this value, how much we will get, by summing over the different possibilities:\n\nI = -\u03a3 pi log(pi)\n\nThis is the information content in one flash.\n\nRed bulb burnt out: pred = 0, pgreen=1, I = -(0 + 0)  = 0\nRed and green equiprobable: pred = 1/2, pgreen = 1/2, I = -(2 * 1/2 * log(1/2)) = log(2)\nThree colors, equiprobable: pi=1/3, I = -(3 * 1/3 * log(1/3)) = log(3)\nGreen and red, green twice as likely: pred=1/3, pgreen=2/3, I = -(1/3 log(1/3) + 2/3 log(2/3)) = log(3) - 2/3 log(2)\n\nThis is the information content, or entropy, of the message. It is maximal when the different symbols are equiprobable. If you're a physicist you use the natural log, if you're a computer scientist you use log2 and get bits.\n"}, "1340": {"topic": "What is \"entropy and information gain\"?", "user_name": "", "text": "\nI really recommend you read about Information Theory, bayesian methods and MaxEnt. The place to start is this (freely available online) book by David Mackay:\nhttp://www.inference.phy.cam.ac.uk/mackay/itila/\nThose inference methods are really far more general than just text mining and I can't really devise how one would learn how to apply this to NLP without learning some of the general basics contained in this book or other introductory books on Machine Learning and MaxEnt bayesian methods.\nThe connection between entropy and probability theory to information processing and storing is really, really deep. To give a taste of it, there's a theorem due to Shannon that states that the maximum amount of information you can pass without error through a noisy communication channel is equal to the entropy of the noise process. There's also a theorem that connects how much you can compress a piece of data to occupy the minimum possible memory in your computer to the entropy of the process that generated the data.\nI don't think it's really necessary that you go learning about all those theorems on communication theory, but it's not possible to learn this without learning the basics about what is entropy, how it's calculated, what is it's relationship with information and inference, etc...\n"}, "1341": {"topic": "What is \"entropy and information gain\"?", "user_name": "VforVitaminVforVitamin", "text": "\nInformally\nentropy is availability of information or knowledge, Lack of information will leads to difficulties in prediction of future which is high entropy (next word prediction in text mining) and availability of information/knowledge will help us more realistic prediction of future (low entropy).\nRelevant information of any type will reduce entropy and helps us predict more realistic future, that information can be word \"meat\" is present in sentence or word \"meat\" is not present. This is called Information Gain\n\nFormally\nentropy is lack of order of predicability\n"}, "1342": {"topic": "What is \"entropy and information gain\"?", "user_name": "BetaBeta", "text": "\nWhen I was implementing an algorithm to calculate the entropy of an image I found these links, see here and here.\nThis is the pseudo-code I used, you'll need to adapt it to work with text rather than images but the principles should be the same.\n//Loop over image array elements and count occurrences of each possible\n//pixel to pixel difference value. Store these values in prob_array\nfor j = 0, ysize-1 do $\n    for i = 0, xsize-2 do begin\n       diff = array(i+1,j) - array(i,j)\n       if diff lt (array_size+1)/2 and diff gt -(array_size+1)/2 then begin\n            prob_array(diff+(array_size-1)/2) = prob_array(diff+(array_size-1)/2) + 1\n       endif\n     endfor\n\n//Convert values in prob_array to probabilities and compute entropy\nn = total(prob_array)\n\nentrop = 0\nfor i = 0, array_size-1 do begin\n    prob_array(i) = prob_array(i)/n\n\n    //Base 2 log of x is Ln(x)/Ln(2). Take Ln of array element\n    //here and divide final sum by Ln(2)\n    if prob_array(i) ne 0 then begin\n        entrop = entrop - prob_array(i)*alog(prob_array(i))\n    endif\nendfor\n\nentrop = entrop/alog(2)\n\nI got this code from somewhere, but I can't dig out the link.\n"}, "1343": {"topic": "What is \"entropy and information gain\"?", "user_name": "Rafael S. CalsaveriniRafael S. Calsaverini", "text": "\nAs you are reading a book about NLTK it would be interesting you read about MaxEnt Classifier Module http://www.nltk.org/api/nltk.classify.html#module-nltk.classify.maxent\nFor text mining classification the steps could be: pre-processing (tokenization, steaming, feature selection with Information Gain ...), transformation to numeric (frequency or TF-IDF) (I think that this is the key step to understand when using text as input to a algorithm that only accept numeric) and then classify with MaxEnt, sure this is just an example.\n"}, "1344": {"topic": "Import NLTK : no module NLTK corpus", "user_name": "John Vandenberg", "text": "\nI have installed NLTK. Here's an image of the installation log.\n\nWhen i use import nltk i get an error:\n\n\"No module named NLTK.corpus\"\n\nHere is a screenshot.\n\nWhat could be the cause?\n"}, "1345": {"topic": "Import NLTK : no module NLTK corpus", "user_name": "manishamanisha", "text": "\nI think I had the same problem. So, downloading all the packages at once (since question didn't specify).\nStart python and then import the packages, exit python and upgrade nltk. Modify the 'all' to download a specific corpus. Took me awhile to complete the 'all' download, I separately downloaded framenet_v15 and restarted the 'all' after. Upgrade nltk when the download is complete.\n$ python\n>>>import nltk\n>>>nltk.download('all')\n\nexit python\n$ pip install --upgrade nltk\n\n"}, "1346": {"topic": "Import NLTK : no module NLTK corpus", "user_name": "Derek_PDerek_P", "text": "\nTo fix this, you should rename your file to something else, say nltkXXX.py. Also make sure to remove \"nltk.pyc\" from your directory if it exists, since this will also be loaded (it's the byte compiled version of your code). After that, it should work fine.\n"}, "1347": {"topic": "Import NLTK : no module NLTK corpus", "user_name": "EdwardEdward", "text": "\nIf you are using the latest version of python, then try installing nltk using pip and the wheel downloaded from here: \n    http://www.lfd.uci.edu/~gohlke/pythonlibs/\nThen in command prompt, use the command:\n    pip3 install \nThis should install nltk correctly.\nAfter that check the installation in python using the command:\n    import nltk\nand download the nltk data required using:\n    nltk.download()\n"}, "1348": {"topic": "Import NLTK : no module NLTK corpus", "user_name": "AnujAnuj", "text": "\nIf you find (Import NLTK : no module NLTK corpus) that type of error .\nMake sure your saved file not be the name like (nltk.py).\nso just rename your file name (like rename nltk.py to example.py ) or something else:\nI hope it will help you.\nthanks\n"}, "1349": {"topic": "Import NLTK : no module NLTK corpus", "user_name": "", "text": "\nIf you has using PyCharm IDE, you should have install NLTK from the IDE's own tools [File -> Settings -> Projetct Interpreter -> Install (button '+') -> Install Package].\n"}, "1350": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "user1502248user1502248", "text": "\nI'm trying to extract named entities from my text using NLTK. I find that NLTK NER is not very accurate for my purpose and I want to add some more tags of my own as well. I've been trying to find a way to train my own NER, but I don't seem to be able to find the right resources. \nI have a couple of questions regarding NLTK-\n\nCan I use my own data to train an Named Entity Recognizer in NLTK?\nIf I can train using my own data, is the named_entity.py the file to be modified?\nDoes the input file format have to be in IOB eg. Eric NNP B-PERSON ?\nAre there any resources - apart from the nltk cookbook and nlp with python that I can use?\n\nI would really appreciate help in this regard\n"}, "1351": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "jjdubsjjdubs", "text": "\nAre you committed to using NLTK/Python?  I ran into the same problems as you, and had much better results using Stanford's named-entity recognizer: http://nlp.stanford.edu/software/CRF-NER.shtml.  The process for training the classifier using your own data is very well-documented in the FAQ.  \nIf you really need to use NLTK, I'd hit up the mailing list for some advice from other users: http://groups.google.com/group/nltk-users.  \nHope this helps!\n"}, "1352": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "Rohan AmruteRohan Amrute", "text": "\nYou can easily use the Stanford NER alongwith nltk.\nThe python script is like\nfrom nltk.tag.stanford import NERTagger\nimport os\njava_path = \"/Java/jdk1.8.0_45/bin/java.exe\"\nos.environ['JAVAHOME'] = java_path\nst = NERTagger('../ner-model.ser.gz','../stanford-ner.jar')\ntagging = st.tag(text.split())   \n\nTo train your own data and to create a model you can refer to the first question on Stanford NER FAQ.\nThe link is http://nlp.stanford.edu/software/crf-faq.shtml\n"}, "1353": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "aroparop", "text": "\nI also had this issue, but I managed to work it out. \nYou can use your own training data. I documented the main requirements/steps for this in my github repository.\nI used NLTK-trainer, so basicly you have to get the training data in the right format (token NNP B-tag), and run the training script. Check my repository for more info.\n"}, "1354": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "xjixji", "text": "\nThere are some functions in the nltk.chunk.named_entity module that train a NER tagger. However, they were specifically written for ACE corpus and not totally cleaned up, so one will need to write their own training procedures with those as a reference.\nThere are also two relatively recent guides (1 2) online detailing the process of using NLTK to train the GMB corpus.\nHowever, as mentioned in answers above, now that many tools are available, one really should not need to resort to NLTK if streamlined training process is desired. Toolkits such as CoreNLP and spaCy do a much better job. As using NLTK is not that much different to writing your own training code from scratch, there is not that much value in doing so. NLTK and OpenNLP can be regarded as somehow belonging to a past era before the explosion of recent progress in NLP.\n"}, "1355": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "Thang PhamThang Pham", "text": "\n\n\nAre there any resources - apart from the nltk cookbook and nlp with python that I can use?\n\n\nYou can consider using spaCy to train your own custom data for NER task. Here is an example from this thread to train a model on a custom training set to detect a new entity ANIMAL. The code was fixed and updated for easier reading.\nimport random\nimport spacy\nfrom spacy.training import Example\n\nLABEL = 'ANIMAL'\nTRAIN_DATA = [\n    (\"Horses are too tall and they pretend to care about your feelings\", {'entities': [(0, 6, LABEL)]}),\n    (\"Do they bite?\", {'entities': []}),\n    (\"horses are too tall and they pretend to care about your feelings\", {'entities': [(0, 6, LABEL)]}),\n    (\"horses pretend to care about your feelings\", {'entities': [(0, 6, LABEL)]}),\n    (\"they pretend to care about your feelings, those horses\", {'entities': [(48, 54, LABEL)]}),\n    (\"horses?\", {'entities': [(0, 6, LABEL)]})\n]\nnlp = spacy.load('en_core_web_sm')  # load existing spaCy model\nner = nlp.get_pipe('ner')\nner.add_label(LABEL)\n\noptimizer = nlp.create_optimizer()\n\n# get names of other pipes to disable them during training\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\nwith nlp.disable_pipes(*other_pipes):  # only train NER\n    for itn in range(20):\n        random.shuffle(TRAIN_DATA)\n        losses = {}\n        for text, annotations in TRAIN_DATA:\n            doc = nlp.make_doc(text)\n            example = Example.from_dict(doc, annotations)\n            nlp.update([example], drop=0.35, sgd=optimizer, losses=losses)\n        print(losses)\n\n# test the trained model\ntest_text = 'Do you like horses?'\ndoc = nlp(test_text)\nprint(\"Entities in '%s'\" % test_text)\nfor ent in doc.ents:\n    print(ent.label_, \" -- \", ent.text)\n\nHere are the outputs:\n{'ner': 9.60289144264557}\n{'ner': 8.875474230820478}\n{'ner': 6.370401408220459}\n{'ner': 6.687456469517201}\n... \n{'ner': 1.3796682589133492e-05}\n{'ner': 1.7709562613218738e-05}\n\nEntities in 'Do you like horses?'\nANIMAL  --  horses\n\n"}, "1356": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "iEriiiiEriii", "text": "\nTo complete the answer by @Thang M. Pham, you need to label your data before training. To do so, you can use the spacy-annotator.\nHere is an example taken from another answer:\nTrain Spacy NER on Indian Names\n"}, "1357": {"topic": "NLTK and language detection", "user_name": "John Vandenberg", "text": "\nHow do I detect what language a text is written in using NLTK?\nThe examples I've seen use nltk.detect, but when I've installed it on my mac, I cannot find this package.\n"}, "1358": {"topic": "NLTK and language detection", "user_name": "niklassaersniklassaers", "text": "\nHave you come across the following code snippet?\nenglish_vocab = set(w.lower() for w in nltk.corpus.words.words())\ntext_vocab = set(w.lower() for w in text if w.lower().isalpha())\nunusual = text_vocab.difference(english_vocab) \n\nfrom http://groups.google.com/group/nltk-users/browse_thread/thread/a5f52af2cbc4cfeb?pli=1&safe=active\nOr the following demo file?\nhttps://web.archive.org/web/20120202055535/http://code.google.com/p/nltk/source/browse/trunk/nltk_contrib/nltk_contrib/misc/langid.py\n"}, "1359": {"topic": "NLTK and language detection", "user_name": "Mark Cramer", "text": "\nThis library is not from NLTK either but certainly helps. \n\n$ sudo pip install langdetect \n\nSupported Python versions 2.6, 2.7, 3.x.\n>>> from langdetect import detect\n\n>>> detect(\"War doesn't show who's right, just who's left.\")\n'en'\n>>> detect(\"Ein, zwei, drei, vier\")\n'de'\n\nhttps://pypi.python.org/pypi/langdetect?\nP.S.: Don't expect this to work correctly always:\n>>> detect(\"today is a good day\")\n'so'\n>>> detect(\"today is a good day.\")\n'so'\n>>> detect(\"la vita e bella!\")\n'it'\n>>> detect(\"khoobi? khoshi?\")\n'so'\n>>> detect(\"wow\")\n'pl'\n>>> detect(\"what a day\")\n'en'\n>>> detect(\"yay!\")\n'so'\n\n"}, "1360": {"topic": "NLTK and language detection", "user_name": "William NiuWilliam Niu", "text": "\nAlthough this is not in the NLTK, I have had great results with another Python-based library : \nhttps://github.com/saffsd/langid.py\nThis is very simple to import and includes a large number of languages in its model.\n"}, "1361": {"topic": "NLTK and language detection", "user_name": "Mona Jalal", "text": "\nSuper late but, you could use textcat classifier in nltk, here. This paper discusses the algorithm. \nIt returns a country code in ISO 639-3, so I would use pycountry to get the  full name.\nFor example, load the libraries\nimport nltk\nimport pycountry\nfrom nltk.stem import SnowballStemmer\n\nNow let's look at two phrases, and guess their language:\nphrase_one = \"good morning\"\nphrase_two = \"goeie more\"\n\ntc = nltk.classify.textcat.TextCat() \nguess_one = tc.guess_language(phrase_one)\nguess_two = tc.guess_language(phrase_two)\n\nguess_one_name = pycountry.languages.get(alpha_3=guess_one).name\nguess_two_name = pycountry.languages.get(alpha_3=guess_two).name\nprint(guess_one_name)\nprint(guess_two_name)\n\nEnglish\nAfrikaans\n\nYou could then pass them into other nltk functions, for example:\nstemmer = SnowballStemmer(guess_one_name.lower())\ns1 = \"walking\"\nprint(stemmer.stem(s1))\nwalk\n\nDisclaimer obviously this will not always work, especially for sparse data\nExtreme example\nguess_example = tc.guess_language(\"hello\")\nprint(pycountry.languages.get(alpha_3=guess_example).name)\nKonkani (individual language)\n\n"}, "1362": {"topic": "NLTK and language detection", "user_name": "SVKSVK", "text": "\npolyglot.detect can detect the language:\nfrom polyglot.detect import Detector\n\nforeign = 'Este libro ha sido uno de los mejores libros que he leido.'\nprint(Detector(foreign).language)\n\nname: Spanish     code: es       confidence:  98.0 read bytes:   865\n\n"}, "1363": {"topic": "Error installing NLTK Python", "user_name": "jasonjason", "text": "\nI am trying to install NLTK (https://pypi.python.org/pypi/nltk).  I have Python 3.6 installed on my Windows 10 (64 bit) computer.  When I run the NLTK installer, I get the following error:\n\"Python version -32 required, which was not found in the registry\"\nDoes anyone have any experience with this or know how to resolve the error?\n"}, "1364": {"topic": "Error installing NLTK Python", "user_name": "TakuTaku", "text": "\nNltk itself is os independent, but the Windows msi installer is not, it's specifically for 32-bits pythons. Alternatively, you can use pip to install nltk, which will install the os independent source file. Simply in cmd, type this:\npip3 install nltk\n# pip/pip3 doesn't matter only if there's multiple pythons, but if that does not work (command not found) type:\npy -3 -m pip install nltk\n\n"}, "1365": {"topic": "Error installing NLTK Python", "user_name": "Priscilla SimPriscilla Sim", "text": "\nThis works for me:\npy -m pip install nltk \n"}, "1366": {"topic": "Error installing NLTK Python", "user_name": "GenieGenie", "text": "\nI found the issue and was able to solved my problem:\nWindows 10 users with Python 64 bit might encounter a RuntimeError when trying to run import nltk.  A recent Windows 10 update has a known bug when running the most recent version of NumPy 1.19.4 on the Python 64 bit version.\nSolution: uninstall NumPy version 1.19.4 and reinstall 1.19.3.\nFrom the command prompt:\npip uninstall numpy\npip install numpy==1.19.3\nIf you are running a Mac and/or Python 32 bit the import nltk command should work fine.\nFor more information on the Windows bug: https://developercommunity.visualstudio.com/content/problem/1207405/fmod-after-an-update-to-windows-2004-is-causing-a.html\nBest,\nGenie\n"}, "1367": {"topic": "Error installing NLTK Python", "user_name": "Code_Like_a_NewbieCode_Like_a_Newbie", "text": "\nafter running\npy -m pip install nltk   as suggested by Priscilla's comment above\nI close my VS code and reopen it and everything is working !\n"}, "1368": {"topic": "nltk NgramModel error", "user_name": "istewartistewart", "text": "\nI've previously used the NgramModel class in nltk without error. However, I recently updated nltk to version 3.1 and I can no longer find the NgramModel class. When I try to import as usual:  \nfrom nltk.model import NgramModel\nI get the error\nImportError: No module named 'nltk.model'.\nHas NgramModel or the model module been recently replaced?\n"}, "1369": {"topic": "nltk NgramModel error", "user_name": "eriperip", "text": "\nThis is an open issue because of bugs.\nThis is noted in the issue:\n\nIf you're currently using the version from github, you can switch to the \"model\" branch, which includes the NgramModel code, though it's currently significantly behind the \"develop\" branch and hasn't picked up all the newest bug fixes.\n\nThe link to the model branch is here.\n"}, "1370": {"topic": "NLTK for Named Entity Recognition", "user_name": "Franck Dernoncourt", "text": "\nI am trying to use NLTK toolkit to get extract place, date and time from text messages. I just installed the toolkit on my machine and I wrote this quick snippet to test it out:\nsentence = \"Let's meet tomorrow at 9 pm\";\ntokens = nltk.word_tokenize(sentence)\npos_tags = nltk.pos_tag(tokens)\nprint nltk.ne_chunk(pos_tags, binary=True)\n\nI was assuming that it will identify the date (tomorrow) and time (9 pm). But, surprisingly it failed to recognize that. I get the following result when I run my above code:\n(S (GPE Let/NNP) 's/POS meet/NN tomorrow/NN at/IN 9/CD pm/NN)\n\nCan someone help me understand if I am missing something or NLTK is just not mature enough to tag time and date properly. Thanks!\n"}, "1371": {"topic": "NLTK for Named Entity Recognition", "user_name": "Darth.VaderDarth.Vader", "text": "\nThe default NE chunker in nltk is a maximum entropy chunker trained on the ACE corpus (http://catalog.ldc.upenn.edu/LDC2005T09). It has not been trained to recognise dates and times, so you need to train your own classifier if you want to do that. \nHave a look at http://mattshomepage.com/articles/2016/May/23/nltk_nec/, the whole process is explained very well. \nAlso, there is a module called timex in nltk_contrib which might help you with your needs. https://github.com/nltk/nltk_contrib/blob/master/nltk_contrib/timex.py\n"}, "1372": {"topic": "NLTK for Named Entity Recognition", "user_name": "", "text": "\nNamed entity recognition is not an easy problem, do not expect any library to be 100% accurate. You shouldn't make any conclusions about NLTK's performance based on one sentence. Here's another example:\nsentence = \"I went to New York to meet John Smith\";\n\nI get\n(S\n  I/PRP\n  went/VBD\n  to/TO\n  (NE New/NNP York/NNP)\n  to/TO\n  meet/VB\n  (NE John/NNP Smith/NNP))\n\nAs you can see, NLTK does very well here. However, I couldn't get NLTK to recognise today or tomorrow as temporal expressions. You can try Stanford SUTime, it is a part of Stanford CoreNLP- I have used it before I it works quite well (it is in Java though).\n"}, "1373": {"topic": "NLTK for Named Entity Recognition", "user_name": "Viktor VojnovskiViktor Vojnovski", "text": "\nIf you wish to correctly identify the date or time from the text messages you can use Stanford's NER. \nIt uses the CRF(Conditional Random Fields) Classifier. CRF is a sequential classifier. So it takes the sequences of words into consideration.\nHow you frame or design a sentence, accordingly you will get the classified data.\nIf your input sentence would have been Let's meet on wednesday at 9am., then Stanford NER would have correctly identified wednesday as date and 9am as time.\nNLTK supports Stanford NER. Try using it.\n"}, "1374": {"topic": "Error importing NLTK on PyCharm", "user_name": "TKR", "text": "\nI'm trying to import NLTK in PyCharm, and get the following error. I'm on Mac OS 10.5.8 with Python 2.7.6. What could be going on? I'm completely new to programming, so sorry if there's something basic that I'm missing.\nInstall packages failed: Error occurred when installing package nltk. \nThe following command was executed:\n\npackaging_tool.py install --build-dir /private/var/folders/NG/NGoQZknvH94yHKezwiiT+k+++TI/-Tmp-/pycharm-packaging3166068946358630595.tmp nltk\n\nThe error output of the command:\nDownloading/unpacking nltk\nCould not fetch URL https://pypi.python.org/simple/nltk/: There was a problem confirming the ssl certificate: <urlopen error [Errno 1] _ssl.c:507: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed>\nWill skip URL https://pypi.python.org/simple/nltk/ when looking for download links for nltk\nCould not fetch URL https://pypi.python.org/simple/: There was a problem confirming the ssl certificate: <urlopen error [Errno 1] _ssl.c:507: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed>\nWill skip URL https://pypi.python.org/simple/ when looking for download links for nltk\nCannot fetch index base URL https://pypi.python.org/simple/\nCould not fetch URL https://pypi.python.org/simple/nltk/: There was a problem confirming the ssl certificate: <urlopen error [Errno 1] _ssl.c:507: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed>\nWill skip URL https://pypi.python.org/simple/nltk/ when looking for download links for nltk\nCould not find any downloads that satisfy the requirement nltk\nCleaning up...\nNo distributions at all found for nltk\nStoring complete log in /Users/Tom/.pip/pip.log\n\nETA: OK, now I've successfully installed NLTK from the command line, and then was able to install it in PyCharm -- but only for the Python 2.5.1 interpreter. If I try it with Python 2.7.6, I still get the error above. Does this matter, or should I not worry about it and just use it with 2.5.1?\n"}, "1375": {"topic": "Error importing NLTK on PyCharm", "user_name": "TKRTKR", "text": "\nYou'd be much better off sticking with the latest version of pip (1.5.6) and just telling it that you don't care about the security of your python packages:\npip install --allow-all-external --allow-unverified ntlk nltk\n\nIf you really want to be sure an install runs without complaint, you can also tell it not to overwrite any existing installations:\npip install --upgrade --force-reinstall --allow-all-external --allow-unverified ntlk nltk\nAnd sudo if you get file write permission errors.\n"}, "1376": {"topic": "Error importing NLTK on PyCharm", "user_name": "hobshobs", "text": "\nI use PyCharm but never install packages through PyCharm, I always use Terminal and install them with mostly pip or easy_install (in my virtual environment). Maybe you can just install the package from terminal..\nsudo pip install nltk (https://pypi.python.org/pypi/nltk)\nor \nsudo easy_install nltk (if you don't have pip installed)\nAnd then in PyCharm, make sure in preferences you set your Project Interpreter to the python path with your installed packages.\n"}, "1377": {"topic": "Error importing NLTK on PyCharm", "user_name": "LonosheaLonoshea", "text": "\nI have run into this (and just did again), I don't remember exactly where I found the answer, but it's an openssl version + local certificate issue (spoken like someone who's only vaguely familiar with the concepts).  The way I have worked around this is to downgrade pip:\neasy_install pip==1.2.1\n\nAfter that you should be able to pip install again. \n"}, "1378": {"topic": "Error importing NLTK on PyCharm", "user_name": "Matt SavoieMatt Savoie", "text": "\nhi import like this in pycharm :\n\nOpen File > Settings > Project from the PyCharm menu.\nSelect your current project.\nClick the Python Interpreter tab within your project tab.\nClick the small + symbol to add a new library to the project.\nNow type in the library to be installed, in your example \"nltk\" without quotes, and click Install Package.\nWait for the installation to terminate and close all popup windows.\n\n"}, "1379": {"topic": "Alternative source for nltk data", "user_name": "MortzMortz", "text": "\nI am trying to install the nltk corpora through these commands as mentioned in the documentation - \nimport nltk\nnltk.download()\n\nHowever, I am doing this from my stupid organization which has blocked github, which is what the download function above tries to connect to. \nIs there an alternate repository for the nltk data from where I can try this out? Trying to whitelist github and associated websites will only get tangled in red tape.\nThank you\n"}, "1380": {"topic": "Alternative source for nltk data", "user_name": "m00amm00am", "text": "\nYou can try downloading the Arch Linux package for nltk, which contains all the files you need. \n\nDownload the package from Archlinux packages website, using the Download from Mirror link in the Package Actions box on the right, or you can just use this link.\nExtract the file (it is an xzipped tar archive). I used ark on linux, not sure what is the appropriate software for your system (on windows 7zip and winrar should be able to handle this).\nYou find the files in the folder usr/share/nltk_data.\nMove the nltk_data folder to the appropriate path on your machine.\n\n"}, "1381": {"topic": "Alternative source for nltk data", "user_name": "John VandenbergJohn Vandenberg", "text": "\nThere was a brief period when GitHub actually blocked all fetches of nltk_data, resulting in issue 1787 which is still open and contains many workarounds, and plans to avoid relying on GitHub hosting.\nThe current 'official' answer is:\nPATH_TO_NLTK_DATA=/home/username/nltk_data/\nwget https://github.com/nltk/nltk_data/archive/gh-pages.zip\nunzip gh-pages.zip\nmv nltk_data-gh-pages/ $PATH_TO_NLTK_DATA\n\n"}, "1382": {"topic": "Alternative source for nltk data", "user_name": "John VandenbergJohn Vandenberg", "text": "\nDue to issue 1787 , I started building RPMs in openSUSE Build Service (OBS) repository home:jayvdb:nltk_data.\nFor example, for the punkt data, the .spec file is here.  It is very easy to copy that for other data packs.\nTo install from OBS on Fedora Rawhide:\ndnf config-manager --add-repo http://download.opensuse.org/repositories/home:jayvdb:nltk_data/Fedora_Rawhide/home:jayvdb:nltk_data.repo\ndnf install nltk-data-punkt\n\nMore download instructions available from the OBS download page.\n"}, "1383": {"topic": "Alternative source for nltk data", "user_name": "alexisalexis", "text": "\nThe layout of the nltk data is pretty straightforward. Run nltk.download() on a computer that has access to github, download the resources you are interested in (if you don't know yet, I recommend the \"book\" bundle), then find the generated nltk_data folder and just copy the hierarchy to your work computer at a location where the nltk can find it. (E.g., see where the downloader tried to install it).\n"}, "1384": {"topic": "counting n-gram frequency in python nltk", "user_name": "RkzRkz", "text": "\nI have the following code. I know that I can use apply_freq_filter function to filter out collocations that are less than a frequency count. However, I don't know how to get the frequencies of all the n-gram tuples (in my case bi-gram) in a document, before I decide what frequency to set for filtering. As you can see I am using the nltk collocations class.\nimport nltk\nfrom nltk.collocations import *\nline = \"\"\nopen_file = open('a_text_file','r')\nfor val in open_file:\n    line += val\ntokens = line.split()\n\nbigram_measures = nltk.collocations.BigramAssocMeasures()\nfinder = BigramCollocationFinder.from_words(tokens)\nfinder.apply_freq_filter(3)\nprint finder.nbest(bigram_measures.pmi, 100)\n\n"}, "1385": {"topic": "counting n-gram frequency in python nltk", "user_name": "", "text": "\nNLTK comes with its own bigrams generator, as well as a convenient FreqDist() function.\nf = open('a_text_file')\nraw = f.read()\n\ntokens = nltk.word_tokenize(raw)\n\n#Create your bigrams\nbgs = nltk.bigrams(tokens)\n\n#compute frequency distribution for all the bigrams in the text\nfdist = nltk.FreqDist(bgs)\nfor k,v in fdist.items():\n    print k,v\n\nOnce you have access to the BiGrams and the frequency distributions, you can filter according to your needs.\nHope that helps.\n"}, "1386": {"topic": "counting n-gram frequency in python nltk", "user_name": "Ram NarasimhanRam Narasimhan", "text": "\nThe finder.ngram_fd.viewitems() function works\n"}, "1387": {"topic": "counting n-gram frequency in python nltk", "user_name": "RkzRkz", "text": "\nI tried all the above and found a simpler solution. NLTK comes with a simple Most Common freq Ngrams.\nfiltered_sentence is my word tokens\nimport nltk\nfrom nltk.util import ngrams\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures\n\nword_fd = nltk.FreqDist(filtered_sentence)\nbigram_fd = nltk.FreqDist(nltk.bigrams(filtered_sentence))\n\nbigram_fd.most_common()\n\nThis should give the output as:\n[(('working', 'hours'), 31),\n (('9', 'hours'), 14),\n (('place', 'work'), 13),\n (('reduce', 'working'), 11),\n (('improve', 'experience'), 9)]\n\n"}, "1388": {"topic": "counting n-gram frequency in python nltk", "user_name": "avinash naharavinash nahar", "text": "\nfrom nltk import FreqDist\nfrom nltk.util import ngrams    \ndef compute_freq():\n   textfile = open('corpus.txt','r')\n\n   bigramfdist = FreqDist()\n   threeramfdist = FreqDist()\n\n   for line in textfile:\n        if len(line) > 1:\n        tokens = line.strip().split(' ')\n\n        bigrams = ngrams(tokens, 2)\n        bigramfdist.update(bigrams)\ncompute_freq()\n\n"}, "1389": {"topic": "Is there a corpus of English words in nltk?", "user_name": "Andrea Gasparini", "text": "\nIs there any way to get the list of English words in python nltk library?\nI tried to find it but the only thing I have found is wordnet from nltk.corpus. But based on documentation, it does not have what I need (it finds synonyms for a word).\nI know how to find the list of this words by myself (this answer covers it in details), so I am interested whether I can do this by only using nltk library.\n"}, "1390": {"topic": "Is there a corpus of English words in nltk?", "user_name": "Salvador DaliSalvador Dali", "text": "\nYes, from nltk.corpus import words\nAnd check using:\n>>> \"fine\" in words.words()\nTrue\n\nReference: Section 4.1 (Wordlist Corpora), chapter 2 of Natural Language Processing with Python.\n"}, "1391": {"topic": "Is there a corpus of English words in nltk?", "user_name": "", "text": "\nOther than the nltk.corpus.words that @salvadordali has highlighted,:\n>>> from nltk.corpus import words\n>>> print words.readme()\nWordlists\n\nen: English, http://en.wikipedia.org/wiki/Words_(Unix)\nen-basic: 850 English words: C.K. Ogden in The ABC of Basic English (1932)\n>>> print words.words()[:10]\n[u'A', u'a', u'aa', u'aal', u'aalii', u'aam', u'Aani', u'aardvark', u'aardwolf', u'Aaron']\n\nDo note that nltk.corpus.words is a list of words without frequencies so it's not exactly a corpora of natural text.\nThe corpus package that contains various corpora, some of which are English corpora, see http://www.nltk.org/nltk_data/. E.g. nltk.corpus.brown:\n>>> from nltk.corpus import brown\n>>> brown.words()[:10]\n[u'The', u'Fulton', u'County', u'Grand', u'Jury', u'said', u'Friday', u'an', u'investigation', u'of']\n\nTo get a word list from a natural text corpus:\n>>> wordlist = set(brown.words())\n>>> print len(wordlist)\n56057\n>>> wordlist_lowercased = set(i.lower() for i in brown.words())\n>>> print len(wordlist_lowercased)\n49815\n\nNote that the brown.words() contains words with both lower and upper cases like natural text.\nIn most cases, a list of words is not very useful without frequencies, so you can use the FreqDist:\n>>> from nltk import FreqDist\n>>> from nltk.corpus import brown\n>>> frequency_list = FreqDist(i.lower() for i in brown.words())\n>>> frequency_list.most_common()[:10]\n[(u'the', 69971), (u',', 58334), (u'.', 49346), (u'of', 36412), (u'and', 28853), (u'to', 26158), (u'a', 23195), (u'in', 21337), (u'that', 10594), (u'is', 10109)]\n\nFor more, see http://www.nltk.org/book/ch01.html on how to access corpora and process them in NLTK\n"}, "1392": {"topic": "Using the Python NLTK (2.0b5) on the Google App Engine", "user_name": "Nick Johnson", "text": "\nI have been trying to make the NLTK (Natural Language Toolkit) work on the Google App Engine.  The steps I followed are:\n\nDownload the installer and run it (a .dmg file, as I am using a Mac).\ncopy the nltk folder out of the python site-packages directory and place it as a sub-folder in my project folder.\nCreate a python module in the folder that contains the nltk sub-folder and add the line: from nltk.tokenize import * \n\nUnfortunately, after launching it I get this error (note that this error is raised deep within NLTK and I'm seeing it for my system installation of python as opposed to the one that is in the sub-folder of the GAE project):\n <type 'exceptions.ImportError'>: No module named nltk\nTraceback (most recent call last):\n  File \"/base/data/home/apps/xxxx/1.335654715894946084/main.py\", line 13, in <module>\n    from lingua import reducer\n  File \"/base/data/home/apps/xxxx/1.335654715894946084/lingua/reducer.py\", line 11, in <module>\n    from nltk.tokenizer import *\n  File \"/base/data/home/apps/xxxx/1.335654715894946084/lingua/nltk/__init__.py\", line 73, in <module>\n    from internals import config_java\n  File \"/base/data/home/apps/xxxx/1.335654715894946084/lingua/nltk/internals.py\", line 19, in <module>\n    from nltk import __file__\n\nNote: this is how the error looks in the logs when uploaded to GAE.  If I run it locally I get the same error (except it seems to originate inside my site-packages instance of NLTK ... so no difference there).  And \"xxxx\" signifies the project name.\nSo in summary:\n\nIs what I am trying to do even possible?  Will NLTK even run on the App Engine?\nIs there something I missed?  That is: copying \"nltk\" to the GAE project isn't enough?\n\nEDIT: fixed typo and removed unnecessary step\n"}, "1393": {"topic": "Using the Python NLTK (2.0b5) on the Google App Engine", "user_name": "Ryan DelucchiRyan Delucchi", "text": "\noakmad has managed to successfully work through\ndeploying SEVERAL NLTK modules to GAE. Hope this helps.\nBut , but be honest, I still don't think it's true even after read the post.\n"}, "1394": {"topic": "Using the Python NLTK (2.0b5) on the Google App Engine", "user_name": "sunqiangsunqiang", "text": "\nThe problem here is that nltk is attempting to do recursive imports: When nltk/__init__.py is imported, it imports nltk/internals.py, which then attempts to import nltk again. Since nltk is in the middle of being imported itself, it fails with a (rather unhelpful) error. Whatever they're doing is pretty weird anyway - it's unsurprising something like from nltk import __file__ breaks.\nThis looks like a problem with nltk itself - does it work when imported directly from a Python console? If so, they must be doing some sort of trickery in the installed version. I'd suggest asking on the nltk groups what they're up to and how to work around it.\n"}, "1395": {"topic": "Using the Python NLTK (2.0b5) on the Google App Engine", "user_name": "icktoofay", "text": "\nI've forked NLTK 2.0.3 on github to run it on app engine; tokenizing and simple POS tagging working with the MaxEnt Treebank tagger.\n"}, "1396": {"topic": "Using the Python NLTK (2.0b5) on the Google App Engine", "user_name": "Nick JohnsonNick Johnson", "text": "\nNLTK, I believe, does try its best to be pure-Python as a fallback (graceful degradation) when it can't have the C-coded accelerator extensions it would like. However one always needs to be moving with utter care to boldly inject such a rich package (recursively zipping up all of the .py files and using zipimport might be less flaky).\nMy installed NLTK, 0.95 I believe, has no ntlk.tokenizer -- it does have an nltk.tokenize, no trailing R, but obviously even the most minute such typo is 100% intolerable when you're trying to tell a computer exactly what you want, so I assume this is not a typo on your part but rather your use of a completely different and incompatible release of NLTK, so, WHAT release is it that has a subpackage named tokenizer rather than tokenize?\nIf you find a zero-tolerance policy for one-char typos hard to bear, computers and their programming are unlikely to be tolerable to you...;-)\n"}, "1397": {"topic": "Drive issue with python NLTK", "user_name": "Slayer", "text": "\nI am trying to use nltk in python, but am receiving a pop up error (windows) describing that I am missing a drive at the moment I call import nltk\nDoes anyone know why or how to fix this?\nThe error is below:\n\"There is no disk in the drive. Please insert a disk into drive \\Device\\Harddisk4\\DR4.\"\n"}, "1398": {"topic": "Drive issue with python NLTK", "user_name": "chasechase", "text": "\nNLTK searches for nltk_data directory until it finds one.\nOn Windows, these locations are scanned: %userprofile%\\nltk_data, C:\\nltk_data, D:\\nltk_data, and so on.\nInstalling NLTK data or creating an empty directory solves the error.\nhttp://www.nltk.org/data.html\nMy installation is Win 10, Python 3.5.2 64-bit, nltk 3.2.1 (Christoph Gohlke's binary).\n"}, "1399": {"topic": "Drive issue with python NLTK", "user_name": "shiratorishiratori", "text": "\nWhile i am not sure exactly where the problem arises, I had this same error happen to me (it started 'overnight' - the code had been working, i hand not re-installed nltk, so i have no idea what caused it to start happening). I still had the problem after upgrading to the latest version of nltk (3.2.1), and re-downloading the nltk data. \nshiratori's answer helped me solve my problem, although at least for me it was slightly more complicated. Specifically, my nltk data was stored in C:\\Users\\USERNAME\\AppData\\Roaming\\nltk_data (i think this is a default location). This is where it had always been stored, and always had worked fine, however suddenly nltk did not seem to be recognizing this location, and hence looked in the next drive. To solve it, I copied and pasted all the data in that folder to C:\\nltk_data and now it is running fine again.\nAnyway, not sure if this is Windows induced problem, or what exactly changed to cause code that was working to stop working, but this solved it.\n"}, "1400": {"topic": "Drive issue with python NLTK", "user_name": "kyreniakyrenia", "text": "\nI had the same problem (Win 7, Python 3.6.5, nltk 3.3).\nA simpler solution is to just define the Windows environment variable NLTK_DATA. In my case (like @kyrenia), the data is in C:\\Users\\USERNAME\\AppData\\Roaming\\nltk_data.\nThis solution is described in the nltk 3.3 documentation: http://www.nltk.org/data.html\n\nIf you did not install the data to one of the above central locations, you will need to set the NLTK_DATA environment variable to specify the location of the data. (On a Windows machine, right click on \u201cMy Computer\u201d then select Properties > Advanced > Environment Variables > User Variables > New...)\n\n"}, "1401": {"topic": "What is \"entropy and information gain\"?", "user_name": "rayryeng", "text": "\nI am reading this book (NLTK) and it is confusing.  Entropy is defined as:\n\nEntropy is the sum of the probability of each label\n  times the log probability of that same label\n\nHow can I apply entropy and maximum entropy in terms of text mining?  Can someone give me a easy, simple example (visual)?\n"}, "1402": {"topic": "What is \"entropy and information gain\"?", "user_name": "TIMEXTIMEX", "text": "\nI assume entropy was mentioned in the context of building decision trees.\nTo illustrate, imagine the task of learning to classify first-names into male/female groups. That is given a list of names each labeled with either m or f, we want to learn a model that fits the data and can be used to predict the gender of a new unseen first-name.\nname       gender\n-----------------        Now we want to predict \nAshley        f              the gender of \"Amro\" (my name)\nBrian         m\nCaroline      f\nDavid         m\n\nFirst step is deciding what features of the data are relevant to the target class we want to predict. Some example features include: first/last letter, length, number of vowels, does it end with a vowel, etc.. So after feature extraction, our data looks like:\n# name    ends-vowel  num-vowels   length   gender\n# ------------------------------------------------\nAshley        1         3           6        f\nBrian         0         2           5        m\nCaroline      1         4           8        f\nDavid         0         2           5        m\n\nThe goal is to build a decision tree. An example of a tree would be:\nlength<7\n|   num-vowels<3: male\n|   num-vowels>=3\n|   |   ends-vowel=1: female\n|   |   ends-vowel=0: male\nlength>=7\n|   length=5: male\n\nbasically each node represent a test performed on a single attribute, and we go left or right depending on the result of the test. We keep traversing the tree until we reach a leaf node which contains the class prediction (m or f)\nSo if we run the name Amro down this tree, we start by testing \"is the length<7?\" and the answer is yes, so we go down that branch. Following the branch, the next test \"is the number of vowels<3?\" again evaluates to true. This leads to a leaf node labeled m, and thus the prediction is male (which I happen to be, so the tree predicted the outcome correctly).\nThe decision tree is built in a top-down fashion, but the question is how do you choose which attribute to split at each node? The answer is find the feature that best splits the target class into the purest possible children nodes (ie: nodes that don't contain a mix of both male and female, rather pure nodes with only one class).\nThis measure of purity is called the information. It represents the expected amount of information that would be needed to specify whether a new instance (first-name) should be classified male or female, given the example that reached the node. We calculate it\nbased on the number of male and female classes at the node.\nEntropy on the other hand is a measure of impurity (the opposite). It is defined for a binary class with values a/b as:\nEntropy = - p(a)*log(p(a)) - p(b)*log(p(b))\n\nThis binary entropy function is depicted in the figure below (random variable can take one of two values). It reaches its maximum when the probability is p=1/2, meaning that p(X=a)=0.5 or similarlyp(X=b)=0.5 having a 50%/50% chance of being either a or b (uncertainty is at a maximum). The entropy function is at zero minimum when probability is p=1 or p=0 with complete certainty (p(X=a)=1 or p(X=a)=0 respectively, latter implies p(X=b)=1).\n\nOf course the definition of entropy can be generalized for a discrete random variable X with N outcomes (not just two):\n\n(the log in the formula is usually taken as the logarithm to the base 2)\n\nBack to our task of name classification, lets look at an example. Imagine at some point during the process of constructing the tree, we were considering the following split:\n     ends-vowel\n      [9m,5f]          <--- the [..,..] notation represents the class\n    /          \\            distribution of instances that reached a node\n   =1          =0\n -------     -------\n [3m,4f]     [6m,1f]\n\nAs you can see, before the split we had 9 males and 5 females, i.e. P(m)=9/14 and P(f)=5/14. According to the definition of entropy:\nEntropy_before = - (5/14)*log2(5/14) - (9/14)*log2(9/14) = 0.9403\n\nNext we compare it with the entropy computed after considering the split by looking at two child branches. In the left branch of ends-vowel=1, we have:\nEntropy_left = - (3/7)*log2(3/7) - (4/7)*log2(4/7) = 0.9852\n\nand the right branch of ends-vowel=0, we have:\nEntropy_right = - (6/7)*log2(6/7) - (1/7)*log2(1/7) = 0.5917\n\nWe combine the left/right entropies using the number of instances down each branch as weight factor (7 instances went left, and 7 instances went right), and get the final entropy after the split:\nEntropy_after = 7/14*Entropy_left + 7/14*Entropy_right = 0.7885\n\nNow by comparing the entropy before and after the split, we obtain a measure of information gain, or how much information we gained by doing the split using that particular feature:\nInformation_Gain = Entropy_before - Entropy_after = 0.1518\n\nYou can interpret the above calculation as following: by doing the split with the end-vowels feature, we were able to reduce uncertainty in the sub-tree prediction outcome by a small amount of 0.1518 (measured in bits as units of information).\nAt each node of the tree, this calculation is performed for every feature, and the feature with the largest information gain is chosen for the split in a greedy manner (thus favoring features that produce pure splits with low uncertainty/entropy). This process is applied recursively from the root-node down, and stops when a leaf node contains instances all having the same class (no need to split it further).\nNote that I skipped over some details which are beyond the scope of this post, including how to handle numeric features, missing values, overfitting and pruning trees, etc..\n"}, "1403": {"topic": "What is \"entropy and information gain\"?", "user_name": "tne", "text": "\nTo begin with, it would be best to understand the measure of information. \nHow do we measure the information?\nWhen something unlikely happens, we say it's a big news. Also, when we say something predictable, it's not really interesting. So to quantify this interesting-ness, the function should satisfy\n\nif the probability of the event is 1 (predictable), then the function gives 0\nif the probability of the event is close to 0, then the function should give high number\nif probability 0.5 events happens it give one bit of information.\n\nOne natural measure that satisfy the constraints is\nI(X) = -log_2(p)\n\nwhere p is the probability of the event X. And the unit is in bit, the same bit computer uses. 0 or 1.\nExample 1\nFair coin flip : \nHow much information can we get from one coin flip?\nAnswer : -log(p) = -log(1/2) = 1 (bit)\nExample 2\nIf a meteor strikes the Earth tomorrow, p=2^{-22} then we can get 22 bits of information.\nIf the Sun rises tomorrow, p ~ 1 then it is 0 bit of information.\nEntropy\nSo if we take expectation on the interesting-ness of an event Y, then it is the entropy.\ni.e. entropy is an expected value of the interesting-ness of an event.\nH(Y) = E[ I(Y)]\n\nMore formally, the entropy is the expected number of bits of an event.\nExample\nY = 1 : an event X occurs with probability p\nY = 0 : an event X does not occur with probability 1-p\nH(Y) = E[I(Y)] = p I(Y==1) + (1-p) I(Y==0) \n     = - p log p - (1-p) log (1-p)\n\nLog base 2 for all log.\n"}, "1404": {"topic": "What is \"entropy and information gain\"?", "user_name": "AmroAmro", "text": "\nI can't give you graphics, but maybe I can give a clear explanation.\nSuppose we have an information channel, such as a light that flashes once every day either red or green. How much information does it convey? The first guess might be one bit per day. But what if we add blue, so that the sender has three options? We would like to have a measure of information that can handle things other than powers of two, but still be additive (the way that multiplying the number of possible messages by two adds one bit). We could do this by taking log2(number of possible messages), but it turns out there's a more general way.\nSuppose we're back to red/green, but the red bulb has burned out (this is common knowledge) so that the lamp must always flash green. The channel is now useless, we know what the next flash will be so the flashes convey no information, no news. Now we repair the bulb but impose a rule that the red bulb may not flash twice in a row. When the lamp flashes red, we know what the next flash will be. If you try to send a bit stream by this channel, you'll find that you must encode it with more flashes than you have bits (50% more, in fact). And if you want to describe a sequence of flashes, you can do so with fewer bits. The same applies if each flash is independent (context-free), but green flashes are more common than red: the more skewed the probability the fewer bits you need to describe the sequence, and the less information it contains, all the way to the all-green, bulb-burnt-out limit.\nIt turns out there's a way to measure the amount of information in a signal, based on the the probabilities of the different symbols. If the probability of receiving symbol xi is pi, then consider the quantity\n\n-log pi\n\nThe smaller pi, the larger this value. If xi becomes twice as unlikely, this value increases by a fixed amount (log(2)). This should remind you of adding one bit to a message.\nIf we don't know what the symbol will be (but we know the probabilities) then we can calculate the average of this value, how much we will get, by summing over the different possibilities:\n\nI = -\u03a3 pi log(pi)\n\nThis is the information content in one flash.\n\nRed bulb burnt out: pred = 0, pgreen=1, I = -(0 + 0)  = 0\nRed and green equiprobable: pred = 1/2, pgreen = 1/2, I = -(2 * 1/2 * log(1/2)) = log(2)\nThree colors, equiprobable: pi=1/3, I = -(3 * 1/3 * log(1/3)) = log(3)\nGreen and red, green twice as likely: pred=1/3, pgreen=2/3, I = -(1/3 log(1/3) + 2/3 log(2/3)) = log(3) - 2/3 log(2)\n\nThis is the information content, or entropy, of the message. It is maximal when the different symbols are equiprobable. If you're a physicist you use the natural log, if you're a computer scientist you use log2 and get bits.\n"}, "1405": {"topic": "What is \"entropy and information gain\"?", "user_name": "", "text": "\nI really recommend you read about Information Theory, bayesian methods and MaxEnt. The place to start is this (freely available online) book by David Mackay:\nhttp://www.inference.phy.cam.ac.uk/mackay/itila/\nThose inference methods are really far more general than just text mining and I can't really devise how one would learn how to apply this to NLP without learning some of the general basics contained in this book or other introductory books on Machine Learning and MaxEnt bayesian methods.\nThe connection between entropy and probability theory to information processing and storing is really, really deep. To give a taste of it, there's a theorem due to Shannon that states that the maximum amount of information you can pass without error through a noisy communication channel is equal to the entropy of the noise process. There's also a theorem that connects how much you can compress a piece of data to occupy the minimum possible memory in your computer to the entropy of the process that generated the data.\nI don't think it's really necessary that you go learning about all those theorems on communication theory, but it's not possible to learn this without learning the basics about what is entropy, how it's calculated, what is it's relationship with information and inference, etc...\n"}, "1406": {"topic": "What is \"entropy and information gain\"?", "user_name": "VforVitaminVforVitamin", "text": "\nInformally\nentropy is availability of information or knowledge, Lack of information will leads to difficulties in prediction of future which is high entropy (next word prediction in text mining) and availability of information/knowledge will help us more realistic prediction of future (low entropy).\nRelevant information of any type will reduce entropy and helps us predict more realistic future, that information can be word \"meat\" is present in sentence or word \"meat\" is not present. This is called Information Gain\n\nFormally\nentropy is lack of order of predicability\n"}, "1407": {"topic": "What is \"entropy and information gain\"?", "user_name": "BetaBeta", "text": "\nWhen I was implementing an algorithm to calculate the entropy of an image I found these links, see here and here.\nThis is the pseudo-code I used, you'll need to adapt it to work with text rather than images but the principles should be the same.\n//Loop over image array elements and count occurrences of each possible\n//pixel to pixel difference value. Store these values in prob_array\nfor j = 0, ysize-1 do $\n    for i = 0, xsize-2 do begin\n       diff = array(i+1,j) - array(i,j)\n       if diff lt (array_size+1)/2 and diff gt -(array_size+1)/2 then begin\n            prob_array(diff+(array_size-1)/2) = prob_array(diff+(array_size-1)/2) + 1\n       endif\n     endfor\n\n//Convert values in prob_array to probabilities and compute entropy\nn = total(prob_array)\n\nentrop = 0\nfor i = 0, array_size-1 do begin\n    prob_array(i) = prob_array(i)/n\n\n    //Base 2 log of x is Ln(x)/Ln(2). Take Ln of array element\n    //here and divide final sum by Ln(2)\n    if prob_array(i) ne 0 then begin\n        entrop = entrop - prob_array(i)*alog(prob_array(i))\n    endif\nendfor\n\nentrop = entrop/alog(2)\n\nI got this code from somewhere, but I can't dig out the link.\n"}, "1408": {"topic": "What is \"entropy and information gain\"?", "user_name": "Rafael S. CalsaveriniRafael S. Calsaverini", "text": "\nAs you are reading a book about NLTK it would be interesting you read about MaxEnt Classifier Module http://www.nltk.org/api/nltk.classify.html#module-nltk.classify.maxent\nFor text mining classification the steps could be: pre-processing (tokenization, steaming, feature selection with Information Gain ...), transformation to numeric (frequency or TF-IDF) (I think that this is the key step to understand when using text as input to a algorithm that only accept numeric) and then classify with MaxEnt, sure this is just an example.\n"}, "1409": {"topic": "Stop pip from failing on single package when installing with requirements.txt", "user_name": "vvvvv", "text": "\nI am installing packages from requirements.txt\npip install -r requirements.txt\n\nThe requirements.txt file reads:\nPillow\nlxml\ncssselect\njieba\nbeautifulsoup\nnltk\n\nlxml is the only package failing to install and this leads to everything failing (expected results as pointed out by larsks in the comments). However, after lxml fails pip still runs through and downloads the rest of the packages. \nFrom what I understand the pip install -r requirements.txt command will fail if any of the packages listed in the requirements.txt fail to install.\nIs there any argument I can pass when running pip install -r requirements.txt to tell it to install what it can and skip the packages that it cannot, or to exit as soon as it sees something fail?\n"}, "1410": {"topic": "Stop pip from failing on single package when installing with requirements.txt", "user_name": "e he h", "text": "\nRunning each line with pip install may be a workaround.\ncat requirements.txt | xargs -n 1 pip install\n\nNote: -a parameter is not available under MacOS, so old cat is more portable.\n"}, "1411": {"topic": "Stop pip from failing on single package when installing with requirements.txt", "user_name": "sorin", "text": "\nThis solution handles empty lines, whitespace lines, # comment lines, whitespace-then-# comment lines in your requirements.txt.\ncat requirements.txt | sed -e '/^\\s*#.*$/d' -e '/^\\s*$/d' | xargs -n 1 pip install\n\nHat tip to this answer for the sed magic.\n"}, "1412": {"topic": "Stop pip from failing on single package when installing with requirements.txt", "user_name": "MZDMZD", "text": "\nFor windows users, you can use this:\nFOR /F %k in (requirements.txt) DO ( if NOT # == %k ( pip install %k ) )\n\nLogic: for every dependency in file(requirements.txt), install them and ignore those start with \"#\".\n"}, "1413": {"topic": "Stop pip from failing on single package when installing with requirements.txt", "user_name": "roublerouble", "text": "\nFor Windows:\npip version >=18\nimport sys\nfrom pip._internal import main as pip_main\n\ndef install(package):\n    pip_main(['install', package])\n\nif __name__ == '__main__':\n    with open(sys.argv[1]) as f:\n        for line in f:\n            install(line)\n\npip version <18\nimport sys\nimport pip\n\ndef install(package):\n    pip.main(['install', package])\n\nif __name__ == '__main__':\n    with open(sys.argv[1]) as f:\n        for line in f:\n            install(line)\n\n"}, "1414": {"topic": "Stop pip from failing on single package when installing with requirements.txt", "user_name": "LysanderLysander", "text": "\nThe xargs solution works but can have portability issues (BSD/GNU) and/or be cumbersome if you have comments or blank lines in your requirements file.\nAs for the usecase where such a behavior would be required, I use for instance two separate requirement files, one which is only listing core dependencies that need to be always installed and another file with non-core dependencies that are in 90% of the cases not needed for most usecases. That would be an equivalent of the Recommends section of a debian package.\nI use the following shell script (requires sed) to install optional dependencies:\n#!/bin/sh\n\nwhile read dependency; do\n    dependency_stripped=\"$(echo \"${dependency}\" | sed -e 's/^[[:space:]]*//' -e 's/[[:space:]]*$//')\"\n    # Skip comments\n    if [[ $dependency_stripped == \\#* ]]; then\n        continue\n    # Skip blank lines\n    elif [ -z \"$dependency_stripped\" ]; then\n        continue\n    else\n        if pip install \"$dependency_stripped\"; then\n            echo \"$dependency_stripped is installed\"\n        else\n            echo \"Could not install $dependency_stripped, skipping\"\n        fi\n    fi\ndone < recommends.txt\n\n"}, "1415": {"topic": "Stop pip from failing on single package when installing with requirements.txt", "user_name": "randers", "text": "\nBuilding on the answer by @MZD, here's a solution to filter out all text starting with a comment sign #\ncat requirements.txt | grep -Eo '(^[^#]+)' | xargs -n 1 pip install\n\n"}, "1416": {"topic": "Stop pip from failing on single package when installing with requirements.txt", "user_name": "Etienne ProthonEtienne Prothon", "text": "\nFor Windows using PowerShell:\nforeach($line in Get-Content requirements.txt) {\n    if(!($line.StartsWith('#'))){\n        pip install $line\n    }\n}\n\n"}, "1417": {"topic": "Stop pip from failing on single package when installing with requirements.txt", "user_name": "Leo CavailleLeo Cavaille", "text": "\nOne line PowerShell:\nGet-Content .\\requirements.txt | ForEach-Object {pip install $_}\nIf you need to ignore certain lines then:\nGet-Content .\\requirements.txt | ForEach-Object {if (!$_.startswith(\"#\")){pip install $_}}\nOR\nGet-Content .\\requirements.txt | ForEach-Object {if ($_ -notmatch \"#\"){pip install $_}}\n"}, "1418": {"topic": "Stop pip from failing on single package when installing with requirements.txt", "user_name": "Flair", "text": "\nThanks, Etienne Prothon for windows cases. \nBut, after upgrading to pip 18, pip package don't expose main to public. So you may need to change code like this. \n # This code install line by line a list of pip package \n import sys\n from pip._internal import main as pip_main\n\n def install(package):\n    pip_main(['install', package])\n\n if __name__ == '__main__':\n    with open(sys.argv[1]) as f:\n        for line in f:\n            install(line)\n\n"}, "1419": {"topic": "Stop pip from failing on single package when installing with requirements.txt", "user_name": "user3611228user3611228", "text": "\nAnother option is to use pip install --dry-run  to get a list of packages that you need to install and then keep trying it and remove the ones that don't work.\n"}, "1420": {"topic": "Stop pip from failing on single package when installing with requirements.txt", "user_name": "wslwsl", "text": "\nA very general solution\nThe following code installs all requirements for:\n\nmultiple requirement files (requirements1.txt, requirements2.txt)\nignores lines with comments #\nskips packages, which are not instalable\nruns pip install each line (not each word as in some other answers)\n\n$ (cat requirements1.txt; echo \"\"; cat requirements2.txt) | grep \"^[^#]\" | xargs -L 1 pip install\n\n"}, "1421": {"topic": "Stop pip from failing on single package when installing with requirements.txt", "user_name": "", "text": "\nFor Windows:\nimport os\nfrom pip.__main__ import _main as main\n\nerror_log = open('error_log.txt', 'w')\n\ndef install(package):\n    try:\n        main(['install'] + [str(package)])\n    except Exception as e:\n        error_log.write(str(e))\n\nif __name__ == '__main__':\n    f = open('requirements1.txt', 'r')\n    for line in f:\n        install(line)\n    f.close()\n    error_log.close()\n\n\nCreate a local directory, and put your requirements.txt file in it.\nCopy the code above and save it as a python file in the same directory. Remember to use .py extension, for instance, install_packages.py\nRun this file using a cmd: python install_packages.py\nAll the packages mentioned will be installed in one go without stopping at all. :)\n\nYou can add other parameters in install function. Like:\n main(['install'] + [str(package)] + ['--update'])\n"}, "1422": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "yprez", "text": "\nMy Code:\nimport nltk.data\ntokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n\nERROR Message:\n[ec2-user@ip-172-31-31-31 sentiment]$ python mapper_local_v1.0.py\nTraceback (most recent call last):\nFile \"mapper_local_v1.0.py\", line 16, in <module>\n\n    tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n\nFile \"/usr/lib/python2.6/site-packages/nltk/data.py\", line 774, in load\n\n    opened_resource = _open(resource_url)\n\nFile \"/usr/lib/python2.6/site-packages/nltk/data.py\", line 888, in _open\n\n    return find(path_, path + ['']).open()\n\nFile \"/usr/lib/python2.6/site-packages/nltk/data.py\", line 618, in find\n\n    raise LookupError(resource_not_found)\n\nLookupError:\n\nResource u'tokenizers/punkt/english.pickle' not found.  Please\nuse the NLTK Downloader to obtain the resource:\n\n    >>>nltk.download()\n\nSearched in:\n- '/home/ec2-user/nltk_data'\n- '/usr/share/nltk_data'\n- '/usr/local/share/nltk_data'\n- '/usr/lib/nltk_data'\n- '/usr/local/lib/nltk_data'\n- u''\n\nI'm trying to run this program in Unix machine:\nAs per the error message, I logged into python shell from my unix machine then I used the below commands:\nimport nltk\nnltk.download()\n\nand then I downloaded all the available things using d- down loader and l- list options but still the problem persists.\nI tried my best to find the solution in internet but I got the same solution what I did as I mentioned in my above steps.\n"}, "1423": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "Supreeth MekaSupreeth Meka", "text": "\nTo add to alvas' answer, you can download only the punkt corpus:\nnltk.download('punkt')\n\nDownloading all sounds like overkill to me. Unless that's what you want.\n"}, "1424": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "CommunityBot", "text": "\nIf you're looking to only download the punkt model:\nimport nltk\nnltk.download('punkt')\n\nIf you're unsure which data/model you need, you can install the popular datasets, models and taggers from NLTK:\nimport nltk\nnltk.download('popular')\n\nWith the above command, there is no need to use the GUI to download the datasets. \n"}, "1425": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "yprezyprez", "text": "\nI got the solution:\nimport nltk\nnltk.download()\n\nonce the NLTK Downloader starts\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\nDownloader> d\nDownload which package (l=list; x=cancel)?\n  Identifier> punkt\n"}, "1426": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "", "text": "\nFrom the shell you can execute:\nsudo python -m nltk.downloader punkt \n\nIf you want to install the popular NLTK corpora/models:\nsudo python -m nltk.downloader popular\n\nIf you want to install all NLTK corpora/models:\nsudo python -m nltk.downloader all\n\nTo list the resources you have downloaded:\npython -c 'import os; import nltk; print os.listdir(nltk.data.find(\"corpora\"))'\npython -c 'import os; import nltk; print os.listdir(nltk.data.find(\"tokenizers\"))'\n\n"}, "1427": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "alvasalvas", "text": "\nimport nltk\nnltk.download('punkt')\n\nOpen the Python prompt and run the above statements.\nThe sent_tokenize function uses an instance of PunktSentenceTokenizer from the\nnltk.tokenize.punkt module. This instance has already been trained and works well for\nmany European languages. So it knows what punctuation and characters mark the end of a\nsentence and the beginning of a new sentence.\n"}, "1428": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "Supreeth MekaSupreeth Meka", "text": "\nThe same thing happened to me recently, you just need to download the \"punkt\" package and it should work.\nWhen you execute \"list\" (l) after having \"downloaded all the available things\", is everything marked like the following line?:  \n[*] punkt............... Punkt Tokenizer Models\n\nIf you see this line with the star, it means you have it, and nltk should be able to load it.\n"}, "1429": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "", "text": "\nGo to python console by typing \n\n$ python\n\nin your terminal. Then, type the following 2 commands in your python shell to install the respective packages:\n\n>> nltk.download('punkt')\n  >> nltk.download('averaged_perceptron_tagger')\n\nThis solved the issue for me.\n"}, "1430": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "Franck DernoncourtFranck Dernoncourt", "text": "\nI was getting an error despite importing the following, \nimport nltk\nnltk.download()\n\nbut for google colab this solved my issue. \n   !python3 -c \"import nltk; nltk.download('all')\"\n\n"}, "1431": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "Ramineni Ravi TejaRamineni Ravi Teja", "text": "\nAfter adding this line of code, the issue will be fixed:\nnltk.download('punkt')\n\n"}, "1432": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "eeelnicoeeelnico", "text": "\nMy issue was that I called nltk.download('all') as the root user, but the process that eventually used nltk was another user who didn't have access to /root/nltk_data where the content was downloaded.  \nSo I simply recursively copied everything from the download location to one of the paths where NLTK was looking to find it like this:\ncp -R /root/nltk_data/ /home/ubuntu/nltk_data\n\n"}, "1433": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "Dharani ManneDharani Manne", "text": "\nSimple nltk.download() will not solve this issue. I tried the below and it worked for me:\nin the nltk folder create a tokenizers folder and copy your punkt folder into tokenizers folder.\nThis will work.!\nthe folder structure needs to be as shown in the picture\n"}, "1434": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "sarguptasargupta", "text": "\n\nExecute the following code:\nimport nltk\nnltk.download()\n\nAfter this, NLTK downloader will pop out.\nSelect All packages.\nDownload punkt.\n\n"}, "1435": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "elcortegano", "text": "\nYou need to rearrange your folders\nMove your tokenizers folder into nltk_data folder.\nThis doesn't work if you have nltk_data folder containing corpora folder containing  tokenizers folder\n"}, "1436": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "Ankit RaiAnkit Rai", "text": "\nFor me nothing of the above worked, so I just downloaded all the files by hand from the web site http://www.nltk.org/nltk_data/ and I put them also by hand in a file \"tokenizers\" inside of \"nltk_data\" folder. Not a pretty solution but still a solution. \n"}, "1437": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "RajRaj", "text": "\nJust make sure you are using Jupyter Notebook and in a notebook, do the following:\nimport nltk\n\nnltk.download()\n\nThen one popup window will appear (showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml) \nFrom that you have to download everything.\nThen rerun your code.\n"}, "1438": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "Deepthi KarnamDeepthi Karnam", "text": "\nI faced same issue. After downloading everything, still 'punkt' error was there. I searched package on my windows machine at C:\\Users\\vaibhav\\AppData\\Roaming\\nltk_data\\tokenizers and I can see 'punkt.zip' present there. I realized that somehow the zip has not been extracted into C:\\Users\\vaibhav\\AppData\\Roaming\\nltk_data\\tokenizers\\punk.\nOnce I extracted the zip, it worked like music. \n"}, "1439": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "Tot Zam", "text": "\nFor me it got solved by using \"nltk:\"\nhttp://www.nltk.org/howto/data.html\nFailed loading english.pickle with nltk.data.load\nsent_tokenizer=nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n\n"}, "1440": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "Mayank KumarMayank Kumar", "text": "\nAdd the following lines into your script. This will automatically download the punkt data.\nimport nltk\nnltk.download('punkt')\n\n"}, "1441": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "yprez", "text": "\nMy Code:\nimport nltk.data\ntokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n\nERROR Message:\n[ec2-user@ip-172-31-31-31 sentiment]$ python mapper_local_v1.0.py\nTraceback (most recent call last):\nFile \"mapper_local_v1.0.py\", line 16, in <module>\n\n    tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n\nFile \"/usr/lib/python2.6/site-packages/nltk/data.py\", line 774, in load\n\n    opened_resource = _open(resource_url)\n\nFile \"/usr/lib/python2.6/site-packages/nltk/data.py\", line 888, in _open\n\n    return find(path_, path + ['']).open()\n\nFile \"/usr/lib/python2.6/site-packages/nltk/data.py\", line 618, in find\n\n    raise LookupError(resource_not_found)\n\nLookupError:\n\nResource u'tokenizers/punkt/english.pickle' not found.  Please\nuse the NLTK Downloader to obtain the resource:\n\n    >>>nltk.download()\n\nSearched in:\n- '/home/ec2-user/nltk_data'\n- '/usr/share/nltk_data'\n- '/usr/local/share/nltk_data'\n- '/usr/lib/nltk_data'\n- '/usr/local/lib/nltk_data'\n- u''\n\nI'm trying to run this program in Unix machine:\nAs per the error message, I logged into python shell from my unix machine then I used the below commands:\nimport nltk\nnltk.download()\n\nand then I downloaded all the available things using d- down loader and l- list options but still the problem persists.\nI tried my best to find the solution in internet but I got the same solution what I did as I mentioned in my above steps.\n"}, "1442": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "Supreeth MekaSupreeth Meka", "text": "\nTo add to alvas' answer, you can download only the punkt corpus:\nnltk.download('punkt')\n\nDownloading all sounds like overkill to me. Unless that's what you want.\n"}, "1443": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "CommunityBot", "text": "\nIf you're looking to only download the punkt model:\nimport nltk\nnltk.download('punkt')\n\nIf you're unsure which data/model you need, you can install the popular datasets, models and taggers from NLTK:\nimport nltk\nnltk.download('popular')\n\nWith the above command, there is no need to use the GUI to download the datasets. \n"}, "1444": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "yprezyprez", "text": "\nI got the solution:\nimport nltk\nnltk.download()\n\nonce the NLTK Downloader starts\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\nDownloader> d\nDownload which package (l=list; x=cancel)?\n  Identifier> punkt\n"}, "1445": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "", "text": "\nFrom the shell you can execute:\nsudo python -m nltk.downloader punkt \n\nIf you want to install the popular NLTK corpora/models:\nsudo python -m nltk.downloader popular\n\nIf you want to install all NLTK corpora/models:\nsudo python -m nltk.downloader all\n\nTo list the resources you have downloaded:\npython -c 'import os; import nltk; print os.listdir(nltk.data.find(\"corpora\"))'\npython -c 'import os; import nltk; print os.listdir(nltk.data.find(\"tokenizers\"))'\n\n"}, "1446": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "alvasalvas", "text": "\nimport nltk\nnltk.download('punkt')\n\nOpen the Python prompt and run the above statements.\nThe sent_tokenize function uses an instance of PunktSentenceTokenizer from the\nnltk.tokenize.punkt module. This instance has already been trained and works well for\nmany European languages. So it knows what punctuation and characters mark the end of a\nsentence and the beginning of a new sentence.\n"}, "1447": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "Supreeth MekaSupreeth Meka", "text": "\nThe same thing happened to me recently, you just need to download the \"punkt\" package and it should work.\nWhen you execute \"list\" (l) after having \"downloaded all the available things\", is everything marked like the following line?:  \n[*] punkt............... Punkt Tokenizer Models\n\nIf you see this line with the star, it means you have it, and nltk should be able to load it.\n"}, "1448": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "", "text": "\nGo to python console by typing \n\n$ python\n\nin your terminal. Then, type the following 2 commands in your python shell to install the respective packages:\n\n>> nltk.download('punkt')\n  >> nltk.download('averaged_perceptron_tagger')\n\nThis solved the issue for me.\n"}, "1449": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "Franck DernoncourtFranck Dernoncourt", "text": "\nI was getting an error despite importing the following, \nimport nltk\nnltk.download()\n\nbut for google colab this solved my issue. \n   !python3 -c \"import nltk; nltk.download('all')\"\n\n"}, "1450": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "Ramineni Ravi TejaRamineni Ravi Teja", "text": "\nAfter adding this line of code, the issue will be fixed:\nnltk.download('punkt')\n\n"}, "1451": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "eeelnicoeeelnico", "text": "\nMy issue was that I called nltk.download('all') as the root user, but the process that eventually used nltk was another user who didn't have access to /root/nltk_data where the content was downloaded.  \nSo I simply recursively copied everything from the download location to one of the paths where NLTK was looking to find it like this:\ncp -R /root/nltk_data/ /home/ubuntu/nltk_data\n\n"}, "1452": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "Dharani ManneDharani Manne", "text": "\nSimple nltk.download() will not solve this issue. I tried the below and it worked for me:\nin the nltk folder create a tokenizers folder and copy your punkt folder into tokenizers folder.\nThis will work.!\nthe folder structure needs to be as shown in the picture\n"}, "1453": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "sarguptasargupta", "text": "\n\nExecute the following code:\nimport nltk\nnltk.download()\n\nAfter this, NLTK downloader will pop out.\nSelect All packages.\nDownload punkt.\n\n"}, "1454": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "elcortegano", "text": "\nYou need to rearrange your folders\nMove your tokenizers folder into nltk_data folder.\nThis doesn't work if you have nltk_data folder containing corpora folder containing  tokenizers folder\n"}, "1455": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "Ankit RaiAnkit Rai", "text": "\nFor me nothing of the above worked, so I just downloaded all the files by hand from the web site http://www.nltk.org/nltk_data/ and I put them also by hand in a file \"tokenizers\" inside of \"nltk_data\" folder. Not a pretty solution but still a solution. \n"}, "1456": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "RajRaj", "text": "\nJust make sure you are using Jupyter Notebook and in a notebook, do the following:\nimport nltk\n\nnltk.download()\n\nThen one popup window will appear (showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml) \nFrom that you have to download everything.\nThen rerun your code.\n"}, "1457": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "Deepthi KarnamDeepthi Karnam", "text": "\nI faced same issue. After downloading everything, still 'punkt' error was there. I searched package on my windows machine at C:\\Users\\vaibhav\\AppData\\Roaming\\nltk_data\\tokenizers and I can see 'punkt.zip' present there. I realized that somehow the zip has not been extracted into C:\\Users\\vaibhav\\AppData\\Roaming\\nltk_data\\tokenizers\\punk.\nOnce I extracted the zip, it worked like music. \n"}, "1458": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "Tot Zam", "text": "\nFor me it got solved by using \"nltk:\"\nhttp://www.nltk.org/howto/data.html\nFailed loading english.pickle with nltk.data.load\nsent_tokenizer=nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n\n"}, "1459": {"topic": "Resource u'tokenizers/punkt/english.pickle' not found", "user_name": "Mayank KumarMayank Kumar", "text": "\nAdd the following lines into your script. This will automatically download the punkt data.\nimport nltk\nnltk.download('punkt')\n\n"}, "1460": {"topic": "NLTK python error: \"TypeError: 'dict_keys' object is not subscriptable\"", "user_name": "Pavel Fedotov", "text": "\nI am following instructions for a class homework assignment and I am supposed to look up the top 200 most frequently used words in a text file. \nHere's the last part of the code:\nfdist1 = FreqDist(NSmyText)\nvocab=fdist1.keys()\nvocab[:200]\n\nBut when I press enter after the vocab 200 line, it returns:\n Traceback (most recent call last):\n File \"<stdin>\", line 1, in <module>\nTypeError: 'dict_keys' object is not subscriptable\n\nAny suggestions on how to fix this so it can correctly return an answer?\n"}, "1461": {"topic": "NLTK python error: \"TypeError: 'dict_keys' object is not subscriptable\"", "user_name": "user3760644user3760644", "text": "\nLooks like you are using Python 3. In Python 3 dict.keys() returns an iterable but not indexable object. The most simple (but not so efficient) solution would be:\nvocab = list(fdist1.keys())[:200]\n\nIn some situations it is desirable to continue working with an  iterator object instead of a list. This can be done with itertools.islice():\nimport itertools\nvocab_iterator = itertools.islice(fdist1.keys(), 200)\n\n"}, "1462": {"topic": "NLTK python error: \"TypeError: 'dict_keys' object is not subscriptable\"", "user_name": "", "text": "\nI am using python 3.5 and I meet the same problem of   TypeError. \nUsing vocab = list(fdist1.keys()) does not give me the top 50 most frequently used words.\nBut fdist1.most_common(50) does.\nFurther,if you just want to show those top 50 words not with their frequency,you can try : \n[word for (word, freq) in fdist1.most_common(50)]\n"}, "1463": {"topic": "NLTK python error: \"TypeError: 'dict_keys' object is not subscriptable\"", "user_name": "Klaus D.Klaus D.", "text": "\nIf you want to get elements as keys and values (word and frequency), you can use:\nlist(fdist1.items())[:200]\n"}, "1464": {"topic": "NLTK python error: \"TypeError: 'dict_keys' object is not subscriptable\"", "user_name": "Roy ChenRoy Chen", "text": "\nTo print the most frequently used 200 words use:\n    fdist1.most_common(200)\nThe above line of code will return the 200 most frequently used words as key-frequency pair.\n"}, "1465": {"topic": "NLTK python error: \"TypeError: 'dict_keys' object is not subscriptable\"", "user_name": "zaroobazarooba", "text": "\nIf your using python 3 try: \nfdist1.most_common(200)\n\ninstead, to get the 200 most frequent words.\n"}, "1466": {"topic": "NLTK python error: \"TypeError: 'dict_keys' object is not subscriptable\"", "user_name": "Shikhar GuptaShikhar Gupta", "text": "\nfdist1 = FreqDist(NSmyText)\nvocab=fdist1.keys()\nThis code is using in Python2.7.\nSo you should do some change.\ndic.keys() returns an iteratable. So using:\nlist(fdist1.keys())\n"}, "1467": {"topic": "How to config nltk data directory from code?", "user_name": "alvas", "text": "\nHow to config nltk data directory from code?\n"}, "1468": {"topic": "How to config nltk data directory from code?", "user_name": "Juanjo ContiJuanjo Conti", "text": "\nJust change items of nltk.data.path, it's a simple list.\n"}, "1469": {"topic": "How to config nltk data directory from code?", "user_name": "Tim McNamaraTim McNamara", "text": "\nFrom the code, http://www.nltk.org/_modules/nltk/data.html: \n\n``nltk:path``: Specifies the file stored in the NLTK data\n package at *path*.  NLTK will search for these files in the\n directories specified by ``nltk.data.path``.\n\n\nThen within the code:\n######################################################################\n# Search Path\n######################################################################\n\npath = []\n\"\"\"A list of directories where the NLTK data package might reside.\n   These directories will be checked in order when looking for a\n   resource in the data package.  Note that this allows users to\n   substitute in their own versions of resources, if they have them\n   (e.g., in their home directory under ~/nltk_data).\"\"\"\n\n# User-specified locations:\npath += [d for d in os.environ.get('NLTK_DATA', str('')).split(os.pathsep) if d]\nif os.path.expanduser('~/') != '~/':\n    path.append(os.path.expanduser(str('~/nltk_data')))\n\nif sys.platform.startswith('win'):\n    # Common locations on Windows:\n    path += [\n        str(r'C:\\nltk_data'), str(r'D:\\nltk_data'), str(r'E:\\nltk_data'),\n        os.path.join(sys.prefix, str('nltk_data')),\n        os.path.join(sys.prefix, str('lib'), str('nltk_data')),\n        os.path.join(os.environ.get(str('APPDATA'), str('C:\\\\')), str('nltk_data'))\n    ]\nelse:\n    # Common locations on UNIX & OS X:\n    path += [\n        str('/usr/share/nltk_data'),\n        str('/usr/local/share/nltk_data'),\n        str('/usr/lib/nltk_data'),\n        str('/usr/local/lib/nltk_data')\n    ]\n\nTo modify the path, simply append to the list of possible paths:\nimport nltk\nnltk.data.path.append(\"/home/yourusername/whateverpath/\")\n\nOr in windows:\nimport nltk\nnltk.data.path.append(\"C:\\somewhere\\farfar\\away\\path\")\n\n"}, "1470": {"topic": "How to config nltk data directory from code?", "user_name": "alvasalvas", "text": "\nI use append, example\nnltk.data.path.append('/libs/nltk_data/')\n\n"}, "1471": {"topic": "How to config nltk data directory from code?", "user_name": "Tushar Gupta - curioustushar", "text": "\nInstead of adding nltk.data.path.append('your/path/to/nltk_data') to every script, NLTK accepts NLTK_DATA environment variable. (code link)\nOpen ~/.bashrc (or ~/.profile) with text editor (e.g. nano, vim, gedit), and add following line:  \nexport NLTK_DATA=\"your/path/to/nltk_data\"\n\nExecute source to load environmental variable  \nsource ~/.bashrc\n\n\nTest\nOpen python and execute following lines\nimport nltk\nnltk.data.path\n\nYour can see your nltk data path already in there.\nReference: @alvations's answer on\nnltk/nltk #1997 \n"}, "1472": {"topic": "How to config nltk data directory from code?", "user_name": "bahlumbahlum", "text": "\nFor those using uwsgi: \nI was having trouble because I wanted a uwsgi app (running as a different user than myself) to have access to nltk data that I had previously downloaded. What worked for me was adding the following line to myapp_uwsgi.ini:\nenv = NLTK_DATA=/home/myuser/nltk_data/\n\nThis sets the environment variable NLTK_DATA, as suggested by @schemacs.\nYou may need to restart your uwsgi process after making this change. \n"}, "1473": {"topic": "How to config nltk data directory from code?", "user_name": "", "text": "\nUsing fnjn's advice above on printing out the path:\nprint(nltk.data.path)\n\nI saw the path strings in this type of format on windows:\nC:\\\\Users\\\\my_user_name\\\\AppData\\\\Roaming\\\\SPB_Data\n\nSo I switched my path from the python type forward slash '/', to a double backslash '\\\\' when I used path.append:\nnltk.data.path.append(\"C:\\\\workspace\\\\my_project\\\\data\\\\nltk_books\")\n\nThe exception went away.\n"}, "1474": {"topic": "How to config nltk data directory from code?", "user_name": "fnjnfnjn", "text": "\nAnother solution is to get ahead of it. \ntry \n    import nltk \n    nltk.download()  \nWhen the window box pops up asking if you want to download the corpus , you can specify there which directory it is to be downloaded to. \n"}, "1475": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "Abu Shoeb", "text": "\nIs it possible to use Stanford Parser in NLTK? (I am not talking about Stanford POS.)\n"}, "1476": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "ThanaDarayThanaDaray", "text": "\nNote that this answer applies to NLTK v 3.0, and not to more recent versions.\nSure, try the following in Python:\nimport os\nfrom nltk.parse import stanford\nos.environ['STANFORD_PARSER'] = '/path/to/standford/jars'\nos.environ['STANFORD_MODELS'] = '/path/to/standford/jars'\n\nparser = stanford.StanfordParser(model_path=\"/location/of/the/englishPCFG.ser.gz\")\nsentences = parser.raw_parse_sents((\"Hello, My name is Melroy.\", \"What is your name?\"))\nprint sentences\n\n# GUI\nfor line in sentences:\n    for sentence in line:\n        sentence.draw()\n\nOutput:\n\n[Tree('ROOT', [Tree('S', [Tree('INTJ', [Tree('UH', ['Hello'])]),\n  Tree(',', [',']), Tree('NP', [Tree('PRP$', ['My']), Tree('NN',\n  ['name'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('ADJP', [Tree('JJ',\n  ['Melroy'])])]), Tree('.', ['.'])])]), Tree('ROOT', [Tree('SBARQ',\n  [Tree('WHNP', [Tree('WP', ['What'])]), Tree('SQ', [Tree('VBZ',\n  ['is']), Tree('NP', [Tree('PRP$', ['your']), Tree('NN', ['name'])])]),\n  Tree('.', ['?'])])])]\n\nNote 1:\nIn this example both the parser & model jars are in the same folder.\nNote 2:\n\nFile name of stanford parser is: stanford-parser.jar \nFile name of stanford models is: stanford-parser-x.x.x-models.jar\n\nNote 3:\nThe englishPCFG.ser.gz file can be found inside the models.jar file (/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz). Please use come archive manager to 'unzip' the models.jar file.\nNote 4:\nBe sure you are using Java JRE (Runtime Environment) 1.8 also known as Oracle JDK 8. Otherwise you will get: Unsupported major.minor version 52.0.\nInstallation\n\nDownload NLTK v3 from: https://github.com/nltk/nltk. And install NLTK:\nsudo python setup.py install\nYou can use the NLTK downloader to get Stanford Parser, using Python:\nimport nltk\nnltk.download()\n\nTry my example! (don't forget the change the jar paths and change the model path to the ser.gz location)\n\nOR:\n\nDownload and install NLTK v3, same as above.\nDownload the latest version from (current version filename is stanford-parser-full-2015-01-29.zip):\nhttp://nlp.stanford.edu/software/lex-parser.shtml#Download\nExtract the standford-parser-full-20xx-xx-xx.zip. \nCreate a new folder ('jars' in my example). Place the extracted files into this jar folder:  stanford-parser-3.x.x-models.jar and stanford-parser.jar.\nAs shown above you can use the environment variables (STANFORD_PARSER & STANFORD_MODELS) to point to this 'jars' folder. I'm using Linux, so if you use Windows please use something like: C://folder//jars.\nOpen the stanford-parser-3.x.x-models.jar using an Archive manager (7zip).\nBrowse inside the jar file; edu/stanford/nlp/models/lexparser. Again, extract the file called 'englishPCFG.ser.gz'. Remember the location where you extract this ser.gz file.\nWhen creating a StanfordParser instance, you can provide the model path as parameter. This is the complete path to the model, in our case /location/of/englishPCFG.ser.gz.\nTry my example! (don't forget the change the jar paths and change the model path to the ser.gz location)\n\n"}, "1477": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "Servy", "text": "\nDeprecated Answer\nThe answer below is deprecated, please use the solution on https://stackoverflow.com/a/51981566/610569 for NLTK v3.3 and above.\n\nEDITED\nNote: The following answer will only work on:\n\nNLTK version >=3.2.4\nStanford Tools compiled since 2015-04-20\nPython 2.7, 3.4 and 3.5 (Python 3.6 is not yet officially supported)\n\nAs both tools changes rather quickly and the API might look very different 3-6 months later. Please treat the following answer as temporal and not an eternal fix.\nAlways refer to https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software for the latest instruction on how to interface Stanford NLP tools using NLTK!! \n\nTL;DR\ncd $HOME\n\n# Update / Install NLTK\npip install -U nltk\n\n# Download the Stanford NLP tools\nwget http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip\nwget http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip\nwget http://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip\n# Extract the zip file.\nunzip stanford-ner-2015-04-20.zip \nunzip stanford-parser-full-2015-04-20.zip \nunzip stanford-postagger-full-2015-04-20.zip\n\n\nexport STANFORDTOOLSDIR=$HOME\n\nexport CLASSPATH=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/stanford-postagger.jar:$STANFORDTOOLSDIR/stanford-ner-2015-04-20/stanford-ner.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-04-20/stanford-parser.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar\n\nexport STANFORD_MODELS=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/models:$STANFORDTOOLSDIR/stanford-ner-2015-04-20/classifiers\n\nThen:\n>>> from nltk.tag.stanford import StanfordPOSTagger\n>>> st = StanfordPOSTagger('english-bidirectional-distsim.tagger')\n>>> st.tag('What is the airspeed of an unladen swallow ?'.split())\n[(u'What', u'WP'), (u'is', u'VBZ'), (u'the', u'DT'), (u'airspeed', u'NN'), (u'of', u'IN'), (u'an', u'DT'), (u'unladen', u'JJ'), (u'swallow', u'VB'), (u'?', u'.')]\n\n>>> from nltk.tag import StanfordNERTagger\n>>> st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') \n>>> st.tag('Rami Eid is studying at Stony Brook University in NY'.split())\n[(u'Rami', u'PERSON'), (u'Eid', u'PERSON'), (u'is', u'O'), (u'studying', u'O'), (u'at', u'O'), (u'Stony', u'ORGANIZATION'), (u'Brook', u'ORGANIZATION'), (u'University', u'ORGANIZATION'), (u'in', u'O'), (u'NY', u'O')]\n\n\n>>> from nltk.parse.stanford import StanfordParser\n>>> parser=StanfordParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n>>> list(parser.raw_parse(\"the quick brown fox jumps over the lazy dog\"))\n[Tree('ROOT', [Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['quick']), Tree('JJ', ['brown']), Tree('NN', ['fox'])]), Tree('NP', [Tree('NP', [Tree('NNS', ['jumps'])]), Tree('PP', [Tree('IN', ['over']), Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['lazy']), Tree('NN', ['dog'])])])])])])]\n\n>>> from nltk.parse.stanford import StanfordDependencyParser\n>>> dep_parser=StanfordDependencyParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n>>> print [parse.tree() for parse in dep_parser.raw_parse(\"The quick brown fox jumps over the lazy dog.\")]\n[Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over', 'the', 'lazy'])])]\n\n\nIn Long:\n\nFirstly, one must note that the Stanford NLP tools are written in Java and NLTK is written in Python. The way NLTK is interfacing the tool is through the call the Java tool through the command line interface. \nSecondly, the NLTK API to the Stanford NLP tools have changed quite a lot since the version 3.1. So it is advisable to update your NLTK package to v3.1.\nThirdly, the NLTK API to Stanford NLP Tools wraps around the individual NLP tools, e.g. Stanford POS tagger, Stanford NER Tagger, Stanford Parser. \nFor the POS and NER tagger, it DOES NOT wrap around the Stanford Core NLP package. \nFor the Stanford Parser, it's a special case where it wraps around both the Stanford Parser and the Stanford Core NLP (personally, I have not used the latter using NLTK, i would rather follow @dimazest's demonstration on http://www.eecs.qmul.ac.uk/~dm303/stanford-dependency-parser-nltk-and-anaconda.html )\nNote that as of NLTK v3.1, the STANFORD_JAR and STANFORD_PARSER variables is deprecated and NO LONGER used\n\nIn Longer:\n\nSTEP 1\nAssuming that you have installed Java appropriately on your OS.\nNow, install/update your NLTK version (see http://www.nltk.org/install.html):\n\nUsing pip: sudo pip install -U nltk\nDebian distro (using apt-get): sudo apt-get install python-nltk\n\nFor Windows (Use the 32-bit binary installation):\n\nInstall Python 3.4: http://www.python.org/downloads/ (avoid the 64-bit versions)\nInstall Numpy (optional): http://sourceforge.net/projects/numpy/files/NumPy/ (the version that specifies pythnon3.4)\nInstall NLTK: http://pypi.python.org/pypi/nltk\nTest installation: Start>Python34, then type import nltk\n\n(Why not 64 bit? See https://github.com/nltk/nltk/issues/1079)\n\nThen out of paranoia, recheck your nltk version inside python:\nfrom __future__ import print_function\nimport nltk\nprint(nltk.__version__)\n\nOr on the command line:\npython3 -c \"import nltk; print(nltk.__version__)\"\n\nMake sure that you see 3.1 as the output.\nFor even more paranoia, check that all your favorite Stanford NLP tools API are available:\nfrom nltk.parse.stanford import StanfordParser\nfrom nltk.parse.stanford import StanfordDependencyParser\nfrom nltk.parse.stanford import StanfordNeuralDependencyParser\nfrom nltk.tag.stanford import StanfordPOSTagger, StanfordNERTagger\nfrom nltk.tokenize.stanford import StanfordTokenizer\n\n(Note: The imports above will ONLY ensure that you are using a correct NLTK version that contains these APIs. Not seeing errors in the import doesn't mean that you have successfully configured the NLTK API to use the Stanford Tools)\n\nSTEP 2\nNow that you have checked that you have the correct version of NLTK that contains the necessary Stanford NLP tools interface. You need to download and extract all the necessary Stanford NLP tools.\nTL;DR, in Unix:\ncd $HOME\n\n# Download the Stanford NLP tools\nwget http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip\nwget http://nlp.stanford.edu/software/stanford-postagger-full-2015-04-20.zip\nwget http://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip\n# Extract the zip file.\nunzip stanford-ner-2015-04-20.zip \nunzip stanford-parser-full-2015-04-20.zip \nunzip stanford-postagger-full-2015-04-20.zip\n\nIn Windows / Mac:\n\nDownload and unzip the parser from http://nlp.stanford.edu/software/lex-parser.shtml#Download\nDownload and unizp the FULL VERSION tagger from http://nlp.stanford.edu/software/tagger.shtml#Download\nDownload and unizp the NER tagger from http://nlp.stanford.edu/software/CRF-NER.shtml#Download\n\n\nSTEP 3\nSetup the environment variables such that NLTK can find the relevant file path automatically. You have to set the following variables:\n\nAdd the appropriate Stanford NLP .jar file to the  CLASSPATH environment variable.\n\ne.g. for the NER, it will be stanford-ner-2015-04-20/stanford-ner.jar\ne.g. for the POS, it will be stanford-postagger-full-2015-04-20/stanford-postagger.jar\ne.g. for the parser, it will be stanford-parser-full-2015-04-20/stanford-parser.jar and the parser model jar file, stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar\n\nAdd the appropriate model directory to the STANFORD_MODELS variable (i.e. the directory where you can find where the pre-trained models are saved)\n\ne.g. for the NER, it will be in stanford-ner-2015-04-20/classifiers/\ne.g. for the POS, it will be in stanford-postagger-full-2015-04-20/models/\ne.g. for the Parser, there won't be a model directory.\n\n\nIn the code, see that it searches for the STANFORD_MODELS directory before appending the model name. Also see that, the API also automatically tries to search the OS environments for the `CLASSPATH)\nNote that as of NLTK v3.1, the STANFORD_JAR variables is deprecated and NO LONGER used. Code snippets found in the following Stackoverflow questions might not work:\n\nStanford Dependency Parser Setup and NLTK\nnltk interface to stanford parser\ntrouble importing stanford pos tagger into nltk\nStanford Entity Recognizer (caseless) in Python Nltk\nHow to improve speed with Stanford NLP Tagger and NLTK\nHow can I get the stanford NLTK python module?\nStanford Parser and NLTK windows\nStanford Named Entity Recognizer (NER) functionality with NLTK\nStanford parser with NLTK produces empty output\nExtract list of Persons and Organizations using Stanford NER Tagger in NLTK\nError using Stanford POS Tagger in NLTK Python\n\nTL;DR for STEP 3 on Ubuntu\nexport STANFORDTOOLSDIR=/home/path/to/stanford/tools/\n\nexport CLASSPATH=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/stanford-postagger.jar:$STANFORDTOOLSDIR/stanford-ner-2015-04-20/stanford-ner.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-04-20/stanford-parser.jar:$STANFORDTOOLSDIR/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar\n\nexport STANFORD_MODELS=$STANFORDTOOLSDIR/stanford-postagger-full-2015-04-20/models:$STANFORDTOOLSDIR/stanford-ner-2015-04-20/classifiers\n\n(For Windows: See https://stackoverflow.com/a/17176423/610569 for instructions for setting environment variables)\nYou MUST set the variables as above before starting python, then:\n>>> from nltk.tag.stanford import StanfordPOSTagger\n>>> st = StanfordPOSTagger('english-bidirectional-distsim.tagger')\n>>> st.tag('What is the airspeed of an unladen swallow ?'.split())\n[(u'What', u'WP'), (u'is', u'VBZ'), (u'the', u'DT'), (u'airspeed', u'NN'), (u'of', u'IN'), (u'an', u'DT'), (u'unladen', u'JJ'), (u'swallow', u'VB'), (u'?', u'.')]\n\n>>> from nltk.tag import StanfordNERTagger\n>>> st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') \n>>> st.tag('Rami Eid is studying at Stony Brook University in NY'.split())\n[(u'Rami', u'PERSON'), (u'Eid', u'PERSON'), (u'is', u'O'), (u'studying', u'O'), (u'at', u'O'), (u'Stony', u'ORGANIZATION'), (u'Brook', u'ORGANIZATION'), (u'University', u'ORGANIZATION'), (u'in', u'O'), (u'NY', u'O')]\n\n\n>>> from nltk.parse.stanford import StanfordParser\n>>> parser=StanfordParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n>>> list(parser.raw_parse(\"the quick brown fox jumps over the lazy dog\"))\n[Tree('ROOT', [Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['quick']), Tree('JJ', ['brown']), Tree('NN', ['fox'])]), Tree('NP', [Tree('NP', [Tree('NNS', ['jumps'])]), Tree('PP', [Tree('IN', ['over']), Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['lazy']), Tree('NN', ['dog'])])])])])])]\n\n\nAlternatively, you could try add the environment variables inside python, as the previous answers have suggested but you can also directly tell the parser/tagger to initialize to the direct path where you kept the .jar file and your models. \nThere is NO need to set the environment variables if you use the following method BUT when the API changes its parameter names, you will need to change accordingly. That is why it is MORE advisable to set the environment variables than to modify your python code to suit the NLTK version.\nFor example (without setting any environment variables):\n# POS tagging:\n\nfrom nltk.tag import StanfordPOSTagger\n\nstanford_pos_dir = '/home/alvas/stanford-postagger-full-2015-04-20/'\neng_model_filename= stanford_pos_dir + 'models/english-left3words-distsim.tagger'\nmy_path_to_jar= stanford_pos_dir + 'stanford-postagger.jar'\n\nst = StanfordPOSTagger(model_filename=eng_model_filename, path_to_jar=my_path_to_jar) \nst.tag('What is the airspeed of an unladen swallow ?'.split())\n\n\n# NER Tagging:\nfrom nltk.tag import StanfordNERTagger\n\nstanford_ner_dir = '/home/alvas/stanford-ner/'\neng_model_filename= stanford_ner_dir + 'classifiers/english.all.3class.distsim.crf.ser.gz'\nmy_path_to_jar= stanford_ner_dir + 'stanford-ner.jar'\n\nst = StanfordNERTagger(model_filename=eng_model_filename, path_to_jar=my_path_to_jar) \nst.tag('Rami Eid is studying at Stony Brook University in NY'.split())\n\n# Parsing:\nfrom nltk.parse.stanford import StanfordParser\n\nstanford_parser_dir = '/home/alvas/stanford-parser/'\neng_model_path = stanford_parser_dir  + \"edu/stanford/nlp/models/lexparser/englishRNN.ser.gz\"\nmy_path_to_models_jar = stanford_parser_dir  + \"stanford-parser-3.5.2-models.jar\"\nmy_path_to_jar = stanford_parser_dir  + \"stanford-parser.jar\"\n\nparser=StanfordParser(model_path=eng_model_path, path_to_models_jar=my_path_to_models_jar, path_to_jar=my_path_to_jar)\n\n"}, "1478": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "Melroy van den BergMelroy van den Berg", "text": "\nAs of NLTK v3.3, users should avoid the Stanford NER or POS taggers from nltk.tag, and avoid Stanford tokenizer/segmenter from nltk.tokenize.\nInstead use the new nltk.parse.corenlp.CoreNLPParser API. \nPlease see https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK\n\n(Avoiding link only answer, I've pasted the docs from NLTK github wiki below)\nFirst, update your NLTK\npip3 install -U nltk # Make sure is >=3.3\n\nThen download the necessary CoreNLP packages:\ncd ~\nwget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-02-27.zip\nunzip stanford-corenlp-full-2018-02-27.zip\ncd stanford-corenlp-full-2018-02-27\n\n# Get the Chinese model \nwget http://nlp.stanford.edu/software/stanford-chinese-corenlp-2018-02-27-models.jar\nwget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-chinese.properties \n\n# Get the Arabic model\nwget http://nlp.stanford.edu/software/stanford-arabic-corenlp-2018-02-27-models.jar\nwget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-arabic.properties \n\n# Get the French model\nwget http://nlp.stanford.edu/software/stanford-french-corenlp-2018-02-27-models.jar\nwget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-french.properties \n\n# Get the German model\nwget http://nlp.stanford.edu/software/stanford-german-corenlp-2018-02-27-models.jar\nwget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-german.properties \n\n\n# Get the Spanish model\nwget http://nlp.stanford.edu/software/stanford-spanish-corenlp-2018-02-27-models.jar\nwget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-spanish.properties \n\nEnglish\nStill in the stanford-corenlp-full-2018-02-27 directory, start the server:\njava -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-preload tokenize,ssplit,pos,lemma,ner,parse,depparse \\\n-status_port 9000 -port 9000 -timeout 15000 & \n\nThen in Python:\n>>> from nltk.parse import CoreNLPParser\n\n# Lexical Parser\n>>> parser = CoreNLPParser(url='http://localhost:9000')\n\n# Parse tokenized text.\n>>> list(parser.parse('What is the airspeed of an unladen swallow ?'.split()))\n[Tree('ROOT', [Tree('SBARQ', [Tree('WHNP', [Tree('WP', ['What'])]), Tree('SQ', [Tree('VBZ', ['is']), Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('NN', ['airspeed'])]), Tree('PP', [Tree('IN', ['of']), Tree('NP', [Tree('DT', ['an']), Tree('JJ', ['unladen'])])]), Tree('S', [Tree('VP', [Tree('VB', ['swallow'])])])])]), Tree('.', ['?'])])])]\n\n# Parse raw string.\n>>> list(parser.raw_parse('What is the airspeed of an unladen swallow ?'))\n[Tree('ROOT', [Tree('SBARQ', [Tree('WHNP', [Tree('WP', ['What'])]), Tree('SQ', [Tree('VBZ', ['is']), Tree('NP', [Tree('NP', [Tree('DT', ['the']), Tree('NN', ['airspeed'])]), Tree('PP', [Tree('IN', ['of']), Tree('NP', [Tree('DT', ['an']), Tree('JJ', ['unladen'])])]), Tree('S', [Tree('VP', [Tree('VB', ['swallow'])])])])]), Tree('.', ['?'])])])]\n\n# Neural Dependency Parser\n>>> from nltk.parse.corenlp import CoreNLPDependencyParser\n>>> dep_parser = CoreNLPDependencyParser(url='http://localhost:9000')\n>>> parses = dep_parser.parse('What is the airspeed of an unladen swallow ?'.split())\n>>> [[(governor, dep, dependent) for governor, dep, dependent in parse.triples()] for parse in parses]\n[[(('What', 'WP'), 'cop', ('is', 'VBZ')), (('What', 'WP'), 'nsubj', ('airspeed', 'NN')), (('airspeed', 'NN'), 'det', ('the', 'DT')), (('airspeed', 'NN'), 'nmod', ('swallow', 'VB')), (('swallow', 'VB'), 'case', ('of', 'IN')), (('swallow', 'VB'), 'det', ('an', 'DT')), (('swallow', 'VB'), 'amod', ('unladen', 'JJ')), (('What', 'WP'), 'punct', ('?', '.'))]]\n\n\n# Tokenizer\n>>> parser = CoreNLPParser(url='http://localhost:9000')\n>>> list(parser.tokenize('What is the airspeed of an unladen swallow?'))\n['What', 'is', 'the', 'airspeed', 'of', 'an', 'unladen', 'swallow', '?']\n\n# POS Tagger\n>>> pos_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='pos')\n>>> list(pos_tagger.tag('What is the airspeed of an unladen swallow ?'.split()))\n[('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'), ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]\n\n# NER Tagger\n>>> ner_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\n>>> list(ner_tagger.tag(('Rami Eid is studying at Stony Brook University in NY'.split())))\n[('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'STATE_OR_PROVINCE')]\n\nChinese\nStart the server a little differently, still from the `stanford-corenlp-full-2018-02-27 directory:\njava -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-serverProperties StanfordCoreNLP-chinese.properties \\\n-preload tokenize,ssplit,pos,lemma,ner,parse \\\n-status_port 9001  -port 9001 -timeout 15000\n\nIn Python:\n>>> parser = CoreNLPParser('http://localhost:9001')\n>>> list(parser.tokenize(u'\u6211\u5bb6\u6ca1\u6709\u7535\u8111\u3002'))\n['\u6211\u5bb6', '\u6ca1\u6709', '\u7535\u8111', '\u3002']\n\n>>> list(parser.parse(parser.tokenize(u'\u6211\u5bb6\u6ca1\u6709\u7535\u8111\u3002')))\n[Tree('ROOT', [Tree('IP', [Tree('IP', [Tree('NP', [Tree('NN', ['\u6211\u5bb6'])]), Tree('VP', [Tree('VE', ['\u6ca1\u6709']), Tree('NP', [Tree('NN', ['\u7535\u8111'])])])]), Tree('PU', ['\u3002'])])])]\n\nArabic\nStart the server:\njava -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-serverProperties StanfordCoreNLP-arabic.properties \\\n-preload tokenize,ssplit,pos,parse \\\n-status_port 9005  -port 9005 -timeout 15000\n\nIn Python:\n>>> from nltk.parse import CoreNLPParser\n>>> parser = CoreNLPParser('http://localhost:9005')\n>>> text = u'\u0627\u0646\u0627 \u062d\u0627\u0645\u0644'\n\n# Parser.\n>>> parser.raw_parse(text)\n<list_iterator object at 0x7f0d894c9940>\n>>> list(parser.raw_parse(text))\n[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('PRP', ['\u0627\u0646\u0627'])]), Tree('NP', [Tree('NN', ['\u062d\u0627\u0645\u0644'])])])])]\n>>> list(parser.parse(parser.tokenize(text)))\n[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('PRP', ['\u0627\u0646\u0627'])]), Tree('NP', [Tree('NN', ['\u062d\u0627\u0645\u0644'])])])])]\n\n# Tokenizer / Segmenter.\n>>> list(parser.tokenize(text))\n['\u0627\u0646\u0627', '\u062d\u0627\u0645\u0644']\n\n# POS tagg\n>>> pos_tagger = CoreNLPParser('http://localhost:9005', tagtype='pos')\n>>> list(pos_tagger.tag(parser.tokenize(text)))\n[('\u0627\u0646\u0627', 'PRP'), ('\u062d\u0627\u0645\u0644', 'NN')]\n\n\n# NER tag\n>>> ner_tagger = CoreNLPParser('http://localhost:9005', tagtype='ner')\n>>> list(ner_tagger.tag(parser.tokenize(text)))\n[('\u0627\u0646\u0627', 'O'), ('\u062d\u0627\u0645\u0644', 'O')]\n\nFrench\nStart the server:\njava -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-serverProperties StanfordCoreNLP-french.properties \\\n-preload tokenize,ssplit,pos,parse \\\n-status_port 9004  -port 9004 -timeout 15000\n\nIn Python:\n>>> parser = CoreNLPParser('http://localhost:9004')\n>>> list(parser.parse('Je suis enceinte'.split()))\n[Tree('ROOT', [Tree('SENT', [Tree('NP', [Tree('PRON', ['Je']), Tree('VERB', ['suis']), Tree('AP', [Tree('ADJ', ['enceinte'])])])])])]\n>>> pos_tagger = CoreNLPParser('http://localhost:9004', tagtype='pos')\n>>> pos_tagger.tag('Je suis enceinte'.split())\n[('Je', 'PRON'), ('suis', 'VERB'), ('enceinte', 'ADJ')]\n\nGerman\nStart the server:\njava -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-serverProperties StanfordCoreNLP-german.properties \\\n-preload tokenize,ssplit,pos,ner,parse \\\n-status_port 9002  -port 9002 -timeout 15000\n\nIn Python:\n>>> parser = CoreNLPParser('http://localhost:9002')\n>>> list(parser.raw_parse('Ich bin schwanger'))\n[Tree('ROOT', [Tree('NUR', [Tree('S', [Tree('PPER', ['Ich']), Tree('VAFIN', ['bin']), Tree('AP', [Tree('ADJD', ['schwanger'])])])])])]\n>>> list(parser.parse('Ich bin schwanger'.split()))\n[Tree('ROOT', [Tree('NUR', [Tree('S', [Tree('PPER', ['Ich']), Tree('VAFIN', ['bin']), Tree('AP', [Tree('ADJD', ['schwanger'])])])])])]\n\n\n>>> pos_tagger = CoreNLPParser('http://localhost:9002', tagtype='pos')\n>>> pos_tagger.tag('Ich bin schwanger'.split())\n[('Ich', 'PPER'), ('bin', 'VAFIN'), ('schwanger', 'ADJD')]\n\n>>> pos_tagger = CoreNLPParser('http://localhost:9002', tagtype='pos')\n>>> pos_tagger.tag('Ich bin schwanger'.split())\n[('Ich', 'PPER'), ('bin', 'VAFIN'), ('schwanger', 'ADJD')]\n\n>>> ner_tagger = CoreNLPParser('http://localhost:9002', tagtype='ner')\n>>> ner_tagger.tag('Donald Trump besuchte Angela Merkel in Berlin.'.split())\n[('Donald', 'PERSON'), ('Trump', 'PERSON'), ('besuchte', 'O'), ('Angela', 'PERSON'), ('Merkel', 'PERSON'), ('in', 'O'), ('Berlin', 'LOCATION'), ('.', 'O')]\n\nSpanish\nStart the server:\njava -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-serverProperties StanfordCoreNLP-spanish.properties \\\n-preload tokenize,ssplit,pos,ner,parse \\\n-status_port 9003  -port 9003 -timeout 15000\n\nIn Python:\n>>> pos_tagger = CoreNLPParser('http://localhost:9003', tagtype='pos')\n>>> pos_tagger.tag(u'Barack Obama sali\u00f3 con Michael Jackson .'.split())\n[('Barack', 'PROPN'), ('Obama', 'PROPN'), ('sali\u00f3', 'VERB'), ('con', 'ADP'), ('Michael', 'PROPN'), ('Jackson', 'PROPN'), ('.', 'PUNCT')]\n>>> ner_tagger = CoreNLPParser('http://localhost:9003', tagtype='ner')\n>>> ner_tagger.tag(u'Barack Obama sali\u00f3 con Michael Jackson .'.split())\n[('Barack', 'PERSON'), ('Obama', 'PERSON'), ('sali\u00f3', 'O'), ('con', 'O'), ('Michael', 'PERSON'), ('Jackson', 'PERSON'), ('.', 'O')]\n\n"}, "1479": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "", "text": "\nDeprecated Answer\nThe answer below is deprecated, please use the solution on https://stackoverflow.com/a/51981566/610569 for NLTK v3.3 and above.\n\nEdited\nAs of the current Stanford parser (2015-04-20), the default output for the lexparser.sh has changed so the script below will not work.\nBut this answer is kept for legacy sake, it will still work with http://nlp.stanford.edu/software/stanford-parser-2012-11-12.zip though.\n\nOriginal Answer\nI suggest you don't mess with Jython, JPype. Let python do python stuff and let java do java stuff, get the Stanford Parser output through the console.\nAfter you've installed the Stanford Parser in your home directory ~/, just use this python recipe to get the flat bracketed parse:\nimport os\nsentence = \"this is a foo bar i want to parse.\"\n\nos.popen(\"echo '\"+sentence+\"' > ~/stanfordtemp.txt\")\nparser_out = os.popen(\"~/stanford-parser-2012-11-12/lexparser.sh ~/stanfordtemp.txt\").readlines()\n\nbracketed_parse = \" \".join( [i.strip() for i in parser_out if i.strip()[0] == \"(\"] )\nprint bracketed_parse\n\n"}, "1480": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "alvasalvas", "text": "\nThere is python interface for stanford parser\nhttp://projects.csail.mit.edu/spatial/Stanford_Parser\n"}, "1481": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "", "text": "\nThe Stanford Core NLP software page has a list of python wrappers:\nhttp://nlp.stanford.edu/software/corenlp.shtml#Extensions\n"}, "1482": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "alvasalvas", "text": "\nIf I remember well, the Stanford parser is a java library, therefore you must have a Java interpreter running on your server/computer.\nI used it once a server, combined with a php script. The script used php's exec() function to make a command-line call to the parser like so:\n<?php\n\nexec( \"java -cp /pathTo/stanford-parser.jar -mx100m edu.stanford.nlp.process.DocumentPreprocessor /pathTo/fileToParse > /pathTo/resultFile 2>/dev/null\" );\n\n?>\n\nI don't remember all the details of this command, it basically opened the fileToParse, parsed it, and wrote the output in the resultFile. PHP would then open the result file for further use.\nThe end of the command directs the parser's verbose to NULL, to prevent unnecessary command line information from disturbing the script.\nI don't know much about Python, but there might be a way to make command line calls.\nIt might not be the exact route you were hoping for, but hopefully it'll give you some inspiration. Best of luck.\n"}, "1483": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "", "text": "\nNote that this answer applies to NLTK v 3.0, and not to more recent versions.\nHere is an adaptation of danger98's code that works with nltk3.0.0 on windoze, and presumably the other platforms as well, adjust directory names as appropriate for your setup:\nimport os\nfrom nltk.parse import stanford\nos.environ['STANFORD_PARSER'] = 'd:/stanford-parser'\nos.environ['STANFORD_MODELS'] = 'd:/stanford-parser'\nos.environ['JAVAHOME'] = 'c:/Program Files/java/jre7/bin'\n\nparser = stanford.StanfordParser(model_path=\"d:/stanford-grammars/englishPCFG.ser.gz\")\nsentences = parser.raw_parse_sents((\"Hello, My name is Melroy.\", \"What is your name?\"))\nprint sentences\n\nNote that the parsing command has changed (see the source code at www.nltk.org/_modules/nltk/parse/stanford.html), and that you need to define the JAVAHOME variable.  I tried to get it to read the grammar file in situ in the jar, but have so far failed to do that.\n"}, "1484": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "alvasalvas", "text": "\nYou can use the Stanford Parsers output to create a Tree in nltk (nltk.tree.Tree).\nAssuming the stanford parser gives you a file in which there is exactly one parse tree for every sentence.\nThen this example works, though it might not look very pythonic:\nf = open(sys.argv[1]+\".output\"+\".30\"+\".stp\", \"r\")\nparse_trees_text=[]\ntree = \"\"\nfor line in f:\n  if line.isspace():\n    parse_trees_text.append(tree)\ntree = \"\"\n  elif \"(. ...))\" in line:\n#print \"YES\"\ntree = tree+')'\nparse_trees_text.append(tree)\ntree = \"\"\n  else:\ntree = tree + line\n\nparse_trees=[]\nfor t in parse_trees_text:\n  tree = nltk.Tree(t)\n  tree.__delitem__(len(tree)-1) #delete \"(. .))\" from tree (you don't need that)\n  s = traverse(tree)\n  parse_trees.append(tree)\n\n"}, "1485": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "RohithRohith", "text": "\nNote that this answer applies to NLTK v 3.0, and not to more recent versions.\nSince nobody really mentioned and it's somehow troubled me a lot, here is an alternative way to use Stanford parser in python:\nstanford_parser_jar = '../lib/stanford-parser-full-2015-04-20/stanford-parser.jar'\nstanford_model_jar = '../lib/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar'    \nparser = StanfordParser(path_to_jar=stanford_parser_jar, \n                        path_to_models_jar=stanford_model_jar)\n\nin this way, you don't need to worry about the path thing anymore.\nFor those who cannot use it properly on Ubuntu or run the code in Eclipse.\n"}, "1486": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "silverasmsilverasm", "text": "\nI am on a windows machine and you can simply run the parser normally as you do from the command like but as in a different directory so you don't need to edit the lexparser.bat file. Just put in the full path. \ncmd = r'java -cp \\Documents\\stanford_nlp\\stanford-parser-full-2015-01-30 edu.stanford.nlp.parser.lexparser.LexicalizedParser -outputFormat \"typedDependencies\" \\Documents\\stanford_nlp\\stanford-parser-full-2015-01-30\\stanford-parser-3.5.1-models\\edu\\stanford\\nlp\\models\\lexparser\\englishFactored.ser.gz stanfordtemp.txt'\nparse_out = os.popen(cmd).readlines()\n\nThe tricky part for me was realizing how to run a java program from a different path. There must be a better way but this works.\n"}, "1487": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "bob dopebob dope", "text": "\nNote that this answer applies to NLTK v 3.0, and not to more recent versions.\nA slight update (or simply alternative) on danger89's comprehensive answer on using Stanford Parser in NLTK and Python\nWith stanford-parser-full-2015-04-20, JRE 1.8 and nltk 3.0.4 (python 2.7.6), it appears that you no longer need to extract the englishPCFG.ser.gz from stanford-parser-x.x.x-models.jar or setting up any os.environ\nfrom nltk.parse.stanford import StanfordParser\n\nenglish_parser = StanfordParser('path/stanford-parser.jar', 'path/stanford-parser-3.5.2-models.jar')\n\ns = \"The real voyage of discovery consists not in seeking new landscapes, but in having new eyes.\"\n\nsentences = english_parser.raw_parse_sents((s,))\nprint sentences #only print <listiterator object> for this version\n\n#draw the tree\nfor line in sentences:\n    for sentence in line:\n        sentence.draw()\n\n"}, "1488": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "Servy", "text": "\nNote that this answer applies to NLTK v 3.0, and not to more recent versions.\nHere is the windows version of alvas's answer\nsentences = ('. '.join(['this is sentence one without a period','this is another foo bar sentence '])+'.').encode('ascii',errors = 'ignore')\ncatpath =r\"YOUR CURRENT FILE PATH\"\n\nf = open('stanfordtemp.txt','w')\nf.write(sentences)\nf.close()\n\nparse_out = os.popen(catpath+r\"\\nlp_tools\\stanford-parser-2010-08-20\\lexparser.bat \"+catpath+r\"\\stanfordtemp.txt\").readlines()\n\nbracketed_parse = \" \".join( [i.strip() for i in parse_out if i.strip() if i.strip()[0] == \"(\"] )\nbracketed_parse = \"\\n(ROOT\".join(bracketed_parse.split(\" (ROOT\")).split('\\n')\naa = map(lambda x :ParentedTree.fromstring(x),bracketed_parse)\n\nNOTES:\n\nIn lexparser.bat  you need to change all the paths into absolute path to avoid java errors such as \"class not found\"\nI strongly recommend you to apply this method under windows since I Tried several answers on the page and   all the methods communicates python with Java fails.\nwish to hear from you if you succeed on windows and wish you can tell me how you overcome all these problems.\nsearch python wrapper for stanford coreNLP to get the python version\n\n\n"}, "1489": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "Avery AndrewsAvery Andrews", "text": "\nI took many hours and finally found a simple solution for Windows users. Basically its summarized version of an existing answer by alvas, but made easy to follow(hopefully) for those who are new to stanford NLP and are Window users.\n1) Download the module you want to use, such as NER, POS etc. In my case i wanted to use NER, so i downloaded the module from http://nlp.stanford.edu/software/stanford-ner-2015-04-20.zip\n2) Unzip the file.\n3) Set the environment variables(classpath and stanford_modules) from the unzipped folder.\nimport os\nos.environ['CLASSPATH'] = \"C:/Users/Downloads/stanford-ner-2015-04-20/stanford-ner.jar\"\nos.environ['STANFORD_MODELS'] = \"C:/Users/Downloads/stanford-ner-2015-04-20/classifiers/\"\n\n4) set the environment variables for JAVA, as in where you have JAVA installed. for me it was below\nos.environ['JAVAHOME'] = \"C:/Program Files/Java/jdk1.8.0_102/bin/java.exe\"\n\n5) import the module you want\nfrom nltk.tag import StanfordNERTagger\n\n6) call the pretrained model which is present in classifier folder in the unzipped folder. add \".gz\" in the end for file extension. for me the model i wanted to use was english.all.3class.distsim.crf.ser\nst = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n\n7) Now execute the parser!! and we are done!!\nst.tag('Rami Eid is studying at Stony Brook University in NY'.split())\n\n"}, "1490": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "Sad\u0131kSad\u0131k", "text": "\nDeprecated Answer\nThe answer below is deprecated, please use the solution on https://stackoverflow.com/a/51981566/610569 for NLTK v3.3 and above.\n\nEDITED\nNote: The following answer will only work on:\n\nNLTK version ==3.2.5\nStanford Tools compiled since 2016-10-31\nPython 2.7, 3.5 and 3.6\n\nAs both tools changes rather quickly and the API might look very different 3-6 months later. Please treat the following answer as temporal and not an eternal fix.\nAlways refer to https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software for the latest instruction on how to interface Stanford NLP tools using NLTK!!\nTL;DR\nThe follow code comes from https://github.com/nltk/nltk/pull/1735#issuecomment-306091826\nIn terminal:\nwget http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip\nunzip stanford-corenlp-full-2016-10-31.zip && cd stanford-corenlp-full-2016-10-31\n\njava -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n-preload tokenize,ssplit,pos,lemma,parse,depparse \\\n-status_port 9000 -port 9000 -timeout 15000\n\nIn Python:\n>>> from nltk.tag.stanford import CoreNLPPOSTagger, CoreNLPNERTagger\n>>> from nltk.parse.corenlp import CoreNLPParser\n\n>>> stpos, stner = CoreNLPPOSTagger(), CoreNLPNERTagger()\n\n>>> stpos.tag('What is the airspeed of an unladen swallow ?'.split())\n[(u'What', u'WP'), (u'is', u'VBZ'), (u'the', u'DT'), (u'airspeed', u'NN'), (u'of', u'IN'), (u'an', u'DT'), (u'unladen', u'JJ'), (u'swallow', u'VB'), (u'?', u'.')]\n\n>>> stner.tag('Rami Eid is studying at Stony Brook University in NY'.split())\n[(u'Rami', u'PERSON'), (u'Eid', u'PERSON'), (u'is', u'O'), (u'studying', u'O'), (u'at', u'O'), (u'Stony', u'ORGANIZATION'), (u'Brook', u'ORGANIZATION'), (u'University', u'ORGANIZATION'), (u'in', u'O'), (u'NY', u'O')]\n\n\n>>> parser = CoreNLPParser(url='http://localhost:9000')\n\n>>> next(\n...     parser.raw_parse('The quick brown fox jumps over the lazy dog.')\n... ).pretty_print()  # doctest: +NORMALIZE_WHITESPACE\n                     ROOT\n                      |\n                      S\n       _______________|__________________________\n      |                         VP               |\n      |                _________|___             |\n      |               |             PP           |\n      |               |     ________|___         |\n      NP              |    |            NP       |\n  ____|__________     |    |     _______|____    |\n DT   JJ    JJ   NN  VBZ   IN   DT      JJ   NN  .\n |    |     |    |    |    |    |       |    |   |\nThe quick brown fox jumps over the     lazy dog  .\n\n>>> (parse_fox, ), (parse_wolf, ) = parser.raw_parse_sents(\n...     [\n...         'The quick brown fox jumps over the lazy dog.',\n...         'The quick grey wolf jumps over the lazy fox.',\n...     ]\n... )\n\n>>> parse_fox.pretty_print()  # doctest: +NORMALIZE_WHITESPACE\n                     ROOT\n                      |\n                      S\n       _______________|__________________________\n      |                         VP               |\n      |                _________|___             |\n      |               |             PP           |\n      |               |     ________|___         |\n      NP              |    |            NP       |\n  ____|__________     |    |     _______|____    |\n DT   JJ    JJ   NN  VBZ   IN   DT      JJ   NN  .\n |    |     |    |    |    |    |       |    |   |\nThe quick brown fox jumps over the     lazy dog  .\n\n>>> parse_wolf.pretty_print()  # doctest: +NORMALIZE_WHITESPACE\n                     ROOT\n                      |\n                      S\n       _______________|__________________________\n      |                         VP               |\n      |                _________|___             |\n      |               |             PP           |\n      |               |     ________|___         |\n      NP              |    |            NP       |\n  ____|_________      |    |     _______|____    |\n DT   JJ   JJ   NN   VBZ   IN   DT      JJ   NN  .\n |    |    |    |     |    |    |       |    |   |\nThe quick grey wolf jumps over the     lazy fox  .\n\n>>> (parse_dog, ), (parse_friends, ) = parser.parse_sents(\n...     [\n...         \"I 'm a dog\".split(),\n...         \"This is my friends ' cat ( the tabby )\".split(),\n...     ]\n... )\n\n>>> parse_dog.pretty_print()  # doctest: +NORMALIZE_WHITESPACE\n        ROOT\n         |\n         S\n  _______|____\n |            VP\n |    ________|___\n NP  |            NP\n |   |         ___|___\nPRP VBP       DT      NN\n |   |        |       |\n I   'm       a      dog\n\nPlease take a look at http://www.nltk.org/_modules/nltk/parse/corenlp.html  for more information on of the Stanford API. Take a look at the docstrings!\n"}, "1491": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "Servy", "text": "\nNote that this answer applies to NLTK v 3.0, and not to more recent versions.\nI cannot leave this as a comment because of reputation, but since I spent (wasted?) some time solving this I would rather share my problem/solution to get this parser to work in NLTK.\nIn the excellent answer from alvas, it is mentioned that:\n\ne.g. for the Parser, there won't be a model directory.\n\nThis led me wrongly to:\n\nnot be careful to the value I put to STANFORD_MODELS  (and only care about my CLASSPATH)\nleave ../path/tostanford-parser-full-2015-2012-09/models directory * virtually empty* (or with a jar file whose name did not match nltk regex)!\n\nIf the OP, like me, just wanted to use the parser, it may be confusing that when not downloading anything else (no POStagger, no NER,...) and following all these instructions, we still get an error.\nEventually, for any CLASSPATH given (following examples and explanations in answers from this thread) I would still get the error:\n\nNLTK was unable to find stanford-parser-(\\d+)(.(\\d+))+-models.jar!\n  Set the CLASSPATH environment variable. For more information, on\n  stanford-parser-(\\d+)(.(\\d+))+-models.jar,\n\nsee:\n    http://nlp.stanford.edu/software/lex-parser.shtml\nOR:\n\nNLTK was unable to find stanford-parser.jar! Set the CLASSPATH\n  environment variable. For more information, on stanford-parser.jar,\n  see: http://nlp.stanford.edu/software/lex-parser.shtml\n\nThough, importantly, I could correctly load and use the parser if I called the function with all arguments and path fully specified, as in:\nstanford_parser_jar = '../lib/stanford-parser-full-2015-04-20/stanford-parser.jar'\nstanford_model_jar = '../lib/stanford-parser-full-2015-04-20/stanfor-parser-3.5.2-models.jar'    \nparser = StanfordParser(path_to_jar=stanford_parser_jar, \n                    path_to_models_jar=stanford_model_jar)\n\nSolution for Parser alone:\nTherefore the error came from NLTK and how it is looking for jars using the supplied STANFORD_MODELS and CLASSPATH environment variables. To solve this, the *-models.jar, with the correct formatting (to match the regex in NLTK code, so no -corenlp-....jar) must be located in the folder designated by STANFORD_MODELS.\nNamely, I first created:\nmkdir stanford-parser-full-2015-12-09/models\n\nThen added in .bashrc:\nexport STANFORD_MODELS=/path/to/stanford-parser-full-2015-12-09/models\n\nAnd finally, by copying stanford-parser-3.6.0-models.jar (or corresponding version), into:\npath/to/stanford-parser-full-2015-12-09/models/\n\nI could get StanfordParser to load smoothly in python with the classic CLASSPATH that points to stanford-parser.jar. Actually, as such, you can call StanfordParser with no parameters, the default will just work.\n"}, "1492": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "Zhong ZhuZhong Zhu", "text": "\nI am using nltk version 3.2.4. And following code worked for me.\nfrom nltk.internals import find_jars_within_path\nfrom nltk.tag import StanfordPOSTagger\nfrom nltk import word_tokenize\n\n# Alternatively to setting the CLASSPATH add the jar and model via their \npath:\njar = '/home/ubuntu/stanford-postagger-full-2017-06-09/stanford-postagger.jar'\nmodel = '/home/ubuntu/stanford-postagger-full-2017-06-09/models/english-left3words-distsim.tagger'\n\npos_tagger = StanfordPOSTagger(model, jar)\n\n# Add other jars from Stanford directory\nstanford_dir = pos_tagger._stanford_jar.rpartition('/')[0]\nstanford_jars = find_jars_within_path(stanford_dir)\npos_tagger._stanford_jar = ':'.join(stanford_jars)\n\ntext = pos_tagger.tag(word_tokenize(\"Open app and play movie\"))\nprint(text)\n\nOutput:\n[('Open', 'VB'), ('app', 'NN'), ('and', 'CC'), ('play', 'VB'), ('movie', 'NN')]\n\n"}, "1493": {"topic": "How to use Stanford Parser in NLTK using Python", "user_name": "Ted PetrouTed Petrou", "text": "\nA new development of the Stanford parser based on a neural model, trained using Tensorflow is very recently made available to be used as a python API. This model is supposed to be far more accurate than the Java-based moel. You can certainly integrate with an NLTK pipeline.\nLink to the parser. Ther repository contains pre-trained parser models for 53 languages.\n"}, "1494": {"topic": "Ordinal numbers replacement", "user_name": "skornosskornos", "text": "\nI am currently looking for the way to replace words like first, second, third,...with appropriate ordinal number representation (1st, 2nd, 3rd).\nI have been googling for the last week and I didn't find any useful standard tool or any function from NLTK.\nSo is there any or should I write some regular expressions manually?\nThanks for any advice\n"}, "1495": {"topic": "Ordinal numbers replacement", "user_name": "", "text": "\nThe package number-parser can parse ordinal words (\"first\", \"second\", etc) to integers.\nfrom number_parser import parse_ordinal\nn = parse_ordinal(\"first\")\n\nTo convert an integer to \"1st\", \"2nd\", etc, you can use the following:\ndef ordinal(n: int):\n    if 11 <= (n % 100) <= 13:\n        suffix = 'th'\n    else:\n        suffix = ['th', 'st', 'nd', 'rd', 'th'][min(n % 10, 4)]\n    return str(n) + suffix\n\nHere is a more terse but less readable version (taken from Gareth on codegolf):\nordinal = lambda n: \"%d%s\" % (n,\"tsnrhtdd\"[(n//10%10!=1)*(n%10<4)*n%10::4])\n\nThis works on any number:\nprint([ordinal(n) for n in range(1,32)])\n\n['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th',\n '11th', '12th', '13th', '14th', '15th', '16th', '17th', '18th', '19th',\n '20th', '21st', '22nd', '23rd', '24th', '25th', '26th', '27th', '28th',\n '29th', '30th', '31st']\n\n"}, "1496": {"topic": "Ordinal numbers replacement", "user_name": "Ben DavisBen Davis", "text": "\nIf you don't want to pull in an additional dependency on an external library (as suggested by luckydonald) but also don't want the future maintainer of the code to haunt you down and kill you (because you used golfed code in production) then here's a short-but-maintainable variant:\ndef make_ordinal(n):\n    '''\n    Convert an integer into its ordinal representation::\n\n        make_ordinal(0)   => '0th'\n        make_ordinal(3)   => '3rd'\n        make_ordinal(122) => '122nd'\n        make_ordinal(213) => '213th'\n    '''\n    n = int(n)\n    if 11 <= (n % 100) <= 13:\n        suffix = 'th'\n    else:\n        suffix = ['th', 'st', 'nd', 'rd', 'th'][min(n % 10, 4)]\n    return str(n) + suffix\n\n"}, "1497": {"topic": "Ordinal numbers replacement", "user_name": "", "text": "\nHow about this:\nsuf = lambda n: \"%d%s\"%(n,{1:\"st\",2:\"nd\",3:\"rd\"}.get(n%100 if (n%100)<20 else n%10,\"th\"))\nprint [suf(n) for n in xrange(1,32)]\n\n['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th',\n '11th', '12th', '13th', '14th', '15th', '16th', '17th', '18th', '19th',\n '20th', '21st', '22nd', '23rd', '24th', '25th', '26th', '27th', '28th',\n '29th', '30th', '31st']\n\n"}, "1498": {"topic": "Ordinal numbers replacement", "user_name": "Florian BruckerFlorian Brucker", "text": "\nAnother solution to format numbers to 1th, 2nd, 3rd, ... is the num2words library (pip | github).\nIt especially offers different languages, so localization/internationalization (aka. l10n/i18n) is a no-brainer.\nUsage is easy after you installed it with pip install num2words:\nfrom num2words import num2words\n# english is default\nnum2words(4458, to=\"ordinal_num\")\n'4458th'\n\n# examples for other languages\nnum2words(4458, lang=\"en\", to=\"ordinal_num\")\n'4458th'\n\nnum2words(4458, lang=\"es\", to=\"ordinal_num\")\n'4458\u00ba'\n\nnum2words(4458, lang=\"de\", to=\"ordinal_num\")\n'4458.'\n\nnum2words(4458, lang=\"id\", to=\"ordinal_num\")\n'ke-4458'\n\nBonus:\nnum2words(4458, lang=\"en\", to=\"ordinal\")\n'four thousand, four hundred and fifty-eighth'\n\n\nIf you need to parse the words \"first\", \"second\", \"third\", ... to numbers 1, 2, 3 first (as asked in the question, too), you can use the number-parser library (pip | github) to do that:\nfrom number_parser import parse_ordinal\nparse_ordinal(\"twenty third\")\n23\n\nNote that it only supports English, Hindi, Spanish, Ukrainian and Russian at the time of writing this answer.\n"}, "1499": {"topic": "Ordinal numbers replacement", "user_name": "t\u00f6rzsm\u00f3kus", "text": "\nThe accepted answer to a previous question has an algorithm for half of this: it turns \"first\" into 1. To go from there to \"1st\", do something like:\nsuffixes = [\"th\", \"st\", \"nd\", \"rd\", ] + [\"th\"] * 16\nsuffixed_num = str(num) + suffixes[num % 100]\n\nThis only works for numbers 0-19.\n"}, "1500": {"topic": "Ordinal numbers replacement", "user_name": "evandrixevandrix", "text": "\nI found myself doing something similar, needing to convert addresses with ordinal numbers ('Third St') to a format that a geocoder could comprehend ('3rd St').  While this isn't very elegant, one quick and dirty solution is to use the inflect.py to generate a dictionary for translation.\ninflect.py has a number_to_words() function, that will turn a number (e.g. 2) to its word form (e.g. 'two').  Additionally, there is an ordinal() function that will take any number (numeral or word form) and turn it into its ordinal form (e.g. 4 -> fourth, six -> sixth).  Neither of those, on their own, do what you're looking for, but together you can use them to generate a dictionary to translate any supplied ordinal-number-word (within a reasonable range) to its respective numeral ordinal.  Take a look:\n>>> import inflect\n>>> p = inflect.engine()\n>>> word_to_number_mapping = {}\n>>>\n>>> for i in range(1, 100):\n...     word_form = p.number_to_words(i)  # 1 -> 'one'\n...     ordinal_word = p.ordinal(word_form)  # 'one' -> 'first'\n...     ordinal_number = p.ordinal(i)  # 1 -> '1st'\n...     word_to_number_mapping[ordinal_word] = ordinal_number  # 'first': '1st'\n...\n>>> print word_to_number_mapping['sixth']\n6th\n>>> print word_to_number_mapping['eleventh']\n11th\n>>> print word_to_number_mapping['forty-third']\n43rd\n\nIf you're willing to commit some time, it might be possible to examine inflect.py's inner-workings in both of those functions and build your own code to do this dynamically (I haven't tried to do this).\n"}, "1501": {"topic": "Ordinal numbers replacement", "user_name": "", "text": "\nI wanted to use ordinals for a project of mine and after a few prototypes I think this method although not small will work for any positive integer, yes any integer.\nIt works by determiniting if the number is above or below 20, if the number is below 20 it will turn the int 1 into the string 1st , 2 , 2nd; 3, 3rd; and the rest will have \"st\" added to it. \nFor numbers over 20 it will take the last and second to last digits, which I have called the tens and unit respectively and test them to see what to add to the number. \nThis is in python by the way, so I'm not sure if other languages will be able to find the last or second to last digit on a string if they do it should translate pretty easily.\ndef o(numb):\n    if numb < 20: #determining suffix for < 20\n        if numb == 1: \n            suffix = 'st'\n        elif numb == 2:\n            suffix = 'nd'\n        elif numb == 3:\n            suffix = 'rd'\n        else:\n            suffix = 'th'  \n    else:   #determining suffix for > 20\n        tens = str(numb)\n        tens = tens[-2]\n        unit = str(numb)\n        unit = unit[-1]\n        if tens == \"1\":\n           suffix = \"th\"\n        else:\n            if unit == \"1\": \n                suffix = 'st'\n            elif unit == \"2\":\n                suffix = 'nd'\n            elif unit == \"3\":\n                suffix = 'rd'\n            else:\n                suffix = 'th'\n    return str(numb)+ suffix\n\nI called the function \"o\" for ease of use and can be called by importing the file name which I called \"ordinal\" by import ordinal then ordinal.o(number).\nLet me know what you think :D\n"}, "1502": {"topic": "Ordinal numbers replacement", "user_name": "luckydonaldluckydonald", "text": "\nIf using django, you could do:\nfrom django.contrib.humanize.templatetags.humanize import ordinal\nvar = ordinal(number)\n\n(or use ordinal in a django template as the template filter it was intended to be, though calling it like this from python code works as well)\nIf not using django you could steal their implementation which is very neat.\n"}, "1503": {"topic": "Ordinal numbers replacement", "user_name": "CommunityBot", "text": "\nThere's an ordinal function in humanize\npip install humanize\n>>> [(x, humanize.ordinal(x)) for x in (1, 2, 3, 4, 20, 21, 22, 23, 24, 100, 101,\n...                                     102, 103, 113, -1, 0, 1.2, 13.6)]\n[(1, '1st'), (2, '2nd'), (3, '3rd'), (4, '4th'), (20, '20th'), (21, '21st'),\n (22, '22nd'), (23, '23rd'), (24, '24th'), (100, '100th'), (101, '101st'),\n (102, '102nd'), (103, '103rd'), (113, '113th'), (-1, '-1th'), (0, '0th'),\n (1.2, '1st'), (13.6, '13th')]\n\n\n"}, "1504": {"topic": "Ordinal numbers replacement", "user_name": "lvclvc", "text": "\nHere is a more complicated solution I just wrote that takes into account compounded ordinals. So it works from first all the way to nine hundred and ninety ninth. I needed it to convert string street names to the number ordinals:\nimport re\nfrom collections import OrderedDict\n\nONETHS = {\n    'first': '1ST', 'second': '2ND', 'third': '3RD', 'fourth': '4TH', 'fifth': '5TH', 'sixth': '6TH', 'seventh': '7TH',\n    'eighth': '8TH', 'ninth': '9TH'\n}\n\nTEENTHS = {\n    'tenth': '10TH', 'eleventh': '11TH', 'twelfth': '12TH', 'thirteenth': '13TH',\n    'fourteenth': '14TH', 'fifteenth': '15TH', 'sixteenth': '16TH', 'seventeenth': '17TH', 'eighteenth': '18TH',\n    'nineteenth': '19TH'\n}\n\nTENTHS = {\n    'twentieth': '20TH', 'thirtieth': '30TH', 'fortieth': '40TH', 'fiftieth': '50TH', 'sixtieth': '60TH',\n    'seventieth': '70TH', 'eightieth': '80TH', 'ninetieth': '90TH',\n}\n\nHUNDREDTH = {'hundredth': '100TH'}  # HUNDREDTH not s\n\nONES = {'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7', 'eight': '8',\n        'nine': '9'}\n\nTENS = {'twenty': '20', 'thirty': '30', 'forty': '40', 'fifty': '50', 'sixty': '60', 'seventy': '70', 'eighty': '80',\n        'ninety': '90'}\n\nHUNDRED = {'hundred': '100'}\n\n# Used below for ALL_ORDINALS\nALL_THS = {}\nALL_THS.update(ONETHS)\nALL_THS.update(TEENTHS)\nALL_THS.update(TENTHS)\nALL_THS.update(HUNDREDTH)\n\nALL_ORDINALS = OrderedDict()\nALL_ORDINALS.update(ALL_THS)\nALL_ORDINALS.update(TENS)\nALL_ORDINALS.update(HUNDRED)\nALL_ORDINALS.update(ONES)\n\n\ndef split_ordinal_word(word):\n    ordinals = []\n    if not word:\n        return ordinals \n\n    for key, value in ALL_ORDINALS.items():\n        if word.startswith(key):\n            ordinals.append(key)\n            ordinals += split_ordinal_word(word[len(key):])\n            break\n    return ordinals\n\ndef get_ordinals(s):\n    ordinals, start, end = [], [], []\n    s = s.strip().replace('-', ' ').replace('and', '').lower()\n    s = re.sub(' +',' ', s)  # Replace multiple spaces with a single space\n    s = s.split(' ')\n\n    for word in s:\n        found_ordinals = split_ordinal_word(word)\n        if found_ordinals:\n            ordinals += found_ordinals\n        else:  # else if word, for covering blanks\n            if ordinals:  # Already have some ordinals\n                end.append(word)\n            else:\n                start.append(word)\n    return start, ordinals, end\n\n\ndef detect_ordinal_pattern(ordinals):\n    ordinal_length = len(ordinals)\n    ordinal_string = '' # ' '.join(ordinals)\n    if ordinal_length == 1:\n        ordinal_string = ALL_ORDINALS[ordinals[0]]\n    elif ordinal_length == 2:\n        if ordinals[0] in ONES.keys() and ordinals[1] in HUNDREDTH.keys():\n            ordinal_string = ONES[ordinals[0]] + '00TH'\n        elif ordinals[0] in HUNDRED.keys() and ordinals[1] in ONETHS.keys():\n            ordinal_string = HUNDRED[ordinals[0]][:-1] + ONETHS[ordinals[1]]\n        elif ordinals[0] in TENS.keys() and ordinals[1] in ONETHS.keys():\n            ordinal_string = TENS[ordinals[0]][0] + ONETHS[ordinals[1]]\n    elif ordinal_length == 3:\n        if ordinals[0] in HUNDRED.keys() and ordinals[1] in TENS.keys() and ordinals[2] in ONETHS.keys():\n            ordinal_string = HUNDRED[ordinals[0]][0] + TENS[ordinals[1]][0] + ONETHS[ordinals[2]]\n        elif ordinals[0] in ONES.keys() and ordinals[1] in HUNDRED.keys() and ordinals[2] in ALL_THS.keys():\n            ordinal_string =  ONES[ordinals[0]] + ALL_THS[ordinals[2]]\n    elif ordinal_length == 4:\n        if ordinals[0] in ONES.keys() and ordinals[1] in HUNDRED.keys() and ordinals[2] in TENS.keys() and \\\n           ordinals[3] in ONETHS.keys():\n                ordinal_string = ONES[ordinals[0]] + TENS[ordinals[2]][0] + ONETHS[ordinals[3]]\n\n    return ordinal_string\n\nAnd here is some sample usage:\n# s = '32 one   hundred and forty-third st toronto, on'\n#s = '32 forty-third st toronto, on'\n#s = '32 one-hundredth st toronto, on'\n#s = '32 hundred and third st toronto, on'\n#s = '32 hundred and thirty first st toronto, on'\n# s = '32 nine hundred and twenty third st toronto, on'\n#s = '32 nine hundred and ninety ninth st toronto, on'\ns = '32 sixty sixth toronto, on'\n\nst, ords, en = get_ordinals(s)\nprint st, detect_ordinal_pattern(ords), en\n\n"}, "1505": {"topic": "Ordinal numbers replacement", "user_name": "", "text": "\nthis function works well for each number n.  If n is negative, it is converted to positive. If n is not integer, it is converted to integer. \ndef ordinal( n ):\n\n    suffix = ['th', 'st', 'nd', 'rd', 'th', 'th', 'th', 'th', 'th', 'th']\n\n    if n < 0:\n        n *= -1\n\n    n = int(n)\n\n    if n % 100 in (11,12,13):\n        s = 'th'\n    else:\n        s = suffix[n % 10]\n\n    return str(n) + s\n\n"}, "1506": {"topic": "Ordinal numbers replacement", "user_name": "alukachalukach", "text": "\nIf you don't want to import an external module and prefer a one-line solution, then the following is probably (slightly) more readable than the accepted answer:\ndef suffix(i):\n    return {1:\"st\", 2:\"nd\", 3:\"rd\"}.get(i%10*(i%100 not in [11,12,13]), \"th\"))\n\nIt uses dictionary .get, as suggested by https://codereview.stackexchange.com/a/41300/90593 and https://stackoverflow.com/a/36977549/5069869.\nI made use of multiplication with a boolean to handle the special cases (11,12,13) without having to start an if-block. If the condition (i%100 not in [11,12,13]) evaluates to False, the whole number is 0 and we get the default 'th' case.\n"}, "1507": {"topic": "Ordinal numbers replacement", "user_name": "", "text": "\nGareth's code expressed using the modern .format()\nordinal = lambda n: \"{}{}\".format(n,\"tsnrhtdd\"[(n/10%10!=1)*(n%10<4)*n%10::4])\n\n"}, "1508": {"topic": "Ordinal numbers replacement", "user_name": "HounganHoungan", "text": "\nThis can handle any length number, the exceptions for ...#11 to ...#13 and negative integers.\ndef ith(i):return(('th'*(10<(abs(i)%100)<14))+['st','nd','rd',*['th']*7][(abs(i)-1)%10])[0:2]\n\nI suggest using ith() as a name to avoid overriding the builtin ord().\n# test routine\nfor i in range(-200,200):\n    print(i,ith(i))\n\nNote: Tested with Python 3.6; The abs() function was available without explicitly including a math module.\n"}, "1509": {"topic": "Ordinal numbers replacement", "user_name": "Monika SulikMonika Sulik", "text": "\nTry this \nimport sys\n\na = int(sys.argv[1])\n\nfor i in range(1,a+1):\n\nj = i\nif(j%100 == 11 or j%100 == 12 or j%100 == 13):\n    print(\"%dth Hello\"%(j))\n    continue            \ni %= 10\nif ((j%10 == 1) and ((i%10 != 0) or (i%10 != 1))):\n    print(\"%dst Hello\"%(j))\nelif ((j%10 == 2) and ((i%10 != 0) or (i%10 != 1))):\n    print(\"%dnd Hello\"%(j))\nelif ((j%10 == 3) and ((i%10 != 0) or (i%10 != 1))):\n    print(\"%drd Hello\"%(j))\nelse:\n    print(\"%dth Hello\"%(j))\n\n"}, "1510": {"topic": "Ordinal numbers replacement", "user_name": "Tim DielsTim Diels", "text": "\nI salute Gareth's lambda code. So elegant. I only half-understand how it works though. So I tried to deconstruct it and came up with this:\ndef ordinal(integer):\n\n    int_to_string = str(integer)\n\n    if int_to_string == '1' or int_to_string == '-1':\n        print int_to_string+'st'\n        return int_to_string+'st';\n    elif int_to_string == '2' or int_to_string == '-2':\n        print int_to_string+'nd'\n        return int_to_string+'nd';\n    elif int_to_string == '3' or int_to_string == '-3':\n        print int_to_string+'rd'\n        return int_to_string+'rd';\n\n    elif int_to_string[-1] == '1' and int_to_string[-2] != '1':\n        print int_to_string+'st'\n        return int_to_string+'st';\n    elif int_to_string[-1] == '2' and int_to_string[-2] != '1':\n        print int_to_string+'nd'\n        return int_to_string+'nd';\n    elif int_to_string[-1] == '3' and int_to_string[-2] != '1':\n        print int_to_string+'rd'\n        return int_to_string+'rd';\n\n    else:\n        print int_to_string+'th'\n        return int_to_string+'th';\n\n\n>>> print [ordinal(n) for n in range(1,25)]\n1st\n2nd\n3rd\n4th\n5th\n6th\n7th\n8th\n9th\n10th\n11th\n12th\n13th\n14th\n15th\n16th\n17th\n18th\n19th\n20th\n21st\n22nd\n23rd\n24th\n['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th',             \n'11th', '12th', '13th', '14th', '15th', '16th', '17th', '18th', '19th', \n'20th', '21st', '22nd', '23rd', '24th']\n\n"}, "1511": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "agiledatabase", "text": "\nI get the following error when trying to install Punkt for nltk:\nnltk.download('punkt')    \n [nltk_data] Error loading Punkt: <urlopen error [SSL:\n [nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed\n [nltk_data]     (_ssl.c:590)>\nFalse\n\n"}, "1512": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "user3429986user3429986", "text": "\nTLDR: Here is a better solution: https://github.com/gunthercox/ChatterBot/issues/930#issuecomment-322111087\nNote that when you run nltk.download(), a window will pop up and let you select which packages to download (Download is not automatically started right away).\nTo complement the accepted answer, the following is a complete list of directories that will be searched on Mac (not limited to the one mentioned in the accepted answer):\n\n    - '/Users/YOUR_USERNAME/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/YOUR_USERNAME/YOUR_VIRTUAL_ENV_DIRECTORY/nltk_data'\n    - '/Users/YOUR_USERNAME/YOUR_VIRTUAL_ENV_DIRECTORY/share/nltk_data'\n    - '/Users/YOUR_USERNAME/YOUR_VIRTUAL_ENV_DIRECTORY/lib/nltk_data'\n\nIn case the link above dies, here is the solution pasted in its entirety:\nimport nltk\nimport ssl\n\ntry:\n    _create_unverified_https_context = ssl._create_unverified_context\nexcept AttributeError:\n    pass\nelse:\n    ssl._create_default_https_context = _create_unverified_https_context\n\nnltk.download()\n\nRun the above code in your favourite Python IDE or via the command line.\n"}, "1513": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "pookie", "text": "\nThis works by disabling SSL check!\nimport nltk\nimport ssl\n\ntry:\n    _create_unverified_https_context = ssl._create_unverified_context\nexcept AttributeError:\n    pass\nelse:\n    ssl._create_default_https_context = _create_unverified_https_context\n\nnltk.download()\n\n"}, "1514": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "fstangfstang", "text": "\nRun the Python interpreter and type the commands:\nimport nltk\nnltk.download()\n\nfrom here: http://www.nltk.org/data.html\nif you get an SSL/Certificate error, run the following command\nbash /Applications/Python 3.6/Install Certificates.command\nfrom here: ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:749)\n"}, "1515": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "ishwardgretishwardgret", "text": "\nSearch 'Install Certificates.command' in the finder and open it.\nThen do the following steps in the terminal:\npython3\nimport nltk\nnltk.download()\n\n"}, "1516": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "Selvaram G", "text": "\nThe downloader script is broken. As a temporal workaround can manually download the punkt tokenizer from here and then place the unzipped folder in the corresponding location. The default folders for each OS are:\n\nWindows: C:\\nltk_data\\tokenizers\nOSX: /usr/local/share/nltk_data/tokenizers\nUnix: /usr/share/nltk_data/tokenizers\n\n"}, "1517": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "AdiAdi", "text": "\nThis is how I solved it for MAC OS.\nInitially after installing nltk, I was getting the SSL error.\nSolution:\nGoto\ncd /Applications/Python\\ 3.8\n\nRun the command\n./Install\\ Certificates.command\n\nNow if you try again, it should work!\nThanks a lot to this article!\n"}, "1518": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "xxx", "text": "\nYou just need to Install the certificate doing this simple step \nIn the python application folder double-click on the file 'Certificates.command'\nthis will make a prompt window show in your screen and basically will automatically install the certificate for you, close this window and try again.\n"}, "1519": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "yashyash", "text": "\nMy solution is:\n\nDownload punkt.zip from here and unzip\nCreate nltk_data/tokenizers folders under home folder\nPut punkt folder under tokenizers folder\n\n"}, "1520": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "elyaseelyase", "text": "\nThere is a very simple way to fix all of this as written in the formal bug report for anyone else coming across this problem recently (e.g. 2019) and using MacOS. From the bug report at https://bugs.python.org/issue28150:\n\n...there is a simple double-clickable or command-line-runnable script (\"/Applications/Python 3.6/Install Certificates.command\") that does two things: 1. uses pip to install certifi and 2. creates a symlink in the OpenSSL directory to certifi's installed bundle location. \n\nSimply running the \"Install Certificates.command\" script worked for me on MacOS (10.15 beta as of this writing) and I was off and running.\n"}, "1521": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "Sreekiran A RSreekiran A R", "text": "\nMy solution after nothing worked. I navigated, via the GUI to the Python 3.7 folder, opened the 'Certificates.command' file in terminal and the SSL issue was immediately resolved. \n"}, "1522": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "thiago89thiago89", "text": "\nA bit late to the party but I just entered Certificates.command into Spotlight which found it and ran it. All fixed in seconds.\nI'm running mac Catalina and using python 3.7 installed by Homebrew\n"}, "1523": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "gocengocen", "text": "\nIt means that you are not using HTTPS to work consistently with other run time dependencies for Python etc.\nIf you are using Linux (Ubuntu)\n~$ sudo apt-get install ca-certificates\n\nShould solve the issue.\nIf you are using this in a script with a docker file, you have to make sure you have install the the ca-certificates modules in your docker file.\n"}, "1524": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "Michael HawkinsMichael Hawkins", "text": "\nFor mac users,\njust copy paste the following in the terminal:\n/Applications/Python\\ 3.10/Install\\ Certificates.command ; exit;\n\n"}, "1525": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "Cormac O'KeeffeCormac O'Keeffe", "text": "\nFirst go to the path  /Applications/Python 3.6/ and run \nInstall Certificates.command\nYou will admin rights for the same.\nIf you are unable to download it, then as other answer suggest you can download directly and place it. You need to place them in the following directory structure.\n> nltk_data\n          > corpora\n                   > brown\n                   > conll2000\n                   > movie_reviews\n                   > wordnet\n          > taggers\n                   > averaged_perceptron_tagger\n          > tokenizers\n                      > punkt\n\n"}, "1526": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "ChezChez", "text": "\nUpdating the python certificates worked for me.\nAt the top of your script, keep:\nimport nltk\nnltk.download('punkt')\n\nIn a separate terminal run (Mac):\nbash /Applications/Python <version>/Install Certificates.command\n\n"}, "1527": {"topic": "NLTK download SSL: Certificate verify failed", "user_name": "Sibeesh Venu", "text": "\nFor me, the solution was much simpler: I was still connected to my corporate network/VPN which blocks certain types of downloads. Switching the network made the SSL error disappear.\n"}, "1528": {"topic": "wordnet lemmatization and pos tagging in python", "user_name": "user1946217user1946217", "text": "\nI wanted to use wordnet lemmatizer in python and I have learnt that the default pos tag is NOUN and that it does not output the correct lemma for a verb, unless the pos tag is explicitly specified as VERB.\nMy question is what is the best shot inorder to perform the above lemmatization accurately?\nI did the pos tagging using nltk.pos_tag and I am lost in integrating the tree bank pos tags to wordnet compatible pos tags. Please help\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlmtzr = WordNetLemmatizer()\ntagged = nltk.pos_tag(tokens)\n\nI get the output tags in NN,JJ,VB,RB. How do I change these to wordnet compatible tags?\nAlso do I have to train nltk.pos_tag() with a tagged corpus or can I use it directly on my data to evaluate?\n"}, "1529": {"topic": "wordnet lemmatization and pos tagging in python", "user_name": "", "text": "\nFirst of all, you can use nltk.pos_tag() directly without training it. \nThe function will load a pretrained tagger from a file. You can see the file name \nwith nltk.tag._POS_TAGGER:\nnltk.tag._POS_TAGGER\n>>> 'taggers/maxent_treebank_pos_tagger/english.pickle' \n\nAs it was trained with the Treebank corpus, it also uses the Treebank tag set.\nThe following function would map the treebank tags to WordNet part of speech names: \nfrom nltk.corpus import wordnet\n\ndef get_wordnet_pos(treebank_tag):\n\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return ''\n\nYou can then use the return value with the lemmatizer:\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nlemmatizer.lemmatize('going', wordnet.VERB)\n>>> 'go'\n\nCheck the return value before passing it to the Lemmatizer because an empty string would give a KeyError. \n"}, "1530": {"topic": "wordnet lemmatization and pos tagging in python", "user_name": "SuzanaSuzana", "text": "\n\nSteps to convert : Document->Sentences->Tokens->POS->Lemmas\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\n#example text text = 'What can I say about this place. The staff of these restaurants is nice and the eggplant is not bad'\n\nclass Splitter(object):\n    \"\"\"\n    split the document into sentences and tokenize each sentence\n    \"\"\"\n    def __init__(self):\n        self.splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n\n    def split(self,text):\n        \"\"\"\n        out : ['What', 'can', 'I', 'say', 'about', 'this', 'place', '.']\n        \"\"\"\n        # split into single sentence\n        sentences = self.splitter.tokenize(text)\n        # tokenization in each sentences\n        tokens = [self.tokenizer.tokenize(sent) for sent in sentences]\n        return tokens\n\n\nclass LemmatizationWithPOSTagger(object):\n    def __init__(self):\n        pass\n    def get_wordnet_pos(self,treebank_tag):\n        \"\"\"\n        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n        \"\"\"\n        if treebank_tag.startswith('J'):\n            return wordnet.ADJ\n        elif treebank_tag.startswith('V'):\n            return wordnet.VERB\n        elif treebank_tag.startswith('N'):\n            return wordnet.NOUN\n        elif treebank_tag.startswith('R'):\n            return wordnet.ADV\n        else:\n            # As default pos in lemmatization is Noun\n            return wordnet.NOUN\n\n    def pos_tag(self,tokens):\n        # find the pos tagginf for each tokens [('What', 'WP'), ('can', 'MD'), ('I', 'PRP') ....\n        pos_tokens = [nltk.pos_tag(token) for token in tokens]\n\n        # lemmatization using pos tagg   \n        # convert into feature set of [('What', 'What', ['WP']), ('can', 'can', ['MD']), ... ie [original WORD, Lemmatized word, POS tag]\n        pos_tokens = [ [(word, lemmatizer.lemmatize(word,self.get_wordnet_pos(pos_tag)), [pos_tag]) for (word,pos_tag) in pos] for pos in pos_tokens]\n        return pos_tokens\n\nlemmatizer = WordNetLemmatizer()\nsplitter = Splitter()\nlemmatization_using_pos_tagger = LemmatizationWithPOSTagger()\n\n#step 1 split document into sentence followed by tokenization\ntokens = splitter.split(text)\n\n#step 2 lemmatization using pos tagger \nlemma_pos_token = lemmatization_using_pos_tagger.pos_tag(tokens)\nprint(lemma_pos_token)\n\n"}, "1531": {"topic": "wordnet lemmatization and pos tagging in python", "user_name": "", "text": "\nAs in the source code of nltk.corpus.reader.wordnet (http://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html)\n#{ Part-of-speech constants\n ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n#}\nPOS_LIST = [NOUN, VERB, ADJ, ADV]\n\n"}, "1532": {"topic": "wordnet lemmatization and pos tagging in python", "user_name": "DeepakDeepak", "text": "\nYou can create a map using the python default dict and take advantage of the fact that for the lemmatizer the default tag is Noun.\nfrom nltk.corpus import wordnet as wn\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk import word_tokenize, pos_tag\nfrom collections import defaultdict\n\ntag_map = defaultdict(lambda : wn.NOUN)\ntag_map['J'] = wn.ADJ\ntag_map['V'] = wn.VERB\ntag_map['R'] = wn.ADV\n\ntext = \"Another way of achieving this task\"\ntokens = word_tokenize(text)\nlmtzr = WordNetLemmatizer()\n\nfor token, tag in pos_tag(tokens):\n    lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n    print(token, \"=>\", lemma)\n\n"}, "1533": {"topic": "wordnet lemmatization and pos tagging in python", "user_name": "pg2455pg2455", "text": "\n@Suzana_K was working. But I there are some case result in KeyError as @ Clock Slave mention. \nConvert treebank tags to Wordnet tag\nfrom nltk.corpus import wordnet\n\ndef get_wordnet_pos(treebank_tag):\n\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return None # for easy if-statement \n\nNow, we only input pos into lemmatize function only if we have wordnet tag \nfrom nltk.stem.wordnet import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\ntagged = nltk.pos_tag(tokens)\nfor word, tag in tagged:\n    wntag = get_wordnet_pos(tag)\n    if wntag is None:# not supply tag in case of None\n        lemma = lemmatizer.lemmatize(word) \n    else:\n        lemma = lemmatizer.lemmatize(word, pos=wntag) \n\n"}, "1534": {"topic": "wordnet lemmatization and pos tagging in python", "user_name": "", "text": "\nYou can do as following:\nimport nltk\nfrom nltk.corpus import wordnet\n\nwordnet_map = {\n    \"N\": wordnet.NOUN,\n    \"V\": wordnet.VERB,\n    \"J\": wordnet.ADJ,\n    \"R\": wordnet.ADV\n}\n\n\ndef pos_tag_wordnet(text):\n    \"\"\"\n        Create pos_tag with wordnet format\n    \"\"\"\n    pos_tagged_text = nltk.pos_tag(text)\n\n    # map the pos tagging output with wordnet output\n    pos_tagged_text = [\n        (word, wordnet_map.get(pos_tag[0])) if pos_tag[0] in wordnet_map.keys()\n        else (word, wordnet.NOUN)\n        for (word, pos_tag) in pos_tagged_text\n    ]\n\n    return pos_tagged_text\n\n"}, "1535": {"topic": "wordnet lemmatization and pos tagging in python", "user_name": "Shuchita BanthiaShuchita Banthia", "text": "\nYou can do this in one line:\nwnpos = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'\n\nThen use wnpos(nltk_pos) to get the POS to give to .lemmatize().  In your case, lmtzr.lemmatize(word=tagged[0][0], pos=wnpos(tagged[0][1])).\n"}, "1536": {"topic": "wordnet lemmatization and pos tagging in python", "user_name": "Haha TTproHaha TTpro", "text": "\nAfter searching from internet, I've found this solution: from sentence to \"bag of words\" derived after splitting, pos_tagging, lemmatizing and cleaning (from punctuation and \"stopping words\") operations.\nHere's my code:\nfrom nltk.corpus import wordnet as wn\nfrom nltk.wsd import lesk\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\n\npunctuation = u\",.?!()-_\\\"\\'\\\\\\n\\r\\t;:+*<>@#\u00a7^$%&|/\"\nstop_words_eng = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\ntag_dict = {\"J\": wn.ADJ,\n            \"N\": wn.NOUN,\n            \"V\": wn.VERB,\n            \"R\": wn.ADV}\n\ndef extract_wnpostag_from_postag(tag):\n    #take the first letter of the tag\n    #the second parameter is an \"optional\" in case of missing key in the dictionary \n    return tag_dict.get(tag[0].upper(), None)\n\ndef lemmatize_tupla_word_postag(tupla):\n    \"\"\"\n    giving a tupla of the form (wordString, posTagString) like ('guitar', 'NN'), return the lemmatized word\n    \"\"\"\n    tag = extract_wnpostag_from_postag(tupla[1])    \n    return lemmatizer.lemmatize(tupla[0], tag) if tag is not None else tupla[0]\n\ndef bag_of_words(sentence, stop_words=None):\n    if stop_words is None:\n        stop_words = stop_words_eng\n    original_words = word_tokenize(sentence)\n    tagged_words = nltk.pos_tag(original_words) #returns a list of tuples: (word, tagString) like ('And', 'CC')\n    original_words = None\n    lemmatized_words = [ lemmatize_tupla_word_postag(ow) for ow in tagged_words ]\n    tagged_words = None\n    cleaned_words = [ w for w in lemmatized_words if (w not in punctuation) and (w not in stop_words) ]\n    lemmatized_words = None\n    return cleaned_words\n\nsentence = \"Two electric guitar rocks players, and also a better bass player, are standing off to two sides reading corpora while walking\"\nprint(sentence, \"\\n\\n bag of words:\\n\", bag_of_words(sentence) )\n\n"}, "1537": {"topic": "How to get most informative features for scikit-learn classifiers?", "user_name": "desertnaut", "text": "\nThe classifiers in machine learning packages like liblinear and nltk offer a method show_most_informative_features(), which is really helpful for debugging features:\nviagra = None          ok : spam     =      4.5 : 1.0\nhello = True           ok : spam     =      4.5 : 1.0\nhello = None           spam : ok     =      3.3 : 1.0\nviagra = True          spam : ok     =      3.3 : 1.0\ncasino = True          spam : ok     =      2.0 : 1.0\ncasino = None          ok : spam     =      1.5 : 1.0\n\nMy question is if something similar is implemented for the classifiers in scikit-learn. I searched the documentation, but couldn't find anything the like.\nIf there is no such function yet, does somebody know a workaround how to get to those values?\n"}, "1538": {"topic": "How to get most informative features for scikit-learn classifiers?", "user_name": "tobiguetobigue", "text": "\nThe classifiers themselves do not record feature names, they just see numeric arrays. However, if you extracted your features using a Vectorizer/CountVectorizer/TfidfVectorizer/DictVectorizer, and you are using a linear model (e.g. LinearSVC or Naive Bayes) then you can apply the same trick that the document classification example uses. Example (untested, may contain a bug or two):\ndef print_top10(vectorizer, clf, class_labels):\n    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n    feature_names = vectorizer.get_feature_names()\n    for i, class_label in enumerate(class_labels):\n        top10 = np.argsort(clf.coef_[i])[-10:]\n        print(\"%s: %s\" % (class_label,\n              \" \".join(feature_names[j] for j in top10)))\n\nThis is for multiclass classification; for the binary case, I think you should use clf.coef_[0] only. You may have to sort the class_labels.\n"}, "1539": {"topic": "How to get most informative features for scikit-learn classifiers?", "user_name": "Jorge Leitao", "text": "\nWith the help of larsmans code I came up with this code for the binary case:\ndef show_most_informative_features(vectorizer, clf, n=20):\n    feature_names = vectorizer.get_feature_names()\n    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n    for (coef_1, fn_1), (coef_2, fn_2) in top:\n        print \"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2)\n\n"}, "1540": {"topic": "How to get most informative features for scikit-learn classifiers?", "user_name": "Fred FooFred Foo", "text": "\nTo add an update, RandomForestClassifier now supports the .feature_importances_ attribute. This attribute tells you how much of the observed variance is explained by that feature. Obviously, the sum of all these values must be <= 1. \nI find this attribute very useful when performing feature engineering. \nThanks to the scikit-learn team and contributors for implementing this!\nedit: This works for both RandomForest and GradientBoosting. So RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier and GradientBoostingRegressor all support this. \n"}, "1541": {"topic": "How to get most informative features for scikit-learn classifiers?", "user_name": "", "text": "\nWe've recently released a library (https://github.com/TeamHG-Memex/eli5) which allows to do that: it handles variuos classifiers from scikit-learn, binary / multiclass cases, allows to highlight text according to feature values, integrates with IPython, etc.\n"}, "1542": {"topic": "How to get most informative features for scikit-learn classifiers?", "user_name": "tobiguetobigue", "text": "\nI actually had to find out Feature Importance on my NaiveBayes classifier and although I used the above functions, I was not able to get feature importance based on classes. I went through the scikit-learn's documentation and tweaked the above functions a bit to find it working for my problem. Hope it helps you too!\ndef important_features(vectorizer,classifier,n=20):\n    class_labels = classifier.classes_\n    feature_names =vectorizer.get_feature_names()\n\n    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n\n    print(\"Important words in negative reviews\")\n\n    for coef, feat in topn_class1:\n        print(class_labels[0], coef, feat)\n\n    print(\"-----------------------------------------\")\n    print(\"Important words in positive reviews\")\n\n    for coef, feat in topn_class2:\n        print(class_labels[1], coef, feat)\n\n\n\nNote that your classifier(in my case it's NaiveBayes) must have attribute feature_count_ for this to work.\n\n"}, "1543": {"topic": "How to get most informative features for scikit-learn classifiers?", "user_name": "", "text": "\nYou can also do something like this to create a graph of importance features by order:\nimportances = clf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf.estimators_],\n         axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\n#print(\"Feature ranking:\")\n\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(train[features].shape[1]), importances[indices],\n   color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(train[features].shape[1]), indices)\nplt.xlim([-1, train[features].shape[1]])\nplt.show()\n\n"}, "1544": {"topic": "How to get most informative features for scikit-learn classifiers?", "user_name": "ClimbsRocksClimbsRocks", "text": "\nRandomForestClassifier does not yet have a coef_ attrubute, but it will in the 0.17 release, I think. However, see the RandomForestClassifierWithCoef class in Recursive feature elimination on Random Forest using scikit-learn. This may give you some ideas to work around the limitation above.\n"}, "1545": {"topic": "How to get most informative features for scikit-learn classifiers?", "user_name": "Mikhail KorobovMikhail Korobov", "text": "\nNot exactly what you are looking for, but a quick way to get the largest magnitude coefficients (assuming a pandas dataframe columns are your feature names): \nYou trained the model like: \nlr = LinearRegression()\nX_train, X_test, y_train, y_test = train_test_split(df, Y, test_size=0.25)\nlr.fit(X_train, y_train)\n\nGet the 10 largest negative coefficient values (or change to reverse=True for largest positive) like: \nsorted(list(zip(feature_df.columns, lr.coef_)), key=lambda x: x[1], \nreverse=False)[:10]\n\n"}, "1546": {"topic": "How to get most informative features for scikit-learn classifiers?", "user_name": "Manimaran Paneerselvam", "text": "\nFirst make a list, I give this list the name label.  After that extracting all features name and column name I add in label list. Here I use naive bayes model. In naive bayes model, feature_log_prob_ give probability of features.\ndef top20(model,label):\n\n  feature_prob=(abs(model.feature_log_prob_))\n\n  for i in range(len(feature_prob)):\n\n    print ('top 20 features for {} class'.format(i))\n\n    clas = feature_prob[i,:]\n\n    dictonary={}\n\n    for count,ele in enumerate(clas,0): \n\n      dictonary[count]=ele\n\n    dictonary=dict(sorted(dictonary.items(), key=lambda x: x[1], reverse=True)[:20])\n\n    keys=list(dictonary.keys())\n\n    for i in keys:\n\n      print(label[i])\n\n    print('*'*1000)\n\n"}, "1547": {"topic": "English grammar for parsing in NLTK", "user_name": "Fred Foo", "text": "\nIs there a ready-to-use English grammar that I can just load it and use in NLTK? I've searched around examples of parsing with NLTK, but it seems like that I have to manually specify grammar before parsing a sentence. \nThanks a lot!\n"}, "1548": {"topic": "English grammar for parsing in NLTK", "user_name": "roborenroboren", "text": "\nYou can take a look at pyStatParser, a simple python statistical parser that returns NLTK parse Trees. It comes with public treebanks and it generates the grammar model only the first time you instantiate a Parser object (in about 8 seconds). It uses a CKY algorithm and it parses average length sentences (like the one below) in under a second.\n>>> from stat_parser import Parser\n>>> parser = Parser()\n>>> print parser.parse(\"How can the net amount of entropy of the universe be massively decreased?\")\n(SBARQ\n  (WHADVP (WRB how))\n  (SQ\n    (MD can)\n    (NP\n      (NP (DT the) (JJ net) (NN amount))\n      (PP\n        (IN of)\n        (NP\n          (NP (NNS entropy))\n          (PP (IN of) (NP (DT the) (NN universe))))))\n    (VP (VB be) (ADJP (RB massively) (VBN decreased))))\n  (. ?))\n\n"}, "1549": {"topic": "English grammar for parsing in NLTK", "user_name": "emilmontemilmont", "text": "\nMy library, spaCy, provides a high performance dependency parser.\nInstallation:\npip install spacy\npython -m spacy.en.download all\n\nUsage:\nfrom spacy.en import English\nnlp = English()\ndoc = nlp(u'A whole document.\\nNo preprocessing require.   Robust to arbitrary formating.')\nfor sent in doc:\n    for token in sent:\n        if token.is_alpha:\n            print token.orth_, token.tag_, token.head.lemma_\n\nChoi et al. (2015) found spaCy to be the fastest dependency parser available. It processes over 13,000 sentences a second, on a single thread. On the standard WSJ evaluation it scores 92.7%, over 1% more accurate than any of CoreNLP's models.\n"}, "1550": {"topic": "English grammar for parsing in NLTK", "user_name": "syllogism_syllogism_", "text": "\nThere are a few grammars in the nltk_data distribution. In your Python interpreter, issue nltk.download().\n"}, "1551": {"topic": "English grammar for parsing in NLTK", "user_name": "Fred FooFred Foo", "text": "\nThere is a Library called Pattern. It is quite fast and easy to use.\n>>> from pattern.en import parse\n>>>  \n>>> s = 'The mobile web is more important than mobile apps.'\n>>> s = parse(s, relations=True, lemmata=True)\n>>> print s\n\n'The/DT/B-NP/O/NP-SBJ-1/the mobile/JJ/I-NP/O/NP-SBJ-1/mobile' ... \n\n"}, "1552": {"topic": "English grammar for parsing in NLTK", "user_name": "user3798928user3798928", "text": "\nUse the MaltParser, there you have a pretrained english-grammar, and also some other pretrained languages.\nAnd the Maltparser is a dependency parser and not some simple bottom-up, or top-down Parser.\nJust download the MaltParser from http://www.maltparser.org/index.html and use the NLTK like this:\nimport nltk\nparser = nltk.parse.malt.MaltParser()\n\n"}, "1553": {"topic": "English grammar for parsing in NLTK", "user_name": "blackmambablackmamba", "text": "\nI've tried NLTK, PyStatParser, Pattern. IMHO Pattern is best English parser introduced in above article. Because it supports pip install and There is a fancy document on the website (http://www.clips.ua.ac.be/pages/pattern-en). I couldn't find reasonable document for NLTK (And it gave me inaccurate result for me by its default. And I couldn't find how to tune it). pyStatParser is much slower than described above in my Environment. (About one minute for initialization and It took couple of seconds to parse long sentences. Maybe I didn't use it correctly).  \n"}, "1554": {"topic": "English grammar for parsing in NLTK", "user_name": "Piyo HogePiyo Hoge", "text": "\nDid you try POS tagging in NLTK?\ntext = word_tokenize(\"And now for something completely different\")\nnltk.pos_tag(text)\n\nThe answer is something like this\n[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'),('completely', 'RB'), ('different', 'JJ')]\n\nGot this example from here NLTK_chapter03\n"}, "1555": {"topic": "English grammar for parsing in NLTK", "user_name": "maverik_akagamimaverik_akagami", "text": "\nI'm found out that nltk working good with parser grammar developed by Stanford.\nSyntax Parsing with Stanford CoreNLP and NLTK\nIt is very easy to start to use Stanford CoreNLP and NLTK. All you need is small preparation, after that you can parse sentences with following code:\nfrom nltk.parse.corenlp import CoreNLPParser\nparser = CoreNLPParser()\nparse = next(parser.raw_parse(\"I put the book in the box on the table.\"))\n\nPreparation:\n\nDownload Java Stanford model\nRun CoreNLPServer\n\nYou can use following code to run CoreNLPServer:\nimport os\nfrom nltk.parse.corenlp import CoreNLPServer\n# The server needs to know the location of the following files:\n#   - stanford-corenlp-X.X.X.jar\n#   - stanford-corenlp-X.X.X-models.jar\nSTANFORD = os.path.join(\"models\", \"stanford-corenlp-full-2018-02-27\")\n# Create the server\nserver = CoreNLPServer(\n   os.path.join(STANFORD, \"stanford-corenlp-3.9.1.jar\"),\n   os.path.join(STANFORD, \"stanford-corenlp-3.9.1-models.jar\"),    \n)\n# Start the server in the background\nserver.start()\n\n\nDo not forget stop server with executing server.stop() \n\n"}, "1556": {"topic": "How to extract common / significant phrases from a series of text entries", "user_name": "ROMANIA_engineer", "text": "\nI have a series of text items- raw HTML from a MySQL database. I want to find the most common phrases in these entries (not the single most common phrase, and ideally, not enforcing word-for-word matching). \nMy example is any review on Yelp.com, that shows 3 snippets from hundreds of reviews of a given restaurant, in the format: \n\"Try the hamburger\" (in 44 reviews)\ne.g., the \"Review Highlights\" section of this page: \nhttp://www.yelp.com/biz/sushi-gen-los-angeles/\nI have NLTK installed and I've played around with it a bit, but am honestly overwhelmed by the options. This seems like a rather common problem and I haven't been able to find a straightforward solution by searching here.\n"}, "1557": {"topic": "How to extract common / significant phrases from a series of text entries", "user_name": "arronskyarronsky", "text": "\nI suspect you don't just want the most common phrases, but rather you want the most interesting collocations. Otherwise, you could end up with an overrepresentation of phrases made up of common words and fewer interesting and informative phrases. \nTo do this, you'll essentially want to extract n-grams from your data and then find the ones that have the highest point wise mutual information (PMI). That is, you want to find the words that co-occur together much more than you would expect them to by chance. \nThe NLTK collocations how-to covers how to do this in a about 7 lines of code, e.g.:\nimport nltk\nfrom nltk.collocations import *\nbigram_measures = nltk.collocations.BigramAssocMeasures()\ntrigram_measures = nltk.collocations.TrigramAssocMeasures()\n\n# change this to read in your data\nfinder = BigramCollocationFinder.from_words(\n    nltk.corpus.genesis.words('english-web.txt'))\n\n# only bigrams that appear 3+ times\nfinder.apply_freq_filter(3)\n\n# return the 10 n-grams with the highest PMI\nfinder.nbest(bigram_measures.pmi, 10)\n\n"}, "1558": {"topic": "How to extract common / significant phrases from a series of text entries", "user_name": "Soroush", "text": "\nI think what you're looking for is chunking. I recommended reading chapter 7 of the NLTK book or maybe my own article on chunk extraction. Both of these assume knowledge of part-of-speech tagging, which is covered in chapter 5.\n"}, "1559": {"topic": "How to extract common / significant phrases from a series of text entries", "user_name": "dmcerdmcer", "text": "\nif you just want to get to larger than 3 ngrams you can try this.  I'm assuming you've stripped out all the junk like HTML etc.\nimport nltk\nngramlist=[]\nraw=<yourtextfile here>\n\nx=1\nngramlimit=6\ntokens=nltk.word_tokenize(raw)\n\nwhile x <= ngramlimit:\n  ngramlist.extend(nltk.ngrams(tokens, x))\n  x+=1\n\nProbably not very pythonic as I've only been doing this a month or so myself, but might be of help!\n"}, "1560": {"topic": "How to extract common / significant phrases from a series of text entries", "user_name": "Adnan Azmat", "text": "\nWell, for a start you would probably have to remove all HTML tags (search for \"<[^>]*>\" and replace it with \"\"). After that, you could try the naive approach of looking for the longest common substrings between every two text items, but I don't think you'd get very good results.\nYou might do better by normalizing the words (reducing them to their base form, removing all accents, setting everything to lower or upper case) first and then analyse. Again, depending on what you want to accomplish, you might be able to cluster the text items better if you allow for some word order flexibility, i.e. treat the text items as bags of normalized words and measure bag content similarity.\nI've commented on a similar (although not identical) topic here.\n"}, "1561": {"topic": "How do I download NLTK data?", "user_name": "Q-ximi", "text": "\nUpdated answer:NLTK works for 2.7 well. I had 3.2. I uninstalled 3.2 and installed 2.7. Now it works!!\nI have installed NLTK and tried to download NLTK Data. What I did was to follow the instrution on this site: http://www.nltk.org/data.html\nI downloaded NLTK, installed it, and then tried to run the following code:\n>>> import nltk\n>>> nltk.download()\n\nIt gave me the error message like below:\nTraceback (most recent call last):\n  File \"<pyshell#6>\", line 1, in <module>\n    nltk.download()\nAttributeError: 'module' object has no attribute 'download'\n Directory of C:\\Python32\\Lib\\site-packages\n\nTried both nltk.download() and nltk.downloader(), both gave me error messages.\nThen I used help(nltk) to pull out the package, it shows the following info:\nNAME\n    nltk\n\nPACKAGE CONTENTS\n    align\n    app (package)\n    book\n    ccg (package)\n    chat (package)\n    chunk (package)\n    classify (package)\n    cluster (package)\n    collocations\n    corpus (package)\n    data\n    decorators\n    downloader\n    draw (package)\n    examples (package)\n    featstruct\n    grammar\n    help\n    inference (package)\n    internals\n    lazyimport\n    metrics (package)\n    misc (package)\n    model (package)\n    parse (package)\n    probability\n    sem (package)\n    sourcedstring\n    stem (package)\n    tag (package)\n    test (package)\n    text\n    tokenize (package)\n    toolbox\n    tree\n    treetransforms\n    util\n    yamltags\n\nFILE\n    c:\\python32\\lib\\site-packages\\nltk\n\nI do see Downloader there, not sure why it does not work. Python 3.2.2, system Windows vista.\n"}, "1562": {"topic": "How do I download NLTK data?", "user_name": "Q-ximiQ-ximi", "text": "\nTL;DR\nTo download a particular dataset/models, use the nltk.download() function, e.g. if you are looking to download the punkt sentence tokenizer, use:\n$ python3\n>>> import nltk\n>>> nltk.download('punkt')\n\nIf you're unsure of which data/model you need, you can start out with the basic list of data + models with:\n>>> import nltk\n>>> nltk.download('popular')\n\nIt will download a list of \"popular\" resources, these includes:\n<collection id=\"popular\" name=\"Popular packages\">\n      <item ref=\"cmudict\" />\n      <item ref=\"gazetteers\" />\n      <item ref=\"genesis\" />\n      <item ref=\"gutenberg\" />\n      <item ref=\"inaugural\" />\n      <item ref=\"movie_reviews\" />\n      <item ref=\"names\" />\n      <item ref=\"shakespeare\" />\n      <item ref=\"stopwords\" />\n      <item ref=\"treebank\" />\n      <item ref=\"twitter_samples\" />\n      <item ref=\"omw\" />\n      <item ref=\"wordnet\" />\n      <item ref=\"wordnet_ic\" />\n      <item ref=\"words\" />\n      <item ref=\"maxent_ne_chunker\" />\n      <item ref=\"punkt\" />\n      <item ref=\"snowball_data\" />\n      <item ref=\"averaged_perceptron_tagger\" />\n    </collection>\n\n\nEDITED\nIn case anyone is avoiding errors from downloading larger datasets from nltk, from https://stackoverflow.com/a/38135306/610569\n$ rm /Users/<your_username>/nltk_data/corpora/panlex_lite.zip\n$ rm -r /Users/<your_username>/nltk_data/corpora/panlex_lite\n$ python\n\n>>> import nltk\n>>> dler = nltk.downloader.Downloader()\n>>> dler._update_index()\n>>> dler._status_cache['panlex_lite'] = 'installed' # Trick the index to treat panlex_lite as it's already installed.\n>>> dler.download('popular')\n\nUpdated\nFrom v3.2.5, NLTK has a more informative error message when nltk_data resource is not found, e.g.:\n>>> from nltk import word_tokenize\n>>> word_tokenize('x')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/l/alvas/git/nltk/nltk/tokenize/__init__.py\", line 128, in word_tokenize\n    sentences = [text] if preserve_line else sent_tokenize(text, language)\n  File \"/Users//alvas/git/nltk/nltk/tokenize/__init__.py\", line 94, in sent_tokenize\n    tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))\n  File \"/Users/alvas/git/nltk/nltk/data.py\", line 820, in load\n    opened_resource = _open(resource_url)\n  File \"/Users/alvas/git/nltk/nltk/data.py\", line 938, in _open\n    return find(path_, path + ['']).open()\n  File \"/Users/alvas/git/nltk/nltk/data.py\", line 659, in find\n    raise LookupError(resource_not_found)\nLookupError: \n**********************************************************************\n  Resource punkt not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  >>> import nltk\n  >>> nltk.download('punkt')\n\n  Searched in:\n    - '/Users/alvas/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n\nRelated\n\nTo find nltk_data directory (auto-magically), see https://stackoverflow.com/a/36383314/610569 \nTo download nltk_data to a different path, see https://stackoverflow.com/a/48634212/610569\nTo config nltk_data path (i.e. set a different path for NLTK to find nltk_data), see https://stackoverflow.com/a/22987374/610569\n\n"}, "1563": {"topic": "How do I download NLTK data?", "user_name": "", "text": "\nTry \nnltk.download('all')\nthis will download all the data and no need to download individually.\n"}, "1564": {"topic": "How do I download NLTK data?", "user_name": "alvasalvas", "text": "\nInstall Pip: run in terminal : sudo easy_install pip\nInstall Numpy (optional): run : sudo pip install -U numpy\nInstall NLTK: run : sudo pip install -U nltk\nTest installation: run:  python \nthen type : import nltk\nTo download the corpus \nrun : python -m nltk.downloader all\n"}, "1565": {"topic": "How do I download NLTK data?", "user_name": "Noordeen", "text": "\nDo not name your file nltk.py I used the same code and name it nltk, and got the same error as you have, I changed the file name and it went well.\n"}, "1566": {"topic": "How do I download NLTK data?", "user_name": "B KB K", "text": "\nThis worked for me:\nnltk.set_proxy('http://user:password@proxy.example.com:8080')\nnltk.download()\n\n"}, "1567": {"topic": "How do I download NLTK data?", "user_name": "", "text": "\nPlease Try\nimport nltk\n\nnltk.download()\n\nAfter running this you get something like this\nNLTK Downloader\n---------------------------------------------------------------------------\n   d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\n\nThen, Press d\nDo As Follows:\nDownloader> d all\n\nYou will get following message on completion, and Prompt then Press q\nDone downloading collection all\n"}, "1568": {"topic": "How do I download NLTK data?", "user_name": "NoordeenNoordeen", "text": "\nyou can't have a saved python file called nltk.py because the interpreter is reading from that and not from the actual file. \nChange the name of your file that the python shell is reading from and try what you were doing originally: \nimport nltk and then nltk.download()\n"}, "1569": {"topic": "How do I download NLTK data?", "user_name": "GerardGerard", "text": "\nIt's very simple....\n\nOpen pyScripter or any editor\nCreate a python file eg: install.py\nwrite the below code in it.\n\nimport nltk\nnltk.download()\n\n\nA pop-up window will apper and click on download .\n\n\n"}, "1570": {"topic": "How do I download NLTK data?", "user_name": "Morteza MashayekhiMorteza Mashayekhi", "text": "\nI had the similar issue. Probably check if you are using proxy. \nIf yes, set up the proxy before doing download:\nnltk.set_proxy('http://proxy.example.com:3128', ('USERNAME', 'PASSWORD'))\n\n"}, "1571": {"topic": "How do I download NLTK data?", "user_name": "Ankit Jayaswal", "text": "\nIf you are running a really old version of nltk, then there is indeed no download module available (reference)\nTry this:\nimport nltk\nprint(nltk.__version__)\n\nAs per the reference, anything after 0.9.5 should be fine\n"}, "1572": {"topic": "How do I download NLTK data?", "user_name": "balabala", "text": "\nyou should add python to your PATH during installation of python...after installation.. open cmd prompt type  command-pip   install nltk\nthen go to IDLE and open a new file..save it as file.py..then open file.py\ntype the following:\nimport nltk\nnltk.download()\n\n"}, "1573": {"topic": "How do I download NLTK data?", "user_name": "mrsrinivas", "text": "\nTry download the zip files from http://www.nltk.org/nltk_data/ and then unzip, save in your Python folder, such as  C:\\ProgramData\\Anaconda3\\nltk_data\n"}, "1574": {"topic": "How do I download NLTK data?", "user_name": "user3682157user3682157", "text": "\nif you have already saved a file name nltk.py and again rename as my_nltk_script.py. check whether you have still the file nltk.py existing. If yes, then delete them and run the file my_nltk.scripts.py it should work!\n"}, "1575": {"topic": "How do I download NLTK data?", "user_name": "layog", "text": "\njust do like\nimport nltk\nnltk.download()\n\nthen you will be show a popup asking what to download , select 'all'. it will take some time because of its size, but eventually we will get it.\nand if you are using Google Colab, you can use\nnltk.download(download_dir='/content/nltkdata')\n\nafter running that you will be asked to select from a list\nNLTK Downloader\n----------------------------------------------------------------- \n----------\nd) Download   l) List    u) Update   c) Config   h) Help   q) \nQuit\n----------------------------------------------------------------- \n----------\nDownloader> d\n\nhere you have to enter d as you want to download.\nafter that you will be asked to enter the identifier that you want to download . You can see the list of available indentifier with l command or if you want all of them just enter 'all' in the input box.\nthen you will see something like -\nDownloading collection 'all'\n       | \n       | Downloading package abc to /content/nltkdata...\n       |   Unzipping corpora/abc.zip.\n       | Downloading package alpino to /content/nltkdata...\n       |   Unzipping corpora/alpino.zip.\n       | Downloading package biocreative_ppi to /content/nltkdata...\n       |   Unzipping corpora/biocreative_ppi.zip.\n       | Downloading package brown to /content/nltkdata...\n       |   Unzipping corpora/brown.zip.\n       | Downloading package brown_tei to /content/nltkdata...\n       |   Unzipping corpora/brown_tei.zip.\n       | Downloading package cess_cat to /content/nltkdata...\n       |   Unzipping corpora/cess_cat.zip.\n.\n.\n. \n |   Unzipping models/wmt15_eval.zip.\n       | Downloading package mwa_ppdb to /content/nltkdata...\n       |   Unzipping misc/mwa_ppdb.zip.\n       | \n     Done downloading collection all\n\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\nDownloader> q\nTrue\n\nat last you can enter q to quit.\n"}, "1576": {"topic": "How do I download NLTK data?", "user_name": "Arun DasArun Das", "text": "\nYou may try:\n>> $ import nltk\n>> $ nltk.download_shell()\n>> $ d\n>> $ *name of the package*\n\nhappy nlp'ing.\n"}, "1577": {"topic": "Spell Checker for Python", "user_name": "User", "text": "\nI'm fairly new to Python and NLTK. I am busy with an application that can perform spell checks (replaces an incorrectly spelled word with the correct one).\nI'm currently using the Enchant library on Python 2.7, PyEnchant and the NLTK library. The code below is a class that handles the correction/replacement. \nfrom nltk.metrics import edit_distance\n\nclass SpellingReplacer:\n    def __init__(self, dict_name='en_GB', max_dist=2):\n        self.spell_dict = enchant.Dict(dict_name)\n        self.max_dist = 2\n\n    def replace(self, word):\n        if self.spell_dict.check(word):\n            return word\n        suggestions = self.spell_dict.suggest(word)\n\n        if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n            return suggestions[0]\n        else:\n            return word\n\nI have written a function that takes in a list of words and executes replace() on each word and then returns a list of those words, but spelled correctly.\ndef spell_check(word_list):\n    checked_list = []\n    for item in word_list:\n        replacer = SpellingReplacer()\n        r = replacer.replace(item)\n        checked_list.append(r)\n    return checked_list\n\n>>> word_list = ['car', 'colour']\n>>> spell_check(words)\n['car', 'color']\n\nNow, I don't really like this because it isn't very accurate and I'm looking for a way to achieve spelling checks and replacements on words. I also need something that can pick up spelling mistakes like \"caaaar\"? Are there better ways to perform spelling checks out there? If so, what are they? How does Google do it? Because their spelling suggester is very good.\nAny suggestions?\n"}, "1578": {"topic": "Spell Checker for Python", "user_name": "Mike BarnesMike Barnes", "text": "\nYou can use the autocorrect lib to spell check in python.\nExample Usage:\nfrom autocorrect import Speller\n\nspell = Speller(lang='en')\n\nprint(spell('caaaar'))\nprint(spell('mussage'))\nprint(spell('survice'))\nprint(spell('hte'))\n\nResult:\ncaesar\nmessage\nservice\nthe\n\n"}, "1579": {"topic": "Spell Checker for Python", "user_name": "Sunil Garg", "text": "\nI'd recommend starting by carefully reading this post by Peter Norvig. (I had to something similar and I found it extremely useful.)\nThe following function, in particular has the ideas that you now need to make your spell checker more sophisticated: splitting, deleting, transposing, and inserting the irregular words to 'correct' them.\ndef edits1(word):\n   splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n   deletes    = [a + b[1:] for a, b in splits if b]\n   transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n   replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n   inserts    = [a + c + b     for a, b in splits for c in alphabet]\n   return set(deletes + transposes + replaces + inserts)\n\nNote: The above is one snippet from Norvig's spelling corrector\nAnd the good news is that you can incrementally add to and keep improving your spell-checker.\nHope that helps.\n"}, "1580": {"topic": "Spell Checker for Python", "user_name": "RakeshRakesh", "text": "\nThe best way for spell checking in python is by: SymSpell, Bk-Tree or Peter Novig's method. \nThe fastest one is SymSpell.\nThis is Method1: Reference link pyspellchecker\nThis library is based on Peter Norvig's implementation.\npip install pyspellchecker\nfrom spellchecker import SpellChecker\n\nspell = SpellChecker()\n\n# find those words that may be misspelled\nmisspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])\n\nfor word in misspelled:\n    # Get the one `most likely` answer\n    print(spell.correction(word))\n\n    # Get a list of `likely` options\n    print(spell.candidates(word))\n\nMethod2: SymSpell Python\npip install -U symspellpy\n"}, "1581": {"topic": "Spell Checker for Python", "user_name": "Ram NarasimhanRam Narasimhan", "text": "\nMaybe it is too late, but I am answering for future searches.\nTO perform spelling mistake correction, you first need to make sure the word is not absurd or from slang like, caaaar, amazzzing etc. with repeated alphabets. So, we first need to get rid of these alphabets. As we know in English language words usually have a maximum of 2 repeated alphabets, e.g., hello., so we remove the extra repetitions from the words first and then check them for spelling. \nFor removing the extra alphabets, you can use Regular Expression module in Python.\nOnce this is done use Pyspellchecker library from Python for correcting spellings.\nFor implementation visit this link: https://rustyonrampage.github.io/text-mining/2017/11/28/spelling-correction-with-python-and-nltk.html\n"}, "1582": {"topic": "Spell Checker for Python", "user_name": "Shaurya UppalShaurya Uppal", "text": "\nTry jamspell - it works pretty well for automatic spelling correction:\nimport jamspell\n\ncorrector = jamspell.TSpellCorrector()\ncorrector.LoadLangModel('en.bin')\n\ncorrector.FixFragment('Some sentnec with error')\n# u'Some sentence with error'\n\ncorrector.GetCandidates(['Some', 'sentnec', 'with', 'error'], 1)\n# ('sentence', 'senate', 'scented', 'sentinel')\n\n"}, "1583": {"topic": "Spell Checker for Python", "user_name": "Rishabh SahrawatRishabh Sahrawat", "text": "\nIN TERMINAL\npip install gingerit\n\nFOR CODE\nfrom gingerit.gingerit import GingerIt\ntext = input(\"Enter text to be corrected\")\nresult = GingerIt().parse(text)\ncorrections = result['corrections']\ncorrectText = result['result']\n\nprint(\"Correct Text:\",correctText)\nprint()\nprint(\"CORRECTIONS\")\nfor d in corrections:\n  print(\"________________\")  \n  print(\"Previous:\",d['text'])  \n  print(\"Correction:\",d['correct'])   \n  print(\"`Definiton`:\",d['definition'])\n \n\n"}, "1584": {"topic": "Spell Checker for Python", "user_name": "jupiterbjy", "text": "\nYou can also try:\npip install textblob\nfrom textblob import TextBlob\ntxt=\"machne learnig\"\nb = TextBlob(txt)\nprint(\"after spell correction: \"+str(b.correct()))\n\nafter spell correction: machine learning\n"}, "1585": {"topic": "Spell Checker for Python", "user_name": "FippoFippo", "text": "\nspell corrector->\nyou need to import a corpus on to your desktop if you store elsewhere change the path in the code i have added a few graphics as well using tkinter and this is only to tackle non word errors!!\ndef min_edit_dist(word1,word2):\n    len_1=len(word1)\n    len_2=len(word2)\n    x = [[0]*(len_2+1) for _ in range(len_1+1)]#the matrix whose last element ->edit distance\n    for i in range(0,len_1+1):  \n        #initialization of base case values\n        x[i][0]=i\n        for j in range(0,len_2+1):\n            x[0][j]=j\n    for i in range (1,len_1+1):\n        for j in range(1,len_2+1):\n            if word1[i-1]==word2[j-1]:\n                x[i][j] = x[i-1][j-1]\n            else :\n                x[i][j]= min(x[i][j-1],x[i-1][j],x[i-1][j-1])+1\n    return x[i][j]\nfrom Tkinter import *\n\n\ndef retrieve_text():\n    global word1\n    word1=(app_entry.get())\n    path=\"C:\\Documents and Settings\\Owner\\Desktop\\Dictionary.txt\"\n    ffile=open(path,'r')\n    lines=ffile.readlines()\n    distance_list=[]\n    print \"Suggestions coming right up count till 10\"\n    for i in range(0,58109):\n        dist=min_edit_dist(word1,lines[i])\n        distance_list.append(dist)\n    for j in range(0,58109):\n        if distance_list[j]<=2:\n            print lines[j]\n            print\" \"   \n    ffile.close()\nif __name__ == \"__main__\":\n    app_win = Tk()\n    app_win.title(\"spell\")\n    app_label = Label(app_win, text=\"Enter the incorrect word\")\n    app_label.pack()\n    app_entry = Entry(app_win)\n    app_entry.pack()\n    app_button = Button(app_win, text=\"Get Suggestions\", command=retrieve_text)\n    app_button.pack()\n    # Initialize GUI loop\n    app_win.mainloop()\n\n"}, "1586": {"topic": "Spell Checker for Python", "user_name": "pouya bararipouya barari", "text": "\npyspellchecker is the one of the best solutions for this problem. pyspellchecker library is based on Peter Norvig\u2019s blog post.\nIt uses a Levenshtein Distance algorithm to find permutations within an edit distance of 2 from the original word.\nThere are two ways to install this library. The official document highly recommends using the pipev package.\n\ninstall using pip\n\npip install pyspellchecker\n\n\ninstall from source\n\ngit clone https://github.com/barrust/pyspellchecker.git\ncd pyspellchecker\npython setup.py install\n\nthe following code is the example provided from the documentation\nfrom spellchecker import SpellChecker\n\nspell = SpellChecker()\n\n# find those words that may be misspelled\nmisspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])\n\nfor word in misspelled:\n    # Get the one `most likely` answer\n    print(spell.correction(word))\n\n    # Get a list of `likely` options\n    print(spell.candidates(word))\n\n"}, "1587": {"topic": "Spell Checker for Python", "user_name": "Mayur PatilMayur Patil", "text": "\nfrom autocorrect import spell\nfor this you need to install, prefer anaconda and it only works for words, not sentences so that's a limitation u gonna face.\nfrom autocorrect import spell\nprint(spell('intrerpreter'))\n# output: interpreter\n\n"}, "1588": {"topic": "Spell Checker for Python", "user_name": "", "text": "\npip install scuse\nfrom scuse import scuse\n\nobj = scuse()\n\ncheckedspell = obj.wordf(\"spelling you want to check\")\n\nprint(checkedspell)\n\n"}, "1589": {"topic": "Spell Checker for Python", "user_name": "\r", "text": "\nSpark NLP is another option that I used and it is working excellent. A simple tutorial can be found here. https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/annotation/english/spell-check-ml-pipeline/Pretrained-SpellCheckML-Pipeline.ipynb\n"}, "1590": {"topic": "str.translate gives TypeError - Translate takes one argument (2 given), worked in Python 2", "user_name": "Martijn Pieters\u2666", "text": "\nI have the following code \nimport nltk, os, json, csv, string, cPickle\nfrom scipy.stats import scoreatpercentile\n\nlmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n\ndef sanitize(wordList): \nanswer = [word.translate(None, string.punctuation) for word in wordList] \nanswer = [lmtzr.lemmatize(word.lower()) for word in answer]\nreturn answer\n\nwords = []\nfor filename in json_list:\n    words.extend([sanitize(nltk.word_tokenize(' '.join([tweet['text'] \n                   for tweet in json.load(open(filename,READ))])))])\n\nI've tested lines 2-4 in a separate testing.py file when I wrote\nimport nltk, os, json, csv, string, cPickle\nfrom scipy.stats import scoreatpercentile\n\nwordList= ['\\'the', 'the', '\"the']\nprint wordList\nwordList2 = [word.translate(None, string.punctuation) for word in wordList]\nprint wordList2\nanswer = [lmtzr.lemmatize(word.lower()) for word in wordList2]\nprint answer\n\nfreq = nltk.FreqDist(wordList2)\nprint freq\n\nand the command prompt returns ['the','the','the'], which is what I wanted (removing punctuation).\nHowever, when I put the exact same code in a different file, python returns a TypeError stating that\nFile \"foo.py\", line 8, in <module>\n  for tweet in json.load(open(filename, READ))])))])\nFile \"foo.py\", line 2, in sanitize\n  answer = [word.translate(None, string.punctuation) for word in wordList]\nTypeError: translate() takes exactly one argument (2 given)\n\njson_list is a list of all the file paths (I printed and check that this list is valid). I'm confused on this TypeError because everything works perfectly fine when I'm just testing it in a different file.\n"}, "1591": {"topic": "str.translate gives TypeError - Translate takes one argument (2 given), worked in Python 2", "user_name": "carebearcarebear", "text": "\nIf all you are looking to accomplish is to do the same thing you were doing in Python 2 in Python 3, here is what I was doing in Python 2.0  to throw away punctuation and numbers:\ntext = text.translate(None, string.punctuation)\ntext = text.translate(None, '1234567890')\n\nHere is my Python 3.0 equivalent:\ntext = text.translate(str.maketrans('','',string.punctuation))\ntext = text.translate(str.maketrans('','','1234567890'))\n\nBasically it says 'translate nothing to nothing' (first two parameters) and translate any punctuation or numbers to None (i.e. remove them).\n"}, "1592": {"topic": "str.translate gives TypeError - Translate takes one argument (2 given), worked in Python 2", "user_name": "drchuckdrchuck", "text": "\nI suspect your issue has to do with the differences between str.translate and unicode.translate (these are also the differences between str.translate on Python 2 versus Python 3). I suspect your original code is being sent unicode instances while your test code is using regular 8-bit str instances.\nI don't suggest converting Unicode strings back to regular str instances, since unicode is a much better type for handling text data (and it is the future!). Instead, you should just adapt to the new unicode.translate syntax. With regular str.translate (on Python 2), you can pass an optional deletechars argument and the characters in it would be removed from the string. For unicode.translate (and str.translate on Python 3), the extra argument is no longer allowed, but translation table entries with None as their value will be deleted from the output.\nTo solve the problem you'll need to create an appropriate translation table. A translation table is a dictionary mapping from Unicode ordinals (that is, ints) to ordinals, strings or None. A helper function for making them exists in Python 2 as string.maketrans (and Python 3 as a method of the str type), but the Python 2 version of it doesn't handle the case we care about (putting None values into the table). You can  build an appropriate dictionary yourself with something like {ord(c): None for c in string.punctuation}.\n"}, "1593": {"topic": "str.translate gives TypeError - Translate takes one argument (2 given), worked in Python 2", "user_name": "", "text": "\nPython 3.0:\ntext = text.translate(str.maketrans('','','1234567890'))\n\n\nstatic str.maketrans(x[, y[, z]]) \nThis static method returns a\n  translation table usable for str.translate().\n\nIf there is only one argument, it must be a dictionary mapping Unicode ordinals (integers) or characters (strings of length 1) to Unicode ordinals, strings (of arbitrary lengths) or None. Character keys will then be converted to ordinals.\nIf there are two arguments, they must be strings of equal length, and in the resulting dictionary, each character in x will be mapped to the character at the same position in y. If there is a third argument, it must be a string, whose characters will be mapped to None in the result.\nhttps://docs.python.org/3/library/stdtypes.html?highlight=maketrans#str.maketrans\n"}, "1594": {"topic": "str.translate gives TypeError - Translate takes one argument (2 given), worked in Python 2", "user_name": "BlckknghtBlckknght", "text": "\nIf you just want to implement something like this: \"123hello.jpg\".translate(None, 0123456789\") then try this:\n \"\".join(c for c in \"123hello.jpg\" if c not in \"0123456789\")\n\nOuput: hello.jpg\n"}, "1595": {"topic": "tag generation from a text content", "user_name": "dmcer", "text": "\nI am curious if there is an algorithm/method exists to generate keywords/tags from a given text, by using some weight calculations, occurrence ratio or other tools.\nAdditionally, I will be grateful if you point any Python based solution / library for this.\nThanks\n"}, "1596": {"topic": "tag generation from a text content", "user_name": "HellnarHellnar", "text": "\nOne way to do this would be to extract words that occur more frequently in a document than you would expect them to by chance. For example, say in a larger collection of documents the term 'Markov' is almost never seen. However, in a particular document from the same collection Markov shows up very frequently. This would suggest that Markov might be a good keyword or tag to associate with the document.\nTo identify keywords like this, you could use the point-wise mutual information of the keyword and the document. This is given by PMI(term, doc) = log [ P(term, doc) / (P(term)*P(doc)) ]. This will roughly tell you how much less (or more) surprised you are to come across the term in the specific document as appose to coming across it in the larger collection.\nTo identify the 5 best keywords to associate with a document, you would just sort the terms by their PMI score with the document and pick the 5 with the highest score. \nIf you want to extract multiword tags, see the StackOverflow question How to extract common / significant phrases from a series of text entries. \nBorrowing from my answer to that question, the NLTK collocations how-to covers how to do \nextract interesting multiword expressions using n-gram PMI in a about 7 lines of code, e.g.:\nimport nltk\nfrom nltk.collocations import *\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n\n# change this to read in your data\nfinder = BigramCollocationFinder.from_words(\n   nltk.corpus.genesis.words('english-web.txt'))\n\n# only bigrams that appear 3+ times\nfinder.apply_freq_filter(3) \n\n# return the 5 n-grams with the highest PMI\nfinder.nbest(bigram_measures.pmi, 5)  \n\n"}, "1597": {"topic": "tag generation from a text content", "user_name": "CommunityBot", "text": "\nFirst, the key python library for computational linguistics is NLTK (\"Natural Language Toolkit\"). This is a stable, mature library created and maintained by professional computational linguists. It also has an extensive collection of tutorials, FAQs, etc. I recommend it highly.\nBelow is a simple template, in python code, for the problem raised in your Question; although it's a template it runs--supply any text as a string (as i've done) and it will return a list of word frequencies as well as a ranked list of those words in order of 'importance' (or suitability as keywords) according to a very simple heuristic.\nKeywords for a given document are (obviously) chosen from among important words in a document--ie, those words that are likely to distinguish it from another document. If you had no a priori knowledge of the text's subject matter, a common technique is to infer the importance or weight of a given word/term from its frequency, or importance = 1/frequency.\ntext = \"\"\" The intensity of the feeling makes up for the disproportion of the objects.  Things are equal to the imagination, which have the power of affecting the mind with an equal degree of terror, admiration, delight, or love.  When Lear calls upon the heavens to avenge his cause, \"for they are old like him,\" there is nothing extravagant or impious in this sublime identification of his age with theirs; for there is no other image which could do justice to the agonising sense of his wrongs and his despair! \"\"\"\n\nBAD_CHARS = \".!?,\\'\\\"\"\n\n# transform text into a list words--removing punctuation and filtering small words\nwords = [ word.strip(BAD_CHARS) for word in text.strip().split() if len(word) > 4 ]\n\nword_freq = {}\n\n# generate a 'word histogram' for the text--ie, a list of the frequencies of each word\nfor word in words :\n  word_freq[word] = word_freq.get(word, 0) + 1\n\n# sort the word list by frequency \n# (just a DSU sort, there's a python built-in for this, but i can't remember it)\ntx = [ (v, k) for (k, v) in word_freq.items()]\ntx.sort(reverse=True)\nword_freq_sorted = [ (k, v) for (v, k) in tx ]\n\n# eg, what are the most common words in that text?\nprint(word_freq_sorted)\n# returns: [('which', 4), ('other', 4), ('like', 4), ('what', 3), ('upon', 3)]\n# obviously using a text larger than 50 or so words will give you more meaningful results\n\nterm_importance = lambda word : 1.0/word_freq[word]\n\n# select document keywords from the words at/near the top of this list:\nmap(term_importance, word_freq.keys())\n\n"}, "1598": {"topic": "tag generation from a text content", "user_name": "dmcerdmcer", "text": "\nhttp://en.wikipedia.org/wiki/Latent_Dirichlet_allocation tries to represent each document in a training corpus as mixture of topics, which in turn are distributions mapping words to probabilities.  \nI had used it once to dissect a corpus of product reviews into the latent ideas that were being spoken about across all the documents such as 'customer service', 'product usability', etc.. The basic model does not advocate a way to convert the topic models into a single word describing what a topic is about.. but people have come up with all kinds of heuristics to do that once their model is trained.  \nI recommend you try playing with http://mallet.cs.umass.edu/ and seeing if this model fits your needs..  \nLDA is a completely unsupervised algorithm meaning it doesn't require you to hand annotate anything which is great, but on the flip side, might not deliver you the topics you were expecting it to give.\n"}, "1599": {"topic": "tag generation from a text content", "user_name": "John Paulett", "text": "\nA very simple solution to the problem would be:\n\ncount the occurences of each word in the text\nconsider the most frequent terms as the key phrases\nhave a black-list of 'stop words' to remove common words like the, and, it, is etc\n\nI'm sure there are cleverer, stats based solutions though.\nIf you need a solution to use in a larger project rather than for interests sake, Yahoo BOSS has a key term extraction method.\n"}, "1600": {"topic": "tag generation from a text content", "user_name": "dougdoug", "text": "\nLatent Dirichlet allocation or Hierarchical Dirichlet Process can be used to generate tags for individual texts within a greater corpus (body of texts) by extracting the most important words from the derived topics.\nA basic example would be if we were to run LDA over a corpus and define it to have two topics, and that we find further that a text in the corpus is 70% one topic, and 30% another. The top 70% of the words that define the first topic and 30% that define the second (without duplication) could then be considered as tags for the given text. This method provides strong results where tags generally represent the broader themes of the given texts.\nWith a general reference for preprocessing needed for these codes being found here, we can find tags through the following process using gensim.\nA heuristic way of deriving the optimal number of topics for LDA is found in this answer. Although HDP does not require the number of topics as an input, the standard in such cases is still to use LDA with a derived topic number, as HDP can be problematic. Assume here that the corpus is found to have 10 topics, and we want 5 tags per text:\nfrom gensim.models import LdaModel, HdpModel\nfrom gensim import corpora\n\nnum_topics = 10\nnum_tags = 5\n\nAssume further that we have a variable corpus, which is a preprocessed list of lists, with the subslist entries being word tokens. Initialize a Dirichlet dictionary and create a bag of words where texts are converted to their indexes for their component tokens (words):\ndirichlet_dict = corpora.Dictionary(corpus)\nbow_corpus = [dirichlet_dict.doc2bow(text) for text in corpus]\n\nCreate an LDA or HDP model:\ndirichlet_model = LdaModel(corpus=bow_corpus,\n                           id2word=dirichlet_dict,\n                           num_topics=num_topics,\n                           update_every=1,\n                           chunksize=len(bow_corpus),\n                           passes=20,\n                           alpha='auto')\n\n# dirichlet_model = HdpModel(corpus=bow_corpus, \n#                            id2word=dirichlet_dict,\n#                            chunksize=len(bow_corpus))\n\nThe following code produces ordered lists for the most important words per topic (note that here is where num_tags defines the desired tags per text):\nshown_topics = dirichlet_model.show_topics(num_topics=num_topics, \n                                           num_words=num_tags,\n                                           formatted=False)\nmodel_topics = [[word[0] for word in topic[1]] for topic in shown_topics]\n\nThen find the coherence of the topics across the texts:\ntopic_corpus = dirichlet_model.__getitem__(bow=bow_corpus, eps=0) # cutoff probability to 0 \ntopics_per_text = [text for text in topic_corpus]\n\nFrom here we have the percentage that each text coheres to a given topic, and the words associated with each topic, so we can combine them for tags with the following:\ncorpus_tags = []\n\nfor i in range(len(bow_corpus)):\n    # The complexity here is to make sure that it works with HDP\n    significant_topics = list(set([t[0] for t in topics_per_text[i]]))\n    topic_indexes_by_coherence = [tup[0] for tup in sorted(enumerate(topics_per_text[i]), key=lambda x:x[1])]\n    significant_topics_by_coherence = [significant_topics[i] for i in topic_indexes_by_coherence]\n\n    ordered_topics = [model_topics[i] for i in significant_topics_by_coherence][:num_topics] # subset for HDP\n    ordered_topic_coherences = [topics_per_text[i] for i in topic_indexes_by_coherence][:num_topics] # subset for HDP\n\n    text_tags = []\n    for i in range(num_topics):\n        # Find the number of indexes to select, which can later be extended if the word has already been selected\n        selection_indexes = list(range(int(round(num_tags * ordered_topic_coherences[i]))))\n        if selection_indexes == [] and len(text_tags) < num_tags: \n            # Fix potential rounding error by giving this topic one selection\n            selection_indexes = [0]\n              \n        for s_i in selection_indexes:\n            # ignore_words is a list of words should not be included\n            if ordered_topics[i][s_i] not in text_tags and ordered_topics[i][s_i] not in ignore_words:\n                text_tags.append(ordered_topics[i][s_i])\n            else:\n                selection_indexes.append(selection_indexes[-1] + 1)\n\n    # Fix for if too many were selected\n    text_tags = text_tags[:num_tags]\n\n    corpus_tags.append(text_tags)\n\ncorpus_tags will be a list of tags for each text based on how coherent the text is to the derived topics.\nSee this answer for a similar version of this that generates tags for a whole text corpus.\n"}, "1601": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "user1502248user1502248", "text": "\nI'm trying to extract named entities from my text using NLTK. I find that NLTK NER is not very accurate for my purpose and I want to add some more tags of my own as well. I've been trying to find a way to train my own NER, but I don't seem to be able to find the right resources. \nI have a couple of questions regarding NLTK-\n\nCan I use my own data to train an Named Entity Recognizer in NLTK?\nIf I can train using my own data, is the named_entity.py the file to be modified?\nDoes the input file format have to be in IOB eg. Eric NNP B-PERSON ?\nAre there any resources - apart from the nltk cookbook and nlp with python that I can use?\n\nI would really appreciate help in this regard\n"}, "1602": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "jjdubsjjdubs", "text": "\nAre you committed to using NLTK/Python?  I ran into the same problems as you, and had much better results using Stanford's named-entity recognizer: http://nlp.stanford.edu/software/CRF-NER.shtml.  The process for training the classifier using your own data is very well-documented in the FAQ.  \nIf you really need to use NLTK, I'd hit up the mailing list for some advice from other users: http://groups.google.com/group/nltk-users.  \nHope this helps!\n"}, "1603": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "Rohan AmruteRohan Amrute", "text": "\nYou can easily use the Stanford NER alongwith nltk.\nThe python script is like\nfrom nltk.tag.stanford import NERTagger\nimport os\njava_path = \"/Java/jdk1.8.0_45/bin/java.exe\"\nos.environ['JAVAHOME'] = java_path\nst = NERTagger('../ner-model.ser.gz','../stanford-ner.jar')\ntagging = st.tag(text.split())   \n\nTo train your own data and to create a model you can refer to the first question on Stanford NER FAQ.\nThe link is http://nlp.stanford.edu/software/crf-faq.shtml\n"}, "1604": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "aroparop", "text": "\nI also had this issue, but I managed to work it out. \nYou can use your own training data. I documented the main requirements/steps for this in my github repository.\nI used NLTK-trainer, so basicly you have to get the training data in the right format (token NNP B-tag), and run the training script. Check my repository for more info.\n"}, "1605": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "xjixji", "text": "\nThere are some functions in the nltk.chunk.named_entity module that train a NER tagger. However, they were specifically written for ACE corpus and not totally cleaned up, so one will need to write their own training procedures with those as a reference.\nThere are also two relatively recent guides (1 2) online detailing the process of using NLTK to train the GMB corpus.\nHowever, as mentioned in answers above, now that many tools are available, one really should not need to resort to NLTK if streamlined training process is desired. Toolkits such as CoreNLP and spaCy do a much better job. As using NLTK is not that much different to writing your own training code from scratch, there is not that much value in doing so. NLTK and OpenNLP can be regarded as somehow belonging to a past era before the explosion of recent progress in NLP.\n"}, "1606": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "Thang PhamThang Pham", "text": "\n\n\nAre there any resources - apart from the nltk cookbook and nlp with python that I can use?\n\n\nYou can consider using spaCy to train your own custom data for NER task. Here is an example from this thread to train a model on a custom training set to detect a new entity ANIMAL. The code was fixed and updated for easier reading.\nimport random\nimport spacy\nfrom spacy.training import Example\n\nLABEL = 'ANIMAL'\nTRAIN_DATA = [\n    (\"Horses are too tall and they pretend to care about your feelings\", {'entities': [(0, 6, LABEL)]}),\n    (\"Do they bite?\", {'entities': []}),\n    (\"horses are too tall and they pretend to care about your feelings\", {'entities': [(0, 6, LABEL)]}),\n    (\"horses pretend to care about your feelings\", {'entities': [(0, 6, LABEL)]}),\n    (\"they pretend to care about your feelings, those horses\", {'entities': [(48, 54, LABEL)]}),\n    (\"horses?\", {'entities': [(0, 6, LABEL)]})\n]\nnlp = spacy.load('en_core_web_sm')  # load existing spaCy model\nner = nlp.get_pipe('ner')\nner.add_label(LABEL)\n\noptimizer = nlp.create_optimizer()\n\n# get names of other pipes to disable them during training\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\nwith nlp.disable_pipes(*other_pipes):  # only train NER\n    for itn in range(20):\n        random.shuffle(TRAIN_DATA)\n        losses = {}\n        for text, annotations in TRAIN_DATA:\n            doc = nlp.make_doc(text)\n            example = Example.from_dict(doc, annotations)\n            nlp.update([example], drop=0.35, sgd=optimizer, losses=losses)\n        print(losses)\n\n# test the trained model\ntest_text = 'Do you like horses?'\ndoc = nlp(test_text)\nprint(\"Entities in '%s'\" % test_text)\nfor ent in doc.ents:\n    print(ent.label_, \" -- \", ent.text)\n\nHere are the outputs:\n{'ner': 9.60289144264557}\n{'ner': 8.875474230820478}\n{'ner': 6.370401408220459}\n{'ner': 6.687456469517201}\n... \n{'ner': 1.3796682589133492e-05}\n{'ner': 1.7709562613218738e-05}\n\nEntities in 'Do you like horses?'\nANIMAL  --  horses\n\n"}, "1607": {"topic": "NLTK Named Entity Recognition with Custom Data", "user_name": "iEriiiiEriii", "text": "\nTo complete the answer by @Thang M. Pham, you need to label your data before training. To do so, you can use the spacy-annotator.\nHere is an example taken from another answer:\nTrain Spacy NER on Indian Names\n"}, "1608": {"topic": "Tokenize a paragraph into sentence and then into words in NLTK", "user_name": "Nikhil Raghavendra", "text": "\nI am trying to input an entire paragraph into my word processor to be split into sentences first and then into words. \nI tried the following code but it does not work,\n    #text is the paragraph input\n    sent_text = sent_tokenize(text)\n    tokenized_text = word_tokenize(sent_text.split)\n    tagged = nltk.pos_tag(tokenized_text)\n    print(tagged)\n\nhowever this is not working and gives me errors. So how do I tokenize paragraphs into sentences and then words? \nAn example paragraph:\nThis thing seemed to overpower and astonish the little dark-brown dog,     and wounded him to the heart. He sank down in despair at the child's feet. When the blow was repeated, together with an admonition in childish sentences, he turned over upon his back, and held his paws in a peculiar manner. At the same time with his ears and his eyes he offered a small prayer to the child.\n**WARNING:**This is just a random text from the internet, I do not own the above content.\n"}, "1609": {"topic": "Tokenize a paragraph into sentence and then into words in NLTK", "user_name": "Nikhil RaghavendraNikhil Raghavendra", "text": "\nYou probably intended to loop over sent_text:\nimport nltk\n\nsent_text = nltk.sent_tokenize(text) # this gives us a list of sentences\n# now loop over each sentence and tokenize it separately\nfor sentence in sent_text:\n    tokenized_text = nltk.word_tokenize(sentence)\n    tagged = nltk.pos_tag(tokenized_text)\n    print(tagged)\n\n"}, "1610": {"topic": "Tokenize a paragraph into sentence and then into words in NLTK", "user_name": "", "text": "\nHere's a shorter version. This will give you a data structure with each individual sentence, and each token within the sentence. I prefer the TweetTokenizer for messy, real world language. The sentence tokenizer is considered decent, but be careful not to lower your word case till after this step, as it may impact the accuracy of detecting the boundaries of messy text. \nfrom nltk.tokenize import TweetTokenizer, sent_tokenize\n\ntokenizer_words = TweetTokenizer()\ntokens_sentences = [tokenizer_words.tokenize(t) for t in \nnltk.sent_tokenize(input_text)]\nprint(tokens_sentences)\n\nHere's what the output looks like, which I cleaned up so the structure stands out:\n[\n['This', 'thing', 'seemed', 'to', 'overpower', 'and', 'astonish', 'the', 'little', 'dark-brown', 'dog', ',', 'and', 'wounded', 'him', 'to', 'the', 'heart', '.'], \n['He', 'sank', 'down', 'in', 'despair', 'at', 'the', \"child's\", 'feet', '.'], \n['When', 'the', 'blow', 'was', 'repeated', ',', 'together', 'with', 'an', 'admonition', 'in', 'childish', 'sentences', ',', 'he', 'turned', 'over', 'upon', 'his', 'back', ',', 'and', 'held', 'his', 'paws', 'in', 'a', 'peculiar', 'manner', '.'], \n['At', 'the', 'same', 'time', 'with', 'his', 'ears', 'and', 'his', 'eyes', 'he', 'offered', 'a', 'small', 'prayer', 'to', 'the', 'child', '.']\n]\n\n"}, "1611": {"topic": "Tokenize a paragraph into sentence and then into words in NLTK", "user_name": "sliderslider", "text": "\nimport nltk  \n\ntextsample =\"This thing seemed to overpower and astonish the little dark-brown dog, and wounded him to the heart. He sank down in despair at the child's feet. When the blow was repeated, together with an admonition in childish sentences, he turned over upon his back, and held his paws in a peculiar manner. At the same time with his ears and his eyes he offered a small prayer to the child.\"  \n\nsentences = nltk.sent_tokenize(textsample)  \nwords = nltk.word_tokenize(textsample)  \nsentences \n[w for w in words if w.isalpha()]\n\n\nThe last line above will ensure only words are in the output and not special characters\nThe sentence output is as below\n\n['This thing seemed to overpower and astonish the little dark-brown dog, and wounded him to the heart.',\n \"He sank down in despair at the child's feet.\",\n 'When the blow was repeated, together with an admonition in childish sentences, he turned over upon his back, and held his paws in a peculiar manner.',\n 'At the same time with his ears and his eyes he offered a small prayer to the child.']\n\n\nThe words output is as below after removing special characters\n\n['This',\n 'thing',\n 'seemed',\n 'to',\n 'overpower',\n 'and',\n 'astonish',\n 'the',\n 'little',\n 'dog',\n 'and',\n 'wounded',\n 'him',\n 'to',\n 'the',\n 'heart',\n 'He',\n 'sank',\n 'down',\n 'in',\n 'despair',\n 'at',\n 'the',\n 'child',\n 'feet',\n 'When',\n 'the',\n 'blow',\n 'was',\n 'repeated',\n 'together',\n 'with',\n 'an',\n 'admonition',\n 'in',\n 'childish',\n 'sentences',\n 'he',\n 'turned',\n 'over',\n 'upon',\n 'his',\n 'back',\n 'and',\n 'held',\n 'his',\n 'paws',\n 'in',\n 'a',\n 'peculiar',\n 'manner',\n 'At',\n 'the',\n 'same',\n 'time',\n 'with',\n 'his',\n 'ears',\n 'and',\n 'his',\n 'eyes',\n 'he',\n 'offered',\n 'a',\n 'small',\n 'prayer',\n 'to',\n 'the',\n 'child']\n\n"}, "1612": {"topic": "Scikit Learn TfidfVectorizer : How to get top n terms with highest tf-idf score", "user_name": "Tommaso Di Noto", "text": "\nI am working on keyword extraction problem. Consider the very general case\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n\nt = \"\"\"Two Travellers, walking in the noonday sun, sought the shade of a widespreading tree to rest. As they lay looking up among the pleasant leaves, they saw that it was a Plane Tree.\n\n\"How useless is the Plane!\" said one of them. \"It bears no fruit whatever, and only serves to litter the ground with leaves.\"\n\n\"Ungrateful creatures!\" said a voice from the Plane Tree. \"You lie here in my cooling shade, and yet you say I am useless! Thus ungratefully, O Jupiter, do men receive their blessings!\"\n\nOur best blessings are often the least appreciated.\"\"\"\n\ntfs = tfidf.fit_transform(t.split(\" \"))\nstr = 'tree cat travellers fruit jupiter'\nresponse = tfidf.transform([str])\nfeature_names = tfidf.get_feature_names()\n\nfor col in response.nonzero()[1]:\n    print(feature_names[col], ' - ', response[0, col])\n\nand this gives me\n  (0, 28)   0.443509712811\n  (0, 27)   0.517461475101\n  (0, 8)    0.517461475101\n  (0, 6)    0.517461475101\ntree  -  0.443509712811\ntravellers  -  0.517461475101\njupiter  -  0.517461475101\nfruit  -  0.517461475101\n\nwhich is good. For any new document that comes in, is there a way to get the top n terms with the highest tfidf score?\n"}, "1613": {"topic": "Scikit Learn TfidfVectorizer : How to get top n terms with highest tf-idf score", "user_name": "AbtPstAbtPst", "text": "\nYou have to do a little bit of a song and dance to get the matrices as numpy arrays instead, but this should do what you're looking for:\nfeature_array = np.array(tfidf.get_feature_names())\ntfidf_sorting = np.argsort(response.toarray()).flatten()[::-1]\n\nn = 3\ntop_n = feature_array[tfidf_sorting][:n]\n\nThis gives me:\narray([u'fruit', u'travellers', u'jupiter'], \n  dtype='<U13')\n\nThe argsort call is really the useful one, here are the docs for it. We have to do [::-1] because argsort only supports sorting small to large. We call flatten to reduce the dimensions to 1d so that the sorted indices can be used to index the 1d feature array. Note that including the call to flatten will only work if you're testing one document at at time.\nAlso, on another note, did you mean something like tfs = tfidf.fit_transform(t.split(\"\\n\\n\"))? Otherwise, each term in the multiline string is being treated as a \"document\". Using \\n\\n instead means that we are actually looking at 4 documents (one for each line), which makes more sense when you think about tfidf.\n"}, "1614": {"topic": "Scikit Learn TfidfVectorizer : How to get top n terms with highest tf-idf score", "user_name": "humehume", "text": "\nSolution using sparse matrix itself (without .toarray())!\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(stop_words='english')\ncorpus = [\n    'I would like to check this document',\n    'How about one more document',\n    'Aim is to capture the key words from the corpus',\n    'frequency of words in a document is called term frequency'\n]\n\nX = tfidf.fit_transform(corpus)\nfeature_names = np.array(tfidf.get_feature_names())\n\n\nnew_doc = ['can key words in this new document be identified?',\n           'idf is the inverse document frequency caculcated for each of the words']\nresponses = tfidf.transform(new_doc)\n\n\ndef get_top_tf_idf_words(response, top_n=2):\n    sorted_nzs = np.argsort(response.data)[:-(top_n+1):-1]\n    return feature_names[response.indices[sorted_nzs]]\n  \nprint([get_top_tf_idf_words(response,2) for response in responses])\n\n#[array(['key', 'words'], dtype='<U9'),\n array(['frequency', 'words'], dtype='<U9')]\n\n"}, "1615": {"topic": "Scikit Learn TfidfVectorizer : How to get top n terms with highest tf-idf score", "user_name": "", "text": "\nHere is a quick code for that:\n(documents is a list)\ndef get_tfidf_top_features(documents,n_top=10):\n  tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,  stop_words='english')\n  tfidf = tfidf_vectorizer.fit_transform(documents)\n  importance = np.argsort(np.asarray(tfidf.sum(axis=0)).ravel())[::-1]\n  tfidf_feature_names = np.array(tfidf_vectorizer.get_feature_names())\n  return tfidf_feature_names[importance[:n_top]]\n\n"}, "1616": {"topic": "\"AssertionError: Torch not compiled with CUDA enabled\" in spite upgrading to CUDA version", "user_name": "Tina J", "text": "\nI figured out this is a popular question, but still I couldn't find a solution for that.\nI'm trying to run a simple repo Here which uses PyTorch. Although I just upgraded my Pytorch to the latest CUDA version from pytorch.org (1.2.0), it still throws the same error. I'm on Windows 10 and use conda with python 3.7.\n    raise AssertionError(\"Torch not compiled with CUDA enabled\")\nAssertionError: Torch not compiled with CUDA enabled\n\nHow to fix the problem?\nHere is my conda list:\n# Name                    Version                   Build  Channel\n_ipyw_jlab_nb_ext_conf    0.1.0                    py37_0    anaconda\n_pytorch_select           1.1.0                       cpu    anaconda\n_tflow_select             2.3.0                       mkl    anaconda\nabsl-py                   0.7.1                    pypi_0    pypi\nalabaster                 0.7.12                   py37_0    anaconda\nanaconda                  2019.07                  py37_0    anaconda\nanaconda-client           1.7.2                    py37_0    anaconda\nanaconda-navigator        1.9.7                    py37_0    anaconda\nanaconda-project          0.8.3                      py_0    anaconda\nargparse                  1.4.0                    pypi_0    pypi\nasn1crypto                0.24.0                   py37_0    anaconda\nastor                     0.8.0                    pypi_0    pypi\nastroid                   2.2.5                    py37_0    anaconda\nastropy                   3.2.1            py37he774522_0    anaconda\natomicwrites              1.3.0                    py37_1    anaconda\nattrs                     19.1.0                   py37_1    anaconda\nbabel                     2.7.0                      py_0    anaconda\nbackcall                  0.1.0                    py37_0    anaconda\nbackports                 1.0                        py_2    anaconda\nbackports-csv             1.0.7                    pypi_0    pypi\nbackports-functools-lru-cache 1.5                      pypi_0    pypi\nbackports.functools_lru_cache 1.5                        py_2    anaconda\nbackports.os              0.1.1                    py37_0    anaconda\nbackports.shutil_get_terminal_size 1.0.0                    py37_2    anaconda\nbackports.tempfile        1.0                        py_1    anaconda\nbackports.weakref         1.0.post1                  py_1    anaconda\nbeautifulsoup4            4.7.1                    py37_1    anaconda\nbitarray                  0.9.3            py37he774522_0    anaconda\nbkcharts                  0.2                      py37_0    anaconda\nblas                      1.0                         mkl    anaconda\nbleach                    3.1.0                    py37_0    anaconda\nblosc                     1.16.3               h7bd577a_0    anaconda\nbokeh                     1.2.0                    py37_0    anaconda\nboto                      2.49.0                   py37_0    anaconda\nbottleneck                1.2.1            py37h452e1ab_1    anaconda\nbzip2                     1.0.8                he774522_0    anaconda\nca-certificates           2019.5.15                     0    anaconda\ncertifi                   2019.6.16                py37_0    anaconda\ncffi                      1.12.3           py37h7a1dbc1_0    anaconda\nchainer                   6.2.0                    pypi_0    pypi\nchardet                   3.0.4                    py37_1    anaconda\ncheroot                   6.5.5                    pypi_0    pypi\ncherrypy                  18.1.2                   pypi_0    pypi\nclick                     7.0                      py37_0    anaconda\ncloudpickle               1.2.1                      py_0    anaconda\nclyent                    1.2.2                    py37_1    anaconda\ncolorama                  0.4.1                    py37_0    anaconda\ncomtypes                  1.1.7                    py37_0    anaconda\nconda                     4.7.11                   py37_0    anaconda\nconda-build               3.18.9                   py37_3    anaconda\nconda-env                 2.6.0                         1    anaconda\nconda-package-handling    1.3.11                   py37_0    anaconda\nconda-verify              3.4.2                      py_1    anaconda\nconsole_shortcut          0.1.1                         3    anaconda\nconstants                 0.6.0                    pypi_0    pypi\ncontextlib2               0.5.5                    py37_0    anaconda\ncpuonly                   1.0                           0    pytorch\ncryptography              2.7              py37h7a1dbc1_0    anaconda\ncudatoolkit               10.0.130                      0    anaconda\ncurl                      7.65.2               h2a8f88b_0    anaconda\ncycler                    0.10.0                   py37_0    anaconda\ncython                    0.29.12          py37ha925a31_0    anaconda\ncytoolz                   0.10.0           py37he774522_0    anaconda\ndask                      2.1.0                      py_0    anaconda\ndask-core                 2.1.0                      py_0    anaconda\ndecorator                 4.4.0                    py37_1    anaconda\ndefusedxml                0.6.0                      py_0    anaconda\ndistributed               2.1.0                      py_0    anaconda\ndocutils                  0.14                     py37_0    anaconda\nentrypoints               0.3                      py37_0    anaconda\net_xmlfile                1.0.1                    py37_0    anaconda\nez-setup                  0.9                      pypi_0    pypi\nfastcache                 1.1.0            py37he774522_0    anaconda\nfasttext                  0.9.1                    pypi_0    pypi\nfeedparser                5.2.1                    pypi_0    pypi\nffmpeg                    4.1.3                h6538335_0    conda-forge\nfilelock                  3.0.12                     py_0    anaconda\nfirst                     2.0.2                    pypi_0    pypi\nflask                     1.1.1                      py_0    anaconda\nfreetype                  2.9.1                ha9979f8_1    anaconda\nfuture                    0.17.1                   py37_0    anaconda\ngast                      0.2.2                    py37_0    anaconda\nget                       2019.4.13                pypi_0    pypi\nget_terminal_size         1.0.0                h38e98db_0    anaconda\ngevent                    1.4.0            py37he774522_0    anaconda\nglob2                     0.7                        py_0    anaconda\ngoogle-pasta              0.1.7                    pypi_0    pypi\ngraphviz                  2.38.0                        4    anaconda\ngreenlet                  0.4.15           py37hfa6e2cd_0    anaconda\ngrpcio                    1.22.0                   pypi_0    pypi\nh5py                      2.9.0            py37h5e291fa_0    anaconda\nhdf5                      1.10.4               h7ebc959_0    anaconda\nheapdict                  1.0.0                    py37_2    anaconda\nhtml5lib                  1.0.1                    py37_0    anaconda\nhttp-client               0.1.22                   pypi_0    pypi\nhypothesis                4.34.0                   pypi_0    pypi\nicc_rt                    2019.0.0             h0cc432a_1    anaconda\nicu                       58.2                 ha66f8fd_1    anaconda\nidna                      2.8                      py37_0    anaconda\nimageio                   2.4.1                    pypi_0    pypi\nimageio-ffmpeg            0.3.0                    pypi_0    pypi\nimagesize                 1.1.0                    py37_0    anaconda\nimportlib_metadata        0.17                     py37_1    anaconda\nimutils                   0.5.2                    pypi_0    pypi\nintel-openmp              2019.0                   pypi_0    pypi\nipykernel                 5.1.1            py37h39e3cac_0    anaconda\nipython                   7.6.1            py37h39e3cac_0    anaconda\nipython_genutils          0.2.0                    py37_0    anaconda\nipywidgets                7.5.0                      py_0    anaconda\nisort                     4.3.21                   py37_0    anaconda\nitsdangerous              1.1.0                    py37_0    anaconda\njaraco-functools          2.0                      pypi_0    pypi\njdcal                     1.4.1                      py_0    anaconda\njedi                      0.13.3                   py37_0    anaconda\njinja2                    2.10.1                   py37_0    anaconda\njoblib                    0.13.2                   py37_0    anaconda\njpeg                      9b                   hb83a4c4_2    anaconda\njson5                     0.8.4                      py_0    anaconda\njsonschema                3.0.1                    py37_0    anaconda\njupyter                   1.0.0                    py37_7    anaconda\njupyter_client            5.3.1                      py_0    anaconda\njupyter_console           6.0.0                    py37_0    anaconda\njupyter_core              4.5.0                      py_0    anaconda\njupyterlab                1.0.2            py37hf63ae98_0    anaconda\njupyterlab_server         1.0.0                      py_0    anaconda\nkeras                     2.2.4                         0    anaconda\nkeras-applications        1.0.8                      py_0    anaconda\nkeras-base                2.2.4                    py37_0    anaconda\nkeras-preprocessing       1.1.0                      py_1    anaconda\nkeyring                   18.0.0                   py37_0    anaconda\nkiwisolver                1.1.0            py37ha925a31_0    anaconda\nkrb5                      1.16.1               hc04afaa_7\nlazy-object-proxy         1.4.1            py37he774522_0    anaconda\nlibarchive                3.3.3                h0643e63_5    anaconda\nlibcurl                   7.65.2               h2a8f88b_0    anaconda\nlibiconv                  1.15                 h1df5818_7    anaconda\nliblief                   0.9.0                ha925a31_2    anaconda\nlibmklml                  2019.0.5                      0    anaconda\nlibpng                    1.6.37               h2a8f88b_0    anaconda\nlibprotobuf               3.8.0                h7bd577a_0    anaconda\nlibsodium                 1.0.16               h9d3ae62_0    anaconda\nlibssh2                   1.8.2                h7a1dbc1_0    anaconda\nlibtiff                   4.0.10               hb898794_2    anaconda\nlibxml2                   2.9.9                h464c3ec_0    anaconda\nlibxslt                   1.1.33               h579f668_0    anaconda\nllvmlite                  0.29.0           py37ha925a31_0    anaconda\nlocket                    0.2.0                    py37_1    anaconda\nlxml                      4.3.4            py37h1350720_0    anaconda\nlz4-c                     1.8.1.2              h2fa13f4_0    anaconda\nlzo                       2.10                 h6df0209_2    anaconda\nm2w64-gcc-libgfortran     5.3.0                         6\nm2w64-gcc-libs            5.3.0                         7\nm2w64-gcc-libs-core       5.3.0                         7\nm2w64-gmp                 6.1.0                         2\nm2w64-libwinpthread-git   5.0.0.4634.697f757               2\nmake-dataset              1.0                      pypi_0    pypi\nmarkdown                  3.1.1                    py37_0    anaconda\nmarkupsafe                1.1.1            py37he774522_0    anaconda\nmatplotlib                3.1.0            py37hc8f65d3_0    anaconda\nmccabe                    0.6.1                    py37_1    anaconda\nmenuinst                  1.4.16           py37he774522_0    anaconda\nmistune                   0.8.4            py37he774522_0    anaconda\nmkl                       2019.0                   pypi_0    pypi\nmkl-service               2.0.2            py37he774522_0    anaconda\nmkl_fft                   1.0.12           py37h14836fe_0    anaconda\nmkl_random                1.0.2            py37h343c172_0    anaconda\nmock                      3.0.5                    py37_0    anaconda\nmore-itertools            7.0.0                    py37_0    anaconda\nmoviepy                   1.0.0                    pypi_0    pypi\nmpmath                    1.1.0                    py37_0    anaconda\nmsgpack-python            0.6.1            py37h74a9793_1    anaconda\nmsys2-conda-epoch         20160418                      1\nmultipledispatch          0.6.0                    py37_0    anaconda\nmysqlclient               1.4.2.post1              pypi_0    pypi\nnavigator-updater         0.2.1                    py37_0    anaconda\nnbconvert                 5.5.0                      py_0    anaconda\nnbformat                  4.4.0                    py37_0    anaconda\nnetworkx                  2.3                        py_0    anaconda\nninja                     1.9.0            py37h74a9793_0    anaconda\nnltk                      3.4.4                    py37_0    anaconda\nnose                      1.3.7                    py37_2    anaconda\nnotebook                  6.0.0                    py37_0    anaconda\nnumba                     0.44.1           py37hf9181ef_0    anaconda\nnumexpr                   2.6.9            py37hdce8814_0    anaconda\nnumpy                     1.16.4                   pypi_0    pypi\nnumpy-base                1.16.4           py37hc3f5095_0    anaconda\nnumpydoc                  0.9.1                      py_0    anaconda\nolefile                   0.46                     py37_0    anaconda\nopencv-contrib-python     4.1.0.25                 pypi_0    pypi\nopencv-python             4.1.0.25                 pypi_0    pypi\nopenpyxl                  2.6.2                      py_0    anaconda\nopenssl                   1.1.1c               he774522_1    anaconda\npackaging                 19.0                     py37_0    anaconda\npandas                    0.24.2           py37ha925a31_0    anaconda\npandoc                    2.2.3.2                       0    anaconda\npandocfilters             1.4.2                    py37_1    anaconda\nparso                     0.5.0                      py_0    anaconda\npartd                     1.0.0                      py_0    anaconda\npath.py                   12.0.1                     py_0    anaconda\npathlib2                  2.3.4                    py37_0    anaconda\npatsy                     0.5.1                    py37_0    anaconda\npattern                   3.6                      pypi_0    pypi\npdfminer-six              20181108                 pypi_0    pypi\npep8                      1.7.1                    py37_0    anaconda\npickleshare               0.7.5                    py37_0    anaconda\npillow                    6.1.0            py37hdc69c19_0    anaconda\npip                       19.1.1                   py37_0    anaconda\npkginfo                   1.5.0.1                  py37_0    anaconda\npluggy                    0.12.0                     py_0    anaconda\nply                       3.11                     py37_0    anaconda\nportend                   2.5                      pypi_0    pypi\npost                      2019.4.13                pypi_0    pypi\npowershell_shortcut       0.0.1                         2    anaconda\nproglog                   0.1.9                    pypi_0    pypi\nprometheus_client         0.7.1                      py_0    anaconda\nprompt_toolkit            2.0.9                    py37_0    anaconda\nprotobuf                  3.7.1                    pypi_0    pypi\npsutil                    5.6.3            py37he774522_0    anaconda\npublic                    2019.4.13                pypi_0    pypi\npy                        1.8.0                    py37_0    anaconda\npy-lief                   0.9.0            py37ha925a31_2    anaconda\npybind11                  2.3.0                    pypi_0    pypi\npycodestyle               2.5.0                    py37_0    anaconda\npycosat                   0.6.3            py37hfa6e2cd_0    anaconda\npycparser                 2.19                     py37_0    anaconda\npycrypto                  2.6.1            py37hfa6e2cd_9    anaconda\npycryptodome              3.8.2                    pypi_0    pypi\npycurl                    7.43.0.3         py37h7a1dbc1_0    anaconda\npydot                     1.4.1                    pypi_0    pypi\npyflakes                  2.1.1                    py37_0    anaconda\npygments                  2.4.2                      py_0    anaconda\npylint                    2.3.1                    py37_0    anaconda\npyodbc                    4.0.26           py37ha925a31_0    anaconda\npyopenssl                 19.0.0                   py37_0    anaconda\npyparsing                 2.4.0                      py_0    anaconda\npyqt                      5.9.2            py37h6538335_2    anaconda\npyreadline                2.1                      py37_1    anaconda\npyrsistent                0.14.11          py37he774522_0    anaconda\npysocks                   1.7.0                    py37_0    anaconda\npytables                  3.5.2            py37h1da0976_1    anaconda\npytest                    5.0.1                    py37_0    anaconda\npytest-arraydiff          0.3              py37h39e3cac_0    anaconda\npytest-astropy            0.5.0                    py37_0    anaconda\npytest-doctestplus        0.3.0                    py37_0    anaconda\npytest-openfiles          0.3.2                    py37_0    anaconda\npytest-remotedata         0.3.1                    py37_0    anaconda\npython                    3.7.3                h8c8aaf0_1    anaconda\npython-dateutil           2.8.0                    py37_0    anaconda\npython-docx               0.8.10                   pypi_0    pypi\npython-graphviz           0.11.1                   pypi_0    pypi\npython-libarchive-c       2.8                     py37_11    anaconda\npytorch                   1.2.0               py3.7_cpu_1  [cpuonly]  pytorch\npytube                    9.5.1                    pypi_0    pypi\npytz                      2019.1                     py_0    anaconda\npywavelets                1.0.3            py37h8c2d366_1    anaconda\npywin32                   223              py37hfa6e2cd_1    anaconda\npywinpty                  0.5.5                 py37_1000    anaconda\npyyaml                    5.1.1            py37he774522_0    anaconda\npyzmq                     18.0.0           py37ha925a31_0    anaconda\nqt                        5.9.7            vc14h73c81de_0  [vc14]  anaconda\nqtawesome                 0.5.7                    py37_1    anaconda\nqtconsole                 4.5.1                      py_0    anaconda\nqtpy                      1.8.0                      py_0    anaconda\nquery-string              2019.4.13                pypi_0    pypi\nrequest                   2019.4.13                pypi_0    pypi\nrequests                  2.22.0                   py37_0    anaconda\nrope                      0.14.0                     py_0    anaconda\nruamel_yaml               0.15.46          py37hfa6e2cd_0    anaconda\nscikit-image              0.15.0           py37ha925a31_0    anaconda\nscikit-learn              0.21.2           py37h6288b17_0    anaconda\nscipy                     1.3.0                    pypi_0    pypi\nscipy-stack               0.0.5                    pypi_0    pypi\nseaborn                   0.9.0                    py37_0    anaconda\nsend2trash                1.5.0                    py37_0    anaconda\nsetuptools                41.1.0                   pypi_0    pypi\nsimplegeneric             0.8.1                    py37_2    anaconda\nsingledispatch            3.4.0.3                  py37_0    anaconda\nsip                       4.19.8           py37h6538335_0    anaconda\nsix                       1.12.0                   py37_0    anaconda\nsnappy                    1.1.7                h777316e_3    anaconda\nsnowballstemmer           1.9.0                      py_0    anaconda\nsortedcollections         1.1.2                    py37_0    anaconda\nsortedcontainers          2.1.0                    py37_0    anaconda\nsoupsieve                 1.8                      py37_0    anaconda\nsphinx                    2.1.2                      py_0    anaconda\nsphinxcontrib             1.0                      py37_1    anaconda\nsphinxcontrib-applehelp   1.0.1                      py_0    anaconda\nsphinxcontrib-devhelp     1.0.1                      py_0    anaconda\nsphinxcontrib-htmlhelp    1.0.2                      py_0    anaconda\nsphinxcontrib-jsmath      1.0.1                      py_0    anaconda\nsphinxcontrib-qthelp      1.0.2                      py_0    anaconda\nsphinxcontrib-serializinghtml 1.1.3                      py_0    anaconda\nsphinxcontrib-websupport  1.1.2                      py_0    anaconda\nspyder                    3.3.6                    py37_0    anaconda\nspyder-kernels            0.5.1                    py37_0    anaconda\nsqlalchemy                1.3.5            py37he774522_0    anaconda\nsqlite                    3.29.0               he774522_0    anaconda\nstatsmodels               0.10.0           py37h8c2d366_0    anaconda\nsumma                     1.2.0                    pypi_0    pypi\nsympy                     1.4                      py37_0    anaconda\ntbb                       2019.4               h74a9793_0    anaconda\ntblib                     1.4.0                      py_0    anaconda\ntempora                   1.14.1                   pypi_0    pypi\ntensorboard               1.14.0           py37he3c9ec2_0    anaconda\ntensorboardx              1.8                      pypi_0    pypi\ntensorflow                1.14.0          mkl_py37h7908ca0_0    anaconda\ntensorflow-base           1.14.0          mkl_py37ha978198_0    anaconda\ntensorflow-estimator      1.14.0                     py_0    anaconda\ntensorflow-mkl            1.14.0               h4fcabd2_0    anaconda\ntermcolor                 1.1.0                    pypi_0    pypi\nterminado                 0.8.2                    py37_0    anaconda\ntestpath                  0.4.2                    py37_0    anaconda\ntk                        8.6.8                hfa6e2cd_0    anaconda\ntoolz                     0.10.0                     py_0    anaconda\ntorchvision               0.4.0                  py37_cpu  [cpuonly]  pytorch\ntornado                   6.0.3            py37he774522_0    anaconda\ntqdm                      4.32.1                     py_0    anaconda\ntraitlets                 4.3.2                    py37_0    anaconda\ntyping                    3.6.6                    pypi_0    pypi\ntyping-extensions         3.6.6                    pypi_0    pypi\nunicodecsv                0.14.1                   py37_0    anaconda\nurllib3                   1.24.2                   py37_0    anaconda\nvalidators                0.13.0                   pypi_0    pypi\nvc                        14.1                 h0510ff6_4    anaconda\nvs2015_runtime            14.15.26706          h3a45250_4    anaconda\nwcwidth                   0.1.7                    py37_0    anaconda\nwebencodings              0.5.1                    py37_1    anaconda\nwerkzeug                  0.15.4                     py_0    anaconda\nwheel                     0.33.4                   py37_0    anaconda\nwidgetsnbextension        3.5.0                    py37_0    anaconda\nwin_inet_pton             1.1.0                    py37_0    anaconda\nwin_unicode_console       0.5                      py37_0    anaconda\nwincertstore              0.2                      py37_0    anaconda\nwinpty                    0.4.3                         4    anaconda\nwrapt                     1.11.2           py37he774522_0    anaconda\nxlrd                      1.2.0                    py37_0    anaconda\nxlsxwriter                1.1.8                      py_0    anaconda\nxlwings                   0.15.8                   py37_0    anaconda\nxlwt                      1.3.0                    py37_0    anaconda\nxz                        5.2.4                h2fa13f4_4    anaconda\nyaml                      0.1.7                hc54c509_2    anaconda\nyoutube-dl                2019.8.2                 pypi_0    pypi\nzc-lockfile               1.4                      pypi_0    pypi\nzeromq                    4.3.1                h33f27b4_3    anaconda\nzict                      1.0.0                      py_0    anaconda\nzipp                      0.5.1                      py_0    anaconda\nzlib                      1.2.11               h62dcd97_3    anaconda\nzstd                      1.3.7                h508b16e_0    anaconda\n\n"}, "1617": {"topic": "\"AssertionError: Torch not compiled with CUDA enabled\" in spite upgrading to CUDA version", "user_name": "Tina JTina J", "text": "\nyou dont have to install it via anaconda, you could install cuda from their website. after install ends open a new terminal and check your cuda version with:\n>>> nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2021 NVIDIA Corporation\nBuilt on Thu_Nov_18_09:52:33_Pacific_Standard_Time_2021\nCuda compilation tools, release 11.5, V11.5.119\nBuild cuda_11.5.r11.5/compiler.30672275_0\n\nmy is V11.5\nthen go here and select your os and preferred package manager(pip or anaconda), and the cuda version you installed, and copy the generated install command, I got:\npip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n\nnotice that for me I had python 3.10 installed but my project run over 3.9 so either use virtual environment or run pip of your wanted base interpreter explicitly (for example C:\\Software\\Python\\Python39\\python.exe -m pip install .....)\nelse you will be stuck with Could not find a version that satisfies the requirement torch errors\nthen open python console and check for cuda availability\n>>> import torch\n>>> torch.cuda.is_available()\nTrue\n\n"}, "1618": {"topic": "\"AssertionError: Torch not compiled with CUDA enabled\" in spite upgrading to CUDA version", "user_name": "", "text": "\nHow did you install pytorch? It sounds like you installed pytorch without CUDA support. https://pytorch.org/ has instructions for how to install pytorch with cuda support.\nIn this case, we have the following command:\nconda install pytorch torchvision cudatoolkit=10.1 -c pytorch\nOR the command with latest cudatoolkit version.\n"}, "1619": {"topic": "\"AssertionError: Torch not compiled with CUDA enabled\" in spite upgrading to CUDA version", "user_name": "Eliav LouskiEliav Louski", "text": "\nUninstalling the packages and reinstalling it with pip instead solved it for me.\n1.conda remove pytorch torchvision torchaudio cudatoolkit \n2.pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n"}, "1620": {"topic": "\"AssertionError: Torch not compiled with CUDA enabled\" in spite upgrading to CUDA version", "user_name": "Gilfoyle", "text": "\ntry this:\nconda install pytorch torchvision cudatoolkit=10.2 -c pytorch\n\n"}, "1621": {"topic": "\"AssertionError: Torch not compiled with CUDA enabled\" in spite upgrading to CUDA version", "user_name": "Muhammad Hashir AliMuhammad Hashir Ali", "text": "\nOne more thing to note here is if you are installing PyTorch with CUDA support in an anaconda environment, Please make sure that the Python version should be 3.7-3.9.\nconda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge\n\nI was getting the same \"AssertionError: Torch not compiled with CUDA enabled\" with python 3.10.\n"}, "1622": {"topic": "\"AssertionError: Torch not compiled with CUDA enabled\" in spite upgrading to CUDA version", "user_name": "HusseinHussein", "text": "\nFirst activate your environment. Replace <name> with your environment name.\nconda activate <name>\n\nThen see cuda version in your machine. To see cuda version:\nnvcc --version\n\nNow for CUDA 10.1 use:\nconda install pytorch==1.6.0 torchvision==0.7.0 cudatoolkit=10.1 -c pytorch\n\nFor CUDA 10.0 use:\nconda install pytorch==1.6.0 torchvision==0.7.0 cudatoolkit=10.0 -c pytorch\n\nFor CUDA 9.2 use:\nconda install pytorch==1.6.0 torchvision==0.7.0 cudatoolkit=9.2 -c pytorch\n\nYou can also visit this link to see pytorch and the corresponding torchvision version.\n"}, "1623": {"topic": "\"AssertionError: Torch not compiled with CUDA enabled\" in spite upgrading to CUDA version", "user_name": "Sinh Nguyen", "text": "\nThis error is happening because of incorrect device. Make sure to run this snippet before every experiment.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n"}, "1624": {"topic": "\"AssertionError: Torch not compiled with CUDA enabled\" in spite upgrading to CUDA version", "user_name": "oxaloxal", "text": "\nIt seems like you do not have Pytorch installed with CUDA support.\nTry checking your CUDA version using\nnvcc --version\n\nor\nnvidia-smi\n\nInstall from the original pytorch distribution into your conda environment\nhttps://pytorch.org/get-started/locally/\nConfigure Pytorch for Mac M1 chips\nStep 1: Install Xcode\nInstall the Command Line Tools:\nxcode-select --install\n\nStep 2: Setup a new conda environment\nconda create -n torch-gpu python=3.8\nconda activate torch-gpu\n\nStep 2: Install PyTorch packages\nconda install pytorch torchvision torchaudio -c pytorch-nightly\n\nStep 3: Install Jupyter notebook for validating installation\nconda install -c conda-forge jupyter jupyterlab\njupter-notebook\n\nCreate new notebook file and execute this code\ndtype = torch.float\ndevice = torch.device(\"mps\")\n\n# Create random input and output data\nx = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\ny = torch.sin(x)\n\n# Randomly initialize weights\na = torch.randn((), device=device, dtype=dtype)\nb = torch.randn((), device=device, dtype=dtype)\nc = torch.randn((), device=device, dtype=dtype)\nd = torch.randn((), device=device, dtype=dtype)\n\nlearning_rate = 1e-6\nfor t in range(2000):\n    # Forward pass: compute predicted y\n    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum().item()\n    if t % 100 == 99:\n        print(t, loss)\n\n# Backprop to compute gradients of a, b, c, d with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_a = grad_y_pred.sum()\n    grad_b = (grad_y_pred * x).sum()\n    grad_c = (grad_y_pred * x ** 2).sum()\n    grad_d = (grad_y_pred * x ** 3).sum()\n\n    # Update weights using gradient descent\n    a -= learning_rate * grad_a\n    b -= learning_rate * grad_b\n    c -= learning_rate * grad_c\n    d -= learning_rate * grad_d\n\n\nprint(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n\nIf you don\u2019t see any error, everything works as expected!\nRef:\nhttps://towardsdatascience.com/installing-pytorch-on-apple-m1-chip-with-gpu-acceleration-3351dc44d67c\n"}, "1625": {"topic": "\"AssertionError: Torch not compiled with CUDA enabled\" in spite upgrading to CUDA version", "user_name": "", "text": "\npyenv local 3.8.6\npoetry env use $(pyenv which python)\npoetry run pip install -U pip 'setuptools==59.5.0'\npoetry install\n"}, "1626": {"topic": "\"AssertionError: Torch not compiled with CUDA enabled\" in spite upgrading to CUDA version", "user_name": "Ajeet VermaAjeet Verma", "text": "\nI was have this problem. My solution is non-Russian IP. Pycharm & Keras not work too.\n"}, "1627": {"topic": "Sentiment Analysis Dictionaries", "user_name": "Raj", "text": "\nI was wondering if anybody knew where I could obtain dictionaries of positive and negative words.  I'm looking into sentiment analysis and this is a crucial part of it.\n"}, "1628": {"topic": "Sentiment Analysis Dictionaries", "user_name": "user387049user387049", "text": "\nThe Sentiment Lexicon, at the University of Pittsburgh might be what you are after. It's a lexicon of about 8,000 words with positive/neutral/negative sentiment. It's described in more detail in this paper and released under the GPL.\n"}, "1629": {"topic": "Sentiment Analysis Dictionaries", "user_name": "Tim", "text": "\nSentiment Analysis (Opinion Mining) lexicons\n\nMPQA Subjectivity Lexicon\nBing Liu and Minqing Hu Sentiment Lexicon\nSentiWordNet (Included in NLTK)\nVADER Sentiment Lexicon\nSenticNet\nLIWC (not free)\nHarvard Inquirer (broken link)\nANEW\n\n\nSources:\n\nKeenformatics - Sentiment Analysis lexicons and datasets (my blog)\nHutto, C. J., and Eric Gilbert. \"Vader: A parsimonious rule-based model for sentiment analysis of social media text.\" Eighth International AAAI Conference on Weblogs and Social Media. 2014.\nSentiment Symposium Tutorial by Christopher Potts\nPersonal experience\n\n"}, "1630": {"topic": "Sentiment Analysis Dictionaries", "user_name": "StompchickenStompchicken", "text": "\nArriving a bit late I'll just note that dictionaries have a limited contribution for sentiment analysis. \nSome sentiment bearing sentences do not contain any \"sentiment\" word - e.g. \"read the book\" which could be positive in a book review while negative in a movie review. \nSimilarly, the sentiment word \"unpredictable\" could be positive in the context of a thriller but negative when describing the breaks system of the Toyota. \nand there are many more...\n"}, "1631": {"topic": "Sentiment Analysis Dictionaries", "user_name": "", "text": "\nProfessor Bing Liu provide an English Lexicon of about 6800 word, you can download form this link:\nOpinion Mining, Sentiment Analysis, and Opinion Spam Detection\n"}, "1632": {"topic": "Sentiment Analysis Dictionaries", "user_name": "Kurt BourbakiKurt Bourbaki", "text": "\nThis paper from 2002 describes an algorithm for deriving such a dictionary from text samples automatically, using only two words as a seed set.\n"}, "1633": {"topic": "Sentiment Analysis Dictionaries", "user_name": "ScienceFrictionScienceFriction", "text": "\nAFINN you can find here and also create it dynamically. Like whenever unknown +ve word comes add it with +1. Like banana is new +ve word and appearing twice then it will become +2. \nAs much articles and data you craws your dictionary would become stronger!\n"}, "1634": {"topic": "Sentiment Analysis Dictionaries", "user_name": "CommunityBot", "text": "\nThe Harvard-IV dictionary directory http://www.wjh.harvard.edu/~inquirer/homecat.htm  has at least two sets of ready-to-use dictionaries for positive/negative orientation.\n"}, "1635": {"topic": "Sentiment Analysis Dictionaries", "user_name": "rodobastiasrodobastias", "text": "\nYou can use vader sentiment lexicon\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nsentence='APPle is good for health'\nsid = SentimentIntensityAnalyzer()\nss = sid.polarity_scores(sentence)  \nprint(ss)\n\nit will give you the polarity of sentence.\noutput:\n {'compound': 0.4404, 'neu': 0.58, 'pos': 0.42, 'neg': 0.0}\n\n"}, "1636": {"topic": "Sentiment Analysis Dictionaries", "user_name": "Fred FooFred Foo", "text": "\nSentiwords gives 155,000 words (and their polarity, that is, a score between -1 and 1 for very negative through to very positive). The lexicon is discussed here\n"}, "1637": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "Benyamin Jafari", "text": "\n\n\n\n\n\n\nClosed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.                    \n\n\n\n\n\n\n\n\n\n\n We don\u2019t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\n\n\nClosed 8 years ago.\n\n\n\n\n\n\n\r\n                        Improve this question\r\n                    \n\n\n\nI'm looking for an open source implementation, preferably in python, of Textual Sentiment Analysis (http://en.wikipedia.org/wiki/Sentiment_analysis). Is anyone familiar with such open source implementation I can use?\nI'm writing an application that searches twitter for some search term, say \"youtube\", and counts \"happy\" tweets vs. \"sad\" tweets. \nI'm using Google's appengine, so it's in python. I'd like to be able to classify the returned search results from twitter and I'd like to do that in python.\nI haven't been able to find such sentiment analyzer so far, specifically not in python. \nAre you familiar with such open source implementation I can use? Preferably this is already in python, but if not, hopefully I can translate it to python.\nNote, the texts I'm analyzing are VERY short, they are tweets. So ideally, this classifier is optimized for such short texts.\nBTW, twitter does support the \":)\" and \":(\" operators in search, which aim to do just this, but unfortunately, the classification provided by them isn't that great, so I figured I might give this a try myself.\nThanks!\nBTW, an early demo is here and the code I have so far is here and I'd love to opensource it with any interested developer.\n"}, "1638": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "RanRan", "text": "\nGood luck with that.\nSentiment is enormously contextual, and tweeting culture makes the problem worse because you aren't given the context for most tweets.  The whole point of twitter is that you can leverage the huge amount of shared \"real world\" context to pack meaningful communication in a very short message.\nIf they say the video is bad, does that mean bad, or bad?\n\nA linguistics professor was lecturing\n  to her class one day. \"In English,\"\n  she said, \"A double negative forms a\n  positive. In some languages, though,\n  such as Russian, a double negative is\n  still a negative. However, there is no\n  language wherein a double positive can\n  form a negative.\"\nA voice from the back of the room\n  piped up, \"Yeah . . .right.\"\n\n"}, "1639": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "dbr", "text": "\nWith most of these kinds of applications, you'll have to roll much of your own code for a statistical classification task. As Lucka suggested, NLTK is the perfect tool for natural language manipulation in Python, so long as your goal doesn't interfere with the non commercial nature of its license.  However, I would suggest other software packages for modeling.  I haven't found many strong advanced machine learning models available for Python, so I'm going to suggest some standalone binaries that easily cooperate with it.\nYou may be interested in The Toolkit for Advanced Discriminative Modeling, which can be easily interfaced with Python.  This has been used for classification tasks in various areas of natural language processing.  You also have a pick of a number of different models.  I'd suggest starting with Maximum Entropy classification so long as you're already familiar with implementing a Naive Bayes classifier.  If not, you may want to look into it and code one up to really get a decent understanding of statistical classification as a machine learning task.\nThe University of Texas at Austin computational linguistics groups have held classes where most of the projects coming out of them have used this great tool.  You can look at the course page for Computational Linguistics II to get an idea of how to make it work and what previous applications it has served.\nAnother great tool which works in the same vein is Mallet.  The difference between Mallet is that there's a bit more documentation and some more models available, such as decision trees, and it's in Java, which, in my opinion, makes it a little slower.  Weka is a whole suite of different machine learning models in one big package that includes some graphical stuff, but it's really mostly meant for pedagogical purposes, and isn't really something I'd put into production.\nGood luck with your task.  The real difficult part will probably be the amount of knowledge engineering required up front for you to classify the 'seed set' off of which your model will learn.  It needs to be pretty sizeable, depending on whether you're doing binary classification (happy vs sad) or a whole range of emotions (which will require even more).  Make sure to hold out some of this engineered data for testing, or run some tenfold or remove-one tests to make sure you're actually doing a good job predicting before you put it out there. And most of all, have fun!  This is the best part of NLP and AI, in my opinion.\n"}, "1640": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "MarkusQMarkusQ", "text": "\nThanks everyone for your suggestions, they were indeed very useful!\nI ended up using a Naive Bayesian classifier, which I borrowed from here. \nI started by feeding it with a list of good/bad keywords and then added a \"learn\" feature by employing user feedback. It turned out to work pretty nice.\nThe full details of my work as in a blog post.\nAgain, your help was very useful, so thank you!\n"}, "1641": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "Robert ElwellRobert Elwell", "text": "\nI have constructed a word list labeled with sentiment. You can access it from here:\nhttp://www2.compute.dtu.dk/pubdb/views/edoc_download.php/6010/zip/imm6010.zip\nYou will find a short Python program on my blog: \nhttp://finnaarupnielsen.wordpress.com/2011/06/20/simplest-sentiment-analysis-in-python-with-af/\nThis post displays how to use the word list with single sentences as well as with Twitter.\nWord lists approaches have their limitations. You will find a investigation of the limitations of my word list in the article \"A new ANEW: Evaluation of a word list for sentiment analysis in microblogs\". That article is available from my homepage.\nPlease note a unicode(s, 'utf-8') is missing from the code (for paedagogic reasons).\n"}, "1642": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "RanRan", "text": "\nA lot of research papers indicate that a good starting point for sentiment analysis is looking at adjectives, e.g., are they positive adjectives or negative adjectives. For a short block of text this is pretty much your only option... There are papers that look at entire documents, or sentence level analysis, but as you say tweets are quite short... There is no real magic approach to understanding the sentiment of a sentence, so I think your best bet would be hunting down one of these research papers and trying to get their data-set of positively/negatively oriented adjectives.\nNow, this having been said, sentiment is domain specific, and you might find it difficult to get a high-level of accuracy with a general purpose data-set.\nGood luck.\n"}, "1643": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "", "text": "\nI think you may find it difficult to find what you're after. The closest thing that I know of is LingPipe, which has some sentiment analysis functionality and is available under a limited kind of open-source licence, but is written in Java.\nAlso, sentiment analysis systems are usually developed by training a system on product/movie review data which is significantly different from the average tweet. They are going to be optimised for text with several sentences, all about the same topic. I suspect you would do better coming up with a rule-based system yourself, perhaps based on a lexicon of sentiment terms like the one the University of Pittsburgh provide.\nCheck out We Feel Fine for an implementation of similar idea with a really beautiful interface (and twitrratr).\n"}, "1644": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "Finn \u00c5rup NielsenFinn \u00c5rup Nielsen", "text": "\nTake a look at Twitter sentiment analysis tool. It's written in python, and it uses Naive Bayes classifier with semi-supervised machine learning. The source can be found here.\n"}, "1645": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "        Ben CoeBen Coe", "text": "\nMaybe TextBlob (based on NLTK and pattern) is the right sentiment analysis tool for you.\n"}, "1646": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "", "text": "\nI came across Natural Language Toolkit a while ago. You could probably use it as a starting point. It also has a lot of modules and addons, so maybe they already have something similar.\n"}, "1647": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "StompchickenStompchicken", "text": "\nSomewhat wacky thought: you could try using the Twitter API to download a large set of tweets, and then classifying a subset of that set using emoticons: one positive group for  \":)\", \":]\", \":D\", etc, and another negative group with \":(\", etc.\nOnce you have that crude classification, you could search for more clues with frequency or ngram analysis or something along those lines.\nIt may seem silly, but serious research has been done on this (search for \"sentiment analysis\" and emoticon). Worth a look.  \n"}, "1648": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "Garrett Hyde", "text": "\nThere's a Twitter Sentiment API by TweetFeel that does advanced linguistic analysis of tweets, and can retrieve positive/negative tweets. See http://www.webservius.com/corp/docs/tweetfeel_sentiment.htm\n"}, "1649": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "cyhexcyhex", "text": "\nFor those interested in coding Twitter Sentiment Analyis from scratch, there is a Coursera course \"Data Science\" with python code on GitHub (as part of assignment 1 - link). The sentiments are part of the AFINN-111.\nYou can find working solutions, for example here. In addition to the AFINN-111 sentiment list, there is a simple implementation of builing a dynamic term list based on frequency of terms in tweets that have a pos/neg score (see here).\n"}, "1650": {"topic": "Sentiment Analysis of Entity (Entity-level Sentiment Analysis)", "user_name": "Mona Jalal", "text": "\nI've been working on document level sentiment analysis since past 1 year. Document level sentiment analysis provides the sentiment of the complete document. For example - The text \"Nokia is good but vodafone sucks big time\" would have a negative polarity associated with it as it would be agnostic to the entities Nokia and Vodafone. How would it be possible to get entity level sentiment, like positive for Nokia but negative for Vodafone ? Are there any research papers providing a solution to such problems ?\n"}, "1651": {"topic": "Sentiment Analysis of Entity (Entity-level Sentiment Analysis)", "user_name": "JasneetJasneet", "text": "\nYou can try Aspect-level or Entity-level Sentiment Analysis. There are good efforts have been already done to find the opinions about the aspects in a sentence. You can find some of works here. You can also go further and deeper and review those papers that are related to feature (aspect) extraction. What does it mean? Let me give you an example:\n\"The quality of screen is great, however, the battery life is short.\" \nDocument-level sentiment analysis may not give us the real sense of this document here because we have one positive and one negative sentence in the document. However, by aspect-based (aspect-level) opinion mining, we can figure out the senses/polarities towards different entities in the document separately. By doing feature extraction, in the first step, you try to find the features (aspects) in different sentences (in here \"quality of screen\" or simply \"quality\" and \"battery life\"). Afterwards, when you have these aspects, you try to extract opinions related to these aspects (\"great\" for \"quality\" and \"short\" for \"battery life\"). In researches and academic papers, we also name features (aspects) as target words (those words or entities on which users comment), and the opinions as opinion words, the comments that have been stated about the target words.\nBy searching the keywords that I have just mentioned, you can become more familiar with these concepts.\n"}, "1652": {"topic": "Sentiment Analysis of Entity (Entity-level Sentiment Analysis)", "user_name": "", "text": "\nYou could look for entities and their coreferents, and have a simple heuristic like giving each entity sentiment from the closest sentiment term, perhaps closest by distance in a dependency parse tree, as opposed to linearly.  Each of those steps seems to be an open research topic.\nhttp://scholar.google.com/scholar?q=entity+identification\nhttp://scholar.google.com/scholar?q=coreference+resolution\nhttp://scholar.google.com/scholar?q=sentiment+phrase\nhttp://scholar.google.com/scholar?q=dependency+parsing\n"}, "1653": {"topic": "Sentiment Analysis of Entity (Entity-level Sentiment Analysis)", "user_name": "PedramPedram", "text": "\nThis can be achieved using Google Cloud Natural Language API.\n\n"}, "1654": {"topic": "Sentiment Analysis of Entity (Entity-level Sentiment Analysis)", "user_name": "Gregory MartonGregory Marton", "text": "\nI also tried getting research articles on this but haven't found any. I would suggest you to try using the aspect based sentiment analysis algorithms. The similarity i found is there we recognize aspects of a single entity in a sentence and then find the sentiment of each aspect.Similarly we can train our model using the same algorithm which can detect the entities as it does for aspects and find the sentiment of such entities. I didn't try this but I am going to.Let me know if this worked or not.Also there are various ways to do this. The following are the links for few articles.\nhttp://arxiv.org/pdf/1605.08900v1.pdf\nhttps://cs224d.stanford.edu/reports/MarxElliot.pdf\n"}, "1655": {"topic": "Unsupervised Sentiment Analysis", "user_name": "Holger Just", "text": "\nI've been reading a lot of articles that explain the need for an initial set of texts that are classified as either 'positive' or 'negative' before a sentiment analysis system will really work.\nMy question is: Has anyone attempted just doing a rudimentary check of 'positive' adjectives vs 'negative' adjectives, taking into account any simple negators to avoid classing 'not happy' as positive? If so, are there any articles that discuss just why this strategy isn't realistic?\n"}, "1656": {"topic": "Unsupervised Sentiment Analysis", "user_name": "TrindazTrindaz", "text": "\nA classic paper by Peter Turney (2002) explains a method to do unsupervised sentiment analysis (positive/negative classification) using only the words excellent and poor as a seed set. Turney uses the mutual information of other words with these two adjectives to achieve an accuracy of 74%.\n"}, "1657": {"topic": "Unsupervised Sentiment Analysis", "user_name": "ruben_pants", "text": "\nI haven't tried doing untrained sentiment analysis such as you are describing, but off the top of my head I'd say you're oversimplifying the problem.  Simply analyzing adjectives is not enough to get a good grasp of the sentiment of a text; for example, consider the word 'stupid.'  Alone, you would classify that as negative, but if a product review were to have '... [x] product makes their competitors look stupid for not thinking of this feature first...' then the sentiment in there would definitely be positive. The greater context in which words appear definitely matters in something like this.  This is why an untrained bag-of-words approach alone (let alone an even more limited bag-of-adjectives) is not enough to tackle this problem adequately.\nThe pre-classified data ('training data') helps in that the problem shifts from trying to determine whether a text is of positive or negative sentiment from scratch, to trying to determine if the text is more similar to positive texts or negative texts, and classify it that way.  The other big point is that textual analyses such as sentiment analysis are often affected greatly by the differences of the characteristics of texts depending on domain.  This is why having a good set of data to train on (that is, accurate data from within the domain in which you are working, and is hopefully representative of the texts you are going to have to classify) is as important as building a good system to classify with.\nNot exactly an article, but hope that helps.\n"}, "1658": {"topic": "Unsupervised Sentiment Analysis", "user_name": "Fred FooFred Foo", "text": "\nThe paper of Turney (2002) mentioned by larsmans is a good basic one. In a newer research, Li and He [2009] introduce an approach using Latent Dirichlet Allocation (LDA) to train a model that can classify an article's overall sentiment and topic simultaneously in a totally unsupervised manner. The accuracy they achieve is 84.6%.\n"}, "1659": {"topic": "Unsupervised Sentiment Analysis", "user_name": "waffle paradoxwaffle paradox", "text": "\nI tried several methods of Sentiment Analysis for opinion mining in Reviews. \nWhat worked the best for me is the method described in Liu book: http://www.cs.uic.edu/~liub/WebMiningBook.html In this Book Liu and others, compared many strategies and discussed different papers on Sentiment Analysis and Opinion Mining.\nAlthough my main goal was to extract features in the opinions, I implemented a sentiment classifier to detect positive and negative classification of this features. \nI used NLTK for the pre-processing (Word tokenization, POS tagging) and the trigrams creation. Then also I used the Bayesian Classifiers inside this tookit to compare with other strategies Liu was pinpointing. \nOne of the methods relies on tagging as pos/neg every trigrram expressing this information, and using some classifier on this data. \nOther method I tried, and worked better (around 85% accuracy in my dataset), was calculating  the sum of scores of PMI (punctual mutual information) for every word in the sentence and the words excellent/poor as seeds of pos/neg class. \n"}, "1660": {"topic": "Unsupervised Sentiment Analysis", "user_name": "        user325117\r", "text": "\nI tried spotting keywords using a dictionary of affect to predict the sentiment label at sentence level. Given the generality of the vocabulary (non domain dependent), the results were just about 61%. The paper is available in my homepage.\nIn a somewhat improved version, negation adverbs were considered. The whole system, named EmoLib, is available for demo:\nhttp://dtminredis.housing.salle.url.edu:8080/EmoLib/\nRegards,\n"}, "1661": {"topic": "Unsupervised Sentiment Analysis", "user_name": "Trung HuynhTrung Huynh", "text": "\nDavid, \nI'm not sure if this helps but you may want to look into Jacob Perkin's blog post on using NLTK for sentiment analysis.\n"}, "1662": {"topic": "Unsupervised Sentiment Analysis", "user_name": "LuchuxLuchux", "text": "\nThere are no magic \"shortcuts\" in sentiment analysis, as with any other sort of text analysis that seeks to discover the underlying \"aboutness,\" of a chunk of text. Attempting to short cut proven text analysis methods through simplistic \"adjective\" checking or similar approaches leads to ambiguity, incorrect classification, etc., that at the end of the day give you a poor accuracy read on sentiment. The more terse the source (e.g. Twitter), the more difficult the problem.\n"}, "1663": {"topic": "Training data for sentiment analysis [closed]", "user_name": "Iterator", "text": "\n\n\n\n\n\n\nClosed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.                    \n\n\n\n\n\n\n\n\n\n\n We don\u2019t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\n\n\nClosed 7 years ago.\n\n\n\n\n\n\n\r\n                        Improve this question\r\n                    \n\n\n\nWhere can I get a corpus of documents that have already been classified as positive/negative for sentiment in the corporate domain? I want a large corpus of documents that provide reviews for companies, like reviews of companies provided by analysts and media.\nI find corpora that have reviews of products and movies. Is there a corpus for the business domain including reviews of companies, that match the language of business?\n"}, "1664": {"topic": "Training data for sentiment analysis [closed]", "user_name": "London guyLondon guy", "text": "\nhttp://www.cs.cornell.edu/home/llee/data/\nhttp://mpqa.cs.pitt.edu/corpora/mpqa_corpus\nYou can use twitter, with its smileys, like this: http://web.archive.org/web/20111119181304/http://deepthoughtinc.com/wp-content/uploads/2011/01/Twitter-as-a-Corpus-for-Sentiment-Analysis-and-Opinion-Mining.pdf\nHope that gets you started.  There's more in the literature, if you're interested in specific subtasks like negation, sentiment scope, etc.\nTo get a focus on companies, you might pair a method with topic detection, or cheaply just a lot of mentions of a given company.  Or you could get your data annotated by Mechanical Turkers.\n"}, "1665": {"topic": "Training data for sentiment analysis [closed]", "user_name": "        user325117\r", "text": "\nThis is a list I wrote a few weeks ago, from my blog. Some of these datasets have been recently included in the NLTK Python platform.\nLexicons\n\nOpinion Lexicon by Bing Liu\n\nURL: http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon\nPAPERS: Mining and summarizing customer reviews\nNOTES: Included in the NLTK Python platform\n\n\nMPQA Subjectivity Lexicon\n\nURL: http://mpqa.cs.pitt.edu/#subj_lexicon\nPAPERS: Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis (Theresa Wilson, Janyce Wiebe, and Paul Hoffmann, 2005).\n\n\nSentiWordNet\n\nURL: http://sentiwordnet.isti.cnr.it\nNOTES: Included in the NLTK Python platform\n\n\nHarvard General Inquirer\n\nURL: http://www.wjh.harvard.edu/~inquirer\nPAPERS: The General Inquirer: A Computer Approach to Content Analysis (Stone, Philip J; Dexter C. Dunphry; Marshall S. Smith; and Daniel M. Ogilvie. 1966)\n\n\nLinguistic Inquiry and Word Counts (LIWC)\n\nURL: http://www.liwc.net\n\n\nVader Lexicon\n\nURLs: https://github.com/cjhutto/vaderSentiment, http://comp.social.gatech.edu/papers\nPAPERS: Vader: A parsimonious rule-based model for sentiment analysis of social media text (Hutto, Gilbert.  2014)\n\n\n\n\nDatasets\n\nMPQA Datasets\n\nURL: http://mpqa.cs.pitt.edu\n\nNOTES: GNU Public License.\n\nPolitical Debate data\nProduct Debate data\nSubjectivity Sense Annotations\n\n\n\n\nSentiment140 (Tweets)\n\nURL: http://help.sentiment140.com/for-students\nPAPERS: Twitter Sent classification using Distant Supervision (Go, Alec, Richa Bhayani, and Lei Huang)\nURLs: http://help.sentiment140.com, https://groups.google.com/forum/#!forum/sentiment140\n\n\nSTS-Gold (Tweets)\n\nURL: http://www.tweenator.com/index.php?page_id=13\nPAPERS: Evaluation datasets for twitter sentiment analysis (Saif, Fernandez, He, Alani)\nNOTES: As Sentiment140, but the dataset is smaller and with human annotators. It comes with 3 files: tweets, entities (with their sentiment) and an aggregate set.\n\n\nCustomer Review Dataset (Product reviews)\n\nURL: http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets\n\nPAPERS: Mining and summarizing customer reviews\n\nNOTES: Title of review, product feature, positive/negative label with opinion strength, other info (comparisons, pronoun resolution, etc.)\nIncluded in the NLTK Python platform\n\n\n\nPros and Cons Dataset (Pros and cons sentences)\n\nURL: http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets\n\nPAPERS: Mining Opinions in Comparative Sentences (Ganapathibhotla, Liu 2008)\n\nNOTES: A list of sentences tagged <pros> or <cons>\nIncluded in the NLTK Python platform\n\n\n\nComparative Sentences (Reviews)\n\nURL: http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets\n\nPAPERS: Identifying Comparative Sentences in Text Documents (Nitin Jindal and Bing Liu), Mining Opinion Features in Customer Reviews (Minqing Hu and Bing Liu)\n\nNOTES: Sentence, POS-tagged sentence, entities, comparison type (non-equal, equative, superlative, non-gradable)\nIncluded in the NLTK Python platform\n\n\n\nSanders Analytics Twitter Sentiment Corpus (Tweets)\n\nURL: http://www.sananalytics.com/lab/twitter-sentiment\n\n\n5513 hand-classified tweets wrt 4 different topics. Because of Twitter\u2019s ToS, a small Python script is included to download all of the tweets. The sentiment classifications themselves are provided free of charge and without restrictions. They may be used for commercial products. They may be redistributed. They may be modified.\n\n\nSpanish tweets (Tweets)\n\nURL: http://www.daedalus.es/TASS2013/corpus.php\n\n\nSemEval 2014 (Tweets)\n\nURL: http://alt.qcri.org/semeval2014/task9\n\n\nYou MUST NOT re-distribute the tweets, the annotations or the corpus obtained (from the readme file)\n\n\nVarious Datasets (Reviews)\n\nURL: https://personalwebs.coloradocollege.edu/~mwhitehead/html/opinion_mining.html\nPAPERS: Building a General Purpose Cross-Domain Sentiment Mining Model (Whitehead and Yaeger), Sentiment Mining Using Ensemble Classification Models (Whitehead and Yaeger)\n\n\nVarious Datasets #2 (Reviews)\n\nURL: http://www.text-analytics101.com/2011/07/user-review-datasets_20.html\n\n\n\n\nReferences:\n\nKeenformatics - Sentiment Analysis lexicons and datasets (my blog)\nPersonal experience\n\n"}, "1666": {"topic": "Training data for sentiment analysis [closed]", "user_name": "Gregory MartonGregory Marton", "text": "\nHere are a few more;\nhttp://inclass.kaggle.com/c/si650winter11\nhttp://alias-i.com/lingpipe/demos/tutorial/sentiment/read-me.html\n"}, "1667": {"topic": "Training data for sentiment analysis [closed]", "user_name": "", "text": "\nIf you have some resources (media channels, blogs, etc) about the domain you want to explore, you can create your own corpus. \nI do this in python: \n\nusing Beautiful Soup http://www.crummy.com/software/BeautifulSoup/ for parsing the content that I want to classify. \nseparate those sentences meaning positive/negative opinions about companies. \nUse NLTK to process this sentences, tokenize words, POS tagging, etc. \nUse NLTK PMI to calculate bigrams or trigrams mos frequent in only one class\n\nCreating corpus is a hard work of pre-processing, checking, tagging, etc, but has the benefits of preparing a model for a specific domain many times increasing the accuracy. If you can get already prepared corpus, just go ahead with the sentiment analysis ;) \n"}, "1668": {"topic": "Training data for sentiment analysis [closed]", "user_name": "Kurt BourbakiKurt Bourbaki", "text": "\nI'm not aware of any such corpus being freely available, but you could try an unsupervised method on an unlabeled dataset.\n"}, "1669": {"topic": "Training data for sentiment analysis [closed]", "user_name": "y2py2p", "text": "\nYou can get a large select of online reviews from Datafiniti.  Most of the reviews come with rating data, which would provide more granularity on sentiment than positive / negative.  Here's a list of businesses with reviews, and here's a list of products with reviews.\n"}, "1670": {"topic": "Sentiment analysis", "user_name": "pekkypekky", "text": "\nwhile performing sentiment analysis, how can I make the machine understand that I'm referring apple (the iphone), instead of apple (the fruit)? \nThanks for the advise ! \n"}, "1671": {"topic": "Sentiment analysis", "user_name": "CommunityBot", "text": "\nWell, there are several methods,\nI would start with checking Capital letter, usually, when referring to a name, first letter is capitalized.\nBefore doing sentiment analysis, I would use some Part-of-speech and Named Entity Recognition to tag the relevant words.\nStanford CoreNLP is a good text analysis project to start with, it will teach\nyou the basic concepts.\nExample from CoreNLP:\n\nYou can see how the tags can help you.\nAnd check out more info\n"}, "1672": {"topic": "Sentiment analysis", "user_name": "OfirisOfiris", "text": "\nAs described by Ofiris, NER is only one way to do solve your problem. I feel it's more effective to use word embedding to represent your words. In that way machine automatically recognize the context of the word. As an example \"Apple\" is mostly coming together with \"eat\" and But if the given input \"Apple\" is present with \"mobile\" or any other word in that domain, Machine will understand it's \"iPhone apple\" instead of \"apple fruit\". There are 2 popular ways to generate word embeddings such as word2vec and fasttext. \nGensim provides more reliable implementations for both word2vec and fasttext. \nhttps://radimrehurek.com/gensim/models/word2vec.html\nhttps://radimrehurek.com/gensim/models/fasttext.html\n"}, "1673": {"topic": "Sentiment analysis", "user_name": "LahiruLahiru", "text": "\nIn presence of dates, famous brands, vip or historical figures you can use a NER (named entity recognition) algorithm; in such case, as suggested by Ofiris, the Stanford CoreNLP offers a good Named entity recognizer.\nFor a more general disambiguation of polysemous words (i.e., words having more than one sense, such as \"good\") you could use a POS tagger coupled with a Word Sense Disambiguation (WSD) algorithm. An example of the latter can be found HERE, but I do not know any freely downloadable library for this purpose.\n"}, "1674": {"topic": "Sentiment analysis", "user_name": "        user5135328user5135328", "text": "\nThis problem has already been solved by many open source pre-trained NER models. Anyways you can try retraining an existing NER models to finetune them to solve this issue. \nYou can find an demo of NER results as done by Spacy NER here.\n\n"}, "1675": {"topic": "How can I display full (non-truncated) dataframe information in HTML when converting from Pandas dataframe to HTML?", "user_name": "Peter Mortensen", "text": "\nI converted a Pandas dataframe to an HTML output using the DataFrame.to_html function. When I save this to a separate HTML file, the file shows truncated output.\nFor example, in my TEXT column,\ndf.head(1) will show\nThe film was an excellent effort...\ninstead of\nThe film was an excellent effort in deconstructing the complex social sentiments that prevailed during this period.\nThis rendition is fine in the case of a screen-friendly format of a massive Pandas dataframe, but I need an HTML file that will show complete tabular data contained in the dataframe, that is, something that will show the latter text element rather than the former text snippet.\nHow would I be able to show the complete, non-truncated text data for each element in my TEXT column in the HTML version of the information? I would imagine that the HTML table would have to display long cells to show the complete data, but as far as I understand, only column-width parameters can be passed into the DataFrame.to_html function.\n"}, "1676": {"topic": "How can I display full (non-truncated) dataframe information in HTML when converting from Pandas dataframe to HTML?", "user_name": "AmyAmy", "text": "\nSet the display.max_colwidth option to None (or -1 before version 1.0):\npd.set_option('display.max_colwidth', None)\n\nset_option documentation\nFor example, in IPython, we see that the information is truncated to 50 characters. Anything in excess is ellipsized:\n\nIf you set the display.max_colwidth option, the information will be displayed fully:\n\n"}, "1677": {"topic": "How can I display full (non-truncated) dataframe information in HTML when converting from Pandas dataframe to HTML?", "user_name": "Peter Mortensen", "text": "\npd.set_option('display.max_columns', None)  \n\nid (second argument) can fully show the columns. \n"}, "1678": {"topic": "How can I display full (non-truncated) dataframe information in HTML when converting from Pandas dataframe to HTML?", "user_name": "behzad.nouribehzad.nouri", "text": "\nWhile pd.set_option('display.max_columns', None) sets the number of the maximum columns shown, the option pd.set_option('display.max_colwidth', -1) sets the maximum width of each single field.\nFor my purposes I wrote a small helper function to fully print huge data frames without affecting the rest of the code. It also reformats float numbers and sets the virtual display width. You may adopt it for your use cases.\ndef print_full(x):\n    pd.set_option('display.max_rows', None)\n    pd.set_option('display.max_columns', None)\n    pd.set_option('display.width', 2000)\n    pd.set_option('display.float_format', '{:20,.2f}'.format)\n    pd.set_option('display.max_colwidth', None)\n    print(x)\n    pd.reset_option('display.max_rows')\n    pd.reset_option('display.max_columns')\n    pd.reset_option('display.width')\n    pd.reset_option('display.float_format')\n    pd.reset_option('display.max_colwidth')\n\n"}, "1679": {"topic": "How can I display full (non-truncated) dataframe information in HTML when converting from Pandas dataframe to HTML?", "user_name": "rafaelvalle", "text": "\nJupyter Users\nWhenever I need this for just one cell, I use this:\nwith pd.option_context('display.max_colwidth', None):\n  display(df)\n\n"}, "1680": {"topic": "How can I display full (non-truncated) dataframe information in HTML when converting from Pandas dataframe to HTML?", "user_name": "user7579768user7579768", "text": "\nTry this too:\npd.set_option(\"max_columns\", None) # show all cols\npd.set_option('max_colwidth', None) # show full width of showing cols\npd.set_option(\"expand_frame_repr\", False) # print cols side by side as it's supposed to be\n\n"}, "1681": {"topic": "How can I display full (non-truncated) dataframe information in HTML when converting from Pandas dataframe to HTML?", "user_name": "Peter Mortensen", "text": "\nThe following code results in the error below:\npd.set_option('display.max_colwidth', -1)\n\nFutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\nInstead, use:\npd.set_option('display.max_colwidth', None)\n\nThis accomplishes the task and complies with versions of Pandas following version 1.0.\n"}, "1682": {"topic": "How can I display full (non-truncated) dataframe information in HTML when converting from Pandas dataframe to HTML?", "user_name": "Karl AdlerKarl Adler", "text": "\nAnother way of viewing the full content of the cells in a Pandas dataframe is to use IPython's display functions:\nfrom IPython.display import HTML\n\nHTML(df.to_html())\n\n"}, "1683": {"topic": "How can I display full (non-truncated) dataframe information in HTML when converting from Pandas dataframe to HTML?", "user_name": "", "text": "\nDisplay the full dataframe for a specific cell:\nimport pandas as pd\nwith pd.option_context('display.max_colwidth', None,\n                       'display.max_columns', None,\n                       'display.max_rows', None):\n    display(df)\n\nThe method above can be extended with more options.\nUpdated helper function from Karl Adler:\ndef display_full(x):\n    with pd.option_context('display.max_rows', None,\n                           'display.max_columns', None,\n                           'display.width', 2000,\n                           'display.float_format', '{:20,.2f}'.format,\n                           'display.max_colwidth', None):\n        display(x)\n\nChange display options for all cells:\npd.set_option('display.max_colwidth', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\ndisplay(df)\n\n"}, "1684": {"topic": "How can I display full (non-truncated) dataframe information in HTML when converting from Pandas dataframe to HTML?", "user_name": "iamyojimboiamyojimbo", "text": "\nFor those looking to do this in Dask:\nI could not find a similar option in Dask, but if I simply do this in same notebook for Pandas it works for Dask too.\nimport pandas as pd\nimport dask.dataframe as dd\npd.set_option('display.max_colwidth', -1) # This will set the no truncate for Pandas as well as for Dask. I am not sure how it does for Dask though, but it works.\n\ntrain_data = dd.read_csv('./data/train.csv')\ntrain_data.head(5)\n\n"}, "1685": {"topic": "How can I display full (non-truncated) dataframe information in HTML when converting from Pandas dataframe to HTML?", "user_name": "Peter Mortensen", "text": "\nFor those who like to reduce typing (i.e., everyone!): pd.set_option('max_colwidth', None) does the same thing\n"}, "1686": {"topic": "How can I display full (non-truncated) dataframe information in HTML when converting from Pandas dataframe to HTML?", "user_name": "bitbangbitbang", "text": "\nI would like to offer other methods. If you don't want to always set it as default.\n# First method\nlist(df.itertuples()) # This would force pandas to explicitly display your dataframe, however it's not that beautiful\n\n# Second method\nimport tabulate\nprint(tabulate(df, tablefmt='psql', headers='keys')) \n# `headers` are your columns, `keys` are the current columns\n# `psql` is one type of format for tabulate to organize before, you could pick other format you like in the documentation\n\n"}, "1687": {"topic": "What's so bad about Template Haskell?", "user_name": "CommunityBot", "text": "\nIt seems that Template Haskell is often viewed by the Haskell community as an unfortunate convenience. It's hard to put into words exactly what I have observed in this regard, but consider these few examples\n\nTemplate Haskell listed under \"The Ugly (but necessary)\" in response to the question Which Haskell (GHC) extensions should users use/avoid?\nTemplate Haskell considered a temporary/inferior solution in Unboxed Vectors of newtype'd values thread (libraries mailing list)\nYesod is often criticized for relying too much on Template Haskell (see the blog post in response to this sentiment)\n\nI've seen various blog posts where people do pretty neat stuff with Template Haskell, enabling prettier syntax that simply wouldn't be possible in regular Haskell, as well as tremendous boilerplate reduction. So why is it that Template Haskell is looked down upon in this way? What makes it undesirable? Under what circumstances should Template Haskell be avoided, and why?\n"}, "1688": {"topic": "What's so bad about Template Haskell?", "user_name": "Dan BurtonDan Burton", "text": "\nOne reason for avoiding Template Haskell is that it as a whole isn't type-safe, at all, thus going against much of \"the spirit of Haskell.\" Here are some examples of this:\n\nYou have no control over what kind of Haskell AST a piece of TH code will generate, beyond where it will appear; you can have a value of type Exp, but you don't know if it is an expression that represents a [Char] or a (a -> (forall b . b -> c)) or whatever. TH would be more reliable if one could express that a function may only generate expressions of a certain type, or only function declarations, or only data-constructor-matching patterns, etc.\nYou can generate expressions that don't compile. You generated an expression that references a free variable foo that doesn't exist? Tough luck, you'll only see that when actually using your code generator, and only under the circumstances that trigger the generation of that particular code. It is very difficult to unit test, too.\n\nTH is also outright dangerous:\n\nCode that runs at compile-time can do arbitrary IO, including launching missiles or stealing your credit card. You don't want to have to look through every cabal package you ever download in search for TH exploits.\nTH can access \"module-private\" functions and definitions, completely breaking encapsulation in some cases.\n\nThen there are some problems that make TH functions less fun to use as a library developer:\n\nTH code isn't always composable. Let's say someone makes a generator for lenses, and more often than not, that generator will be structured in such a way that it can only be called directly by the \"end-user,\" and not by other TH code, by for example taking a list of type constructors to generate lenses for as the parameter. It is tricky to generate that list in code, while the user only has to write generateLenses [''Foo, ''Bar].\nDevelopers don't even know that TH code can be composed. Did you know that you can write forM_ [''Foo, ''Bar] generateLens? Q is just a monad, so you can use all of the usual functions on it. Some people don't know this, and because of that, they create multiple overloaded versions of essentially the same functions with the same functionality, and these functions lead to a certain bloat effect. Also, most people write their generators in the Q monad even when they don't have to, which is like writing bla :: IO Int; bla = return 3; you are giving a function more \"environment\" than it needs, and clients of the function are required to provide that environment as an effect of that.\n\nFinally, there are some things that make TH functions less fun to use as an end-user:\n\nOpacity. When a TH function has type Q Dec, it can generate absolutely anything at the top-level of a module, and you have absolutely no control over what will be generated.\nMonolithism. You can't control how much a TH function generates unless the developer allows it; if you find a function that generates a database interface and a JSON serialization interface, you can't say \"No, I only want the database interface, thanks; I'll roll my own JSON interface\"\nRun time. TH code takes a relatively long time to run. The code is interpreted anew every time a file is compiled, and often, a ton of packages are required by the running TH code, that have to be loaded. This slows down compile time considerably.\n\n"}, "1689": {"topic": "What's so bad about Template Haskell?", "user_name": "Dave", "text": "\nThis is solely my own opinion.\n\nIt's ugly to use. $(fooBar ''Asdf) just does not look nice. Superficial, sure, but it contributes.\n\nIt's even uglier to write. Quoting works sometimes, but a lot of the time you have to do manual AST grafting and plumbing. The API is big and unwieldy, there's always a lot of cases you don't care about but still need to dispatch, and the cases you do care about tend to be present in multiple similar but not identical forms (data vs. newtype, record-style vs. normal constructors, and so on). It's boring and repetitive to write and complicated enough to not be mechanical. The reform proposal addresses some of this (making quotes more widely applicable).\n\nThe stage restriction is hell. Not being able to splice functions defined in the same module is the smaller part of it: the other consequence is that if you have a top-level splice, everything after it in the module will be out of scope to anything before it. Other languages with this property (C, C++) make it workable by allowing you to forward declare things, but Haskell doesn't. If you need cyclic references between spliced declarations or their dependencies and dependents, you're usually just screwed.\n\nIt's undisciplined. What I mean by this is that most of the time when you express an abstraction, there is some kind of principle or concept behind that abstraction. For many abstractions, the principle behind them can be expressed in their types. For type classes, you can often formulate laws which instances should obey and clients can assume. If you use GHC's new generics feature to abstract the form of an instance declaration over any datatype (within bounds), you get to say \"for sum types, it works like this, for product types, it works like that\". Template Haskell, on the other hand, is just macros. It's not abstraction at the level of ideas, but abstraction at the level of ASTs, which is better, but only modestly, than abstraction at the level of plain text.*\n\nIt ties you to GHC. In theory another compiler could implement it, but in practice I doubt this will ever happen. (This is in contrast to various type system extensions which, though they might only be implemented by GHC at the moment, I could easily imagine being adopted by other compilers down the road and eventually standardized.)\n\nThe API isn't stable. When new language features are added to GHC and the template-haskell package is updated to support them, this often involves backwards-incompatible changes to the TH datatypes. If you want your TH code to be compatible with more than just one version of GHC you need to be very careful and possibly use CPP.\n\nThere's a general principle that you should use the right tool for the job and the smallest one that will suffice, and in that analogy Template Haskell is something like this. If there's a way to do it that's not Template Haskell, it's generally preferable.\n\n\nThe advantage of Template Haskell is that you can do things with it that you couldn't do any other way, and it's a big one. Most of the time the things TH is used for could otherwise only be done if they were implemented directly as compiler features. TH is extremely beneficial to have both because it lets you do these things, and because it lets you prototype potential compiler extensions in a much more lightweight and reusable way (see the various lens packages, for example).\nTo summarize why I think there are negative feelings towards Template Haskell: It solves a lot of problems, but for any given problem that it solves, it feels like there should be a better, more elegant, disciplined solution better suited to solving that problem, one which doesn't solve the problem by automatically generating the boilerplate, but by removing the need to have the boilerplate.\n* Though I often feel that CPP has a better power-to-weight ratio for those problems that it can solve.\nEDIT 23-04-14: What I was frequently trying to get at in the above, and have only recently gotten at exactly, is that there's an important distinction between abstraction and deduplication. Proper abstraction often results in deduplication as a side effect, and duplication is often a telltale sign of inadequate abstraction, but that's not why it's valuable. Proper abstraction is what makes code correct, comprehensible, and maintainable. Deduplication only makes it shorter. Template Haskell, like macros in general, is a tool for deduplication.\n"}, "1690": {"topic": "What's so bad about Template Haskell?", "user_name": "dflemstrdflemstr", "text": "\nI'd like to address a few of the points dflemstr brings up.\nI don't find the fact that you can't typecheck TH to be that worrying. Why? Because even if there is an error, it will still be compile time. I'm not sure if this strengthens my argument, but this is similar in spirit to the errors that you receive when using templates in C++. I think these errors are more understandable than C++'s errors though, as you'll get a pretty printed version of the generated code.\nIf a TH expression / quasi-quoter does something that's so advanced that tricky corners can hide, then perhaps it's ill-advised?\nI break this rule quite a bit with quasi-quoters I've been working on lately (using haskell-src-exts / meta) - https://github.com/mgsloan/quasi-extras/tree/master/examples . I know this introduces some bugs such as not being able to splice in the generalized list comprehensions. However, I think that there's a good chance that some of the ideas in http://hackage.haskell.org/trac/ghc/blog/Template%20Haskell%20Proposal will end up in the compiler. Until then, the libraries for parsing Haskell to TH trees are a nearly perfect approximation.\nRegarding compilation speed / dependencies, we can use the \"zeroth\" package to inline the generated code. This is at least nice for the users of a given library, but we can't do much better for the case of editing the library. Can TH dependencies bloat generated binaries? I thought it left out everything that's not referenced by the compiled code.\nThe staging restriction / splitting of compilation steps of the Haskell module does suck.\nRE Opacity: This is the same for any library function you call. You have no control over what Data.List.groupBy will do. You just have a reasonable \"guarantee\" / convention that the version numbers tell you something about the compatibility. It is somewhat of a different matter of change when.\nThis is where using zeroth pays off - you're already versioning the generated files - so you'll always know when the form of the generated code has changed. Looking at the diffs might be a bit gnarly, though, for large amounts of generated code, so that's one place where a better developer interface would be handy.\nRE Monolithism: You can certainly post-process the results of a TH expression, using your own compile-time code. It wouldn't be very much code to filter on top-level declaration type / name. Heck, you could imagine writing a function that does this generically. For modifying / de-monolithisizing quasiquoters, you can pattern match on \"QuasiQuoter\" and extract out the transformations used, or make a new one in terms of the old.\n"}, "1691": {"topic": "What's so bad about Template Haskell?", "user_name": "dfeuer", "text": "\nThis answer is in response to the issues brought up by illissius, point by point:\n\n\nIt's ugly to use. $(fooBar ''Asdf) just does not look nice. Superficial, sure, but it contributes.\n\n\nI agree.  I feel like $( ) was chosen to look like it was part of the language - using the familiar symbol pallet of Haskell.  However, that's exactly what you /don't/ want in the symbols used for your macro splicing.  They definitely blend in too much, and this cosmetic aspect is quite important.  I like the look of {{ }} for splices, because they are quite visually distinct.\n\n\nIt's even uglier to write. Quoting works sometimes, but a lot of the time you have to do manual AST grafting and plumbing. The [API][1] is big and unwieldy, there's always a lot of cases you don't care about but still need to dispatch, and the cases you do care about tend to be present in multiple similar but not identical forms (data vs. newtype, record-style vs. normal constructors, and so on). It's boring and repetitive to write and complicated enough to not be mechanical. The [reform proposal][2] addresses some of this (making quotes more widely applicable).\n\n\nI also agree with this, however, as some of the comments in \"New Directions for TH\" observe, the lack of good out-of-the-box AST quoting is not a critical flaw.  In this WIP package, I seek to address these problems in library form: https://github.com/mgsloan/quasi-extras .  So far I allow splicing in a few more places than usual and can pattern match on ASTs.\n\n\nThe stage restriction is hell. Not being able to splice functions defined in the same module is the smaller part of it: the other consequence is that if you have a top-level splice, everything after it in the module will be out of scope to anything before it. Other languages with this property (C, C++) make it workable by allowing you to forward declare things, but Haskell doesn't. If you need cyclic references between spliced declarations or their dependencies and dependents, you're usually just screwed.\n\n\nI've run into the issue of cyclic TH definitions being impossible before...  It's quite annoying.  There is a solution, but it's ugly - wrap the things involved in the cyclic dependency in a TH expression that combines all of the generated declarations.  One of these declarations generators could just be a quasi-quoter that accepts Haskell code.\n\n\nIt's unprincipled. What I mean by this is that most of the time when you express an abstraction, there is some kind of principle or concept behind that abstraction. For many abstractions, the principle behind them can be expressed in their types. When you define a type class, you can often formulate laws which instances should obey and clients can assume. If you use GHC's [new generics feature][3] to abstract the form of an instance declaration over any datatype (within bounds), you get to say \"for sum types, it works like this, for product types, it works like that\". But Template Haskell is just dumb macros. It's not abstraction at the level of ideas, but abstraction at the level of ASTs, which is better, but only modestly, than abstraction at the level of plain text.\n\n\nIt's only unprincipled if you do unprincipled things with it.  The only difference is that with the compiler implemented mechanisms for abstraction, you have more confidence that the abstraction isn't leaky.  Perhaps democratizing language design does sound a bit scary!  Creators of TH libraries need to document well and clearly define the meaning and results of the tools they provide.  A good example of principled TH is the derive package: http://hackage.haskell.org/package/derive - it uses a DSL such that the example of many of the derivations /specifies/ the actual derivation.\n\n\nIt ties you to GHC. In theory another compiler could implement it, but in practice I doubt this will ever happen. (This is in contrast to various type system extensions which, though they might only be implemented by GHC at the moment, I could easily imagine being adopted by other compilers down the road and eventually standardized.)\n\n\nThat's a pretty good point - the TH API is pretty big and clunky.  Re-implementing it seems like it could be tough.  However, there are only really only a few ways to slice the problem of representing Haskell ASTs.  I imagine that copying the TH ADTs, and writing a converter to the internal AST representation would get you a good deal of the way there.  This would be equivalent to the (not insignificant) effort of creating haskell-src-meta.  It could also be simply re-implemented by pretty printing the TH AST and using the compiler's internal parser.\nWhile I could be wrong, I don't see TH as being that complicated of a compiler extension, from an implementation perspective.  This is actually one of the benefits of \"keeping it simple\" and not having the fundamental layer be some theoretically appealing, statically verifiable templating system.\n\n\nThe API isn't stable. When new language features are added to GHC and the template-haskell package is updated to support them, this often involves backwards-incompatible changes to the TH datatypes. If you want your TH code to be compatible with more than just one version of GHC you need to be very careful and possibly use CPP.\n\n\nThis is also a good point, but somewhat dramaticized.  While there have been API additions lately, they haven't been extensively breakage inducing.  Also, I think that with the superior AST quoting I mentioned earlier, the API that actually needs to be used can be very substantially reduced.  If no construction / matching needs distinct functions, and are instead expressed as literals, then most of the API disappears.  Moreover, the code you write would port more easily to AST representations for languages similar to Haskell.\n\nIn summary, I think that TH is a powerful, semi-neglected tool.  Less hate could lead to a more lively eco-system of libraries, encouraging the implementation of more language feature prototypes.  It's been observed that TH is an overpowered tool, that can let you /do/ almost anything.  Anarchy!  Well, it's my opinion that this power can allow you to overcome most of its limitations, and construct systems capable of quite principled meta-programming approaches.  It's worth the usage of ugly hacks to simulate the \"proper\" implementation, as this way the design of the \"proper\" implementation will gradually become clear. \nIn my personal ideal version of nirvana, much of the language would actually move out of the compiler, into libraries of these variety.  The fact that the features are implemented as libraries does not heavily influence their ability to faithfully abstract.\nWhat's the typical Haskell answer to boilerplate code?  Abstraction.  What're our favorite abstractions?  Functions and typeclasses!\nTypeclasses let us define a set of methods, that can then be used in all manner of functions generic on that class.  However, other than this, the only way classes help avoid boilerplate is by offering \"default definitions\".  Now here is an example of an unprincipled feature!\n\nMinimal binding sets are not declarable / compiler checkable.  This could lead to inadvertent definitions that yield bottom due to mutual recursion.\nDespite the great convenience and power this would yield, you cannot specify superclass defaults, due to orphan instances  http://lukepalmer.wordpress.com/2009/01/25/a-world-without-orphans/  These would let us fix the numeric hierarchy gracefully!\nGoing after TH-like capabilities for method defaults led to http://www.haskell.org/haskellwiki/GHC.Generics .  While this is cool stuff, my only experience debugging code using these generics was nigh-impossible, due to the size of the type induced for and ADT as complicated as an AST. https://github.com/mgsloan/th-extra/commit/d7784d95d396eb3abdb409a24360beb03731c88c\nIn other words, this went after the features provided by TH, but it had to lift an entire domain of the language, the construction language, into a type system representation.  While I can see it working well for your common problem, for complex ones, it seems prone to yielding a pile of symbols far more terrifying than TH hackery.\nTH gives you value-level compile-time computation of the output code, whereas generics forces you to lift the pattern matching / recursion part of the code into the type system.  While this does restrict the user in a few fairly useful ways, I don't think the complexity is worth it.\n\nI think that the rejection of TH and lisp-like metaprogramming led to the preference towards things like method-defaults instead of more flexible, macro-expansion like declarations of instances.  The discipline of avoiding things that could lead to unforseen results is wise, however, we should not ignore that Haskell's capable type system allows for more reliable metaprogramming than in many other environments (by checking the generated code).\n"}, "1692": {"topic": "What's so bad about Template Haskell?", "user_name": "glaebhoerlglaebhoerl", "text": "\nOne rather pragmatic problem with Template Haskell is that it only works when GHC's bytecode interpreter is available, which is not the case on all architectures. So if your program uses Template Haskell or relies on libraries that use it, it will not run on machines with an ARM, MIPS, S390 or PowerPC CPU.\nThis is relevant in practice: git-annex is a tool written in Haskell that makes sense to run on machines worrying about storage, such machines often have non-i386-CPUs. Personally, I run git-annex on a NSLU 2 (32 MB of RAM, 266MHz CPU; did you know Haskell works fine on such hardware?) If it would use Template Haskell, this is not possible.\n(The situation about GHC on ARM is improving these days a lot and I think 7.4.2 even works, but the point still stands).\n"}, "1693": {"topic": "What's so bad about Template Haskell?", "user_name": "", "text": "\nWhy is TH bad? For me, it comes down to this:\n\nIf you need to produce so much repetitive code that you find yourself trying to use TH to auto-generate it, you're doing it wrong!\n\nThink about it. Half the appeal of Haskell is that its high-level design allows you to avoid huge amounts of useless boilerplate code that you have to write in other languages. If you need compile-time code generation, you're basically saying that either your language or your application design has failed you. And we programmers don't like to fail.\nSometimes, of course, it's necessary. But sometimes you can avoid needing TH by just being a bit more clever with your designs.\n(The other thing is that TH is quite low-level. There's no grand high-level design; a lot of GHC's internal implementation details are exposed. And that makes the API prone to change...)\n"}, "1694": {"topic": "How do you implement a good profanity filter?", "user_name": "S.S. Anne", "text": "\nMany of us need to deal with user input, search queries, and situations where the input text can potentially contain profanity or undesirable language. Oftentimes this needs to be filtered out.\nWhere can one find a good list of swear words in various languages and dialects? \nAre there APIs available to sources that contain good lists? Or maybe an API that simply says \"yes this is clean\" or \"no this is dirty\" with some parameters?\nWhat are some good methods for catching folks trying to trick the system, like a$$, azz, or a55?\nBonus points if you offer solutions for PHP. :)\nEdit: Response to answers that say simply avoid the programmatic issue:\nI think there is a place for this kind of filter when, for instance, a user can use public image search to find pictures that get added to a sensitive community pool. If they can search for \"penis\", then they will likely get many pictures of, yep. If we don't want pictures of that, then preventing the word as a search term is a good gatekeeper, though admittedly not a foolproof method. Getting the list of words in the first place is the real question.\nSo I'm really referring to a way to figure out of a single token is dirty or not and then simply disallow it. I'd not bother preventing a sentiment like the totally hilarious \"long necked giraffe\" reference. Nothing you can do there. :)\n"}, "1695": {"topic": "How do you implement a good profanity filter?", "user_name": "Ben ThroopBen Throop", "text": "\nObscenity Filters: Bad Idea, or Incredibly Intercoursing Bad Idea?\nAlso, one can't forget The Untold History of Toontown's SpeedChat, where even using a \"safe-word whitelist\" resulted in a 14-year-old quickly circumventing it with:\n\"I want to stick my long-necked Giraffe up your fluffy white bunny.\"\nBottom line: Ultimately, for any system that you implement, there is absolutely no substitute for human review (whether peer or otherwise). Feel free to implement a rudimentary tool to get rid of the drive-by's, but for the determined troll, you absolutely must have a non-algorithm-based approach.\nA system that removes anonymity and introduces accountability (something that Stack Overflow does well) is helpful also, particularly in order to help combat John Gabriel's G.I.F.T.\nYou also asked where you can get profanity lists to get you started -- one open-source project to check out is Dansguardian -- check out the source code for their default profanity lists. There is also an additional third party Phrase List that you can download for the proxy that may be a helpful gleaning point for you.\nEdit in response to the question edit: Thanks for the clarification on what you're trying to do. In that case, if you're just trying to do a simple word filter, there are two ways you can do it. One is to create a single long regexp with all of the banned phrases that you want to censor, and merely do a regex find/replace with it. A regex like:\n$filterRegex = \"(boogers|snot|poop|shucks|argh)\"\n\nand run it on your input string using preg_match() to wholesale test for a hit,\nor preg_replace() to blank them out.\nYou can also load those functions up with arrays rather than a single long regex, and for long word lists, it may be more manageable. See the preg_replace() for some good examples as to how arrays can be used flexibly.\nFor additional PHP programming examples, see this page for a somewhat advanced generic class for word filtering that *'s out the center letters from censored words, and this previous Stack Overflow question that also has a PHP example (the main valuable part in there is the SQL-based filtered word approach -- the leet-speak compensator can be dispensed with if you find it unnecessary).\nYou also added: \"Getting the list of words in the first place is the real question.\" -- in addition to some of the previous Dansgaurdian links, you may find this handy .zip of 458 words to be helpful.\n"}, "1696": {"topic": "How do you implement a good profanity filter?", "user_name": "", "text": "\nWhilst I know that this question is fairly old, but it's a commonly occurring question...\nThere is both a reason and a distinct need for profanity filters (see Wikipedia entry here), but they often fall short of being 100% accurate for very distinct reasons; Context and accuracy.\nIt depends (wholly) on what you're trying to achieve - at it's most basic, you're probably trying to cover the \"seven dirty words\" and then some... Some businesses need to filter the most basic of profanity: basic swear words, URLs or even personal information and so on, but others need to prevent illicit account naming (Xbox live is an example) or far more...\nUser generated content doesn't just contain potential swear words, it can also contain offensive references to:\n\nSexual acts \nSexual orientation\nReligion\nEthnicity \nEtc...\n\nAnd potentially, in multiple languages. Shutterstock has developed basic dirty-words lists in 10 languages to date, but it's still basic and very much oriented towards their 'tagging' needs. There are a number of other lists available on the web.\nI agree with the accepted answer that it's not a defined science and as language is a continually evolving challenge but one where a 90% catch rate is better than 0%. It depends purely on your goals - what you're trying to achieve, the level of support you have and how important it is to remove profanities of different types.\nIn building a filter, you need to consider the following elements and how they relate to your project:\n\nWords/phrases\nAcronyms (FOAD/LMFAO etc)\nFalse positives (words, places and names like 'mishit', 'scunthorpe' and 'titsworth')\nURLs (porn sites are an obvious target)\nPersonal information (email, address, phone etc - if applicable)\nLanguage choice (usually English by default)\nModeration (how, if at all, you can interact with user generated content and what you can do with it)\n\nYou can easily build a profanity filter that captures 90%+ of profanities, but you'll never hit 100%. It's just not possible. The closer you want to get to 100%, the harder it becomes... Having built a complex profanity engine in the past that dealt with more than 500K realtime messages per day, I'd offer the following advice:\nA basic filter would involve:\n\nBuilding a list of applicable profanities\nDeveloping a method of dealing with derivations of profanities\n\nA moderately complex filer would involve, (In addition to a basic filter):\n\nUsing complex pattern matching to deal with extended derivations (using advanced regex)\nDealing with Leetspeak (l33t)\nDealing with false positives\n\nA complex filter would involve a number of the following (In addition to a moderate filter):\n\nWhitelists and blacklists\nNaive bayesian inference filtering of phrases/terms\nSoundex functions (where a word sounds like another)\nLevenshtein distance\nStemming\nHuman moderators to help guide a filtering engine to learn by example or where matches aren't accurate enough without guidance (a self/continually-improving system)\nPerhaps some form of AI engine\n\n"}, "1697": {"topic": "How do you implement a good profanity filter?", "user_name": "\r", "text": "\nI don't know of any good libraries for this, but whatever you do, make sure that you err in the direction of letting stuff through.  I've dealt with systems that wouldn't allow me to use \"mpassell\" as a username, because it contains \"ass\" as a substring.  That's a great way to alienate users!\n"}, "1698": {"topic": "How do you implement a good profanity filter?", "user_name": "\r", "text": "\nDuring a job interview of mine, the company CTO who was interviewing me tried out a word/web game I wrote in Java. Out of a word list of the entire Oxford English dictionary, what was the first word that came up to be guessed?\nOf course, the most foul word in the English language.\nSomehow, I still got the job offer, but I then tracked down a profanity word list (not unlike this one) and wrote a quick script to generate a new dictionary without all of the bad words (without even having to look at the list).\nFor your particular case, I think comparing the search to real words sounds like the way to go with a word list like that. The alternative styles/punctuation require a bit more work, but I doubt users will use that often enough to be an issue.\n"}, "1699": {"topic": "How do you implement a good profanity filter?", "user_name": "", "text": "\na profanity filtering system will never be perfect, even if the programmer is cocksure and keeps abreast of all nude developments\nthat said, any list of 'naughty words' is likely to perform as well as any other list, since the underlying problem is language understanding which is pretty much intractable with current technology\nso, the only practical solution is twofold:\n\nbe prepared to update your dictionary frequently\nhire a human editor to correct false positives (e.g. \"clbuttic\" instead of \"classic\") and false negatives (oops! missed one!)\n\n"}, "1700": {"topic": "How do you implement a good profanity filter?", "user_name": "nickharnickhar", "text": "\nThe only way to prevent offensive user input is to prevent all user input.\nIf you insist on allowing user input and need moderation, then incorporate human moderators.\n"}, "1701": {"topic": "How do you implement a good profanity filter?", "user_name": "Matt PassellMatt Passell", "text": "\nHave a look at CDYNE's Profanity Filter Web Service\nTesting URL\n"}, "1702": {"topic": "How do you implement a good profanity filter?", "user_name": "MatthewMatthew", "text": "\nBeware of localization issues: what is a swearword in one language might be a perfectly normal word in another.\nOne current example of this: ebay uses a dictionary approach to filter \"bad words\" from feedback. If you try to enter the german translation of \"this was a perfect transaction\" (\"das war eine perfekte Transaktion\"), ebay will reject the feedback due to bad words.\nWhy? Because the german word for \"was\" is \"war\", and \"war\" is in ebay dictionary of \"bad words\".\nSo beware of localisation issues.\n"}, "1703": {"topic": "How do you implement a good profanity filter?", "user_name": "", "text": "\nRegarding your \"trick the system\" subquestion, you can handle that by normalizing both the \"bad word\" list and the user-entered text before doing your search.  e.g., Use a series of regexes (or tr if PHP has it) to convert [z$5] to \"s\", [4@] to \"a\", etc., then compare the normalized \"bad word\" list against the normalized text.  Note  that the normalization could potentially lead to additional false positives, although I can't think of any actual cases at the moment.\nThe larger challenge is to come up with something that will let people quote \"The pen is mightier than the sword\" while blocking \"p e n i s\".\n"}, "1704": {"topic": "How do you implement a good profanity filter?", "user_name": "Steven A. LoweSteven A. Lowe", "text": "\nI collected 2200 bad words in 12 languages: en, ar, cs, da, de, eo, es, fa, fi, fr, hi, hu, it, ja, ko, nl, no, pl, pt, ru, sv, th, tlh, tr, zh. \nMySQL dump, JSON, XML or CSV options are available. \nhttps://github.com/turalus/openDB\nI'd suggest you to execute this SQL into your DB and check everytime when user inputs something.\n"}, "1705": {"topic": "How do you implement a good profanity filter?", "user_name": "AxelAxel", "text": "\nIf you can do something like Digg/Stackoverflow where the users can downvote/mark obscene content... do so.\nThen all you need to do is review the \"naughty\" users, and block them if they break the rules.\n"}, "1706": {"topic": "How do you implement a good profanity filter?", "user_name": "Tim CavanaughTim Cavanaugh", "text": "\nI'm a little late to the party, but I have a solution that might work for some who read this. It's in javascript instead of php, but there's a valid reason for it.\n\nFull disclosure, I wrote this plugin... \n\nAnyways. \nThe approach I've gone with is to allow a user to \"Opt-In\" to their profanity filtering. Basically profanity will be allowed by default, but if my users don't want to read it, they don't have to. This also helps with the \"l33t sp3@k\" issue.\nThe concept is a simple jquery plugin that gets injected by the server if the client's account is enabling profanity filtering. From there, it's just a couple simple lines that blot out the swears.\nHere's the demo page\nhttps://chaseflorell.github.io/jQuery.ProfanityFilter/demo/\n<div id=\"foo\">\n    ass will fail but password will not\n</div>\n\n<script>\n    // code:\n    $('#foo').profanityFilter({\n        customSwears: ['ass']\n    });\n</script>\n\nresult \n\n*** will fail but password will not\n\n"}, "1707": {"topic": "How do you implement a good profanity filter?", "user_name": "SamSam", "text": "\nAlso late in the game, but doing some researches and stumbled across here.  As others have mentioned, it's just almost close to impossible if it was automated, but if your design/requirement can involve in some cases (but not all the time) human interactions to review whether it is profane or not, you may consider ML.  https://learn.microsoft.com/en-us/azure/cognitive-services/content-moderator/text-moderation-api#profanity is my current choice right now for multiple reasons:\n\nSupports many localization\nThey keep updating the database, so I don't have to keep up with latest slangs or languages (maintenance issue)\nWhen there is a high probability (I.e. 90% or more) you can just deny it pragmatically\nYou can observe for category which causes a flag that may or may not be profanity, and can have somebody review it to teach that it is or isn't profane.\n\nFor my need, it was/is based on public-friendly commercial service (OK, videogames) which other users may/will see the username, but the design requires that it has to go through profanity filter to reject offensive username.  The sad part about this is the classic \"clbuttic\" issue will most likely occur since usernames are usually single word (up to N characters) of sometimes multiple words concatenated...  Again, Microsoft's cognitive service will not flag \"Assist\" as Text.HasProfanity=true but may flag one of the categories probability to be high.\nAs the OP inquires, what about \"a$$\", here's a result when I passed it through the filter:, as you can see, it has determined it's not profane, but it has high probability that it is, so flags as recommendations of reviewing (human interactions).\nWhen probability is high, I can either return back \"I'm sorry, that name is already taken\" (even if it isn't) so that it is less offensive to anti-censorship persons or something, if we don't want to integrate human review, or return \"Your username have been notified to the live operation department, you may wait for your username to be reviewed and approved or chose another username\". Or whatever...\nBy the way, the cost/price for this service is quite low for my purpose (how often does the username gets changed?), but again, for OP maybe the design demands more intensive queries and may not be ideal to pay/subscribe for ML-services, or cannot have human-review/interactions.  It all depends on the design...  But if design does fit the bill, perhaps this can be OP's solution.\nIf interested, I can list the cons in the comment in the future.\n"}, "1708": {"topic": "How do you implement a good profanity filter?", "user_name": "Dave SherohmanDave Sherohman", "text": "\nOnce you have a good MYSQL table of some bad words you want to filter (I started with one of the links in this thread), you can do something like this:\n$errors = array();  //Initialize error array (I use this with all my PHP form validations)\n\n$SCREENNAME = mysql_real_escape_string($_POST['SCREENNAME']); //Escape the input data to prevent SQL injection when you query the profanity table.\n\n$ProfanityCheckString = strtoupper($SCREENNAME); //Make the input string uppercase (so that 'BaDwOrD' is the same as 'BADWORD').  All your values in the profanity table will need to be UPPERCASE for this to work.\n\n$ProfanityCheckString = preg_replace('/[_-]/','',$ProfanityCheckString); //I allow alphanumeric, underscores, and dashes...nothing else (I control this with PHP form validation).  Pull out non-alphanumeric characters so 'B-A-D-W-O-R-D' shows up as 'BADWORD'.\n\n$ProfanityCheckString = preg_replace('/1/','I',$ProfanityCheckString); //Replace common numeric representations of letters so '84DW0RD' shows up as 'BADWORD'.\n\n$ProfanityCheckString = preg_replace('/3/','E',$ProfanityCheckString);\n\n$ProfanityCheckString = preg_replace('/4/','A',$ProfanityCheckString);\n\n$ProfanityCheckString = preg_replace('/5/','S',$ProfanityCheckString);\n\n$ProfanityCheckString = preg_replace('/6/','G',$ProfanityCheckString);\n\n$ProfanityCheckString = preg_replace('/7/','T',$ProfanityCheckString);\n\n$ProfanityCheckString = preg_replace('/8/','B',$ProfanityCheckString);\n\n$ProfanityCheckString = preg_replace('/0/','O',$ProfanityCheckString); //Replace ZERO's with O's (Capital letter o's).\n\n$ProfanityCheckString = preg_replace('/Z/','S',$ProfanityCheckString); //Replace Z's with S's, another common substitution.  Make sure you replace Z's with S's in your profanity database for this to work properly.  Same with all the numbers too--having S3X7 in your database won't work, since this code would render that string as 'SEXY'.  The profanity table should have the \"rendered\" version of the bad words.\n\n$CheckProfanity = mysql_query(\"SELECT * FROM DATABASE.TABLE p WHERE p.WORD = '\".$ProfanityCheckString.\"'\");\nif(mysql_num_rows($CheckProfanity) > 0) {$errors[] = 'Please select another Screen Name.';} //Check your profanity table for the scrubbed input.  You could get real crazy using LIKE and wildcards, but I only want a simple profanity filter.\n\nif (count($errors) > 0) {foreach($errors as $error) {$errorString .= \"<span class='PHPError'>$error</span><br /><br />\";} echo $errorString;} //Echo any PHP errors that come out of the validation, including any profanity flagging.\n\n\n//You can also use these lines to troubleshoot.\n//echo $ProfanityCheckString;\n//echo \"<br />\";\n//echo mysql_error();\n//echo \"<br />\";\n\nI'm sure there is a more efficient way to do all those replacements, but I'm not smart enough to figure it out (and this seems to work okay, albeit inefficiently).\nI believe that you should err on the side of allowing users to register, and use humans to filter and add to your profanity table as required.  Though it all depends on the cost of a false positive (okay word flagged as bad) versus a false negative (bad word gets through).  That should ultimately govern how aggressive or conservative you are in your filtering strategy.\nI would also be very careful if you want to use wildcards, since they can sometimes behave more onerously than you intend.\n"}, "1709": {"topic": "How do you implement a good profanity filter?", "user_name": "Tural AliTural Ali", "text": "\nI agree with HanClinto's post higher up in this discussion. I generally use regular expressions to string-match input text. And this is a vain effort, as, like you originally mentioned you have to explicitly account for every trick form of writing popular on the net in your \"blocked\" list.\nOn a side note, while others are debating the ethics of censorship, I must agree that some form is necessary on the web. Some people simply enjoy posting vulgarity because it can be instantly offensive to a large body of people, and requires absolutely no thought on the author's part.\nThank you for the ideas.\nHanClinto rules!\n"}, "1710": {"topic": "How do you implement a good profanity filter?", "user_name": "scunliffescunliffe", "text": "\nFrankly, I'd let them get the \"trick the system\" words out and ban them instead, which is just me.  But it also makes the programming simpler.\nWhat I'd do is implement a regex filter like so: /[\\s]dooby (doo?)[\\s]/i or it the word is prefixed on others, /[\\s]doob(er|ed|est)[\\s]/.  These would prevent filtering words like assuaged, which is perfectly valid, but would also require knowledge of the other variants and updating the actual filter if you learn a new one.  Obviously these are all examples, but you'd have to decide how to do it yourself.\nI'm not about to type out all the words I know, not when I don't actually want to know them.\n"}, "1711": {"topic": "How do you implement a good profanity filter?", "user_name": "", "text": "\nDon't. It just leads to problems. One clbuttic personal experience I have with profanity filters is the time where I was kick/banned from an IRC channel for mentioning that I was \"heading over the bridge to Hancock for a couple hours\" or something to that effect.\n"}, "1712": {"topic": "How do you implement a good profanity filter?", "user_name": "Chase FlorellChase Florell", "text": "\nI agree with the futility of the subject, but if you have to have a filter, check out Ning's Boxwood:\n\nBoxwood is a PHP extension for fast replacement of multiple words in a piece of text. It supports case-sensitive and case-insensitive matching. It requires that the text it operates on be encoded as UTF-8.\n\nAlso see this blog post for more details:\n\nFast Multiple String Replacement in PHP\n\n\nWith Boxwood, you can have your list of search terms be as long as you like -- the search and replace algorithm doesn't get slower with more words on the list of words to look for. It works by building a trie  of all the search terms and then scans your subject text just once, walking down elements of the trie and comparing them to characters in your text. It supports US-ASCII and UTF-8, case-sensitive or insensitive matching, and has some English-centric word boundary checking logic.\n\n"}, "1713": {"topic": "How do you implement a good profanity filter?", "user_name": "HidekiAIHidekiAI", "text": "\nI concluded, in order to create a good profanity filter we need 3 main components, or at least it is what I am going to do. These they are:\n\nThe filter: a background service that verify against a blacklist, dictionary or something like that.\nNot allow anonymous account\nReport abuse\n\nA bonus, it will be to reward somehow those who contribute with accurate abuse reporters and punish the offender, e.g. suspend their accounts.\n"}, "1714": {"topic": "How do you implement a good profanity filter?", "user_name": "andrewandrew", "text": "\nDon't.\nBecause:\n\nClbuttic\nProfanity is not OMG EVIL\nProfanity cannot be effectively defined\nMost people quite probably don't appreciate being \"protected\" from profanity\n\nEdit: While I agree with the commenter who said \"censorship is wrong\", that is not the nature of this answer.\n"}, "1715": {"topic": "What is the at sign (@) in a batch file and what does it do?", "user_name": "CommunityBot", "text": "\nOne remotely familiar with windows/dos batch scripting will recognize this line:\n@echo off\n\nFor many-many days, I was happy with the sentiment that the @ is how echo off is meant to be written at the top of the batch and that's it.\nHowever, recently I've came accross a line like this:\n@php foo bar\n\nand another line like this:\n@call \\\\network\\folder\\batch.bat\n\nThis reinforced my suspicion that @ has more to it than just echo mode switching. However @ is not listed in the Windows XP: Command-line reference A-Z which I try to use as a reference and thus I'm not sure how to find definitive information on this:\nWhat is the @ sign in batch, what's the terminology for it, and what does it do?\n"}, "1716": {"topic": "What is the at sign (@) in a batch file and what does it do?", "user_name": "n611x007n611x007", "text": "\nAt symbol - @\nThe @ symbol tells the command processor to be less verbose; to only show the output of the command without showing it being executed or any prompts associated with the execution. When used it is prepended to the beginning of the command, it is not necessary to leave a space between the \"@\" and the command.\nWhen \"echo\" is set to \"off\" it is not necessary to use \"@\" because setting \"echo\" to \"off\" causes this behavior to become automatic. \"Echo\" is usually set to \"on\" by default when the execution of a script begins. This is the reason \"@echo off\" is commonly used, to turn echo off without displaying the act of turning it off.\necho verbose\n@echo less verbose\npause\n\n"}, "1717": {"topic": "What is the at sign (@) in a batch file and what does it do?", "user_name": "        user736893\r", "text": "\nNot only does the \"at\" symbol placed in the beginning hide the command, it can, for some commands, also be used to append command arguments stored in a text file. The syntax is exe @commands.txt. armclang.exe for example supports this option.\n"}, "1718": {"topic": "What is the at sign (@) in a batch file and what does it do?", "user_name": "SunnySunny", "text": "\nSo in simple terms, Mostly imagine this, If we did @echo off, It would've shown echo off right?, Well with @ you can make it so it doesn't show it, Hope this helped!\n"}, "1719": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "Benyamin Jafari", "text": "\n\n\n\n\n\n\nClosed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.                    \n\n\n\n\n\n\n\n\n\n\n We don\u2019t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\n\n\nClosed 8 years ago.\n\n\n\n\n\n\n\r\n                        Improve this question\r\n                    \n\n\n\nI'm looking for an open source implementation, preferably in python, of Textual Sentiment Analysis (http://en.wikipedia.org/wiki/Sentiment_analysis). Is anyone familiar with such open source implementation I can use?\nI'm writing an application that searches twitter for some search term, say \"youtube\", and counts \"happy\" tweets vs. \"sad\" tweets. \nI'm using Google's appengine, so it's in python. I'd like to be able to classify the returned search results from twitter and I'd like to do that in python.\nI haven't been able to find such sentiment analyzer so far, specifically not in python. \nAre you familiar with such open source implementation I can use? Preferably this is already in python, but if not, hopefully I can translate it to python.\nNote, the texts I'm analyzing are VERY short, they are tweets. So ideally, this classifier is optimized for such short texts.\nBTW, twitter does support the \":)\" and \":(\" operators in search, which aim to do just this, but unfortunately, the classification provided by them isn't that great, so I figured I might give this a try myself.\nThanks!\nBTW, an early demo is here and the code I have so far is here and I'd love to opensource it with any interested developer.\n"}, "1720": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "RanRan", "text": "\nGood luck with that.\nSentiment is enormously contextual, and tweeting culture makes the problem worse because you aren't given the context for most tweets.  The whole point of twitter is that you can leverage the huge amount of shared \"real world\" context to pack meaningful communication in a very short message.\nIf they say the video is bad, does that mean bad, or bad?\n\nA linguistics professor was lecturing\n  to her class one day. \"In English,\"\n  she said, \"A double negative forms a\n  positive. In some languages, though,\n  such as Russian, a double negative is\n  still a negative. However, there is no\n  language wherein a double positive can\n  form a negative.\"\nA voice from the back of the room\n  piped up, \"Yeah . . .right.\"\n\n"}, "1721": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "dbr", "text": "\nWith most of these kinds of applications, you'll have to roll much of your own code for a statistical classification task. As Lucka suggested, NLTK is the perfect tool for natural language manipulation in Python, so long as your goal doesn't interfere with the non commercial nature of its license.  However, I would suggest other software packages for modeling.  I haven't found many strong advanced machine learning models available for Python, so I'm going to suggest some standalone binaries that easily cooperate with it.\nYou may be interested in The Toolkit for Advanced Discriminative Modeling, which can be easily interfaced with Python.  This has been used for classification tasks in various areas of natural language processing.  You also have a pick of a number of different models.  I'd suggest starting with Maximum Entropy classification so long as you're already familiar with implementing a Naive Bayes classifier.  If not, you may want to look into it and code one up to really get a decent understanding of statistical classification as a machine learning task.\nThe University of Texas at Austin computational linguistics groups have held classes where most of the projects coming out of them have used this great tool.  You can look at the course page for Computational Linguistics II to get an idea of how to make it work and what previous applications it has served.\nAnother great tool which works in the same vein is Mallet.  The difference between Mallet is that there's a bit more documentation and some more models available, such as decision trees, and it's in Java, which, in my opinion, makes it a little slower.  Weka is a whole suite of different machine learning models in one big package that includes some graphical stuff, but it's really mostly meant for pedagogical purposes, and isn't really something I'd put into production.\nGood luck with your task.  The real difficult part will probably be the amount of knowledge engineering required up front for you to classify the 'seed set' off of which your model will learn.  It needs to be pretty sizeable, depending on whether you're doing binary classification (happy vs sad) or a whole range of emotions (which will require even more).  Make sure to hold out some of this engineered data for testing, or run some tenfold or remove-one tests to make sure you're actually doing a good job predicting before you put it out there. And most of all, have fun!  This is the best part of NLP and AI, in my opinion.\n"}, "1722": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "MarkusQMarkusQ", "text": "\nThanks everyone for your suggestions, they were indeed very useful!\nI ended up using a Naive Bayesian classifier, which I borrowed from here. \nI started by feeding it with a list of good/bad keywords and then added a \"learn\" feature by employing user feedback. It turned out to work pretty nice.\nThe full details of my work as in a blog post.\nAgain, your help was very useful, so thank you!\n"}, "1723": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "Robert ElwellRobert Elwell", "text": "\nI have constructed a word list labeled with sentiment. You can access it from here:\nhttp://www2.compute.dtu.dk/pubdb/views/edoc_download.php/6010/zip/imm6010.zip\nYou will find a short Python program on my blog: \nhttp://finnaarupnielsen.wordpress.com/2011/06/20/simplest-sentiment-analysis-in-python-with-af/\nThis post displays how to use the word list with single sentences as well as with Twitter.\nWord lists approaches have their limitations. You will find a investigation of the limitations of my word list in the article \"A new ANEW: Evaluation of a word list for sentiment analysis in microblogs\". That article is available from my homepage.\nPlease note a unicode(s, 'utf-8') is missing from the code (for paedagogic reasons).\n"}, "1724": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "RanRan", "text": "\nA lot of research papers indicate that a good starting point for sentiment analysis is looking at adjectives, e.g., are they positive adjectives or negative adjectives. For a short block of text this is pretty much your only option... There are papers that look at entire documents, or sentence level analysis, but as you say tweets are quite short... There is no real magic approach to understanding the sentiment of a sentence, so I think your best bet would be hunting down one of these research papers and trying to get their data-set of positively/negatively oriented adjectives.\nNow, this having been said, sentiment is domain specific, and you might find it difficult to get a high-level of accuracy with a general purpose data-set.\nGood luck.\n"}, "1725": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "", "text": "\nI think you may find it difficult to find what you're after. The closest thing that I know of is LingPipe, which has some sentiment analysis functionality and is available under a limited kind of open-source licence, but is written in Java.\nAlso, sentiment analysis systems are usually developed by training a system on product/movie review data which is significantly different from the average tweet. They are going to be optimised for text with several sentences, all about the same topic. I suspect you would do better coming up with a rule-based system yourself, perhaps based on a lexicon of sentiment terms like the one the University of Pittsburgh provide.\nCheck out We Feel Fine for an implementation of similar idea with a really beautiful interface (and twitrratr).\n"}, "1726": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "Finn \u00c5rup NielsenFinn \u00c5rup Nielsen", "text": "\nTake a look at Twitter sentiment analysis tool. It's written in python, and it uses Naive Bayes classifier with semi-supervised machine learning. The source can be found here.\n"}, "1727": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "        Ben CoeBen Coe", "text": "\nMaybe TextBlob (based on NLTK and pattern) is the right sentiment analysis tool for you.\n"}, "1728": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "", "text": "\nI came across Natural Language Toolkit a while ago. You could probably use it as a starting point. It also has a lot of modules and addons, so maybe they already have something similar.\n"}, "1729": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "StompchickenStompchicken", "text": "\nSomewhat wacky thought: you could try using the Twitter API to download a large set of tweets, and then classifying a subset of that set using emoticons: one positive group for  \":)\", \":]\", \":D\", etc, and another negative group with \":(\", etc.\nOnce you have that crude classification, you could search for more clues with frequency or ngram analysis or something along those lines.\nIt may seem silly, but serious research has been done on this (search for \"sentiment analysis\" and emoticon). Worth a look.  \n"}, "1730": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "Garrett Hyde", "text": "\nThere's a Twitter Sentiment API by TweetFeel that does advanced linguistic analysis of tweets, and can retrieve positive/negative tweets. See http://www.webservius.com/corp/docs/tweetfeel_sentiment.htm\n"}, "1731": {"topic": "Sentiment analysis for Twitter in Python [closed]", "user_name": "cyhexcyhex", "text": "\nFor those interested in coding Twitter Sentiment Analyis from scratch, there is a Coursera course \"Data Science\" with python code on GitHub (as part of assignment 1 - link). The sentiments are part of the AFINN-111.\nYou can find working solutions, for example here. In addition to the AFINN-111 sentiment list, there is a simple implementation of builing a dynamic term list based on frequency of terms in tweets that have a pos/neg score (see here).\n"}, "1732": {"topic": "In a C# event handler, why must the \"sender\" parameter be an object?", "user_name": "max630", "text": "\nAccording to Microsoft event naming guidelines, the sender parameter in a C# event handler \"is always of type object, even if it is possible to use a more specific type\".\nThis leads to lots of event handling code like:\nRepeaterItem item = sender as RepeaterItem;\nif (item != null) { /* Do some stuff */ }\n\nWhy does the convention advise against declaring an event handler with a more specific type?\nMyType\n{\n    public event MyEventHander MyEvent;\n}\n\n...\n\ndelegate void MyEventHander(MyType sender, MyEventArgs e);\n\nAm I missing a gotcha?\n\nFor posterity: I agree with the general sentiment in the answers that the convention is to use object (and to pass data via the EventArgs) even when it is possible to use a more specific type, and in real-world programming it is important to follow the convention.\nEdit: bait for search: RSPEC-3906 rule \"Event Handlers should have the correct signature\"\n"}, "1733": {"topic": "In a C# event handler, why must the \"sender\" parameter be an object?", "user_name": "Iain GallowayIain Galloway", "text": "\nWell, it's a pattern rather than a rule. It does mean that one component can forward on an event from another, keeping the original sender even if it's not the normal type raising the event.\nI agree it's a bit strange - but it's probably worth sticking to the convention just for familiarity's sake. (Familiarity for other developers, that is.) I've never been particularly keen on EventArgs myself (given that on its own it conveys no information) but that's another topic. (At least we've got EventHandler<TEventArgs> now - although it would help if there were also an EventArgs<TContent> for the common situation where you just need a single value to be propagated.)\nEDIT: It does make the delegate more general purpose, of course - a single delegate type can be reused across multiple events. I'm not sure I buy that as a particularly good reason - particularly in the light of generics - but I guess it's something...\n"}, "1734": {"topic": "In a C# event handler, why must the \"sender\" parameter be an object?", "user_name": "", "text": "\nI think there's a good reason for this convention.\nLet's take (and expand on) @erikkallen's example:\nvoid SomethingChanged(object sender, EventArgs e) {\n    EnableControls();\n}\n...\nMyRadioButton.Click += SomethingChanged;\nMyCheckbox.Click += SomethingChanged;\nMyDropDown.SelectionChanged += SomethingChanged;\n...\n\nThis is possible (and has been since .Net 1, before generics) because covariance is supported.\nYour question makes total sense if you're going top-down - i.e. you need the event in your code, so you add it to your control.\nHowever the convention is to make it easier when writing the components in the first place. You know that for any event the basic pattern (object sender, EventArgs e) will work.\nWhen you add the event you don't know how it will be used, and you don't want to arbitrarily constrain the developers using your component.\nYour example of a generic, strongly typed event makes good sense in your code, but won't fit with other components written by other developers. For instance if they want to use your component with those above:\n//this won't work\nGallowayClass.Changed += SomethingChanged;\n\nIn this example the additional type-constraint is just creating pain for the remote developer. They now have to create a new delegate just for your component. If they're using a load of your components they might need a delegate for each one.\nI reckon the convention is worth following for anything external or that you expect to be used outside of a close nit team.\nI like the idea of the generic event args - I already use something similar.\n"}, "1735": {"topic": "In a C# event handler, why must the \"sender\" parameter be an object?", "user_name": "Jon SkeetJon Skeet", "text": "\nI use the following delegate when I would prefer a strongly-typed sender.\n/// <summary>\n/// Delegate used to handle events with a strongly-typed sender.\n/// </summary>\n/// <typeparam name=\"TSender\">The type of the sender.</typeparam>\n/// <typeparam name=\"TArgs\">The type of the event arguments.</typeparam>\n/// <param name=\"sender\">The control where the event originated.</param>\n/// <param name=\"e\">Any event arguments.</param>\npublic delegate void EventHandler<TSender, TArgs>(TSender sender, TArgs e) where TArgs : EventArgs;\n\nThis can be used in the following manner:\npublic event EventHandler<TypeOfSender, TypeOfEventArguments> CustomEvent;\n\n"}, "1736": {"topic": "In a C# event handler, why must the \"sender\" parameter be an object?", "user_name": "KeithKeith", "text": "\nGenerics and history would play a big part, especially with the number of controls (etc) that expose similar events. Without generics, you would end up with a lot of events exposing Control, which is largely useless:\n\nyou still have to cast to do anything useful (except maybe a reference check, which you can do just as well with object)\nyou can't re-use the events on non-controls\n\nIf we consider generics, then again all is well, but you then start getting into issues with inheritance; if class B : A, then should events on A be EventHandler<A, ...>, and events on B be EventHandler<B, ...>? Again, very confusing, hard for tooling, and a bit messy in terms of language.\nUntil there is a better option that covers all of these, object works; events are almost always on class instances, so there is no boxing etc - just a cast. And casting isn't very slow.\n"}, "1737": {"topic": "In a C# event handler, why must the \"sender\" parameter be an object?", "user_name": "Chris ShoutsChris Shouts", "text": "\nI guess that's because you should be able to do something like\nvoid SomethingChanged(object sender, EventArgs e) {\n    EnableControls();\n}\n...\nMyRadioButton.Click += SomethingChanged;\nMyCheckbox.Click += SomethingChanged;\n...\n\nWhy do you do the safe cast in your code? If you know that you only use the function as an event handler for the repeater, you know that the argument is always of the correct type and you can use a throwing cast instead, e.g. (Repeater)sender instead of (sender as Repeater).\n"}, "1738": {"topic": "In a C# event handler, why must the \"sender\" parameter be an object?", "user_name": "Marc GravellMarc Gravell", "text": "\nNo good reason at all, now there's covarience and contravarience I think it's fine to use a strongly typed Sender. See discussion in this question\n"}, "1739": {"topic": "In a C# event handler, why must the \"sender\" parameter be an object?", "user_name": "", "text": "\nConventions exist only to impose consistency.\nYou CAN strongly type your event handlers if you wish, but ask yourself if doing so would provide any technical advantage? \nYou should consider that event handlers don't always need to cast the sender... most of the event handling code I've seen in actual practice don't make use of the sender parameter. It is there IF it is needed, but quite often it isn't. \nI often see cases where different events on different objects will share a single common event handler, which works because that event handler isn't concerned with who the sender was.\nIf those delegates were strongly typed, even with clever use of generics, it would be VERY difficult to share an event handler like that. In fact, by strongly typing it you are imposing the assumption that the handlers should care what the sender is, when that isn't the practical reality.\nI guess what you should be asking is why WOULD you strongly type the event handling delegates? By doing so would you be adding any significant functional advantages? Are you making the usage more \"consistent\"? Or are you just imposing assumptions and constraints just for the sake of strong-typing?\n"}, "1740": {"topic": "In a C# event handler, why must the \"sender\" parameter be an object?", "user_name": "erikkallenerikkallen", "text": "\nYou say:\n\nThis leads to lots of event handling\n  code like:-\n\nRepeaterItem item = sender as RepeaterItem\nif (RepeaterItem != null) { /* Do some stuff */ }\n\nIs it really lots of code?\nI'd advise never to use the sender parameter to an event handler. As you've noticed, it's not statically typed. It's not necessarily the direct sender of the event, because sometimes an event is forwarded. So the same event handler may not even get the same sender object type every time it is fired. It's an unnecessary form of implicit coupling.\nWhen you enlist with an event, at that point you must know what object the event is on, and that is what you're most likely to be interested in:\nsomeControl.Exploded += (s, e) => someControl.RepairWindows();\n\nAnd anything else specific to the event ought to be in the EventArgs-derived second parameter.\nBasically the sender parameter is a bit of historical noise, best avoided.\nI asked a similar question here.\n"}, "1741": {"topic": "In a C# event handler, why must the \"sender\" parameter be an object?", "user_name": "CommunityBot", "text": "\nIt's because you can never be sure who fired the event. There is no way to restrict which types are allowed to fire a certain event.\n"}, "1742": {"topic": "In a C# event handler, why must the \"sender\" parameter be an object?", "user_name": "MarkJMarkJ", "text": "\nThe pattern of using EventHandler(object sender, EventArgs e) is meant to provide for all events the means of identifying the event source (sender), and providing a container for all the event's specific payload.\nThe advantage of this pattern is also that it allows to generate a number of different events using the same type of delegate.\nAs for the arguments of this default delegate...\nThe advantage of having a single bag for all the state you want to pass along with the event is fairly obvious, especially if there are many elements in that state.\nUsing object instead of a strong type allows to pass the event along, possibly to assemblies that do not have a reference to your type (in which case you may argue that they won't be able to use the sender anyway, but that's another story - they can still get the event).\nIn my own experience, I agree with Stephen Redd, very often the sender is not used. The only cases I've needed to identify the sender is in the case of UI handlers, with many controls sharing the same event handler (to avoid duplicating code).\nI depart from his position, however, in that I see no problem defining strongly typed delegates, and generating events with strongly typed signatures, in the case where I know that the handler will never care who the sender is (indeed, often it should not have any scope into that type), and I do not want the inconvenience of stuffing state into a bag (EventArg subclass or generic) and unpacking it. If I only have 1 or 2 elements in my state, I'm OK generating that signature.\nIt's a matter of convenience for me: strong typing means the compiler keeps me on my toes, and it reduces the kind of branching like \nFoo foo = sender as Foo;\nif (foo !=null) { ... }\n\nwhich does make the code look better :)\nThis being said, it is just my opinion. I've deviated often from the recommended pattern for events, and I have not suffered any for it. It is important to always be clear about why it is OK to deviate from it.\nGood question!\n.\n"}, "1743": {"topic": "In a C# event handler, why must the \"sender\" parameter be an object?", "user_name": "Stephen M. ReddStephen M. Redd", "text": "\nWell, that's a good question. I think because any other type could use your delegate to declare an event, so you can't be sure that the type of the sender is really \"MyType\".\n"}, "1744": {"topic": "In a C# event handler, why must the \"sender\" parameter be an object?", "user_name": "CommunityBot", "text": "\nI tend to use a specific delegate type for each event (or a small group of similar events).  The useless sender and eventargs simply clutter the api and distract from the actually relevant bits of information.  Being able to \"forward\" events across classes isn't something I've yet to find useful - and if you're forwarding events like that, to an event handler that represents a different type of event, then being forced to wrap the event yourself and provide the appropriate parameters is little effort.  Also, the forwarder tends to have a better idea of how to \"convert\" the event parameters than the final receiver.\nIn short, unless there's some pressing interop reason, dump the useless, confusing parameters.\n"}, "1745": {"topic": "Training data for sentiment analysis [closed]", "user_name": "Iterator", "text": "\n\n\n\n\n\n\nClosed. This question is seeking recommendations for books, tools, software libraries, and more. It does not meet Stack Overflow guidelines. It is not currently accepting answers.                    \n\n\n\n\n\n\n\n\n\n\n We don\u2019t allow questions seeking recommendations for books, tools, software libraries, and more. You can edit the question so it can be answered with facts and citations.\n\n\nClosed 7 years ago.\n\n\n\n\n\n\n\r\n                        Improve this question\r\n                    \n\n\n\nWhere can I get a corpus of documents that have already been classified as positive/negative for sentiment in the corporate domain? I want a large corpus of documents that provide reviews for companies, like reviews of companies provided by analysts and media.\nI find corpora that have reviews of products and movies. Is there a corpus for the business domain including reviews of companies, that match the language of business?\n"}, "1746": {"topic": "Training data for sentiment analysis [closed]", "user_name": "London guyLondon guy", "text": "\nhttp://www.cs.cornell.edu/home/llee/data/\nhttp://mpqa.cs.pitt.edu/corpora/mpqa_corpus\nYou can use twitter, with its smileys, like this: http://web.archive.org/web/20111119181304/http://deepthoughtinc.com/wp-content/uploads/2011/01/Twitter-as-a-Corpus-for-Sentiment-Analysis-and-Opinion-Mining.pdf\nHope that gets you started.  There's more in the literature, if you're interested in specific subtasks like negation, sentiment scope, etc.\nTo get a focus on companies, you might pair a method with topic detection, or cheaply just a lot of mentions of a given company.  Or you could get your data annotated by Mechanical Turkers.\n"}, "1747": {"topic": "Training data for sentiment analysis [closed]", "user_name": "        user325117\r", "text": "\nThis is a list I wrote a few weeks ago, from my blog. Some of these datasets have been recently included in the NLTK Python platform.\nLexicons\n\nOpinion Lexicon by Bing Liu\n\nURL: http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon\nPAPERS: Mining and summarizing customer reviews\nNOTES: Included in the NLTK Python platform\n\n\nMPQA Subjectivity Lexicon\n\nURL: http://mpqa.cs.pitt.edu/#subj_lexicon\nPAPERS: Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis (Theresa Wilson, Janyce Wiebe, and Paul Hoffmann, 2005).\n\n\nSentiWordNet\n\nURL: http://sentiwordnet.isti.cnr.it\nNOTES: Included in the NLTK Python platform\n\n\nHarvard General Inquirer\n\nURL: http://www.wjh.harvard.edu/~inquirer\nPAPERS: The General Inquirer: A Computer Approach to Content Analysis (Stone, Philip J; Dexter C. Dunphry; Marshall S. Smith; and Daniel M. Ogilvie. 1966)\n\n\nLinguistic Inquiry and Word Counts (LIWC)\n\nURL: http://www.liwc.net\n\n\nVader Lexicon\n\nURLs: https://github.com/cjhutto/vaderSentiment, http://comp.social.gatech.edu/papers\nPAPERS: Vader: A parsimonious rule-based model for sentiment analysis of social media text (Hutto, Gilbert.  2014)\n\n\n\n\nDatasets\n\nMPQA Datasets\n\nURL: http://mpqa.cs.pitt.edu\n\nNOTES: GNU Public License.\n\nPolitical Debate data\nProduct Debate data\nSubjectivity Sense Annotations\n\n\n\n\nSentiment140 (Tweets)\n\nURL: http://help.sentiment140.com/for-students\nPAPERS: Twitter Sent classification using Distant Supervision (Go, Alec, Richa Bhayani, and Lei Huang)\nURLs: http://help.sentiment140.com, https://groups.google.com/forum/#!forum/sentiment140\n\n\nSTS-Gold (Tweets)\n\nURL: http://www.tweenator.com/index.php?page_id=13\nPAPERS: Evaluation datasets for twitter sentiment analysis (Saif, Fernandez, He, Alani)\nNOTES: As Sentiment140, but the dataset is smaller and with human annotators. It comes with 3 files: tweets, entities (with their sentiment) and an aggregate set.\n\n\nCustomer Review Dataset (Product reviews)\n\nURL: http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets\n\nPAPERS: Mining and summarizing customer reviews\n\nNOTES: Title of review, product feature, positive/negative label with opinion strength, other info (comparisons, pronoun resolution, etc.)\nIncluded in the NLTK Python platform\n\n\n\nPros and Cons Dataset (Pros and cons sentences)\n\nURL: http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets\n\nPAPERS: Mining Opinions in Comparative Sentences (Ganapathibhotla, Liu 2008)\n\nNOTES: A list of sentences tagged <pros> or <cons>\nIncluded in the NLTK Python platform\n\n\n\nComparative Sentences (Reviews)\n\nURL: http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#datasets\n\nPAPERS: Identifying Comparative Sentences in Text Documents (Nitin Jindal and Bing Liu), Mining Opinion Features in Customer Reviews (Minqing Hu and Bing Liu)\n\nNOTES: Sentence, POS-tagged sentence, entities, comparison type (non-equal, equative, superlative, non-gradable)\nIncluded in the NLTK Python platform\n\n\n\nSanders Analytics Twitter Sentiment Corpus (Tweets)\n\nURL: http://www.sananalytics.com/lab/twitter-sentiment\n\n\n5513 hand-classified tweets wrt 4 different topics. Because of Twitter\u2019s ToS, a small Python script is included to download all of the tweets. The sentiment classifications themselves are provided free of charge and without restrictions. They may be used for commercial products. They may be redistributed. They may be modified.\n\n\nSpanish tweets (Tweets)\n\nURL: http://www.daedalus.es/TASS2013/corpus.php\n\n\nSemEval 2014 (Tweets)\n\nURL: http://alt.qcri.org/semeval2014/task9\n\n\nYou MUST NOT re-distribute the tweets, the annotations or the corpus obtained (from the readme file)\n\n\nVarious Datasets (Reviews)\n\nURL: https://personalwebs.coloradocollege.edu/~mwhitehead/html/opinion_mining.html\nPAPERS: Building a General Purpose Cross-Domain Sentiment Mining Model (Whitehead and Yaeger), Sentiment Mining Using Ensemble Classification Models (Whitehead and Yaeger)\n\n\nVarious Datasets #2 (Reviews)\n\nURL: http://www.text-analytics101.com/2011/07/user-review-datasets_20.html\n\n\n\n\nReferences:\n\nKeenformatics - Sentiment Analysis lexicons and datasets (my blog)\nPersonal experience\n\n"}, "1748": {"topic": "Training data for sentiment analysis [closed]", "user_name": "Gregory MartonGregory Marton", "text": "\nHere are a few more;\nhttp://inclass.kaggle.com/c/si650winter11\nhttp://alias-i.com/lingpipe/demos/tutorial/sentiment/read-me.html\n"}, "1749": {"topic": "Training data for sentiment analysis [closed]", "user_name": "", "text": "\nIf you have some resources (media channels, blogs, etc) about the domain you want to explore, you can create your own corpus. \nI do this in python: \n\nusing Beautiful Soup http://www.crummy.com/software/BeautifulSoup/ for parsing the content that I want to classify. \nseparate those sentences meaning positive/negative opinions about companies. \nUse NLTK to process this sentences, tokenize words, POS tagging, etc. \nUse NLTK PMI to calculate bigrams or trigrams mos frequent in only one class\n\nCreating corpus is a hard work of pre-processing, checking, tagging, etc, but has the benefits of preparing a model for a specific domain many times increasing the accuracy. If you can get already prepared corpus, just go ahead with the sentiment analysis ;) \n"}, "1750": {"topic": "Training data for sentiment analysis [closed]", "user_name": "Kurt BourbakiKurt Bourbaki", "text": "\nI'm not aware of any such corpus being freely available, but you could try an unsupervised method on an unlabeled dataset.\n"}, "1751": {"topic": "Training data for sentiment analysis [closed]", "user_name": "y2py2p", "text": "\nYou can get a large select of online reviews from Datafiniti.  Most of the reviews come with rating data, which would provide more granularity on sentiment than positive / negative.  Here's a list of businesses with reviews, and here's a list of products with reviews.\n"}, "1752": {"topic": "Unsupervised Sentiment Analysis", "user_name": "Holger Just", "text": "\nI've been reading a lot of articles that explain the need for an initial set of texts that are classified as either 'positive' or 'negative' before a sentiment analysis system will really work.\nMy question is: Has anyone attempted just doing a rudimentary check of 'positive' adjectives vs 'negative' adjectives, taking into account any simple negators to avoid classing 'not happy' as positive? If so, are there any articles that discuss just why this strategy isn't realistic?\n"}, "1753": {"topic": "Unsupervised Sentiment Analysis", "user_name": "TrindazTrindaz", "text": "\nA classic paper by Peter Turney (2002) explains a method to do unsupervised sentiment analysis (positive/negative classification) using only the words excellent and poor as a seed set. Turney uses the mutual information of other words with these two adjectives to achieve an accuracy of 74%.\n"}, "1754": {"topic": "Unsupervised Sentiment Analysis", "user_name": "ruben_pants", "text": "\nI haven't tried doing untrained sentiment analysis such as you are describing, but off the top of my head I'd say you're oversimplifying the problem.  Simply analyzing adjectives is not enough to get a good grasp of the sentiment of a text; for example, consider the word 'stupid.'  Alone, you would classify that as negative, but if a product review were to have '... [x] product makes their competitors look stupid for not thinking of this feature first...' then the sentiment in there would definitely be positive. The greater context in which words appear definitely matters in something like this.  This is why an untrained bag-of-words approach alone (let alone an even more limited bag-of-adjectives) is not enough to tackle this problem adequately.\nThe pre-classified data ('training data') helps in that the problem shifts from trying to determine whether a text is of positive or negative sentiment from scratch, to trying to determine if the text is more similar to positive texts or negative texts, and classify it that way.  The other big point is that textual analyses such as sentiment analysis are often affected greatly by the differences of the characteristics of texts depending on domain.  This is why having a good set of data to train on (that is, accurate data from within the domain in which you are working, and is hopefully representative of the texts you are going to have to classify) is as important as building a good system to classify with.\nNot exactly an article, but hope that helps.\n"}, "1755": {"topic": "Unsupervised Sentiment Analysis", "user_name": "Fred FooFred Foo", "text": "\nThe paper of Turney (2002) mentioned by larsmans is a good basic one. In a newer research, Li and He [2009] introduce an approach using Latent Dirichlet Allocation (LDA) to train a model that can classify an article's overall sentiment and topic simultaneously in a totally unsupervised manner. The accuracy they achieve is 84.6%.\n"}, "1756": {"topic": "Unsupervised Sentiment Analysis", "user_name": "waffle paradoxwaffle paradox", "text": "\nI tried several methods of Sentiment Analysis for opinion mining in Reviews. \nWhat worked the best for me is the method described in Liu book: http://www.cs.uic.edu/~liub/WebMiningBook.html In this Book Liu and others, compared many strategies and discussed different papers on Sentiment Analysis and Opinion Mining.\nAlthough my main goal was to extract features in the opinions, I implemented a sentiment classifier to detect positive and negative classification of this features. \nI used NLTK for the pre-processing (Word tokenization, POS tagging) and the trigrams creation. Then also I used the Bayesian Classifiers inside this tookit to compare with other strategies Liu was pinpointing. \nOne of the methods relies on tagging as pos/neg every trigrram expressing this information, and using some classifier on this data. \nOther method I tried, and worked better (around 85% accuracy in my dataset), was calculating  the sum of scores of PMI (punctual mutual information) for every word in the sentence and the words excellent/poor as seeds of pos/neg class. \n"}, "1757": {"topic": "Unsupervised Sentiment Analysis", "user_name": "        user325117\r", "text": "\nI tried spotting keywords using a dictionary of affect to predict the sentiment label at sentence level. Given the generality of the vocabulary (non domain dependent), the results were just about 61%. The paper is available in my homepage.\nIn a somewhat improved version, negation adverbs were considered. The whole system, named EmoLib, is available for demo:\nhttp://dtminredis.housing.salle.url.edu:8080/EmoLib/\nRegards,\n"}, "1758": {"topic": "Unsupervised Sentiment Analysis", "user_name": "Trung HuynhTrung Huynh", "text": "\nDavid, \nI'm not sure if this helps but you may want to look into Jacob Perkin's blog post on using NLTK for sentiment analysis.\n"}, "1759": {"topic": "Unsupervised Sentiment Analysis", "user_name": "LuchuxLuchux", "text": "\nThere are no magic \"shortcuts\" in sentiment analysis, as with any other sort of text analysis that seeks to discover the underlying \"aboutness,\" of a chunk of text. Attempting to short cut proven text analysis methods through simplistic \"adjective\" checking or similar approaches leads to ambiguity, incorrect classification, etc., that at the end of the day give you a poor accuracy read on sentiment. The more terse the source (e.g. Twitter), the more difficult the problem.\n"}, "1760": {"topic": "Why does adding local variables make .NET code slower", "user_name": "CommunityBot", "text": "\nWhy does commenting out the first two lines of this for loop and uncommenting the third result in a 42% speedup?\nint count = 0;\nfor (uint i = 0; i < 1000000000; ++i) {\n    var isMultipleOf16 = i % 16 == 0;\n    count += isMultipleOf16 ? 1 : 0;\n    //count += i % 16 == 0 ? 1 : 0;\n}\n\nBehind the timing is vastly different assembly code: 13 vs. 7 instructions in the loop. The platform is Windows 7 running .NET 4.0 x64. Code optimization is enabled, and the test app was run outside VS2010. [Update: Repro project, useful for verifying project settings.]\nEliminating the intermediate boolean is a fundamental optimization, one of the simplest in my 1980's era Dragon Book. How did the optimization not get applied when generating the CIL or JITing the x64 machine code?\nIs there a \"Really compiler, I would like you to optimize this code, please\" switch? While I sympathize with the sentiment that premature optimization is akin to the love of money, I could see the frustration in trying to profile a complex algorithm that had problems like this scattered throughout its routines. You'd work through the hotspots but have no hint of the broader warm region that could be vastly improved by hand tweaking what we normally take for granted from the compiler. I sure hope I'm missing something here.\nUpdate: Speed differences also occur for x86, but depend on the order that methods are just-in-time compiled. See Why does JIT order affect performance?\nAssembly code (as requested):\n    var isMultipleOf16 = i % 16 == 0;\n00000037  mov         eax,edx \n00000039  and         eax,0Fh \n0000003c  xor         ecx,ecx \n0000003e  test        eax,eax \n00000040  sete        cl \n    count += isMultipleOf16 ? 1 : 0;\n00000043  movzx       eax,cl \n00000046  test        eax,eax \n00000048  jne         0000000000000050 \n0000004a  xor         eax,eax \n0000004c  jmp         0000000000000055 \n0000004e  xchg        ax,ax \n00000050  mov         eax,1 \n00000055  lea         r8d,[rbx+rax] \n\n    count += i % 16 == 0 ? 1 : 0;\n00000037  mov         eax,ecx \n00000039  and         eax,0Fh \n0000003c  je          0000000000000042 \n0000003e  xor         eax,eax \n00000040  jmp         0000000000000047 \n00000042  mov         eax,1 \n00000047  lea         edx,[rbx+rax] \n\n"}, "1761": {"topic": "Why does adding local variables make .NET code slower", "user_name": "Edward BreyEdward Brey", "text": "\nQuestion should be \"Why do I see such a difference on my machine?\". I cannot reproduce such a huge speed difference and suspect there is something specific to your environment. Very difficult to tell what it can be though. Can be some (compiler) options you have set some time ago and forgot about them.\nI have create a console application, rebuild in Release mode (x86) and run outside VS. Results are virtually identical, 1.77 seconds for both methods. Here is the exact code:\nstatic void Main(string[] args)\n{\n    Stopwatch sw = new Stopwatch();\n    sw.Start();\n    int count = 0;\n\n    for (uint i = 0; i < 1000000000; ++i)\n    {\n        // 1st method\n        var isMultipleOf16 = i % 16 == 0;\n        count += isMultipleOf16 ? 1 : 0;\n\n        // 2nd method\n        //count += i % 16 == 0 ? 1 : 0;\n    }\n\n    sw.Stop();\n    Console.WriteLine(string.Format(\"Ellapsed {0}, count {1}\", sw.Elapsed, count));\n    Console.ReadKey();\n}\n\nPlease, anyone who has 5 minutes copy the code, rebuild, run outside VS and post results in comments to this answer. I'd like to avoid saying \"it works on my machine\".\nEDIT\nTo be sure I have created a 64 bit Winforms application and the results are similar as in the the question - the first method is slower (1.57 sec) than the second one (1.05 sec). The difference I observe is 33% - still a lot. Seems there is a bug in .NET4 64 bit JIT compiler.\n"}, "1762": {"topic": "Why does adding local variables make .NET code slower", "user_name": "", "text": "\nI can't speak to the .NET compiler, or its optimizations, or even WHEN it performs its optimizations.\nBut in this specific case, if the compiler folded that boolean variable in to the actual statement, and you were to try and debug this code, the optimized code would not match the code as written. You would not be able to single step over the isMulitpleOf16 assignment and check it value.\nThats just one example of where the optimization may well be turned off. There could be others. The optimization may happen during the load phase of the code, rather than the code generation phase from the CLR.\nThe modern runtimes are pretty complicated, especially if you throw in JIT and dynamic optimization over run time. I feel grateful the code does what it says at all sometimes.\n"}, "1763": {"topic": "Why does adding local variables make .NET code slower", "user_name": "MaciejMaciej", "text": "\nIt's a bug in the .NET Framework.\nWell, really I'm just speculating, but I submitted a bug report on Microsoft Connect to see what they say. After Microsoft deleted that report, I resubmitted it on roslyn project on GitHub.\nUpdate: Microsoft has moved the issue to the coreclr project. From the comments on the issue, calling it a bug seems a bit strong; it's more of a missing optimization.\n"}, "1764": {"topic": "Why does adding local variables make .NET code slower", "user_name": "Will HartungWill Hartung", "text": "\nI think this is related to your other question. When I change your code as follows, the multi-line version wins.\noops, only on x86.  On x64, multi-line is the slowest and the conditional beats them both handily.\nclass Program\n{\n    static void Main()\n    {\n        ConditionalTest();\n        SingleLineTest();\n        MultiLineTest();\n        ConditionalTest();\n        SingleLineTest();\n        MultiLineTest();\n        ConditionalTest();\n        SingleLineTest();\n        MultiLineTest();\n    }\n\n    public static void ConditionalTest()\n    {\n        Stopwatch stopwatch = new Stopwatch();\n        stopwatch.Start();\n        int count = 0;\n        for (uint i = 0; i < 1000000000; ++i) {\n            if (i % 16 == 0) ++count;\n        }\n        stopwatch.Stop();\n        Console.WriteLine(\"Conditional test --> Count: {0}, Time: {1}\", count, stopwatch.ElapsedMilliseconds);\n    }\n\n    public static void SingleLineTest()\n    {\n        Stopwatch stopwatch = new Stopwatch();\n        stopwatch.Start();\n        int count = 0;\n        for (uint i = 0; i < 1000000000; ++i) {\n            count += i % 16 == 0 ? 1 : 0;\n        }\n        stopwatch.Stop();\n        Console.WriteLine(\"Single-line test --> Count: {0}, Time: {1}\", count, stopwatch.ElapsedMilliseconds);\n    }\n\n    public static void MultiLineTest()\n    {\n        Stopwatch stopwatch = new Stopwatch();\n        stopwatch.Start();\n        int count = 0;\n        for (uint i = 0; i < 1000000000; ++i) {\n            var isMultipleOf16 = i % 16 == 0;\n            count += isMultipleOf16 ? 1 : 0;\n        }\n        stopwatch.Stop();\n        Console.WriteLine(\"Multi-line test  --> Count: {0}, Time: {1}\", count, stopwatch.ElapsedMilliseconds);\n    }\n}\n\n"}, "1765": {"topic": "Why does adding local variables make .NET code slower", "user_name": "", "text": "\nI tend to think of it like this: the people who work on the compiler can only do so much stuff per year. If in that time they could implement lambdas or lots of classical optimizations, I'd vote for lambdas. C# is a language that's efficient in terms of code reading and writing effort, rather than in terms of execution time.\nSo it's reasonable for the team to concentrate on features that maximize the reading/writing efficiency, rather than the execution efficiency in a certain corner case (of which there are probably thousands).\nInitially, I believe, the idea was that the JITter would do all the optimization. Unfortunately the JITting takes noticeable amounts of time, and any advanced optimizations will make it worse. So that didn't work out as well as one might have hoped.\nOne thing I found about programming really fast code in C# is that quite often you hit a severe GC bottleneck before any optimization like you mention would make a difference. Like if you allocate millions of objects. C# leaves you very little in terms of avoiding the cost: you can use arrays of structs instead, but the resulting code is really ugly in comparison. My point being that many other decisions about C# and .NET make such specific optimizations less worthwhile than they would be in something like a C++ compiler. Heck, they even dropped the CPU-specific optimizations in NGEN, trading performance for programmer (debugger) efficiency.\nHaving said all this, I'd love C# that actually made use of optimizations that C++ made use of since the 1990s. Just not at the expense of features like, say, async/await.\n"}, "1766": {"topic": "I know its bad to store data in the DOM, but why?", "user_name": "Jordan SitkinJordan Sitkin", "text": "\nI've heard over and over again that it is bad practice to \"use the DOM as a database.\"\nWhile I mostly agree with that sentiment, this question is more about the less black and white cases. Keeping in mind the latest revisions to jQuery's .data() methods and the HTML5 data-attribute spec, is it really so bad to stick some data in the DOM for the sake of convenience?\nFor example, I recently implemented a \"live\" calculation feature on a table full of inputs by doing something like this:\n<table>\n  <tr>\n    <td><input type=\"text\"></td>\n  </tr>\n  <tr>\n    <td><input type=\"text\"></td>\n  </tr>\n</table>\n\njQuery:\n$('table').bind('calculate',function(){\n  var total = 0;\n  $(this).find('tr').each(function(){\n    total += $(this).data('value');\n  });\n  // display total\n});\n\n$('table input').bind('change keyup',function(){\n  $(this).closest('tr').data('value',$(this).val());\n  $(this).closest('table').trigger('calculate');\n});\n\nThis is an over-simplified example because I could skip the calls to .data() and go straight to the input values, but let's imagine a slightly more complex scenario where elements other than inputs are affecting the row values.\nIs it wrong to use the DOM to store simple data in this kind of situation?\n"}, "1767": {"topic": "I know its bad to store data in the DOM, but why?", "user_name": "CommunityBot", "text": "\nIt's fine to store data in DOM objects. And since your question is specific to jQuery's data API, it's important to understand how the data API works. I wrote an answer explaining its inner workings a while back. The data API only keeps a reference to the DOM objects along with the data, and doesn't store anything inside the DOM objects themselves. All data is stored in plain old JavaScript objects.\nThe issue of whether that is a good or a bad approach is a matter of personal taste. John Resig, the creator of jQuery, gave a talk at Tech4Africa in 2010 where he talks about this exact issue, and proposes to do away with a separate storage area and link everything with the DOM using the data API. You can see the talk on YouTube (thanks to @tine2k for providing the link). If you listen to the entire talk, you'll find some good examples of why this approach makes sense and keeps things simple.\nI believe similar arguments can be made for the other end of the spectrum - to have your data in neatly tucked away objects, and classes, and be separate from the view itself. That sort of thinking comes from traditional architectures such as MVC.\nI say it's fine to go with either approach - using the DOM to store data, or using a classical approach. Just don't mix the two, cause then you application model is sprinkled everywhere.\n"}, "1768": {"topic": "I know its bad to store data in the DOM, but why?", "user_name": "AnuragAnurag", "text": "\nThere are some fundamental arguments against using the DOM to store data:\n\nThe DOM is supposed to be a view of data. The properties of DOM elements should be metadata for the elements themselves, not data from the model. \nYou should not add random properties to host objects as you have no idea what they might do with them.\nYou have the same issue as global variables - if it becomes common practice, you will have to adopt a scheme to avoid name collisions.\n\nThere is also the argument that the DOM is a pretty ordinary data store and that object structure with indexes will be much more efficient for anything other than trivial data needs.\nIf you only have small amounts of data and have full control over your pages, then go ahead and put data in data- attributes and properties. But do not store large chunks or complex structures.\nOh, and I don't think there are any performance issues - accessing an element and its properties in the DOM is likely no faster or slower than accessing some part of an object structure, though I'm sure there are faster and slower ways of doing both.\n"}, "1769": {"topic": "I know its bad to store data in the DOM, but why?", "user_name": "RobGRobG", "text": "\nI don't think there is anything wrong with storing data in the DOM, but I think the issue lies with storing a lot of data in the DOM. The browser does have to muddle through the DOM to output the pages we see, and the more data that is in there the more crap it has to sort out.\nI'm sure there are also other reasons as well, this is just my deduction.\n"}, "1770": {"topic": "I know its bad to store data in the DOM, but why?", "user_name": "ChadChad", "text": "\nHaving developed a few single page apps for CE devices that have limited browser power, I have adopted a few extra personal standards about these things. The main thing is that just because JQuery supports scanning through the DOM, that does not suggest it as a performant approach. If I need to know which LI has focus, I could use .index() or .find() or even .each(), but keeping that value in a separate model object is better for the following reasons:\n MVC is a real thing, despite the rampant abuse through doing things like using the DOM as a state holder. Keeping state in the model as a rule makes MVC make sense. \n Scanning through the LI tags is more expensive than using listItems.focussed in code.\n The DOM is busy being manipulated and your queries on it may be delayed or cause delays. \n\nAccording to Matt @ https://github.com/Matt-Esch/virtual-dom, \n\"Reading some DOM node properties even causes side effects.\" I tend to agree with that point, but i will admit I am in search for more evidence on that point, as the guy appointed to chase down our biggest apps performance issues that I am personally convinced are tied mostly to DOM dependent code.\n    "}, "1771": {"topic": "How to interpret scikit's learn confusion matrix and classification report?", "user_name": "john doejohn doe", "text": "\nI have a sentiment analysis task, for this Im using this corpus the opinions have 5 classes (very neg, neg, neu, pos, very pos), from 1 to 5. So I do the classification as follows:\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\ntfidf_vect= TfidfVectorizer(use_idf=True, smooth_idf=True,\n                            sublinear_tf=False, ngram_range=(2,2))\nfrom sklearn.cross_validation import train_test_split, cross_val_score\n\nimport pandas as pd\n\ndf = pd.read_csv('/corpus.csv',\n                     header=0, sep=',', names=['id', 'content', 'label'])\n\nX = tfidf_vect.fit_transform(df['content'].values)\ny = df['label'].values\n\n\nfrom sklearn import cross_validation\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X,\n                                                    y, test_size=0.33)\n\n\nfrom sklearn.svm import SVC\nsvm_1 = SVC(kernel='linear')\nsvm_1.fit(X, y)\nsvm_1_prediction = svm_1.predict(X_test)\n\nThen with the metrics I obtained the following confusion matrix and classification report, as follows:\nprint '\\nClasification report:\\n', classification_report(y_test, svm_1_prediction)\nprint '\\nConfussion matrix:\\n',confusion_matrix(y_test, svm_1_prediction)\n\nThen, this is the result:\nClasification report:\n             precision    recall  f1-score   support\n\n          1       1.00      0.76      0.86        71\n          2       1.00      0.84      0.91        43\n          3       1.00      0.74      0.85        89\n          4       0.98      0.95      0.96       288\n          5       0.87      1.00      0.93       367\n\navg / total       0.94      0.93      0.93       858\n\n\nConfussion matrix:\n[[ 54   0   0   0  17]\n [  0  36   0   1   6]\n [  0   0  66   5  18]\n [  0   0   0 273  15]\n [  0   0   0   0 367]]\n\nHow can I interpret the above confusion matrix and classification report. I tried reading the documentation and this question. But still can interpretate what happened here particularly with this data?. Wny this matrix is somehow \"diagonal\"?. By the other hand what means the recall, precision, f1score and support for this data?. What can I say about this data?. Thanks in advance guys\n"}, "1772": {"topic": "How to interpret scikit's learn confusion matrix and classification report?", "user_name": "AdityaAditya", "text": "\nClassification report must be straightforward - a report of P/R/F-Measure for each element in your test data. In Multiclass problems, it is not a good idea to read Precision/Recall and F-Measure over the whole data any imbalance would make you feel you've reached better results. That's where such reports help.\nComing to confusion matrix, it is much detailed representation of what's going on with your labels. So there were 71 points in the first class (label 0). Out of these, your model was successful in identifying 54 of those correctly in label 0, but 17 were marked as label 4. Similarly look at second row. There were 43 points in class 1, but 36 of them were marked correctly. Your classifier predicted 1 in class 3 and 6 in class 4.\n\nNow you can see the pattern this follows. An ideal classifiers with 100% accuracy would produce a pure diagonal matrix which would have all the points predicted in their correct class. \nComing to Recall/Precision. They are some of the mostly used measures in evaluating how good your system works. Now you had 71 points in first class (call it 0 class). Out of them your classifier was able to get 54 elements correctly. That's your recall. 54/71 = 0.76. Now look only at first column in the table. There is one cell with entry 54, rest all are zeros. This means your classifier marked 54 points in class 0, and all 54 of them were actually in class 0. This is precision. 54/54 = 1. Look at column marked 4. In this column, there are elements scattered in all the five rows. 367 of them were marked correctly. Rest all are incorrect. So that reduces your precision.\nF Measure is harmonic mean of Precision and Recall. \nBe sure you read details about these. https://en.wikipedia.org/wiki/Precision_and_recall\n"}, "1773": {"topic": "How to interpret scikit's learn confusion matrix and classification report?", "user_name": "Christopher ShymanskyChristopher Shymansky", "text": "\nHere's the documentation for scikit-learn's sklearn.metrics.precision_recall_fscore_support method: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support\nIt seems to indicate that the support is the number of occurrences of each particular class in the true responses (responses in your test set). You can calculate it by summing the rows of the confusion matrix. \n"}, "1774": {"topic": "How to interpret scikit's learn confusion matrix and classification report?", "user_name": "raman kumarraman kumar", "text": "\nConfusion Matrix tells us about the distribution of our predicted values across all the actual outcomes.Accuracy_scores, Recall(sensitivity), Precision, Specificity and other similar metrics are subsets of Confusion Matrix.\nF1 scores are the harmonic means of precision and recall.\nSupport columns in Classification_report tell us about the actual counts of each class in test data.\nWell, rest is explained above beautifully.\nThank you.\n"}, "1775": {"topic": "ValueError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]] - Tokenizing BERT / Distilbert Error", "user_name": "Raoof NaushadRaoof Naushad", "text": "\ndef split_data(path):\n  df = pd.read_csv(path)\n  return train_test_split(df , test_size=0.1, random_state=100)\n\ntrain, test = split_data(DATA_DIR)\ntrain_texts, train_labels = train['text'].to_list(), train['sentiment'].to_list() \ntest_texts, test_labels = test['text'].to_list(), test['sentiment'].to_list() \n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=100)\n\nfrom transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased\n\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True)\nvalid_encodings = tokenizer(valid_texts, truncation=True, padding=True)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True)\n\nWhen I tried to split from the dataframe using BERT tokenizers I got an error us such.\n"}, "1776": {"topic": "ValueError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]] - Tokenizing BERT / Distilbert Error", "user_name": "", "text": "\nI had the same error. The problem was that I had None in my list, e.g:\nfrom transformers import DistilBertTokenizerFast\n\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-german-cased')\n\n# create test dataframe\ntexts = ['Vero Moda Damen \u00dcbergangsmantel Kurzmantel Chic Business Coatigan SALE',\n         'Neu Herren Damen Sportschuhe Sneaker Turnschuhe Freizeit 1975 Schuhe Gr. 36-46',\n         'KOMBI-ANGEBOT Zuckerpaste STRONG / SOFT / ZUBEH\u00d6R -Sugaring Wachs Haarentfernung',\n         None]\n\nlabels = [1, 2, 3, 1]\n\nd = {'texts': texts, 'labels': labels} \ntest_df = pd.DataFrame(d)\n\nSo, before I converted the Dataframe columns to list I remove all None rows.\ntest_df = test_df.dropna()\ntexts = test_df[\"texts\"].tolist()\ntexts_encodings = tokenizer(texts, truncation=True, padding=True)\n\nThis worked for me.\n"}, "1777": {"topic": "ValueError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]] - Tokenizing BERT / Distilbert Error", "user_name": "MarkusOdenthalMarkusOdenthal", "text": "\nIn my case I had to set is_split_into_words=True\nhttps://huggingface.co/transformers/main_classes/tokenizer.html\n\nThe sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set is_split_into_words=True (to lift the ambiguity with a batch of sequences).\n\n"}, "1778": {"topic": "ValueError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]] - Tokenizing BERT / Distilbert Error", "user_name": "AhmadAhmad", "text": "\nSimilar to MarkusOdenthal I had a non string type in my list. I fixed it by converting the column to string, then converting it to a list, before splitting it into train and test segments. So you would do\ntrain_texts = train['text'].astype(str).values.to_list()\n\n"}, "1779": {"topic": "ValueError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]] - Tokenizing BERT / Distilbert Error", "user_name": "MsalmanMsalman", "text": "\ndef split_data(path):\n  df = pd.read_csv(path)\n  return train_test_split(df , test_size=0.2, random_state=100)\n\ntrain, test = split_data(DATA_DIR)\ntrain_texts, train_labels = train['text'].to_list(), train['sentiment'].to_list() \ntest_texts, test_labels = test['text'].to_list(), test['sentiment'].to_list() \n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=100)\n\nfrom transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased\n\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True)\nvalid_encodings = tokenizer(valid_texts, truncation=True, padding=True)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True)\n\nTry changing the size of the split. It will work. Which means that the split data wasn't enough for tokenizer to tokenize\n"}, "1780": {"topic": "ValueError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]] - Tokenizing BERT / Distilbert Error", "user_name": "Raoof NaushadRaoof Naushad", "text": "\nin tokenizer first text must be STR for exaample:\ntrain_encodings = tokenizer(str(train_texts), truncation=True, padding=True)\n"}, "1781": {"topic": "What does a backslash by itself ('\\') mean in Python? [duplicate]", "user_name": "Aya", "text": "\n\n\n\n\n\n\nThis question already has answers here:                    \n\n\n\nWhat is the purpose of a backslash at the end of a line?\n\r\n                                (2 answers)\r\n                            \n\nClosed 6 years ago.\n\n\n\nI found this code in the nltk documentation (http://www.nltk.org/_modules/nltk/sentiment/vader.html)\nif (i < len(words_and_emoticons) - 1 and item.lower() == \"kind\" and \\\n            words_and_emoticons[i+1].lower() == \"of\") or \\\n            item.lower() in BOOSTER_DICT:\n            sentiments.append(valence)\n            continue\n\nCan someone explain what this if condition means?\n"}, "1782": {"topic": "What does a backslash by itself ('\\') mean in Python? [duplicate]", "user_name": "Nikhil PrabhuNikhil Prabhu", "text": "\nA backslash at the end of a line tells Python to extend the current logical line over across to the next physical line. See the Line Structure section of the Python reference documentation:\n\n2.1.5. Explicit line joining\nTwo or more physical lines may be joined into logical lines using\n  backslash characters (\\), as follows: when a physical line ends in a\n  backslash that is not part of a string literal or comment, it is\n  joined with the following forming a single logical line, deleting the\n  backslash and the following end-of-line character. For example:\nif 1900 < year < 2100 and 1 <= month <= 12 \\\n   and 1 <= day <= 31 and 0 <= hour < 24 \\\n   and 0 <= minute < 60 and 0 <= second < 60:   # Looks like a valid date\n        return 1\n\n\nThere is also the option to use implicit line joining, by using parentheses or brackets or curly braces; Python will not end the logical line until it finds the matching closing bracket or brace for each opening bracket or brace. This is the recommended code style, the sample you found should really be written as:\nif ((i < len(words_and_emoticons) - 1 and item.lower() == \"kind\" and\n        words_and_emoticons[i+1].lower() == \"of\") or\n        item.lower() in BOOSTER_DICT):\n    sentiments.append(valence)\n    continue\n\nSee the Python Style Guide (PEP 8) (but note the exception; some Python statements don't support (...) parenthesising so backslashes are acceptable there).\nNote that Python is not the only programming language using backslashes for line continuation; bash, C and C++ preprocessor syntax, Falcon, Mathematica and Ruby also use this syntax to extend lines; see Wikipedia.\n"}, "1783": {"topic": "What does a backslash by itself ('\\') mean in Python? [duplicate]", "user_name": "", "text": "\nIn this case, the \\ is escaping the following new line character. Because Python cares about whitespace, this code is using this to allow code to be continued on a new line.\n"}, "1784": {"topic": "What does a backslash by itself ('\\') mean in Python? [duplicate]", "user_name": "Martijn Pieters\u2666Martijn Pieters", "text": "\nIt is used as a line break so the if condition can be written in the next line.\n"}, "1785": {"topic": "What does a backslash by itself ('\\') mean in Python? [duplicate]", "user_name": "DKingDKing", "text": "\nIt escapes end of the line - for readability purpose. (extends line to the next one, as the \\n character is not visible but it has syntactical meaning.)\n"}, "1786": {"topic": "What does a backslash by itself ('\\') mean in Python? [duplicate]", "user_name": "DeepSpaceDeepSpace", "text": "\nThe backslash is used to indicate a line break in this if condition. The PEP8 says:\n\nBackslashes may still be appropriate at times. For example, long, multiple with -statements cannot use implicit continuation, so backslashes are acceptable: \nwith open('/path/to/some/file/you/want/to/read') as file_1, \\\n     open('/path/to/some/file/being/written', 'w') as file_2:\n    file_2.write(file_1.read())\n\n\nApart from these conditions linebreaks are usually indicated by proper indentation.\nEdit:\nApparently the with statement is an exception that does not allow for line breaks just by indentation and therefore uses the backslash, while if should not be used with \\.\n"}, "1787": {"topic": "Best Algorithmic Approach to Sentiment Analysis [closed]", "user_name": "bragboy", "text": "\n\n\n\n\n\n\nClosed. This question is opinion-based. It is not currently accepting answers.                    \n\n\n\n\n\n\n\n\n\n\nWant to improve this question? Update the question so it can be answered with facts and citations by editing this post.\n\n\nClosed 9 years ago.\n\n\n\n\n\n\n\r\n                        Improve this question\r\n                    \n\n\n\nMy requirement is taking in news articles and determining if they are positive or negative about a subject.  I am taking the approach outlined below, but I keep reading NLP may be of use here.  All that I have read has pointed at NLP detecting opinion from fact, which I don't think would matter much in my case. I'm wondering two things:\n1)  Why wouldn't my algorithm work and/or how can I improve it? ( I know sarcasm would probably be a pitfall, but again I don't see that occurring much in the type of news we will be getting)\n2)  How would NLP help, why should I use it?\nMy algorithmic approach (I have dictionaries of positive, negative, and negation words):\n1) Count number of positive and negative words in article\n2) If a negation word is found with 2 or 3 words of the positive or negative word, (ie: NOT the best) negate the score.\n3) Multiply the scores by weights that have been manually assigned to each word. (1.0 to start)\n4) Add up the totals for positive and negative to get the sentiment score.\n"}, "1788": {"topic": "Best Algorithmic Approach to Sentiment Analysis [closed]", "user_name": "user387049user387049", "text": "\nI don't think there's anything particularly wrong with your algorithm, it's a fairly straightforward and practical way to go, but there are a lot of situations where it will get make mistakes.\n\nAmbiguous sentiment words - \"This product works terribly\" vs. \"This product is terribly good\"\nMissed negations - \"I would never in a millions years say that this product is worth buying\"\nQuoted/Indirect text - \"My dad says this product is terrible, but I disagree\"\nComparisons - \"This product is about as useful as a hole in the head\"\nAnything subtle - \"This product is ugly, slow and uninspiring, but it's the only thing on the market that does the job\"\n\nI'm using product reviews for examples instead of news stories, but you get the idea. In fact, news articles are probably harder because they will often try to show both sides of an argument and tend to use a certain style to convey a point. The final example is quite common in opinion pieces, for example.\nAs far as NLP helping you with any of this, word sense disambiguation (or even just part-of-speech tagging) may help with (1), syntactic parsing might help with the long range dependencies in (2), some kind of chunking might help with (3). It's all research level work though, there's nothing that I know of that you can directly use. Issues (4) and (5) are a lot harder, I throw up my hands and give up at this point.\nI'd stick with the approach you have and look at the output carefully to see if it is doing what you want. Of course that then raises the issue of what you want you understand the definition of \"sentiment\" to be in the first place...\n"}, "1789": {"topic": "Best Algorithmic Approach to Sentiment Analysis [closed]", "user_name": "", "text": "\nmy favorite example is \"just read the book\". it contains no explicit sentiment word and it is highly depending on the context. If it apears in a movie review it means that the-movie-sucks-it's-a-waste-of-your-time-but-the-book-is-good. However, if it is in a book review it delivers a positive sentiment. \nAnd what about - \"this is the smallest [mobile] phone in the market\". back in the '90, it was a great praise. Today it may indicate that it is a way too small. \nI think this is the place to start in order to get the complexity of sentiment analysis: http://www.cs.cornell.edu/home/llee/opinion-mining-sentiment-analysis-survey.html (by Lillian Lee of Cornell).\n"}, "1790": {"topic": "Best Algorithmic Approach to Sentiment Analysis [closed]", "user_name": "StompchickenStompchicken", "text": "\nYou may find the OpinionFinder system and the papers describing it useful. \nIt is available at http://www.cs.pitt.edu/mpqa/ with other resources for opinion analysis. \nIt goes beyond polarity classification at the document level, but try to find individual opinions at the sentence level. \n"}, "1791": {"topic": "Best Algorithmic Approach to Sentiment Analysis [closed]", "user_name": "Stompchicken", "text": "\nI believe the best answer to all of the questions that you mentioned is reading the book under the title of \"Sentiment Analysis and opinion mining\" by Professor Bing Liu. This book is the best of its own in the field of sentiment analysis. it is amazing. Just take a look at it and you will find the answer to all your 'why' and 'how' questions! \n"}, "1792": {"topic": "Best Algorithmic Approach to Sentiment Analysis [closed]", "user_name": "ScienceFrictionScienceFriction", "text": "\nMachine-learning techniques are probably better.\nWhitelaw, Garg, and Argamon have a technique that achieves 92% accuracy, using a technique similar to yours for dealing with negation, and support vector machines for text classification.\n"}, "1793": {"topic": "Best Algorithmic Approach to Sentiment Analysis [closed]", "user_name": "zdepablozdepablo", "text": "\nWhy don't you try something similar to how SpamAsassin spam filter works? There really not much difference between intension mining and opinion mining.\n"}, "1794": {"topic": "Scikit learn SVC predict probability doesn't work as expected", "user_name": "KevinOelenKevinOelen", "text": "\nI built sentiment analyzer using SVM classifier. I trained model with probability=True and it can give me probability. But when I pickled my model and load it again later, the probability doesn't work anymore.\nThe model:\nfrom sklearn.svm import SVC, LinearSVC\npipeline_svm = Pipeline([\n    ('bow', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('classifier', SVC(probability=True)),])\n\n# pipeline parameters to automatically explore and tune\nparam_svm = [\n  {'classifier__C': [1, 10, 100, 1000], 'classifier__kernel': ['linear']},\n  {'classifier__C': [1, 10, 100, 1000], 'classifier__gamma': [0.001, 0.0001], 'classifier__kernel': ['rbf']},\n]\n\ngrid_svm = GridSearchCV(\n    pipeline_svm,\n    param_grid=param_svm,\n    refit=True,\n    n_jobs=-1, \n    scoring='accuracy',\n    cv=StratifiedKFold(label_train, n_folds=5),)\n\nsvm_detector_reloaded = cPickle.load(open('svm_sentiment_analyzer.pkl', 'rb'))\nprint(svm_detector_reloaded.predict([\"\"\"\"Today is awesome day\"\"\"])[0])\n\nGives me: \n\nAttributeError: predict_proba is not available when  probability=False\n\n"}, "1795": {"topic": "Scikit learn SVC predict probability doesn't work as expected", "user_name": "eastclintw00d", "text": "\nUse: SVM(probability=True)\nor \ngrid_svm = GridSearchCV(\n    probability=True\n    pipeline_svm,\n    param_grid=param_svm,\n    refit=True,\n    n_jobs=-1, \n    scoring='accuracy',\n    cv=StratifiedKFold(label_train, n_folds=5),)\n\n"}, "1796": {"topic": "Scikit learn SVC predict probability doesn't work as expected", "user_name": "Neofytos NeocleousNeofytos Neocleous", "text": "\nAdding (probability=True) while initializing the classifier as someone above suggested, resolved my error:\nclf = SVC(kernel='rbf', C=1e9, gamma=1e-07, probability=True).fit(xtrain,ytrain)\n\n"}, "1797": {"topic": "Scikit learn SVC predict probability doesn't work as expected", "user_name": "Salma Elshahawy", "text": "\nYou can use CallibratedClassifierCV for probability score output.\nfrom sklearn.calibration import CalibratedClassifierCV\n\nmodel_svc = LinearSVC()\nmodel = CalibratedClassifierCV(model_svc) \nmodel.fit(X_train, y_train)\n\nSave model using pickle.\nimport pickle\nfilename = 'linearSVC.sav'\npickle.dump(model, open(filename, 'wb'))\n\nLoad model using pickle.load.\nmodel = pickle.load(open(filename, 'rb'))\nNow start prediction.\npred_class = model.predict(pred)\nprobability = model.predict_proba(pred)\n\n"}, "1798": {"topic": "Scikit learn SVC predict probability doesn't work as expected", "user_name": "Dinesh MarimuthuDinesh Marimuthu", "text": "\nIf that can help, pickling the model with with:\nimport pickle\npickle.dump(grid_svm, open('svm_sentiment_analyzer.pkl', 'wb'))\n\nand loading the model and predicting with \nsvm_detector_reloaded = pickle.load(open('svm_sentiment_analyzer.pkl', 'rb'))\nprint(svm_detector_reloaded.predict_proba([\"Today is an awesome day\"])[0])\n\nreturned me two probabilities fine, after working on your code to rerun it and training the model on a pandas sents DataFrame with  \ngrid_svm.fit(sents.Sentence.values, sents.Positive.values)\n\nBest practices (e.g. using joblib) on model serialization can be found at https://scikit-learn.org/stable/modules/model_persistence.html\n"}, "1799": {"topic": "Scikit learn SVC predict probability doesn't work as expected", "user_name": "RoboMexRoboMex", "text": "\nUse the predprobs function to calculate the scores or probabilities/scores as asked in the auc(y_true, y_score), the issue is because of y_score.\nyou can convert it as shown in the following line of code\n# Classifier - Algorithm - SVM\n# fit the training dataset on the classifier\nSVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto',probability=True)\nSVM.fit(Train_X_Tfidf,Train_Y)\n# predict the labels on validation dataset\npredictions_SVM = SVM.predict(Test_X_Tfidf)\n# Use accuracy_score function to get the accuracy\n**print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y))**\n\nprobs = SVM.**predict_proba**(Test_X_Tfidf)\npreds = probs[:,1]\nfpr, tpr, threshold = **roc_curve(Test_Y, preds)**\n**print(\"SVM Area under curve -> \",auc(fpr, tpr))**\n\nsee the difference between the accuracy_score and the auc(), you need the scores of predictions.\n"}}