{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cs/voice/korenevskaya-a/nirma/nirma_venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import cuda\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import fasttext\n",
    "from data_module import CustomDataset\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import os.path\n",
    "import pickle\n",
    "from transformers import AutoModel, BertTokenizer, BertForSequenceClassification\n",
    "from ngram_attention import NGramAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cs/voice/korenevskaya-a/nirma/nirma_venv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = BertForSequenceClassification.from_pretrained('cointegrated/rubert-tiny2', num_labels=2).to(\"cuda\")\n",
    "bert_ckpt = torch.load('/mnt/cs/voice/korenevskaya-a/nirma/bert_ckpt.pt')\n",
    "tokenizer = BertTokenizer.from_pretrained('cointegrated/rubert-tiny2')\n",
    "bert.load_state_dict(bert_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cs/voice/korenevskaya-a/nirma/nirma_venv/lib/python3.8/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram = NGramAttention()\n",
    "ngram_ckpt = torch.load('/mnt/cs/voice/korenevskaya-a/nirma/checkpoints_ngram_attention/checkpoint_4.pt')\n",
    "ngram.load_state_dict(ngram_ckpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (248290, 2)\n",
      "TRAIN Dataset: (198632, 2)\n",
      "TEST Dataset: (49658, 2)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "# Creating the dataset and dataloader for BERT model \n",
    "df = pd.read_csv(\"out_data/ToxicRussianComments.csv\")\n",
    "\n",
    "train_size = 0.8\n",
    "train_dataset=df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "MAX_LEN = 267\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders for ngram_attention model\n",
    "\n",
    "with open('fasttext_train.pkl', 'rb') as fp:\n",
    "    training_set = pickle.load(fp) \n",
    "training_set = TensorDataset(training_set[0], training_set[1])        \n",
    "    \n",
    "with open('fasttext_test.pkl', 'rb') as fp:\n",
    "    testing_set = pickle.load(fp)       \n",
    "testing_set = TensorDataset(testing_set[0], testing_set[1]) \n",
    "\n",
    "train_params = {'batch_size': 10,\n",
    "                'shuffle': False,\n",
    "                'drop_last': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': 10,\n",
    "                'shuffle': False,\n",
    "                'drop_last': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4965 [00:00<?, ?it/s]/mnt/cs/voice/korenevskaya-a/nirma/ngram_attention.py:97: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha1 = self.soft1(torch.matmul(x1, self.ngram_context1))\n",
      "/mnt/cs/voice/korenevskaya-a/nirma/ngram_attention.py:102: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha2 = self.soft2(torch.matmul(x2, self.ngram_context2))\n",
      "/mnt/cs/voice/korenevskaya-a/nirma/ngram_attention.py:107: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha3 = self.soft3(torch.matmul(x3, self.ngram_context3))\n",
      "100%|██████████| 4965/4965 [53:59<00:00,  1.53it/s] \n"
     ]
    }
   ],
   "source": [
    "ngram.to(device)\n",
    "ngram.eval()\n",
    "results_ngram_test = []\n",
    "ans_test = []\n",
    "for _, data in enumerate(tqdm(testing_loader), 0):\n",
    "    # preprocessing\n",
    "    sentences = data[0]\n",
    "    targets = data[1]\n",
    "\n",
    "    for idx, sentence in enumerate(sentences): \n",
    "        for i, word in enumerate(sentence):\n",
    "            # if emb is pure zeros, then it is altered into trainable eos embedding\n",
    "            if torch.all(word.eq(torch.zeros_like(word))):\n",
    "                with torch.no_grad():\n",
    "                    sentences[idx][i] = ngram.eos    \n",
    "    sentences = torch.unsqueeze(sentences, 1)    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = ngram(sentences.to(device, dtype=torch.float))\n",
    "        results_ngram_test += outputs\n",
    "        ans_test += targets \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('logits_ngram_test.pkl', 'wb') as f:\n",
    "    pickle.dump(results_ngram_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('logits_ngram_test.pkl', 'rb') as f:\n",
    "    results_ngram_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49658/49658 [04:59<00:00, 166.00it/s]\n"
     ]
    }
   ],
   "source": [
    "results_bert_test = []\n",
    "bert.to(device='cpu')\n",
    "bert.eval()\n",
    "for comment in tqdm(test_dataset['comment']):\n",
    "    input_ids = torch.tensor(tokenizer.encode(comment), device='cpu').unsqueeze(0)\n",
    "    outputs = bert(input_ids)\n",
    "    results_bert_test.append(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = test_dataset['label'].values\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.9223796033994335\n",
      "Acc: 0.9722501913085505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "alpha = 0.8\n",
    "preds = []\n",
    "for i, pair in enumerate(zip(results_bert_test, results_ngram_test)): \n",
    "    pair0 = torch.sigmoid(pair[0].to(device).squeeze(0))\n",
    "    pair1 = torch.sigmoid(pair[1])\n",
    "    pred = pair0*alpha + pair1*(1-alpha)    \n",
    "    preds.append(int(pred.argmax()))\n",
    "f1 = f1_score(ans[:len(preds)], preds)\n",
    "acc = np.sum((ans[:len(preds)]==preds))/len(ans)\n",
    "print(\"F1:\",  f1)\n",
    "print(\"Acc:\",  acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fusion():\n",
    "    def __init__(self, ):\n",
    "        self.l1 = nn.Linear(4, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat(x1, x2)\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        return x          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion = Fusion()\n",
    "fusion.train()\n",
    "\n",
    "\n",
    "for index, data in enumerate(tqdm(training_loader), 0):\n",
    "    # preprocessing\n",
    "    sentences = data[0]\n",
    "    targets_ = data[1]#.to(device, dtype = torch.float)\n",
    "    targets = torch.empty((len(data[1]), 2), dtype=torch.float)\n",
    "    for i, tar in enumerate(targets_):\n",
    "        if tar == 0:\n",
    "            targets[i] = torch.tensor([1.,0.])\n",
    "        else:\n",
    "            targets[i] = torch.tensor([0.,1.])\n",
    "\n",
    "    for idx, sentence in enumerate(sentences): \n",
    "        for i, word in enumerate(sentence):\n",
    "            # if emb is pure zeros, then it is altered into trainable eos embedding\n",
    "            if torch.all(word.eq(torch.zeros_like(word))):\n",
    "                with torch.no_grad():\n",
    "                    sentences[idx][i] = model.eos    \n",
    "    sentences = torch.unsqueeze(sentences, 1)    \n",
    "    \n",
    "    with torch.enable_grad():\n",
    "        outputs = model(sentences.to(device, dtype=torch.float))\n",
    "        #outputs = outputs.reshape(TRAIN_BATCH_SIZE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets.to(device, dtype=torch.float))\n",
    "        if index %50 ==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "scheduler.step()  \n",
    "print(\"Current LR: \", scheduler.get_last_lr())      \n",
    "ckp = model.state_dict()\n",
    "PATH = f\"/mnt/cs/voice/korenevskaya-a/nirma/checkpoints_ngram_attention/checkpoint_{epoch}.pt\"\n",
    "torch.save(ckp, PATH)\n",
    "print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nirma_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
