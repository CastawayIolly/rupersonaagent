{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cs/voice/korenevskaya-a/nirma/nirma_venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, BertTokenizer, BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric, Dataset\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "seed_all(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cs/voice/korenevskaya-a/nirma/nirma_venv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('cointegrated/rubert-tiny2', num_labels=2).to(\"cuda\")\n",
    "tokenizer = BertTokenizer.from_pretrained('cointegrated/rubert-tiny2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_module import CustomDataset\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "# Creating the dataset and dataloader for the neural network\n",
    "df = pd.read_csv(\"out_data/ToxicRussianComments.csv\")\n",
    "\n",
    "MAX_LEN = max([len(comment.strip().split(' ')) for comment in df['comment']])\n",
    "MAX_LEN    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (248290, 2)\n",
      "TRAIN Dataset: (198632, 2)\n",
      "TEST Dataset: (49658, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_dataset=df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    return {'F1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = './bert_results', #–í—ã—Ö–æ–¥–Ω–æ–π –∫–∞—Ç–∞–ª–æ–≥\n",
    "    num_train_epochs = 3, #–ö–æ–ª-–≤–æ —ç–ø–æ—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "    per_device_train_batch_size = 8, #–†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è\n",
    "    per_device_eval_batch_size = 8, #–†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –≤–æ –≤—Ä–µ–º—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "    weight_decay =0.01, #–ü–æ–Ω–∏–∂–µ–Ω–∏–µ –≤–µ—Å–æ–≤\n",
    "    logging_dir = './bert_logs', #–ö–∞—Ç–∞–ª–æ–≥ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∂—É—Ä–Ω–∞–ª–æ–≤\n",
    "    load_best_model_at_end = True, #–ó–∞–≥—Ä—É–∂–∞—Ç—å –ª–∏ –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è\n",
    "    learning_rate = 1e-5, #–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
    "    evaluation_strategy ='epoch', #–í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏ (–º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –ø–æ—Å–ª–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–æ–ª-–≤–∞ —à–∞–≥–æ–≤)\n",
    "    logging_strategy = 'epoch', #–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏\n",
    "    save_strategy = 'epoch', #–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏\n",
    "    save_total_limit = 1,\n",
    "    seed=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cs/voice/korenevskaya-a/nirma/nirma_venv/lib/python3.8/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  tokenizer = tokenizer,\n",
    "                  args = training_args,\n",
    "                  train_dataset = training_set,\n",
    "                  eval_dataset = testing_set,\n",
    "                  compute_metrics = compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cs/voice/korenevskaya-a/nirma/nirma_venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18624' max='18624' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18624/18624 41:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.138700</td>\n",
       "      <td>0.099204</td>\n",
       "      <td>0.904767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.085600</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.921848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>0.085862</td>\n",
       "      <td>0.922990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cs/voice/korenevskaya-a/nirma/nirma_venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/mnt/cs/voice/korenevskaya-a/nirma/nirma_venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18624, training_loss=0.09997392929706377, metrics={'train_runtime': 2495.7919, 'train_samples_per_second': 238.76, 'train_steps_per_second': 7.462, 'total_flos': 2291538159229536.0, 'train_loss': 0.09997392929706377, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'bert_ckpt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine-tune-bert/tokenizer_config.json',\n",
       " 'fine-tune-bert/special_tokens_map.json',\n",
       " 'fine-tune-bert/vocab.txt',\n",
       " 'fine-tune-bert/added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"fine-tune-bert\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cs/voice/korenevskaya-a/nirma/nirma_venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_prediction():\n",
    "    test_pred = trainer.predict(testing_set)\n",
    "    labels = np.argmax(test_pred.predictions, axis = -1)\n",
    "    return labels\n",
    "pred = get_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     40803\n",
      "           1       0.92      0.93      0.92      8855\n",
      "\n",
      "    accuracy                           0.97     49658\n",
      "   macro avg       0.95      0.95      0.95     49658\n",
      "weighted avg       0.97      0.97      0.97     49658\n",
      "\n",
      "0.9218477465818938\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(testing_set.targets, pred))\n",
    "print(f1_score(testing_set.targets, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              comment  label\n",
      "0                                —Å–∫–æ—Ç–∏–Ω–∞! —á—Ç–æ —Å–∫–∞–∑–∞—Ç—å      1\n",
      "1             –∞ –∫–æ–≥–¥–∞ –º—ã —Å—Ç–∞—Ç—É—Å –∞–≥—Ä–æ–≥–æ—Ä–æ–¥–∫–∞ –ø–æ–ª—É—á–∏–ª–∏?      0\n",
      "2   –∫—Ä–∞—Å–æ—Ç–∞..!! –µ—Å–ª–∏ –µ—Å—Ç—å, —á—Ç–æ –ø–æ–∫–∞–∑–∞—Ç—å??!! –ø–æ—á–µ–º—É...      0\n",
      "3   —ç—Ç–∏ —Å–∫–∞—Ç—ã ,–Ω–∞ –≥–µ—Ä–æ–µ–≤ –ø–∏–æ–Ω–µ—Ä–æ–≤, –∞–Ω–µ–∫–¥–æ—Ç–æ–≤ –Ω–∞–≤—ã–¥...      0\n",
      "4                           –∫—Ç–æ –∑–∞—Å–µ–¥–∞—Ç—å –±—É–¥–µ—Ç ..????      0\n",
      "5   –¥–∞. —è –Ω–∞ –Ω–µ–º –∑–∞ 21 –¥–µ–Ω—å –º–∞—Ä–∞—Ñ–æ–Ω–∞ —Å–∫–∏–Ω—É–ª–∞ 5–∫–≥ –∏...      0\n",
      "6                                   —Å–µ–∫—Å –º–æ—Å–∞–∂ —á—Ç–æ–ª–∏?      0\n",
      "7                                        –∫–∞–∫–∏–µ –∂–∞–ª–∫–∏–µ      0\n",
      "8                —Å–∞–º—ã–µ —É–º–Ω—ã–µ –±—ã—Å—Ç—Ä–µ–Ω—å–∫–æ —Ä–∞–∑–≤–µ—Ä–Ω—É–ª–∏—Å—å!      0\n",
      "9                      —ç—Ç–æ –∂–µ –Ω–∞–¥–æ —Ç–∞–∫ –Ω–∞—É—á–∏—Ç—å!!!üëçüëçüòÇüòÇ      0\n",
      "10  –±—Ä–∞—Ç–∫–∞ –ø–æ–∑–¥—Ä–∞–≤–ª—è—é —Ç–µ–±—è —Å –¥–Ω—ë–º —Ä–æ–∂–¥–µ–Ω–∏—è –∂–µ–ª–∞—é –≤...      0\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "example = test_dataset.loc[0:10]\n",
    "print(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = CustomDataset(example, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cs/voice/korenevskaya-a/nirma/nirma_venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "example_pred = trainer.predict(example)\n",
    "labels = np.argmax(example_pred.predictions, axis = -1)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nirma_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
